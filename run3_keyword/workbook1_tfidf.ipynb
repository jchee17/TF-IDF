{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's write the tf-idf functions here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import timeit\n",
      "import string\n",
      "import pickle\n",
      "import math\n",
      "from __future__ import division\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "path_arxiv = '/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/'\n",
      "articles = os.listdir(path_arxiv)\n",
      "length_arxiv = len(articles)\n",
      "\n",
      "concepts = pickle.load(open('./data_objects/concepts.p', 'r'))\n",
      "\n",
      "arxiv_sep_byfile = pickle.load(open('./data_objects/arxiv_sep_byfile.p' ,'r'))\n",
      "arxiv_sep_byword = pickle.load(open('./data_objects/arxiv_sep_byword.p' ,'r'))\n",
      "arxiv_wordsindoc = pickle.load(open('./data_objects/arxiv_wordsindoc.p', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tf-idf functions. specific to arxiv only\n",
      "def tf(word, document):\n",
      "    # must first check if word in document\n",
      "    if (word in arxiv_sep_byfile[document]):\n",
      "        return (arxiv_sep_byfile[document][word] / arxiv_wordsindoc[document])\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "def idf(word):\n",
      "    return math.log(length_arxiv / (1 + arxiv_sep_byword[word]))\n",
      "\n",
      "def tfidf(word, document):\n",
      "    return tf(word, document) * idf(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_arxiv = {}\n",
      "\n",
      "for f in arxiv_sep_byfile:\n",
      "    # initialize dictoinary\n",
      "    tfidf_arxiv[f] = {}\n",
      "    \n",
      "    for word in concepts:\n",
      "        tfidf_arxiv[f][word] = tfidf(word, f)\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(tfidf_arxiv, open('./data_objects/tfidf_arxiv.p', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "arxiv_sep_byfile[articles[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "{'central limit theorem': 3,\n",
        " 'count data': 1,\n",
        " 'cumulant': 10,\n",
        " 'discrete time': 1,\n",
        " 'exponential dispersion model': 3,\n",
        " 'gamma distribution': 1,\n",
        " 'law of large numbers': 1,\n",
        " 'mean': 2,\n",
        " 'normal distribution': 1,\n",
        " 'overdispersion': 1,\n",
        " 'stable distribution': 1,\n",
        " 'variance': 3,\n",
        " 'xplore': 1}"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}