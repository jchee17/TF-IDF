{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import unicode_literals\n",
      "from __future__ import division\n",
      "import os\n",
      "import re\n",
      "import timeit\n",
      "import math\n",
      "import codecs\n",
      "import pickle\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path_wiki = \"/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/\"\n",
      "path_arxiv = \"/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/\"\n",
      "\n",
      "list_wiki = pickle.load(open(\"./data_objects/list_wiki.p\", 'r'))\n",
      "list_arxiv = pickle.load(open(\"./data_objects/list_arxiv.p\", 'r'))\n",
      "\n",
      "vocab = pickle.load(open(\"./data_objects/vocab.p\", 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_regex_c(list_vocab):\n",
      "    \"\"\" reads in list (unicode), outputs dictionary\n",
      "        where key value is keyword in concept,\n",
      "        value is compiled regex\"\"\"\n",
      "    # create output dict\n",
      "    re_vocab_c = {}\n",
      "\n",
      "    # create regex expression for every keyword in concepts\n",
      "    re_keyword = ur''\n",
      "    re_match = ur\"(\\\\-)|(\\\\\u2013)|(\\\\\\s)\"\n",
      "    re_replace = ur\"[-\\s\u2013]\"\n",
      "    for keyword in list_vocab:\n",
      "        re_keyword = ur'\\s' + re.sub(re_match, re_replace, re.escape(keyword)) + ur'\\s'\n",
      "        #re_keyword = re.sub(re_match, re_replace, re.escape(keyword)) \n",
      "        re_vocab_c[keyword] = re.compile(re_keyword)\n",
      "\n",
      "    return re_vocab_c\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_doc_term(path_corpus, list_corpus, list_vocab):\n",
      "    \"\"\" generates document-term matrix given a path to a \n",
      "        corpus and common vocab\n",
      "    \"\"\"\n",
      "    num_docs = len(list_corpus)\n",
      "    num_terms = len(list_vocab)\n",
      "    doc_term = np.zeros((num_docs, num_terms))\n",
      "    \n",
      "    # generate (dict) compiled regex's\n",
      "    re_c_vocab = gen_regex_c(list_vocab)\n",
      "\n",
      "    # iterate over files\n",
      "    fp = None\n",
      "    txt = u''\n",
      "    r = None\n",
      "    num = 0.0\n",
      "        \n",
      "    count = 0\n",
      "    every = 50\n",
      "    start= timeit.default_timer()\n",
      "    checkpoint = 0.0\n",
      "    for i in range(num_docs):\n",
      "        fp = codecs.open(path_corpus+list_corpus[i], 'r', \"utf-8\", errors=\"ignore\")\n",
      "        txt = fp.read()\n",
      "        txt = txt.lower()\n",
      "        fp.close()\n",
      "        \n",
      "        for j in range(num_terms):\n",
      "            r = re_c_vocab[ vocab[j] ]\n",
      "            num = len(r.findall(txt, re.UNICODE))  \n",
      "            doc_term[i,j] = num\n",
      "            \n",
      "        if (count % every == 0):\n",
      "            checkpoint = timeit.default_timer()\n",
      "            print(count, round(checkpoint-start, 2))\n",
      "        count += 1\n",
      "            \n",
      "    \n",
      "    return doc_term\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_term_wiki  = gen_doc_term(path_wiki, list_wiki, vocab)\n",
      "np.save(\"./data_objects/doc_term_wiki.npy\", doc_term_wiki)\n",
      "\n",
      "doc_term_arxiv = gen_doc_term(path_arxiv, list_arxiv, vocab)\n",
      "np.save(\"./data_objects/doc-term_arxiv.npy\", doc_term_arxiv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0, 0.23)\n",
        "(50, 33.6)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(100, 71.11)"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-fe2284b66658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc_term_wiki\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgen_doc_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_objects/doc_term_wiki.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_wiki\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc_term_arxiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_doc_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_arxiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_arxiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data_objects/doc-term_arxiv.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_term_arxiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-8-4da346f7a097>\u001b[0m in \u001b[0;36mgen_doc_term\u001b[0;34m(path_corpus, list_corpus, vocab)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_c_vocab\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mdoc_term\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_inv_counts(doc_term_corpus, list_corpus, list_vocab):\n",
      "    \"\"\" creates np array, for each vocab term how many documents \n",
      "    it can be found in.\n",
      "    \"\"\"\n",
      "    # generate output array\n",
      "    num_docs = len(list_corpus)\n",
      "    num_terms = len(list_vocab)\n",
      "    inv_counts = np.zeros(num_terms)\n",
      "    \n",
      "    count = 0\n",
      "    every = 100\n",
      "    for i in range(num_docs):\n",
      "        for j in range(num_terms):\n",
      "            if doc_term_corpus[i,j] > 0:\n",
      "                inv_counts[j] += 1\n",
      "        \n",
      "        if count % every == 0:\n",
      "            print(count)\n",
      "        count += 1\n",
      "            \n",
      "    return inv_counts\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}