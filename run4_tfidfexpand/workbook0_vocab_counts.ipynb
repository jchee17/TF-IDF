{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I find a common vocab using python set datastructure and word counts for each doc in each corpus,\n",
      "stored in dictionaries."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import unicode_literals\n",
      "from __future__ import division\n",
      "import os\n",
      "import re\n",
      "import timeit\n",
      "import math\n",
      "import codecs\n",
      "import pickle\n",
      "import numpy as np\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "path_wiki = \"/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/\"\n",
      "path_arxiv = \"/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_vocab_wordcounts(path_corpus, counts):\n",
      "    \"\"\" generates common vocab from all wiki and arxiv papers\n",
      "        and word counts in one pass\n",
      "    \"\"\"\n",
      "    vocab = set()\n",
      "    \n",
      "    fp = None\n",
      "    txt = u''\n",
      "    tokens = []\n",
      "    tokenizer = RegexpTokenizer(ur'\\w+')\n",
      "    \n",
      "    i = 0\n",
      "    every = 500\n",
      "    for f in os.listdir(path_corpus):\n",
      "        # read in text\n",
      "        fp = codecs.open(path_corpus+f, 'r', \"utf-8\", errors=\"ignore\")\n",
      "        txt = fp.read()\n",
      "        txt = txt.lower()\n",
      "        \n",
      "        # tokenize\n",
      "        tokens = tokenizer.tokenize(txt)\n",
      "        counts[f] = len(tokens)\n",
      "        \n",
      "        # add tokens to vocab\n",
      "        vocab = vocab.union(tokens)\n",
      "        \n",
      "        # counting iterations\n",
      "        if (i % every == 0):\n",
      "            print(path_corpus,i)\n",
      "        i += 1\n",
      "    \n",
      "    return vocab\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts_wiki = {}\n",
      "counts_arxiv = {}\n",
      "\n",
      "vocab_wiki = gen_vocab_wordcounts(path_wiki, counts_wiki)\n",
      "vocab_arxiv = gen_vocab_wordcounts(path_arxiv, counts_arxiv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 0)\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 1000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 1500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 2000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 2500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 3000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 1000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 1500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 2000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 2500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 3000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 3500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocab = vocab_wiki.intersection(vocab_arxiv)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fp = open('./data_objects/vocab_wiki.p', 'w')\n",
      "pickle.dump(vocab_wiki, fp)\n",
      "fp.close()\n",
      "\n",
      "fp = open('./data_objects/vocab_arxiv.p', 'w')\n",
      "pickle.dump(vocab_arxiv, fp)\n",
      "fp.close()\n",
      "\n",
      "fp = open('./data_objects/counts_wiki.p', 'w')\n",
      "pickle.dump(counts_wiki, fp)\n",
      "fp.close()\n",
      "\n",
      "fp = open('./data_objects/counts_arxiv.p', 'w')\n",
      "pickle.dump(counts_arxiv, fp)\n",
      "fp.close()\n",
      "\n",
      "fp = open('./data_objects/vocab.p', 'w')\n",
      "pickle.dump(vocab, fp)\n",
      "fp.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pickle list of files in wiki, arxiv\n",
      "list_wiki = os.listdir(path_wiki)\n",
      "list_arxiv = os.listdir(path_arxiv)\n",
      "\n",
      "pickle.dump(list_wiki, open('./data_objects/list_wiki.p', 'w'))\n",
      "pickle.dump(list_arxiv, open('./data_objects/list_arxiv.p', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pickle vocab as list\n",
      "vocab = pickle.load(open('./data_objects/vocab.p', 'r'))\n",
      "list_vocab = list(vocab)\n",
      "pickle.dump(list_vocab, open('./data_objects/list_vocab.p', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save counts as np array\n",
      "def gen_counts(path_corpus, list_corpus):\n",
      "    \"\"\" generates counts of number of words in each doc\n",
      "    as numpy array \"\"\"\n",
      "    # output array\n",
      "    counts_corpus = np.zeros(len(list_corpus))\n",
      "    \n",
      "    fp = None\n",
      "    txt = u''\n",
      "    tokens = []\n",
      "    tokenizer = RegexpTokenizer(ur'\\w+')\n",
      "    \n",
      "    count = 0\n",
      "    every = 500\n",
      "    for i in range(len(list_corpus)):\n",
      "        # read in text\n",
      "        fp = codecs.open(path_corpus+list_corpus[i], \n",
      "                         'r', \"utf-8\", errors=\"ignore\")\n",
      "        txt = fp.read()\n",
      "        txt = txt.lower()\n",
      "        \n",
      "        # tokenize\n",
      "        tokens = tokenizer.tokenize(txt)\n",
      "        counts_corpus[i] = len(tokens)\n",
      "        \n",
      "        # counting iterations\n",
      "        if (i % every == 0):\n",
      "            print(path_corpus,i)\n",
      "        i += 1\n",
      "    \n",
      "    return counts_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#list_wiki = pickle.load(open('./data_objects/list_wiki.p', 'r'))\n",
      "#list_arxiv = pickle.load(open('./data_objects/list_arxiv.p', 'r'))\n",
      "\n",
      "counts_wiki = gen_counts(path_wiki, list_wiki)\n",
      "counts_arxiv = gen_counts(path_arxiv, list_arxiv)\n",
      "\n",
      "np.save(\"./data_objects/counts_wiki.npy\", counts_wiki)\n",
      "np.save(\"./data_objects/counts_arxiv.npy\", counts_arxiv)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 0)\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 1000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 1500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 2000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 2500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/wiki_concepts/', 3000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 1000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 1500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 2000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 2500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 3000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/', 3500)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}