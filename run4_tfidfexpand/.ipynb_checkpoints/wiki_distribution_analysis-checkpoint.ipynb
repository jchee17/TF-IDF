{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import unicode_literals\n",
      "from __future__ import division\n",
      "import os\n",
      "import re\n",
      "import pickle\n",
      "import timeit\n",
      "import numpy as np\n",
      "import webbrowser\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "import matplotlib.pyplot as plt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cos_dist_tfidf = np.load(\"./data_objects/cos_dist_tfidf.npy\")\n",
      "#cos_dist_okapi = np.load(\"./data_objects/cos_dist_okapi.npy\")\n",
      "\n",
      "#tfidf_wiki = np.load(\"./data_objects/tfidf_wiki.npy\")\n",
      "#tfidf_arxiv = np.load(\"./data_objects/tfidf_arxiv.npy\")\n",
      "\n",
      "#okapi_bm25_wiki = np.load(\"./data_objects/okapi_bm25_wiki.npy\")\n",
      "#okpapi_bm25_arxiv = np.load(\"./data_objects/okapi_bm25_arxiv.npy\")\n",
      "\n",
      "list_wiki = pickle.load(open(\"./data_objects/list_wiki.p\", 'r'))\n",
      "list_arxiv = pickle.load(open(\"./data_objects/list_arxiv.p\", 'r'))\n",
      "\n",
      "list_vocab = pickle.load(open('./data_objects/list_vocab.p', 'r'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lookup_top_concepts(cos_dist_score, article_index, n):\n",
      "    \"\"\" given the index of article and cos_dist_array,\n",
      "        returns the index of top n concepts\n",
      "    \"\"\"\n",
      "    article_array = cos_dist_score[article_index]\n",
      "    \n",
      "    # give top n partition\n",
      "    sorted_ind = np.argpartition(article_array, -n)[-n:]\n",
      "    \n",
      "    # sort in this top n\n",
      "    sorted_ind = sorted_ind[np.argsort(article_array[sorted_ind])[::-1]]\n",
      "    \n",
      "    return sorted_ind\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lookup_top_words(score_articles, score_concepts, \n",
      "                      article_index, concept_index, n):\n",
      "    \"\"\" gives top n conributions to normalized dot product \"\"\"\n",
      "    article = score_articles[article_index,]\n",
      "    concept = score_concepts[concept_index,]\n",
      "    \n",
      "    # norms\n",
      "    norm_article = np.linalg.norm(article)\n",
      "    norm_concept = np.linalg.norm(concept)\n",
      "    norm = norm_article * norm_concept\n",
      "    \n",
      "    # store dot product separately in np array\n",
      "    len_vocab = score_articles.shape[1]\n",
      "    dot_product = map(lambda x: (article[x] * concept[x]) / (norm), range(len_vocab))\n",
      "    dot_product = np.array(dot_product)\n",
      "        \n",
      "    # give top n partition\n",
      "    sorted_ind = np.argpartition(dot_product, -n)[-n:]\n",
      "    \n",
      "    # sort in this top n\n",
      "    sorted_ind = sorted_ind[np.argsort(dot_product[sorted_ind])[::-1]]\n",
      "    \n",
      "    return sorted_ind"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_concept_analysis(cos_dist_score, score_articles, score_concepts, \n",
      "                           n_top_concepts, n_top_words):\n",
      "    \"\"\" generates 3dim numpy array to store info on top wiki concepts (2nd axis)\n",
      "        for each article (1st article), with top words to the normalized\n",
      "        dot product (3rd axis)\n",
      "    \"\"\"\n",
      "    # output matrix\n",
      "    x = cos_dist_score.shape[0]\n",
      "    y = n_top_concepts\n",
      "    z = n_top_words + 1\n",
      "    concept_analysis = np.zeros((x,y,z))\n",
      "    \n",
      "    top_concepts = []\n",
      "    top_words = []\n",
      "    \n",
      "    start = timeit.default_timer()\n",
      "    checkpoint = 0.0\n",
      "    count = 0\n",
      "    every = 10\n",
      "    for i in range(x):\n",
      "        top_concepts = lookup_top_concepts(cos_dist_score, i, n_top_concepts)\n",
      "        \n",
      "        for j in range(y):\n",
      "            top_words = lookup_top_words(score_articles, score_concepts, i, j, n_top_words)\n",
      "            concept_analysis[i,j,0] = top_concepts[j]\n",
      "            \n",
      "            for k in range(1,z):\n",
      "                concept_analysis[i,j,k] = top_words[k-1]\n",
      "                \n",
      "        if count % every == 0:\n",
      "            checkpoint = timeit.default_timer()\n",
      "            print(count, checkpoint-start)\n",
      "        count += 1\n",
      "                \n",
      "                \n",
      "    return concept_analysis\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_top_wiki = 20\n",
      "n_top_vocab = 10\n",
      "\n",
      "concept_analysis_tfidf = gen_concept_analysis(cos_dist_tfidf, tfidf_arxiv, tfidf_wiki,\n",
      "                                              n_top_wiki, n_top_vocab)\n",
      "                                              "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0, 0.48871493339538574)\n",
        "(10, 5.013047933578491)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(20, 9.467504024505615)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(30, 13.901734828948975)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(40, 18.355907917022705)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(50, 22.76964497566223)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(60, 27.21312689781189)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(70, 31.623541831970215)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(80, 36.202024936676025)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(90, 40.63670992851257)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(100, 45.07425093650818)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(110, 49.78870892524719)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(120, 54.58260798454285)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(130, 59.50122404098511)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(140, 64.15051698684692)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(150, 68.57159399986267)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(160, 73.13970398902893)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(170, 77.97767400741577)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(180, 82.57091188430786)"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-26-34739a3240d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m concept_analysis_tfidf = gen_concept_analysis(cos_dist_tfidf, tfidf_arxiv, tfidf_wiki,\n\u001b[0;32m----> 5\u001b[0;31m                                               n_top_wiki, n_top_vocab)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-25-171f360a4d2d>\u001b[0m in \u001b[0;36mgen_concept_analysis\u001b[0;34m(cos_dist_score, score_articles, score_concepts, n_top_concepts, n_top_words)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_top_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_concepts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mconcept_analysis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_concepts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-17-163d73937941>\u001b[0m in \u001b[0;36mlookup_top_words\u001b[0;34m(score_articles, score_concepts, article_index, concept_index, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# store dot product separately in np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlen_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdot_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconcept\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdot_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-17-163d73937941>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# store dot product separately in np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlen_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdot_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconcept\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mdot_product\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_analysis_tfidf = np.load(\"./data_objects/concept_analysis_tfidf.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concept_analysis_tfidf[0,0,2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "17629.0"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_marginals(concept_analysis_score, \n",
      "                  list_concepts,list_common_vocab):\n",
      "    \"\"\" outputs 2 matrices, giving the the counts of\n",
      "        wiki concepts in one matrix, counts of \n",
      "        vocab words in the other.\n",
      "    \"\"\"\n",
      "    # otuput matrices\n",
      "    num_concepts = len(list_concepts)\n",
      "    num_vocab = len(list_common_vocab)\n",
      "    \n",
      "    marginal_concepts = np.zeros(num_concepts)\n",
      "    marginal_vocab = np.zeros(num_vocab)\n",
      "    \n",
      "    x = concept_analysis_score.shape[0]\n",
      "    y = concept_analysis_score.shape[1]\n",
      "    z = concept_analysis_score.shape[2]\n",
      "    \n",
      "    concept_index = 0\n",
      "    vocab_index = 0\n",
      "    \n",
      "    start = timeit.default_timer()\n",
      "    checkpoint = 0.0\n",
      "    count = 0\n",
      "    every = 100\n",
      "    for i in range(x):\n",
      "        for j in range(y):\n",
      "            concept_index = concept_analysis_score[i,j,0]\n",
      "            marginal_concepts[concept_index] += 1\n",
      "            \n",
      "            for k in range(1,z):\n",
      "                vocab_index = concept_analysis_score[i,j,k]\n",
      "                marginal_vocab[vocab_index] += 1\n",
      "        \n",
      "        if count % every == 0:\n",
      "            checkpoint = timeit.default_timer()\n",
      "            print(count, checkpoint-start)\n",
      "        count += 1\n",
      "    \n",
      "    return (marginal_concepts, marginal_vocab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "marginals_wiki_tfidf = np.load(\"./data_objects/marginals_wiki_tfidf.npy\")\n",
      "marginals_vocab_tfidf = np.load(\"./data_objects/marginals_vocab_tfidf.npy\")\n",
      "\n",
      "marginals_wiki_okapi = np.load(\"./data_objects/marginals_wiki_okapi.npy\")\n",
      "marginals_vocab_okapi = np.load(\"./data_objects/marginals_vocab_okapi.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(marginals_wiki_tfidf, bins=100)\n",
      "plt.title(\"marginals wiki tfidf\")\n",
      "plt.show()\n",
      "\n",
      "plt.hist(marginals_vocab_tfidf, bins=100)\n",
      "plt.title(\"marginals vocab tfidf\")\n",
      "plt.show()\n",
      "\n",
      "plt.hist(marginals_wiki_okapi, bins=100)\n",
      "plt.title(\"marginals wiki okapi\")\n",
      "plt.show()\n",
      "\n",
      "plt.hist(marginals_vocab_okapi, bins=100)\n",
      "plt.title(\"marginals vocab okapi\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def top(array, list_array, n):\n",
      "    \"\"\" returns top n elements in numpy array \"\"\"\n",
      "    # give top n partition\n",
      "    sorted_ind = np.argpartition(array, -n)[-n:]\n",
      "    \n",
      "    # sort in this top n\n",
      "    sorted_ind = sorted_ind[np.argsort(array[sorted_ind])[::-1]]\n",
      "    \n",
      "    out = []\n",
      "    idx = 0\n",
      "    for i in range(n):\n",
      "        idx = sorted_ind[i]\n",
      "        out.append(list_array[idx])\n",
      "        #out.append((idx, list_array[idx]))\n",
      "        \n",
      "    return (out)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top(marginals_wiki_okapi, list_wiki, 100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[u'normal_distribution.txt',\n",
        " u'linear_regression.txt',\n",
        " u'approximate_bayesian_computation.txt',\n",
        " u'ordinary_least_squares.txt',\n",
        " u'maximum_likelihood.txt',\n",
        " u'linear_least_squares_(mathematics).txt',\n",
        " u'numerical_methods_for_linear_least_squares.txt',\n",
        " u'statistics.txt',\n",
        " u'principal_component_analysis.txt',\n",
        " u'confidence_interval.txt',\n",
        " u'pitman\\u2013koopman\\u2013darmois_theorem.txt',\n",
        " u'exponential_family.txt',\n",
        " u'population_variance.txt',\n",
        " u'true_variance.txt',\n",
        " u'variance.txt',\n",
        " u'logistic_regression.txt',\n",
        " u'kalman_filter.txt',\n",
        " u'beta_distribution.txt',\n",
        " u'statistical_hypothesis_testing.txt',\n",
        " u'predictive_analytics.txt',\n",
        " u'least_squares.txt',\n",
        " u\"student's_t-distribution.txt\",\n",
        " u'regression_estimation.txt',\n",
        " u\"lyapunov's_central_limit_theorem.txt\",\n",
        " u'central_limit_theorem.txt',\n",
        " u'time_series.txt',\n",
        " u'time-series_regression.txt',\n",
        " u'variational_bayesian_methods.txt',\n",
        " u'multivariate_normal_distribution.txt',\n",
        " u'mixture_model.txt',\n",
        " u'markov_chain.txt',\n",
        " u'analysis_of_variance.txt',\n",
        " u'sensitivity_analysis.txt',\n",
        " u'neural_network.txt',\n",
        " u'artificial_neural_network.txt',\n",
        " u'pearson_product-moment_correlation_coefficient.txt',\n",
        " u'experimental_uncertainty_analysis.txt',\n",
        " u'monte_carlo_method.txt',\n",
        " u'principal_component_regression.txt',\n",
        " u'karhunen\\u2013lo\\xe8ve_theorem.txt',\n",
        " u'gibbs_sampling.txt',\n",
        " u'info-gap_decision_theory.txt',\n",
        " u'non-homogeneous_poisson_process.txt',\n",
        " u'minimum_mean_square_error.txt',\n",
        " u'support_vector_machine.txt',\n",
        " u'poisson_process.txt',\n",
        " u'cluster_analysis.txt',\n",
        " u'expectation\\u2013maximization_algorithm.txt',\n",
        " u'data_clustering.txt',\n",
        " u'median.txt',\n",
        " u'particle_filter.txt',\n",
        " u'characteristic_function_(probability_theory).txt',\n",
        " u'hierarchical_bayes_model.txt',\n",
        " u'algorithms.txt',\n",
        " u'bayesian_network.txt',\n",
        " u'd-separation.txt',\n",
        " u'robust_statistics.txt',\n",
        " u'nonlinear_dimensionality_reduction.txt',\n",
        " u'bootstrapping_(statistics).txt',\n",
        " u'bayesian_inference.txt',\n",
        " u'factor_analysis.txt',\n",
        " u'meta-analysis.txt',\n",
        " u'sample_standard_deviation.txt',\n",
        " u'standard_deviation.txt',\n",
        " u'statistical_power.txt',\n",
        " u'reliability_engineering.txt',\n",
        " u'resampling_(statistics).txt',\n",
        " u'stochastic_convergence.txt',\n",
        " u'convergence_of_random_variables.txt',\n",
        " u'bayes_estimator.txt',\n",
        " u'singular_spectrum_analysis.txt',\n",
        " u\"chebyshev's_inequality.txt\",\n",
        " u'categorical_distribution.txt',\n",
        " u'machine_learning.txt',\n",
        " u'an_inequality_on_location_and_scale_parameters.txt',\n",
        " u'bienaym\\xe9\\u2013chebyshev_inequality.txt',\n",
        " u'sampling_(statistics).txt',\n",
        " u'generalized_method_of_moments.txt',\n",
        " u'random_sample.txt',\n",
        " u'random_sampling.txt',\n",
        " u'spatial_analysis.txt',\n",
        " u'random_variable.txt',\n",
        " u'chi-squared_distribution.txt',\n",
        " u'pattern_recognition.txt',\n",
        " u'genetic_algorithm.txt',\n",
        " u'p-value.txt',\n",
        " u'estimation_of_covariance_matrices.txt',\n",
        " u'power_law.txt',\n",
        " u'online_nmf.txt',\n",
        " u'non-negative_matrix_factorization.txt',\n",
        " u'information_entropy.txt',\n",
        " u'entropy_(information_theory).txt',\n",
        " u'hidden_markov_model.txt',\n",
        " u'random_walk.txt',\n",
        " u'it\\u014d_diffusion.txt',\n",
        " u'autoregressive_model.txt',\n",
        " u'covariance_matrix.txt',\n",
        " u'statistical_inference.txt',\n",
        " u\"taylor's_law.txt\",\n",
        " u'independent_component_analysis.txt']"
       ]
      }
     ],
     "prompt_number": 36
    }
   ],
   "metadata": {}
  }
 ]
}