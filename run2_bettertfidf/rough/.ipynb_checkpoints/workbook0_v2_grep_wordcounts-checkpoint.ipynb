{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to use grep to find the word counts of all words in our common vocab. Thus when computing tf-idf scores only need to load the dictionaries saved by this file, will save time by negating repetitive computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "\n",
    "# load common vocab\n",
    "vocab = pickle.load(open('../data_objects/vocab.p', 'r'))\n",
    "\n",
    "# data paths\n",
    "path_arxiv = '/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/'\n",
    "path_wiki = '/home/jerry/Data/Hopper_Project/ptm_data/wiki/'\n",
    "\n",
    "# dictionaries\n",
    "wiki_sep_byword = {}\n",
    "wiki_sep_byfile = {}\n",
    "arxiv_sep_byword = {}\n",
    "arxiv_sep_byfile = {}\n",
    "\n",
    "def num_inDirectory(word, path_directory): \n",
    "    process = subprocess.Popen(\"grep {} {} -l | wc -l\".format(word, path_directory+'*'), \n",
    "                               shell=True, stdout=subprocess.PIPE) \n",
    "    return process.communicate()[0].strip('\\n')\n",
    "\n",
    "def num_inFile(word, f, path_directory):\n",
    "    process = subprocess.Popen(\"grep {} {} -c\".format(word, path_directory+f), \n",
    "                               shell=True, stdout=subprocess.PIPE) \n",
    "    return process.communicate()[0].strip('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# populate dictionaries containing counts of words in corpus\n",
    "# currenlty corpuses are wiki, arxiv seperate\n",
    "i = 0\n",
    "for word in vocab:\n",
    "    wiki_sep_byword[word] = num_inDirectory(word, path_wiki)\n",
    "    arxiv_sep_byword[word] = num_inDirectory(word, path_arxiv)\n",
    "    print(i, word)\n",
    "    i+=1\n",
    "\n",
    "wiki_sep_byword['number of documents in wiki'] = len(os.listdir(path_wiki))\n",
    "\n",
    "arxiv_sep_byword['number of documents in arxiv'] = len(os.listdir(path_arxiv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle\n",
    "#pickle.dump(wiki_sep_byword, \n",
    "            open('./data_objects/wiki_sep_byword.p', 'w'))\n",
    "#pickle.dump(arxiv_sep_byword,\n",
    "            open('./data_objects/arxiv_sep_byword.p', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for f in os.listdir(path_wiki):\n",
    "    # establish values in dict as another\n",
    "    # embedded dict\n",
    "    wiki_sep_byfile[f] = {}\n",
    "    \n",
    "    j = 0\n",
    "    for word in vocab:\n",
    "        wiki_sep_byfile[f][word] = num_inFile(word, f, path_wiki)\n",
    "        if (j % 100 == 0): print(j, word)\n",
    "        j += 1\n",
    "        \n",
    "    print(i,f)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes somewhere around 60-90 seconds to finish one file. So estimating for the wiki concepts, should take 70-110 min. For the arxiv papers in total, should take 60-90 hours for all 3600 papers. Computing 100 of the arxiv papers will be find, but if I ever want to compute all of them I will need to parallelize. \n",
    "\n",
    "So I figured out the problem with the empty dictionaries for certain files. Some of the wiki files have parantheses in them. grep needs to change () to \\(\\). Use re to make the swap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of replacement strings.\n",
    "# typing by hand bc difficulty with re\n",
    "\n",
    "# recompute these guys again\n",
    "replacement_list = ['asymptotic_theory_(statistics).txt',\n",
    "               'bias_(statistics).txt',\n",
    "               'coherence_(statistics).txt',\n",
    "               'completeness_(statistics).txt',\n",
    "               'consistency_(statistics).txt',\n",
    "               'decoupling_(probability).txt',\n",
    "               'efficiency_(statistics).txt',\n",
    "               'entropy_(information_theory).txt',\n",
    "               'frequency_(statistics).txt',\n",
    "               'shrinkage_(statistics).txt',\n",
    "               'behrens\\xc3\\xa2\\xc2\\x80\\xc2\\x93fisher_problem.txt',\n",
    "                'kullback\\xc3\\xa2\\xc2\\x80\\xc2\\x93leibler_divergence.txt']\n",
    "\n",
    "replacement_to = ['asymptotic_theory_\\(statistics\\).txt',\n",
    "               'bias_\\(statistics\\).txt',\n",
    "               'coherence_\\(statistics\\).txt',\n",
    "               'completeness_\\(statistics\\).txt',\n",
    "               'consistency_\\(statistics\\).txt',\n",
    "               'decoupling_\\(probability\\).txt',\n",
    "               'efficiency_\\(statistics\\).txt',\n",
    "               'entropy_\\(information_theory\\).txt',\n",
    "               'frequency_\\(statistics\\).txt',\n",
    "               'shrinkage_\\(statistics\\).txt',\n",
    "               'behrensâ??fisher_problem.txt',\n",
    "               'kullbackâ??leibler_divergence.txt']\n",
    "\n",
    "replacement = {replacement_list[i]:replacement_to[i] \n",
    "               for i in range(len(replacement_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "kullbackâleibler_divergence.txt\n",
      "error\n",
      "youden's_j_statistic.txt\n",
      "error\n",
      "peirce's_criterion.txt\n",
      "error\n",
      "galton's_problem.txt\n",
      "error\n",
      "behrensâfisher_problem.txt\n"
     ]
    }
   ],
   "source": [
    "#wiki_sep_byfile = pickle.load(open('./data_objects/wiki_sep_byfile.p', 'r'))\n",
    "\n",
    "# write loop to check that all values are ints\n",
    "for key in wiki_sep_byfile:\n",
    "    for key2 in wiki_sep_byfile[key]:\n",
    "        tmp = wiki_sep_byfile[key][key2]\n",
    "        if (tmp == ''):\n",
    "            print(\"error\")\n",
    "            print(key)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "replacement2_list = [\"youden's_j_statistic.txt\",\n",
    "                     \"peirce's_criterion.txt\",\n",
    "                     \"galton's_problem.txt\"]\n",
    "\n",
    "replacement2_to = [\"youdens_j_statistic.txt\",\n",
    "                     \"peirces_criterion.txt\",\n",
    "                     \"galtons_problem.txt\"]\n",
    "\n",
    "replacement2 = {replacement2_list[i]:replacement2_to[i] for i in range(len(replacement2_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'writings', '0')\n",
      "(1000, u'totality', '0')\n",
      "(2000, u'correction', '0')\n",
      "(3000, u'faulty', '0')\n",
      "(4000, u'plugged', '0')\n",
      "(5000, u'case', '1')\n",
      "(0, 'youdens_j_statistic.txt')\n",
      "(0, u'writings', '0')\n",
      "(1000, u'totality', '0')\n",
      "(2000, u'correction', '0')\n",
      "(3000, u'faulty', '0')\n",
      "(4000, u'plugged', '0')\n",
      "(5000, u'case', '0')\n",
      "(1, 'peirces_criterion.txt')\n",
      "(0, u'writings', '0')\n",
      "(1000, u'totality', '0')\n",
      "(2000, u'correction', '1')\n",
      "(3000, u'faulty', '0')\n",
      "(4000, u'plugged', '0')\n",
      "(5000, u'case', '4')\n",
      "(2, 'galtons_problem.txt')\n"
     ]
    }
   ],
   "source": [
    "# wiki_sep_byfile = pickle.load(open('./data_objects/wiki_sep_byfile.p', 'r'))\n",
    "\n",
    "i = 0\n",
    "for item in replacement2_to:\n",
    "    # establish values in dict as another\n",
    "    # embedded dict\n",
    "    wiki_sep_byfile[item] = {}\n",
    "    \n",
    "    j = 0\n",
    "    for word in vocab:\n",
    "        wiki_sep_byfile[item][word] = num_inFile(word, item, path_wiki)\n",
    "        if (j % 1000 == 0): print(j, word, num_inFile(word, item, path_wiki))\n",
    "        j += 1\n",
    "              \n",
    "    print(i,item)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(wiki_sep_byfile, \n",
    "            open('./data_objects/wiki_sep_byfile.p', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check to make sure grep returned no empty space\n",
    "for f in wiki_sep_byfile:\n",
    "    #print(f)\n",
    "    for key in wiki_sep_byfile[f]:\n",
    "        if wiki_sep_byfile[f][key] == '':\n",
    "            print(f)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's still a problem with these 5 wiki documents. I will just do them by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's fill in arxiv_sep_byfile for a subset of all arxiv papers\n",
    "papers = os.listdir(path_arxiv)\n",
    "subset = papers[1:200]\n",
    "\n",
    "i = 0\n",
    "for f in subset:\n",
    "    # establish values in dict as another\n",
    "    # embedded dict\n",
    "    arxiv_sep_byfile[f] = {}\n",
    "    \n",
    "    j = 0\n",
    "    for word in vocab:\n",
    "        arxiv_sep_byfile[f][word] = num_inFile(word, f, path_arxiv)\n",
    "        if (j % 100 == 0): print(j, word)\n",
    "        if (j % 3000 == 0): print(num_inFile(word, f, path_arxiv))\n",
    "        j += 1\n",
    "        \n",
    "    print(i,f)\n",
    "    i += 1\n",
    "    \n",
    "# pickle it\n",
    "#pickle.dump(arxiv_sep_byfile, \n",
    "            open('./data_objects/arxiv_sep_byfile.p', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets check that arxiv_sep_byfile values seem ok\n",
    "for f in arxiv_sep_byfile:\n",
    "    #print(f)\n",
    "    for key in arxiv_sep_byfile[f]:\n",
    "        if arxiv_sep_byfile[f][key] == '':\n",
    "            print(f)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word count in each doc using textblob\n",
    "#load\n",
    "wiki_sep_byfile = pickle.load(open('./data_objects/wiki_sep_byfile.p', 'r'))\n",
    "arxiv_sep_byfile = pickle.load(open('./data_objects/arxiv_sep_byfile.p', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#for key in wiki_sep_byfile:\n",
    "    #file = open(path_wiki+key, 'r')\n",
    "txt = ''\n",
    "file = None\n",
    "txt_tb = None\n",
    "\n",
    "file = open(path_wiki+'efficiency_(statistics).txt', 'r')\n",
    "txt = file.read()\n",
    "\n",
    "txt_tb = TextBlob(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionaries\n",
    "wiki_wordsindoc = {}\n",
    "arxiv_wordsindoc = {}\n",
    "\n",
    "file = None\n",
    "txt = ''\n",
    "txt_tb = None\n",
    "\n",
    "for f in wiki_sep_byfile:\n",
    "    file = open(path_wiki+f, 'r')\n",
    "    txt = file.read()\n",
    "    file.close()\n",
    "    txt = unicode(txt, errors='ignore')\n",
    "    txt_tb = TextBlob(txt)\n",
    "    wiki_wordsindoc[f] = len(txt_tb.words)\n",
    "    \n",
    "for f in arxiv_sep_byfile:\n",
    "    file = open(path_arxiv+f, 'r')\n",
    "    txt = file.read()\n",
    "    file.close()\n",
    "    txt = unicode(txt, errors='ignore')\n",
    "    txt_tb = TextBlob(txt)\n",
    "    arxiv_wordsindoc[f] = len(txt_tb.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(wiki_wordsindoc, open('./data_objects/wiki_wordsindoc.p', 'w'))\n",
    "pickle.dump(arxiv_wordsindoc, open('./data_objects/arxiv_wordsindoc.p', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
