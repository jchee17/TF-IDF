{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's write the tf-idf functions here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import timeit\n",
      "import string\n",
      "import pickle\n",
      "import math\n",
      "from __future__ import division\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "\n",
      "path_arxiv = '/home/jerry/Data/Hopper_Project/ptm_data/arxiv_processed_trunc/'\n",
      "articles = os.listdir(path_arxiv)\n",
      "length_arxiv = len(articles)\n",
      "\n",
      "concepts = pickle.load(open('./data_objects/master-concepts.p', 'r'))\n",
      "\n",
      "arxiv_sep_byfile = pickle.load(open('./data_objects/arxiv_sep_byfile_caseinsen.p' ,'r'))\n",
      "arxiv_sep_byword = pickle.load(open('./data_objects/arxiv_sep_byword_caseinsen.p' ,'r'))\n",
      "arxiv_wordsindoc = pickle.load(open('./data_objects/arxiv_wordsindoc.p', 'r'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tf-idf functions. specific to arxiv only\n",
      "def tf(word, document):\n",
      "    # must first check if word in document\n",
      "    if (word in arxiv_sep_byfile[document]):\n",
      "        return (arxiv_sep_byfile[document][word] / arxiv_wordsindoc[document])\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "def idf(word):\n",
      "    return math.log(length_arxiv / (1 + arxiv_sep_byword[word]))\n",
      "\n",
      "def tfidf(word, document):\n",
      "    return tf(word, document) * idf(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_arxiv = {}\n",
      "tmp = 0\n",
      "\n",
      "for f in arxiv_sep_byfile:\n",
      "    # initialize dictoinary\n",
      "    tfidf_arxiv[f] = {}\n",
      "    \n",
      "    for word in concepts:\n",
      "        # only save the nonzero values\n",
      "        tmp = tfidf(word, f)\n",
      "        if (tmp != 0):\n",
      "            tfidf_arxiv[f][word] = tmp\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf_arxiv[articles[200]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "{'adaptive estimator': 0.004160427539792417,\n",
        " 'convex hull': 0.013059295062262218,\n",
        " 'dap': 0.0028401392455172573,\n",
        " 'density estimation': 0.016331859175118296,\n",
        " 'empirical': 0.00843853916614201,\n",
        " 'estimator': 0.00540723298760749,\n",
        " 'mean': 0.0010184572325666512,\n",
        " 'minimax': 0.011667133865881535,\n",
        " 'mixing': 0.005119576194949142,\n",
        " 'model selection': 0.018650140742904574,\n",
        " 'prior probability': 0.004426871079308689,\n",
        " 'probability': 0.002408316273932648,\n",
        " 'probability distribution': 0.0019665658000033205,\n",
        " 'risk': 0.0116392203402462,\n",
        " 'statistic': 0.0002236212359315496,\n",
        " 'statistica': 0.000556914004759965}"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(tfidf_arxiv, open('./data_objects/tfidf_arxiv_caseinsen.p', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note: saving only the nonzero tf-idf values saves ALOT of space. Down from 100 MB to 1MB"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}