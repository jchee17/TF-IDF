(dp0
S'efficiency_(statistics).txt'
p1
ccopy_reg
_reconstructor
p2
(ctextblob.blob
TextBlob
p3
c__builtin__
object
p4
Ntp5
Rp6
(dp7
S'pos_tagger'
p8
g2
(ctextblob.en.taggers
NLTKTagger
p9
g4
Ntp10
Rp11
sS'string'
p12
Vin statistics efficiency is a term used in the comparison of various statistical procedures and in particular it refers to a measure of the optimality of an estimator of an experimental design or of a hypothesis testing procedure essentially a more efficient estimator experiment or test needs fewer observations than a less efficient one to achieve a given performance this article primarily deals with efficiency of estimators\u000athe relative efficiency of two procedures is the ratio of their efficiencies although often this term is used where the comparison is made between a given procedure and a notional best possible procedure the efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure but it is often possible to use the asymptotic relative efficiency defined as the limit of the relative efficiencies as the sample size grows as the principal comparison measure\u000aefficiencies are often defined using the variance or mean square error as the measure of desirability\u000a\u000a\u000a estimators \u000athe efficiency of an unbiased estimator t of a parameter  is defined as \u000a\u000awhere  is the fisher information of the sample thus et is the minimum possible variance for an unbiased estimator divided by its actual variance the cramrrao bound can be used to prove that et  1\u000a\u000a\u000a efficient estimators \u000a\u000aif an unbiased estimator of a parameter  attains  for all values of the parameter then the estimator is called efficient\u000aequivalently the estimator achieves equality in the cramrrao inequality for all \u000aan efficient estimator is also the minimum variance unbiased estimator mvue this is because an efficient estimator maintains equality on the cramrrao inequality for all parameter values which means it attains the minimum variance for all parameters the definition of the mvue the mvue estimator even if it exists is not necessarily efficient because minimum does not mean equality holds on the cramrrao inequality\u000athus an efficient estimator need not exist but if it does it is the mvue\u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a example \u000aconsider a sample of size  drawn from a normal distribution of mean  and unit variance ie \u000athe sample mean  of the sample  defined as\u000a\u000athe variance of the mean 1n the square of the standard error is equal to the reciprocal of the fisher information from the sample and thus by the cramrrao inequality the sample mean is efficient in the sense that its efficiency is unity 100\u000anow consider the sample median  this is an unbiased and consistent estimator for  for large  the sample median is approximately normally distributed with mean  and variance  ie\u000a\u000athe efficiency for large  is thus\u000a\u000anote that this is the asymptotic efficiency  that is the efficiency in the limit as sample size  tends to infinity for finite values of  the efficiency is higher than this for example a sample size of 3 gives an efficiency of about 74\u000athe sample mean is thus more efficient than the sample median in this example however there may be measures by which the median performs better for example the median is far more robust to outliers so that if the gaussian model is questionable or approximate there may advantages to using the median see robust statistics\u000a\u000a\u000a dominant estimators \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000a\u000a\u000a relative efficiency \u000athe relative efficiency of two estimators is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000aan alternative to relative efficiency for comparing estimators is the pitman closeness criterion this replaces the comparison of meansquarederrors with comparing how often one estimator produces estimates closer to the true value than another estimator\u000a\u000a\u000a estimators of uid variables \u000ain the case that we are estimating the mean of uncorrelated identically distributed variables we can take advantage of the fact that the variance of the sum is the sum of the variance in this case efficiency can be defined as the square of the coefficient of variation ie\u000a\u000arelative efficiency of two such estimators can thus be interpreted as the relative sample size of one required to achieve the certainty of the other proof\u000a now because  we have  so the relative efficiency expresses the relative sample size of the first estimator needed to match the variance of the second\u000a\u000a\u000a robustness \u000aefficiency of an estimator may change significantly if the distribution changes often dropping this is one of the motivations of robust statistics  an estimator such as the sample mean is an efficient estimator of the population mean of a normal distribution for example but can be an inefficient estimator of a mixture distribution of two normal distributions with the same mean and different variances for example if a distribution is a combination of 98 n  and 2 n 10 the presence of extreme values from the latter distribution often contaminating outliers significantly reduces the efficiency of the sample mean as an estimator of  by contrast the trimmed mean is less efficient for a normal distribution but is more robust less affected by changes in distribution and thus may be more efficient for a mixture distribution similarly the shape of a distribution such as skewness or heavy tails can significantly reduce the efficiency of estimators that assume a symmetric distribution or thin tails\u000a\u000a\u000a uses of inefficient estimators \u000a\u000awhile efficiency is a desirable quality of an estimator it must be weighed against other desiderata and an estimator that is efficient for certain distributions may well be inefficient for other distributions most significantly estimators that are efficient for clean data from a simple distribution such as the normal distribution which is symmetric unimodal and has thin tails may not be robust to contamination by outliers and may be inefficient for more complicated distributions in robust statistics more importance is placed on robustness and applicability to a wide variety of distributions rather than efficiency on a single distribution mestimators are a general class of solutions motivated by these concerns yielding both robustness and high relative efficiency though possibly lower efficiency than traditional estimators for some cases these are potentially very computationally complicated however\u000aa more traditional alternative are lestimators which are very simple statistics that are easy to compute and interpret in many cases robust and often sufficiently efficient for initial estimates see applications of lestimators for further discussion\u000a\u000a\u000a hypothesis tests \u000afor comparing significance tests a meaningful measure of efficiency can be defined based on the sample size required for the test to achieve a given power\u000apitman efficiency and bahadur efficiency or hodgeslehmann efficiency  relate to the comparison of the performance of statistical hypothesis testing procedures the encyclopedia of mathematics provides a brief exposition of these three criteria\u000a\u000a\u000a experimental design \u000a\u000afor experimental designs efficiency relates to the ability of a design to achieve the objective of the study with minimal expenditure of resources such as time and money in simple cases the relative efficiency of designs can be expressed as the ratio of the sample sizes required to achieve a given objective\u000asee optimal design for further discussion\u000a\u000a\u000a notes \u000a\u000a\u000a
p13
sS'tokenizer'
p14
g2
(ctextblob.tokenizers
WordTokenizer
p15
g4
Ntp16
Rp17
sS'stripped'
p18
Vin statistics efficiency is a term used in the comparison of various statistical procedures and in particular it refers to a measure of the optimality of an estimator of an experimental design or of a hypothesis testing procedure essentially a more efficient estimator experiment or test needs fewer observations than a less efficient one to achieve a given performance this article primarily deals with efficiency of estimators\u000athe relative efficiency of two procedures is the ratio of their efficiencies although often this term is used where the comparison is made between a given procedure and a notional best possible procedure the efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure but it is often possible to use the asymptotic relative efficiency defined as the limit of the relative efficiencies as the sample size grows as the principal comparison measure\u000aefficiencies are often defined using the variance or mean square error as the measure of desirability\u000a\u000a\u000a estimators \u000athe efficiency of an unbiased estimator t of a parameter  is defined as \u000a\u000awhere  is the fisher information of the sample thus et is the minimum possible variance for an unbiased estimator divided by its actual variance the cramrrao bound can be used to prove that et  1\u000a\u000a\u000a efficient estimators \u000a\u000aif an unbiased estimator of a parameter  attains  for all values of the parameter then the estimator is called efficient\u000aequivalently the estimator achieves equality in the cramrrao inequality for all \u000aan efficient estimator is also the minimum variance unbiased estimator mvue this is because an efficient estimator maintains equality on the cramrrao inequality for all parameter values which means it attains the minimum variance for all parameters the definition of the mvue the mvue estimator even if it exists is not necessarily efficient because minimum does not mean equality holds on the cramrrao inequality\u000athus an efficient estimator need not exist but if it does it is the mvue\u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a example \u000aconsider a sample of size  drawn from a normal distribution of mean  and unit variance ie \u000athe sample mean  of the sample  defined as\u000a\u000athe variance of the mean 1n the square of the standard error is equal to the reciprocal of the fisher information from the sample and thus by the cramrrao inequality the sample mean is efficient in the sense that its efficiency is unity 100\u000anow consider the sample median  this is an unbiased and consistent estimator for  for large  the sample median is approximately normally distributed with mean  and variance  ie\u000a\u000athe efficiency for large  is thus\u000a\u000anote that this is the asymptotic efficiency  that is the efficiency in the limit as sample size  tends to infinity for finite values of  the efficiency is higher than this for example a sample size of 3 gives an efficiency of about 74\u000athe sample mean is thus more efficient than the sample median in this example however there may be measures by which the median performs better for example the median is far more robust to outliers so that if the gaussian model is questionable or approximate there may advantages to using the median see robust statistics\u000a\u000a\u000a dominant estimators \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000a\u000a\u000a relative efficiency \u000athe relative efficiency of two estimators is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000aan alternative to relative efficiency for comparing estimators is the pitman closeness criterion this replaces the comparison of meansquarederrors with comparing how often one estimator produces estimates closer to the true value than another estimator\u000a\u000a\u000a estimators of uid variables \u000ain the case that we are estimating the mean of uncorrelated identically distributed variables we can take advantage of the fact that the variance of the sum is the sum of the variance in this case efficiency can be defined as the square of the coefficient of variation ie\u000a\u000arelative efficiency of two such estimators can thus be interpreted as the relative sample size of one required to achieve the certainty of the other proof\u000a now because  we have  so the relative efficiency expresses the relative sample size of the first estimator needed to match the variance of the second\u000a\u000a\u000a robustness \u000aefficiency of an estimator may change significantly if the distribution changes often dropping this is one of the motivations of robust statistics  an estimator such as the sample mean is an efficient estimator of the population mean of a normal distribution for example but can be an inefficient estimator of a mixture distribution of two normal distributions with the same mean and different variances for example if a distribution is a combination of 98 n  and 2 n 10 the presence of extreme values from the latter distribution often contaminating outliers significantly reduces the efficiency of the sample mean as an estimator of  by contrast the trimmed mean is less efficient for a normal distribution but is more robust less affected by changes in distribution and thus may be more efficient for a mixture distribution similarly the shape of a distribution such as skewness or heavy tails can significantly reduce the efficiency of estimators that assume a symmetric distribution or thin tails\u000a\u000a\u000a uses of inefficient estimators \u000a\u000awhile efficiency is a desirable quality of an estimator it must be weighed against other desiderata and an estimator that is efficient for certain distributions may well be inefficient for other distributions most significantly estimators that are efficient for clean data from a simple distribution such as the normal distribution which is symmetric unimodal and has thin tails may not be robust to contamination by outliers and may be inefficient for more complicated distributions in robust statistics more importance is placed on robustness and applicability to a wide variety of distributions rather than efficiency on a single distribution mestimators are a general class of solutions motivated by these concerns yielding both robustness and high relative efficiency though possibly lower efficiency than traditional estimators for some cases these are potentially very computationally complicated however\u000aa more traditional alternative are lestimators which are very simple statistics that are easy to compute and interpret in many cases robust and often sufficiently efficient for initial estimates see applications of lestimators for further discussion\u000a\u000a\u000a hypothesis tests \u000afor comparing significance tests a meaningful measure of efficiency can be defined based on the sample size required for the test to achieve a given power\u000apitman efficiency and bahadur efficiency or hodgeslehmann efficiency  relate to the comparison of the performance of statistical hypothesis testing procedures the encyclopedia of mathematics provides a brief exposition of these three criteria\u000a\u000a\u000a experimental design \u000a\u000afor experimental designs efficiency relates to the ability of a design to achieve the objective of the study with minimal expenditure of resources such as time and money in simple cases the relative efficiency of designs can be expressed as the ratio of the sample sizes required to achieve a given objective\u000asee optimal design for further discussion\u000a\u000a\u000a notes
p19
sS'parser'
p20
g2
(ctextblob.en.parsers
PatternParser
p21
g4
Ntp22
Rp23
sS'analyzer'
p24
g2
(ctextblob.en.sentiments
PatternAnalyzer
p25
g4
Ntp26
Rp27
(dp28
S'_trained'
p29
I00
sbsS'raw'
p30
Vin statistics efficiency is a term used in the comparison of various statistical procedures and in particular it refers to a measure of the optimality of an estimator of an experimental design or of a hypothesis testing procedure essentially a more efficient estimator experiment or test needs fewer observations than a less efficient one to achieve a given performance this article primarily deals with efficiency of estimators\u000athe relative efficiency of two procedures is the ratio of their efficiencies although often this term is used where the comparison is made between a given procedure and a notional best possible procedure the efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure but it is often possible to use the asymptotic relative efficiency defined as the limit of the relative efficiencies as the sample size grows as the principal comparison measure\u000aefficiencies are often defined using the variance or mean square error as the measure of desirability\u000a\u000a\u000a estimators \u000athe efficiency of an unbiased estimator t of a parameter  is defined as \u000a\u000awhere  is the fisher information of the sample thus et is the minimum possible variance for an unbiased estimator divided by its actual variance the cramrrao bound can be used to prove that et  1\u000a\u000a\u000a efficient estimators \u000a\u000aif an unbiased estimator of a parameter  attains  for all values of the parameter then the estimator is called efficient\u000aequivalently the estimator achieves equality in the cramrrao inequality for all \u000aan efficient estimator is also the minimum variance unbiased estimator mvue this is because an efficient estimator maintains equality on the cramrrao inequality for all parameter values which means it attains the minimum variance for all parameters the definition of the mvue the mvue estimator even if it exists is not necessarily efficient because minimum does not mean equality holds on the cramrrao inequality\u000athus an efficient estimator need not exist but if it does it is the mvue\u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a example \u000aconsider a sample of size  drawn from a normal distribution of mean  and unit variance ie \u000athe sample mean  of the sample  defined as\u000a\u000athe variance of the mean 1n the square of the standard error is equal to the reciprocal of the fisher information from the sample and thus by the cramrrao inequality the sample mean is efficient in the sense that its efficiency is unity 100\u000anow consider the sample median  this is an unbiased and consistent estimator for  for large  the sample median is approximately normally distributed with mean  and variance  ie\u000a\u000athe efficiency for large  is thus\u000a\u000anote that this is the asymptotic efficiency  that is the efficiency in the limit as sample size  tends to infinity for finite values of  the efficiency is higher than this for example a sample size of 3 gives an efficiency of about 74\u000athe sample mean is thus more efficient than the sample median in this example however there may be measures by which the median performs better for example the median is far more robust to outliers so that if the gaussian model is questionable or approximate there may advantages to using the median see robust statistics\u000a\u000a\u000a dominant estimators \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000a\u000a\u000a relative efficiency \u000athe relative efficiency of two estimators is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000aan alternative to relative efficiency for comparing estimators is the pitman closeness criterion this replaces the comparison of meansquarederrors with comparing how often one estimator produces estimates closer to the true value than another estimator\u000a\u000a\u000a estimators of uid variables \u000ain the case that we are estimating the mean of uncorrelated identically distributed variables we can take advantage of the fact that the variance of the sum is the sum of the variance in this case efficiency can be defined as the square of the coefficient of variation ie\u000a\u000arelative efficiency of two such estimators can thus be interpreted as the relative sample size of one required to achieve the certainty of the other proof\u000a now because  we have  so the relative efficiency expresses the relative sample size of the first estimator needed to match the variance of the second\u000a\u000a\u000a robustness \u000aefficiency of an estimator may change significantly if the distribution changes often dropping this is one of the motivations of robust statistics  an estimator such as the sample mean is an efficient estimator of the population mean of a normal distribution for example but can be an inefficient estimator of a mixture distribution of two normal distributions with the same mean and different variances for example if a distribution is a combination of 98 n  and 2 n 10 the presence of extreme values from the latter distribution often contaminating outliers significantly reduces the efficiency of the sample mean as an estimator of  by contrast the trimmed mean is less efficient for a normal distribution but is more robust less affected by changes in distribution and thus may be more efficient for a mixture distribution similarly the shape of a distribution such as skewness or heavy tails can significantly reduce the efficiency of estimators that assume a symmetric distribution or thin tails\u000a\u000a\u000a uses of inefficient estimators \u000a\u000awhile efficiency is a desirable quality of an estimator it must be weighed against other desiderata and an estimator that is efficient for certain distributions may well be inefficient for other distributions most significantly estimators that are efficient for clean data from a simple distribution such as the normal distribution which is symmetric unimodal and has thin tails may not be robust to contamination by outliers and may be inefficient for more complicated distributions in robust statistics more importance is placed on robustness and applicability to a wide variety of distributions rather than efficiency on a single distribution mestimators are a general class of solutions motivated by these concerns yielding both robustness and high relative efficiency though possibly lower efficiency than traditional estimators for some cases these are potentially very computationally complicated however\u000aa more traditional alternative are lestimators which are very simple statistics that are easy to compute and interpret in many cases robust and often sufficiently efficient for initial estimates see applications of lestimators for further discussion\u000a\u000a\u000a hypothesis tests \u000afor comparing significance tests a meaningful measure of efficiency can be defined based on the sample size required for the test to achieve a given power\u000apitman efficiency and bahadur efficiency or hodgeslehmann efficiency  relate to the comparison of the performance of statistical hypothesis testing procedures the encyclopedia of mathematics provides a brief exposition of these three criteria\u000a\u000a\u000a experimental design \u000a\u000afor experimental designs efficiency relates to the ability of a design to achieve the objective of the study with minimal expenditure of resources such as time and money in simple cases the relative efficiency of designs can be expressed as the ratio of the sample sizes required to achieve a given objective\u000asee optimal design for further discussion\u000a\u000a\u000a notes \u000a\u000a\u000a
p31
sS'np_extractor'
p32
g2
(ctextblob.en.np_extractors
FastNPExtractor
p33
g4
Ntp34
Rp35
(dp36
g29
I00
sbsS'classifier'
p37
NsbsS'convergence_of_random_variables.txt'
p38
g2
(g3
g4
Ntp39
Rp40
(dp41
g8
g11
sg12
Vin probability theory there exist several different notions of convergence of random variables the convergence of sequences of random variables to some limit random variable is an important concept in probability theory and its applications to statistics and stochastic processes the same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied the different possible notions of convergence relate to how such a behaviour can be characterised two readily understood behaviours are that the sequence eventually takes a constant value and that values in the sequence continue to change but can be described by an unchanging probability distribution\u000a\u000a\u000a backgroundedit \u000astochastic convergence formalizes the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle into a pattern the pattern may for instance be\u000aconvergence in the classical sense to a fixed value perhaps itself coming from a random event\u000aan increasing similarity of outcomes to what a purely deterministic function would produce\u000aan increasing preference towards a certain outcome\u000aan increasing aversion against straying far away from a certain outcome\u000asome less obvious more theoretical patterns could be\u000athat the probability distribution describing the next outcome may grow increasingly similar to a certain distribution\u000athat the series formed by calculating the expected value of the outcomes distance from a particular value may converge to 0\u000athat the variance of the random variable describing the next event grows smaller and smaller\u000athese other types of patterns that may arise are reflected in the different types of stochastic convergence that have been studied\u000awhile the above discussion has related to the convergence of a single series to a limiting value the notion of the convergence of two series towards each other is also important but this is easily handled by studying the sequence defined as either the difference or the ratio of the two series\u000afor example if the average of n independent random variables yi i  1  n all having the same finite mean and variance is given by\u000a\u000athen as n tends to infinity xn converges in probability see below to the common mean  of the random variables yi this result is known as the weak law of large numbers other forms of convergence are important in other useful theorems including the central limit theorem\u000athroughout the following we assume that xn is a sequence of random variables and x is a random variable and all of them are defined on the same probability space \u000a\u000a\u000a convergence in distributionedit \u000awith this mode of convergence we increasingly expect to see the next outcome in a sequence of random experiments becoming better and better modeled by a given probability distribution\u000aconvergence in distribution is the weakest form of convergence since it is implied by all other types of convergence mentioned in this article however convergence in distribution is very frequently used in practice most often it arises from application of the central limit theorem\u000a\u000a\u000a definitionedit \u000aa sequence x1 x2  of random variables is said to converge in distribution or converge weakly or converge in law to a random variable x if\u000a\u000afor every number x  r at which f is continuous here fn and f are the cumulative distribution functions of random variables xn and x respectively\u000athe requirement that only the continuity points of f should be considered is essential for example if xn are distributed uniformly on intervals 0 1n then this sequence converges in distribution to a degenerate random variable x  0 indeed fnx  0 for all n when x  0 and fnx  1 for all x  1n when n  0 however for this limiting random variable f0  1 even though fn0  0 for all n thus the convergence of cdfs fails at the point x  0 where f is discontinuous\u000aconvergence in distribution may be denoted as\u000a\u000awhere  is the law probability distribution of x for example if x is standard normal we can write \u000afor random vectors x1 x2   rk the convergence in distribution is defined similarly we say that this sequence converges in distribution to a random kvector x if\u000a\u000afor every a  rk which is a continuity set of x\u000athe definition of convergence in distribution may be extended from random vectors to more general random elements in arbitrary metric spaces and even to the random variables which are not measurable  a situation which occurs for example in the study of empirical processes this is the weak convergence of laws without laws being defined  except asymptotically\u000ain this case the term weak convergence is preferable see weak convergence of measures and we say that a sequence of random elements xn converges weakly to x denoted as xn  x if\u000a\u000afor all continuous bounded functions h here e denotes the outer expectation that is the expectation of a smallest measurable function g that dominates hxn\u000a\u000a\u000a propertiesedit \u000asince fa  prx  a the convergence in distribution means that the probability for xn to be in a given range is approximately equal to the probability that the value of x is in that range provided n is sufficiently large\u000ain general convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge as an example one may consider random variables with densities fnx  1  cos2nx101 these random variables converge in distribution to a uniform u01 whereas their densities do not converge at allhowever scheffs lemma implies that convergence of the probability density functions implies convergence in distribution\u000a\u000athe portmanteau lemma provides several equivalent definitions of convergence in distribution although these definitions are less intuitive they are used to prove a number of statistical theorems the lemma states that xn converges in distribution to x if and only if any of the following statements are trueexn  ex for all bounded continuous functions  where e denotes the expected value\u000aexn  ex for all bounded lipschitz functions \u000alimsup exn   ex for every upper semicontinuous function  bounded from above\u000aliminf exn   ex for every lower semicontinuous function  bounded from below\u000alimsupprxn  c  prx  c for all closed sets c\u000aliminfprxn  u  prx  u for all open sets u\u000alimprxn  a  prx  a for all continuity sets a of random variable x\u000a\u000athe continuous mapping theorem states that for a continuous function g if the sequence xn converges in distribution to x then gxn converges in distribution to gx\u000anote however that convergence in distribution of xn to x and yn to y does in general not imply convergence in distribution of xn  yn to x  y or of xnyn to xy\u000a\u000alvys continuity theorem the sequence xn converges in distribution to x if and only if the sequence of corresponding characteristic functions n converges pointwise to the characteristic function  of x\u000aconvergence in distribution is metrizable by the lvyprokhorov metric\u000aa natural link to convergence in distribution is the skorokhods representation theorem\u000a\u000a\u000a convergence in probabilityedit \u000athe basic idea behind this type of convergence is that the probability of an unusual outcome becomes smaller and smaller as the sequence progresses\u000athe concept of convergence in probability is used very often in statistics for example an estimator is called consistent if it converges in probability to the quantity being estimated convergence in probability is also the type of convergence established by the weak law of large numbers\u000a\u000a\u000a definitionedit \u000aa sequence xn of random variables converges in probability towards the random variable x if for all   0\u000a\u000aformally pick any   0 and any   0 let pn be the probability that xn is outside the ball of radius  centered at x then for xn to converge in probability to x there should exist a number n which will depend on  and  such that for all n  n pn  \u000aconvergence in probability is denoted by adding the letter p over an arrow indicating convergence or using the plim probability limit operator\u000a\u000afor random elements xn on a separable metric space s d convergence in probability is defined similarly by\u000a\u000a\u000a propertiesedit \u000aconvergence in probability implies convergence in distributionproof\u000ain the opposite direction convergence in distribution implies convergence in probability when the limiting random variable x is a constantproof\u000aconvergence in probability does not imply almost sure convergenceproof\u000athe continuous mapping theorem states that for every continuous function g if  then also \u000aconvergence in probability defines a topology on the space of random variables over a fixed probability space this topology is metrizable by the ky fan metric\u000a\u000aor\u000a\u000a\u000a\u000a almost sure convergenceedit \u000athis is the type of stochastic convergence that is most similar to pointwise convergence known from elementary real analysis\u000a\u000a\u000a definitionedit \u000ato say that the sequence xn converges almost surely or almost everywhere or with probability 1 or strongly towards x means that\u000a\u000athis means that the values of xn approach the value of x in the sense see almost surely that events for which xn does not converge to x have probability 0 using the probability space  and the concept of the random variable as a function from  to r this is equivalent to the statement\u000a\u000ausing the notion of the limit inferior of a sequence of sets almost sure convergence can also be defined as follows\u000a\u000aalmost sure convergence is often denoted by adding the letters as over an arrow indicating convergence\u000a\u000afor generic random elements xn on a metric space s d convergence almost surely is defined similarly\u000a\u000a\u000a propertiesedit \u000aalmost sure convergence implies convergence in probability by fatous lemma and hence implies convergence in distribution it is the notion of convergence used in the strong law of large numbers\u000athe concept of almost sure convergence does not come from a topology on the space of random variables this means there is no topology on the space of random variables such that the almost surely convergent sequences are exactly the converging sequences with respect to that topology in particular there is no metric of almost sure convergence\u000a\u000a\u000a sure convergenceedit \u000ato say that the sequence of random variables xn defined over the same probability space ie a random process converges surely or everywhere or pointwise towards x means\u000a\u000awhere  is the sample space of the underlying probability space over which the random variables are defined\u000athis is the notion of pointwise convergence of sequence functions extended to sequence of random variables note that random variables themselves are functions\u000a\u000asure convergence of a random variable implies all the other kinds of convergence stated above but there is no payoff in probability theory by using sure convergence compared to using almost sure convergence the difference between the two only exists on sets with probability zero this is why the concept of sure convergence of random variables is very rarely used\u000a\u000a\u000a convergence in meanedit \u000agiven a real number r  1 we say that the sequence xn converges in the rth mean or in the lrnorm towards the random variable x if the rth absolute moments exnr and exr of xn and x exist and\u000a\u000awhere the operator e denotes the expected value convergence in rth mean tells us that the expectation of the rth power of the difference between xn and x converges to zero\u000athis type of convergence is often denoted by adding the letter lr over an arrow indicating convergence\u000a\u000athe most important cases of convergence in rth mean are\u000awhen xn converges in rth mean to x for r  1 we say that xn converges in mean to x\u000awhen xn converges in rth mean to x for r  2 we say that xn converges in mean square to x\u000aconvergence in the rth mean for r  1 implies convergence in probability by markovs inequality furthermore if r  s  1 convergence in rth mean implies convergence in sth mean hence convergence in mean square implies convergence in mean\u000ait is also worth noticing that if  then\u000a\u000a\u000a propertiesedit \u000aprovided the probability space is complete\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and \u000anone of the above statements are true for convergence in distribution\u000athe chain of implications between the various notions of convergence are noted in their respective sections they are using the arrow notation\u000a\u000athese properties together with a number of other special cases are summarized in the following list\u000a almost sure convergence implies convergence in probabilityproof\u000a\u000a convergence in probability implies there exists a subsequence  which almost surely converges\u000a\u000a convergence in probability implies convergence in distributionproof\u000a\u000a convergence in rth order mean implies convergence in probability\u000a\u000a convergence in rth order mean implies convergence in lower order mean assuming that both orders are greater than or equal to one\u000a provided r  s  1\u000a if xn converges in distribution to a constant c then xn converges in probability to cproof\u000a provided c is a constant\u000a if xn converges in distribution to x and the difference between xn and yn converges in probability to zero then yn also converges in distribution to xproof\u000a\u000a if xn converges in distribution to x and yn converges in distribution to a constant c then the joint vector xn yn converges in distribution to x cproof\u000a provided c is a constant\u000anote that the condition that yn converges to a constant is important if it were to converge to a random variable y then we wouldnt be able to conclude that xn yn converges to x y\u000a if xn converges in probability to x and yn converges in probability to y then the joint vector xn yn converges in probability to x yproof\u000a\u000aif xn converges in probability to x and if pxn  b  1 for all n and some b then xn converges in rth mean to x for all r  1 in other words if xn converges in probability to x and all random variables xn are almost surely bounded above and below then xn converges to x also in any rth mean\u000aalmost sure representation usually convergence in distribution does not imply convergence almost surely however for a given sequence xn which converges in distribution to x0 it is always possible to find a new probability space  f p and random variables yn n  0 1  defined on it such that yn is equal in distribution to xn for each n  0 and yn converges to y0 almost surely\u000aif for all   0\u000a\u000athen we say that xn converges almost completely or almost in probability towards x when xn converges almost completely towards x then it also converges almost surely to x in other words if xn converges in probability to x sufficiently quickly ie the above sequence of tail probabilities is summable for all   0 then xn also converges almost surely to x this is a direct implication from the borelcantelli lemma\u000aif sn is a sum of n real independent random variables\u000a\u000athen sn converges almost surely if and only if sn converges in probability\u000athe dominated convergence theorem gives sufficient conditions for almost sure convergence to imply l1convergence\u000a\u000aa necessary and sufficient condition for l1 convergence is  and the sequence xn is uniformly integrable\u000a\u000a\u000a see alsoedit \u000aproofs of convergence of random variables\u000aconvergence of measures\u000acontinuous stochastic process the question of continuity of a stochastic process is essentially a question of convergence and many of the same concepts and relationships used above apply to the continuity question\u000aasymptotic distribution\u000abig o in probability notation\u000askorokhods representation theorem\u000athe tweedie convergence theorem\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from the citizendium article stochastic convergence which is licensed under the creative commons attributionsharealike 30 unported license but not under the gfdl
p42
sg14
g17
sg18
Vin probability theory there exist several different notions of convergence of random variables the convergence of sequences of random variables to some limit random variable is an important concept in probability theory and its applications to statistics and stochastic processes the same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied the different possible notions of convergence relate to how such a behaviour can be characterised two readily understood behaviours are that the sequence eventually takes a constant value and that values in the sequence continue to change but can be described by an unchanging probability distribution\u000a\u000a\u000a backgroundedit \u000astochastic convergence formalizes the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle into a pattern the pattern may for instance be\u000aconvergence in the classical sense to a fixed value perhaps itself coming from a random event\u000aan increasing similarity of outcomes to what a purely deterministic function would produce\u000aan increasing preference towards a certain outcome\u000aan increasing aversion against straying far away from a certain outcome\u000asome less obvious more theoretical patterns could be\u000athat the probability distribution describing the next outcome may grow increasingly similar to a certain distribution\u000athat the series formed by calculating the expected value of the outcomes distance from a particular value may converge to 0\u000athat the variance of the random variable describing the next event grows smaller and smaller\u000athese other types of patterns that may arise are reflected in the different types of stochastic convergence that have been studied\u000awhile the above discussion has related to the convergence of a single series to a limiting value the notion of the convergence of two series towards each other is also important but this is easily handled by studying the sequence defined as either the difference or the ratio of the two series\u000afor example if the average of n independent random variables yi i  1  n all having the same finite mean and variance is given by\u000a\u000athen as n tends to infinity xn converges in probability see below to the common mean  of the random variables yi this result is known as the weak law of large numbers other forms of convergence are important in other useful theorems including the central limit theorem\u000athroughout the following we assume that xn is a sequence of random variables and x is a random variable and all of them are defined on the same probability space \u000a\u000a\u000a convergence in distributionedit \u000awith this mode of convergence we increasingly expect to see the next outcome in a sequence of random experiments becoming better and better modeled by a given probability distribution\u000aconvergence in distribution is the weakest form of convergence since it is implied by all other types of convergence mentioned in this article however convergence in distribution is very frequently used in practice most often it arises from application of the central limit theorem\u000a\u000a\u000a definitionedit \u000aa sequence x1 x2  of random variables is said to converge in distribution or converge weakly or converge in law to a random variable x if\u000a\u000afor every number x  r at which f is continuous here fn and f are the cumulative distribution functions of random variables xn and x respectively\u000athe requirement that only the continuity points of f should be considered is essential for example if xn are distributed uniformly on intervals 0 1n then this sequence converges in distribution to a degenerate random variable x  0 indeed fnx  0 for all n when x  0 and fnx  1 for all x  1n when n  0 however for this limiting random variable f0  1 even though fn0  0 for all n thus the convergence of cdfs fails at the point x  0 where f is discontinuous\u000aconvergence in distribution may be denoted as\u000a\u000awhere  is the law probability distribution of x for example if x is standard normal we can write \u000afor random vectors x1 x2   rk the convergence in distribution is defined similarly we say that this sequence converges in distribution to a random kvector x if\u000a\u000afor every a  rk which is a continuity set of x\u000athe definition of convergence in distribution may be extended from random vectors to more general random elements in arbitrary metric spaces and even to the random variables which are not measurable  a situation which occurs for example in the study of empirical processes this is the weak convergence of laws without laws being defined  except asymptotically\u000ain this case the term weak convergence is preferable see weak convergence of measures and we say that a sequence of random elements xn converges weakly to x denoted as xn  x if\u000a\u000afor all continuous bounded functions h here e denotes the outer expectation that is the expectation of a smallest measurable function g that dominates hxn\u000a\u000a\u000a propertiesedit \u000asince fa  prx  a the convergence in distribution means that the probability for xn to be in a given range is approximately equal to the probability that the value of x is in that range provided n is sufficiently large\u000ain general convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge as an example one may consider random variables with densities fnx  1  cos2nx101 these random variables converge in distribution to a uniform u01 whereas their densities do not converge at allhowever scheffs lemma implies that convergence of the probability density functions implies convergence in distribution\u000a\u000athe portmanteau lemma provides several equivalent definitions of convergence in distribution although these definitions are less intuitive they are used to prove a number of statistical theorems the lemma states that xn converges in distribution to x if and only if any of the following statements are trueexn  ex for all bounded continuous functions  where e denotes the expected value\u000aexn  ex for all bounded lipschitz functions \u000alimsup exn   ex for every upper semicontinuous function  bounded from above\u000aliminf exn   ex for every lower semicontinuous function  bounded from below\u000alimsupprxn  c  prx  c for all closed sets c\u000aliminfprxn  u  prx  u for all open sets u\u000alimprxn  a  prx  a for all continuity sets a of random variable x\u000a\u000athe continuous mapping theorem states that for a continuous function g if the sequence xn converges in distribution to x then gxn converges in distribution to gx\u000anote however that convergence in distribution of xn to x and yn to y does in general not imply convergence in distribution of xn  yn to x  y or of xnyn to xy\u000a\u000alvys continuity theorem the sequence xn converges in distribution to x if and only if the sequence of corresponding characteristic functions n converges pointwise to the characteristic function  of x\u000aconvergence in distribution is metrizable by the lvyprokhorov metric\u000aa natural link to convergence in distribution is the skorokhods representation theorem\u000a\u000a\u000a convergence in probabilityedit \u000athe basic idea behind this type of convergence is that the probability of an unusual outcome becomes smaller and smaller as the sequence progresses\u000athe concept of convergence in probability is used very often in statistics for example an estimator is called consistent if it converges in probability to the quantity being estimated convergence in probability is also the type of convergence established by the weak law of large numbers\u000a\u000a\u000a definitionedit \u000aa sequence xn of random variables converges in probability towards the random variable x if for all   0\u000a\u000aformally pick any   0 and any   0 let pn be the probability that xn is outside the ball of radius  centered at x then for xn to converge in probability to x there should exist a number n which will depend on  and  such that for all n  n pn  \u000aconvergence in probability is denoted by adding the letter p over an arrow indicating convergence or using the plim probability limit operator\u000a\u000afor random elements xn on a separable metric space s d convergence in probability is defined similarly by\u000a\u000a\u000a propertiesedit \u000aconvergence in probability implies convergence in distributionproof\u000ain the opposite direction convergence in distribution implies convergence in probability when the limiting random variable x is a constantproof\u000aconvergence in probability does not imply almost sure convergenceproof\u000athe continuous mapping theorem states that for every continuous function g if  then also \u000aconvergence in probability defines a topology on the space of random variables over a fixed probability space this topology is metrizable by the ky fan metric\u000a\u000aor\u000a\u000a\u000a\u000a almost sure convergenceedit \u000athis is the type of stochastic convergence that is most similar to pointwise convergence known from elementary real analysis\u000a\u000a\u000a definitionedit \u000ato say that the sequence xn converges almost surely or almost everywhere or with probability 1 or strongly towards x means that\u000a\u000athis means that the values of xn approach the value of x in the sense see almost surely that events for which xn does not converge to x have probability 0 using the probability space  and the concept of the random variable as a function from  to r this is equivalent to the statement\u000a\u000ausing the notion of the limit inferior of a sequence of sets almost sure convergence can also be defined as follows\u000a\u000aalmost sure convergence is often denoted by adding the letters as over an arrow indicating convergence\u000a\u000afor generic random elements xn on a metric space s d convergence almost surely is defined similarly\u000a\u000a\u000a propertiesedit \u000aalmost sure convergence implies convergence in probability by fatous lemma and hence implies convergence in distribution it is the notion of convergence used in the strong law of large numbers\u000athe concept of almost sure convergence does not come from a topology on the space of random variables this means there is no topology on the space of random variables such that the almost surely convergent sequences are exactly the converging sequences with respect to that topology in particular there is no metric of almost sure convergence\u000a\u000a\u000a sure convergenceedit \u000ato say that the sequence of random variables xn defined over the same probability space ie a random process converges surely or everywhere or pointwise towards x means\u000a\u000awhere  is the sample space of the underlying probability space over which the random variables are defined\u000athis is the notion of pointwise convergence of sequence functions extended to sequence of random variables note that random variables themselves are functions\u000a\u000asure convergence of a random variable implies all the other kinds of convergence stated above but there is no payoff in probability theory by using sure convergence compared to using almost sure convergence the difference between the two only exists on sets with probability zero this is why the concept of sure convergence of random variables is very rarely used\u000a\u000a\u000a convergence in meanedit \u000agiven a real number r  1 we say that the sequence xn converges in the rth mean or in the lrnorm towards the random variable x if the rth absolute moments exnr and exr of xn and x exist and\u000a\u000awhere the operator e denotes the expected value convergence in rth mean tells us that the expectation of the rth power of the difference between xn and x converges to zero\u000athis type of convergence is often denoted by adding the letter lr over an arrow indicating convergence\u000a\u000athe most important cases of convergence in rth mean are\u000awhen xn converges in rth mean to x for r  1 we say that xn converges in mean to x\u000awhen xn converges in rth mean to x for r  2 we say that xn converges in mean square to x\u000aconvergence in the rth mean for r  1 implies convergence in probability by markovs inequality furthermore if r  s  1 convergence in rth mean implies convergence in sth mean hence convergence in mean square implies convergence in mean\u000ait is also worth noticing that if  then\u000a\u000a\u000a propertiesedit \u000aprovided the probability space is complete\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and \u000anone of the above statements are true for convergence in distribution\u000athe chain of implications between the various notions of convergence are noted in their respective sections they are using the arrow notation\u000a\u000athese properties together with a number of other special cases are summarized in the following list\u000a almost sure convergence implies convergence in probabilityproof\u000a\u000a convergence in probability implies there exists a subsequence  which almost surely converges\u000a\u000a convergence in probability implies convergence in distributionproof\u000a\u000a convergence in rth order mean implies convergence in probability\u000a\u000a convergence in rth order mean implies convergence in lower order mean assuming that both orders are greater than or equal to one\u000a provided r  s  1\u000a if xn converges in distribution to a constant c then xn converges in probability to cproof\u000a provided c is a constant\u000a if xn converges in distribution to x and the difference between xn and yn converges in probability to zero then yn also converges in distribution to xproof\u000a\u000a if xn converges in distribution to x and yn converges in distribution to a constant c then the joint vector xn yn converges in distribution to x cproof\u000a provided c is a constant\u000anote that the condition that yn converges to a constant is important if it were to converge to a random variable y then we wouldnt be able to conclude that xn yn converges to x y\u000a if xn converges in probability to x and yn converges in probability to y then the joint vector xn yn converges in probability to x yproof\u000a\u000aif xn converges in probability to x and if pxn  b  1 for all n and some b then xn converges in rth mean to x for all r  1 in other words if xn converges in probability to x and all random variables xn are almost surely bounded above and below then xn converges to x also in any rth mean\u000aalmost sure representation usually convergence in distribution does not imply convergence almost surely however for a given sequence xn which converges in distribution to x0 it is always possible to find a new probability space  f p and random variables yn n  0 1  defined on it such that yn is equal in distribution to xn for each n  0 and yn converges to y0 almost surely\u000aif for all   0\u000a\u000athen we say that xn converges almost completely or almost in probability towards x when xn converges almost completely towards x then it also converges almost surely to x in other words if xn converges in probability to x sufficiently quickly ie the above sequence of tail probabilities is summable for all   0 then xn also converges almost surely to x this is a direct implication from the borelcantelli lemma\u000aif sn is a sum of n real independent random variables\u000a\u000athen sn converges almost surely if and only if sn converges in probability\u000athe dominated convergence theorem gives sufficient conditions for almost sure convergence to imply l1convergence\u000a\u000aa necessary and sufficient condition for l1 convergence is  and the sequence xn is uniformly integrable\u000a\u000a\u000a see alsoedit \u000aproofs of convergence of random variables\u000aconvergence of measures\u000acontinuous stochastic process the question of continuity of a stochastic process is essentially a question of convergence and many of the same concepts and relationships used above apply to the continuity question\u000aasymptotic distribution\u000abig o in probability notation\u000askorokhods representation theorem\u000athe tweedie convergence theorem\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from the citizendium article stochastic convergence which is licensed under the creative commons attributionsharealike 30 unported license but not under the gfdl
p43
sg20
g23
sg24
g27
sg30
Vin probability theory there exist several different notions of convergence of random variables the convergence of sequences of random variables to some limit random variable is an important concept in probability theory and its applications to statistics and stochastic processes the same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behaviour that is essentially unchanging when items far enough into the sequence are studied the different possible notions of convergence relate to how such a behaviour can be characterised two readily understood behaviours are that the sequence eventually takes a constant value and that values in the sequence continue to change but can be described by an unchanging probability distribution\u000a\u000a\u000a backgroundedit \u000astochastic convergence formalizes the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle into a pattern the pattern may for instance be\u000aconvergence in the classical sense to a fixed value perhaps itself coming from a random event\u000aan increasing similarity of outcomes to what a purely deterministic function would produce\u000aan increasing preference towards a certain outcome\u000aan increasing aversion against straying far away from a certain outcome\u000asome less obvious more theoretical patterns could be\u000athat the probability distribution describing the next outcome may grow increasingly similar to a certain distribution\u000athat the series formed by calculating the expected value of the outcomes distance from a particular value may converge to 0\u000athat the variance of the random variable describing the next event grows smaller and smaller\u000athese other types of patterns that may arise are reflected in the different types of stochastic convergence that have been studied\u000awhile the above discussion has related to the convergence of a single series to a limiting value the notion of the convergence of two series towards each other is also important but this is easily handled by studying the sequence defined as either the difference or the ratio of the two series\u000afor example if the average of n independent random variables yi i  1  n all having the same finite mean and variance is given by\u000a\u000athen as n tends to infinity xn converges in probability see below to the common mean  of the random variables yi this result is known as the weak law of large numbers other forms of convergence are important in other useful theorems including the central limit theorem\u000athroughout the following we assume that xn is a sequence of random variables and x is a random variable and all of them are defined on the same probability space \u000a\u000a\u000a convergence in distributionedit \u000awith this mode of convergence we increasingly expect to see the next outcome in a sequence of random experiments becoming better and better modeled by a given probability distribution\u000aconvergence in distribution is the weakest form of convergence since it is implied by all other types of convergence mentioned in this article however convergence in distribution is very frequently used in practice most often it arises from application of the central limit theorem\u000a\u000a\u000a definitionedit \u000aa sequence x1 x2  of random variables is said to converge in distribution or converge weakly or converge in law to a random variable x if\u000a\u000afor every number x  r at which f is continuous here fn and f are the cumulative distribution functions of random variables xn and x respectively\u000athe requirement that only the continuity points of f should be considered is essential for example if xn are distributed uniformly on intervals 0 1n then this sequence converges in distribution to a degenerate random variable x  0 indeed fnx  0 for all n when x  0 and fnx  1 for all x  1n when n  0 however for this limiting random variable f0  1 even though fn0  0 for all n thus the convergence of cdfs fails at the point x  0 where f is discontinuous\u000aconvergence in distribution may be denoted as\u000a\u000awhere  is the law probability distribution of x for example if x is standard normal we can write \u000afor random vectors x1 x2   rk the convergence in distribution is defined similarly we say that this sequence converges in distribution to a random kvector x if\u000a\u000afor every a  rk which is a continuity set of x\u000athe definition of convergence in distribution may be extended from random vectors to more general random elements in arbitrary metric spaces and even to the random variables which are not measurable  a situation which occurs for example in the study of empirical processes this is the weak convergence of laws without laws being defined  except asymptotically\u000ain this case the term weak convergence is preferable see weak convergence of measures and we say that a sequence of random elements xn converges weakly to x denoted as xn  x if\u000a\u000afor all continuous bounded functions h here e denotes the outer expectation that is the expectation of a smallest measurable function g that dominates hxn\u000a\u000a\u000a propertiesedit \u000asince fa  prx  a the convergence in distribution means that the probability for xn to be in a given range is approximately equal to the probability that the value of x is in that range provided n is sufficiently large\u000ain general convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge as an example one may consider random variables with densities fnx  1  cos2nx101 these random variables converge in distribution to a uniform u01 whereas their densities do not converge at allhowever scheffs lemma implies that convergence of the probability density functions implies convergence in distribution\u000a\u000athe portmanteau lemma provides several equivalent definitions of convergence in distribution although these definitions are less intuitive they are used to prove a number of statistical theorems the lemma states that xn converges in distribution to x if and only if any of the following statements are trueexn  ex for all bounded continuous functions  where e denotes the expected value\u000aexn  ex for all bounded lipschitz functions \u000alimsup exn   ex for every upper semicontinuous function  bounded from above\u000aliminf exn   ex for every lower semicontinuous function  bounded from below\u000alimsupprxn  c  prx  c for all closed sets c\u000aliminfprxn  u  prx  u for all open sets u\u000alimprxn  a  prx  a for all continuity sets a of random variable x\u000a\u000athe continuous mapping theorem states that for a continuous function g if the sequence xn converges in distribution to x then gxn converges in distribution to gx\u000anote however that convergence in distribution of xn to x and yn to y does in general not imply convergence in distribution of xn  yn to x  y or of xnyn to xy\u000a\u000alvys continuity theorem the sequence xn converges in distribution to x if and only if the sequence of corresponding characteristic functions n converges pointwise to the characteristic function  of x\u000aconvergence in distribution is metrizable by the lvyprokhorov metric\u000aa natural link to convergence in distribution is the skorokhods representation theorem\u000a\u000a\u000a convergence in probabilityedit \u000athe basic idea behind this type of convergence is that the probability of an unusual outcome becomes smaller and smaller as the sequence progresses\u000athe concept of convergence in probability is used very often in statistics for example an estimator is called consistent if it converges in probability to the quantity being estimated convergence in probability is also the type of convergence established by the weak law of large numbers\u000a\u000a\u000a definitionedit \u000aa sequence xn of random variables converges in probability towards the random variable x if for all   0\u000a\u000aformally pick any   0 and any   0 let pn be the probability that xn is outside the ball of radius  centered at x then for xn to converge in probability to x there should exist a number n which will depend on  and  such that for all n  n pn  \u000aconvergence in probability is denoted by adding the letter p over an arrow indicating convergence or using the plim probability limit operator\u000a\u000afor random elements xn on a separable metric space s d convergence in probability is defined similarly by\u000a\u000a\u000a propertiesedit \u000aconvergence in probability implies convergence in distributionproof\u000ain the opposite direction convergence in distribution implies convergence in probability when the limiting random variable x is a constantproof\u000aconvergence in probability does not imply almost sure convergenceproof\u000athe continuous mapping theorem states that for every continuous function g if  then also \u000aconvergence in probability defines a topology on the space of random variables over a fixed probability space this topology is metrizable by the ky fan metric\u000a\u000aor\u000a\u000a\u000a\u000a almost sure convergenceedit \u000athis is the type of stochastic convergence that is most similar to pointwise convergence known from elementary real analysis\u000a\u000a\u000a definitionedit \u000ato say that the sequence xn converges almost surely or almost everywhere or with probability 1 or strongly towards x means that\u000a\u000athis means that the values of xn approach the value of x in the sense see almost surely that events for which xn does not converge to x have probability 0 using the probability space  and the concept of the random variable as a function from  to r this is equivalent to the statement\u000a\u000ausing the notion of the limit inferior of a sequence of sets almost sure convergence can also be defined as follows\u000a\u000aalmost sure convergence is often denoted by adding the letters as over an arrow indicating convergence\u000a\u000afor generic random elements xn on a metric space s d convergence almost surely is defined similarly\u000a\u000a\u000a propertiesedit \u000aalmost sure convergence implies convergence in probability by fatous lemma and hence implies convergence in distribution it is the notion of convergence used in the strong law of large numbers\u000athe concept of almost sure convergence does not come from a topology on the space of random variables this means there is no topology on the space of random variables such that the almost surely convergent sequences are exactly the converging sequences with respect to that topology in particular there is no metric of almost sure convergence\u000a\u000a\u000a sure convergenceedit \u000ato say that the sequence of random variables xn defined over the same probability space ie a random process converges surely or everywhere or pointwise towards x means\u000a\u000awhere  is the sample space of the underlying probability space over which the random variables are defined\u000athis is the notion of pointwise convergence of sequence functions extended to sequence of random variables note that random variables themselves are functions\u000a\u000asure convergence of a random variable implies all the other kinds of convergence stated above but there is no payoff in probability theory by using sure convergence compared to using almost sure convergence the difference between the two only exists on sets with probability zero this is why the concept of sure convergence of random variables is very rarely used\u000a\u000a\u000a convergence in meanedit \u000agiven a real number r  1 we say that the sequence xn converges in the rth mean or in the lrnorm towards the random variable x if the rth absolute moments exnr and exr of xn and x exist and\u000a\u000awhere the operator e denotes the expected value convergence in rth mean tells us that the expectation of the rth power of the difference between xn and x converges to zero\u000athis type of convergence is often denoted by adding the letter lr over an arrow indicating convergence\u000a\u000athe most important cases of convergence in rth mean are\u000awhen xn converges in rth mean to x for r  1 we say that xn converges in mean to x\u000awhen xn converges in rth mean to x for r  2 we say that xn converges in mean square to x\u000aconvergence in the rth mean for r  1 implies convergence in probability by markovs inequality furthermore if r  s  1 convergence in rth mean implies convergence in sth mean hence convergence in mean square implies convergence in mean\u000ait is also worth noticing that if  then\u000a\u000a\u000a propertiesedit \u000aprovided the probability space is complete\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  almost surely\u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and  and \u000aif  and  then  for any real numbers  and \u000anone of the above statements are true for convergence in distribution\u000athe chain of implications between the various notions of convergence are noted in their respective sections they are using the arrow notation\u000a\u000athese properties together with a number of other special cases are summarized in the following list\u000a almost sure convergence implies convergence in probabilityproof\u000a\u000a convergence in probability implies there exists a subsequence  which almost surely converges\u000a\u000a convergence in probability implies convergence in distributionproof\u000a\u000a convergence in rth order mean implies convergence in probability\u000a\u000a convergence in rth order mean implies convergence in lower order mean assuming that both orders are greater than or equal to one\u000a provided r  s  1\u000a if xn converges in distribution to a constant c then xn converges in probability to cproof\u000a provided c is a constant\u000a if xn converges in distribution to x and the difference between xn and yn converges in probability to zero then yn also converges in distribution to xproof\u000a\u000a if xn converges in distribution to x and yn converges in distribution to a constant c then the joint vector xn yn converges in distribution to x cproof\u000a provided c is a constant\u000anote that the condition that yn converges to a constant is important if it were to converge to a random variable y then we wouldnt be able to conclude that xn yn converges to x y\u000a if xn converges in probability to x and yn converges in probability to y then the joint vector xn yn converges in probability to x yproof\u000a\u000aif xn converges in probability to x and if pxn  b  1 for all n and some b then xn converges in rth mean to x for all r  1 in other words if xn converges in probability to x and all random variables xn are almost surely bounded above and below then xn converges to x also in any rth mean\u000aalmost sure representation usually convergence in distribution does not imply convergence almost surely however for a given sequence xn which converges in distribution to x0 it is always possible to find a new probability space  f p and random variables yn n  0 1  defined on it such that yn is equal in distribution to xn for each n  0 and yn converges to y0 almost surely\u000aif for all   0\u000a\u000athen we say that xn converges almost completely or almost in probability towards x when xn converges almost completely towards x then it also converges almost surely to x in other words if xn converges in probability to x sufficiently quickly ie the above sequence of tail probabilities is summable for all   0 then xn also converges almost surely to x this is a direct implication from the borelcantelli lemma\u000aif sn is a sum of n real independent random variables\u000a\u000athen sn converges almost surely if and only if sn converges in probability\u000athe dominated convergence theorem gives sufficient conditions for almost sure convergence to imply l1convergence\u000a\u000aa necessary and sufficient condition for l1 convergence is  and the sequence xn is uniformly integrable\u000a\u000a\u000a see alsoedit \u000aproofs of convergence of random variables\u000aconvergence of measures\u000acontinuous stochastic process the question of continuity of a stochastic process is essentially a question of convergence and many of the same concepts and relationships used above apply to the continuity question\u000aasymptotic distribution\u000abig o in probability notation\u000askorokhods representation theorem\u000athe tweedie convergence theorem\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from the citizendium article stochastic convergence which is licensed under the creative commons attributionsharealike 30 unported license but not under the gfdl
p44
sg32
g35
sg37
NsbsS'consistency_(statistics).txt'
p45
g2
(g3
g4
Ntp46
Rp47
(dp48
g8
g11
sg12
Vin statistics consistency of procedures such as computing confidence intervals or conducting hypothesis tests is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely in particular consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth use of the term in statistics derives from sir ronald fisher in 1922\u000ause of the terms consistency and consistent in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items in complicated applications of statistics there may be several ways in which the number of data items may grow for example records for rainfall within an area might increase in three ways records for additional time periods records for additional sites with a fixed area records for extra sites obtained by extending the size of the area in such cases the property of consistency may be limited to one or more of the possible ways a sample size can grow\u000a\u000a\u000a estimatorsedit \u000a\u000aa consistent estimator is one for which when the estimate is considered as a random variable indexed by the number n of items in the data set as n increases the estimates converge to the value that the estimator is designed to estimate\u000a\u000aan estimator that has fisher consistency is one for which if the estimator were applied to the entire population rather than a sample the true value of the estimated parameter would be obtained\u000a\u000a\u000a testsedit \u000a\u000aa consistent test is one for which the power of the test for a fixed untrue hypothesis increases to one as the number of data items increases\u000a\u000a\u000a classificationedit \u000ain statistical classification a consistent classifier is one for which the probability of correct classification given a training set approaches as the size of the training set increases the best probability theoretically possible if the population distributions were fully known\u000a\u000a\u000a sparsistencyedit \u000alet  be a vector and define the support  where  is the th element of  let  be an estimator for  then sparsistency is the property that the support of the estimator converges to the true support as the number of samples grows to infinity more formally  as \u000a\u000a\u000a see alsoedit \u000areliability statistics\u000a\u000a\u000a referencesedit 
p49
sg14
g17
sg18
Vin statistics consistency of procedures such as computing confidence intervals or conducting hypothesis tests is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely in particular consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth use of the term in statistics derives from sir ronald fisher in 1922\u000ause of the terms consistency and consistent in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items in complicated applications of statistics there may be several ways in which the number of data items may grow for example records for rainfall within an area might increase in three ways records for additional time periods records for additional sites with a fixed area records for extra sites obtained by extending the size of the area in such cases the property of consistency may be limited to one or more of the possible ways a sample size can grow\u000a\u000a\u000a estimatorsedit \u000a\u000aa consistent estimator is one for which when the estimate is considered as a random variable indexed by the number n of items in the data set as n increases the estimates converge to the value that the estimator is designed to estimate\u000a\u000aan estimator that has fisher consistency is one for which if the estimator were applied to the entire population rather than a sample the true value of the estimated parameter would be obtained\u000a\u000a\u000a testsedit \u000a\u000aa consistent test is one for which the power of the test for a fixed untrue hypothesis increases to one as the number of data items increases\u000a\u000a\u000a classificationedit \u000ain statistical classification a consistent classifier is one for which the probability of correct classification given a training set approaches as the size of the training set increases the best probability theoretically possible if the population distributions were fully known\u000a\u000a\u000a sparsistencyedit \u000alet  be a vector and define the support  where  is the th element of  let  be an estimator for  then sparsistency is the property that the support of the estimator converges to the true support as the number of samples grows to infinity more formally  as \u000a\u000a\u000a see alsoedit \u000areliability statistics\u000a\u000a\u000a referencesedit
p50
sg20
g23
sg24
g27
sg30
Vin statistics consistency of procedures such as computing confidence intervals or conducting hypothesis tests is a desired property of their behaviour as the number of items in the data set to which they are applied increases indefinitely in particular consistency requires that the outcome of the procedure with unlimited data should identify the underlying truth use of the term in statistics derives from sir ronald fisher in 1922\u000ause of the terms consistency and consistent in statistics is restricted to cases where essentially the same procedure can be applied to any number of data items in complicated applications of statistics there may be several ways in which the number of data items may grow for example records for rainfall within an area might increase in three ways records for additional time periods records for additional sites with a fixed area records for extra sites obtained by extending the size of the area in such cases the property of consistency may be limited to one or more of the possible ways a sample size can grow\u000a\u000a\u000a estimatorsedit \u000a\u000aa consistent estimator is one for which when the estimate is considered as a random variable indexed by the number n of items in the data set as n increases the estimates converge to the value that the estimator is designed to estimate\u000a\u000aan estimator that has fisher consistency is one for which if the estimator were applied to the entire population rather than a sample the true value of the estimated parameter would be obtained\u000a\u000a\u000a testsedit \u000a\u000aa consistent test is one for which the power of the test for a fixed untrue hypothesis increases to one as the number of data items increases\u000a\u000a\u000a classificationedit \u000ain statistical classification a consistent classifier is one for which the probability of correct classification given a training set approaches as the size of the training set increases the best probability theoretically possible if the population distributions were fully known\u000a\u000a\u000a sparsistencyedit \u000alet  be a vector and define the support  where  is the th element of  let  be an estimator for  then sparsistency is the property that the support of the estimator converges to the true support as the number of samples grows to infinity more formally  as \u000a\u000a\u000a see alsoedit \u000areliability statistics\u000a\u000a\u000a referencesedit 
p51
sg32
g35
sg37
NsbsS'kullback\xc3\xa2\xc2\x80\xc2\x93leibler_divergence.txt'
p52
g2
(g3
g4
Ntp53
Rp54
(dp55
g8
g11
sg12
Vin probability theory and information theory the kullbackleibler divergence also information divergence information gain relative entropy klic or kl divergence is a nonsymmetric measure of the difference between two probability distributions p and q specifically the kullbackleibler divergence of q from p denoted dklpq is a measure of the information lost when q is used to approximate p the kullbackleibler divergence measures the expected number of extra bits so intuitively it is non negative this can be verified by jensens inequality required to code samples from p when using a code optimized for q rather than using the true code optimized for p typically p represents the true distribution of data observations or a precisely calculated theoretical distribution the measure q typically represents a theory model description or approximation of p\u000aalthough it is often intuited as a metric or distance the kullbackleibler divergence is not a true metric  for example it is not symmetric the kullbackleibler divergence from p to q is generally not the same as that from q to p however its infinitesimal form specifically its hessian is a metric tensor it is the fisher information metric\u000akullbackleibler divergence is a special case of a broader class of divergences called fdivergences it was originally introduced by solomon kullback and richard leibler in 1951 as the directed divergence between two distributions it can be derived from a bregman divergence\u000a\u000a\u000a definition \u000afor discrete probability distributions p and q the kullbackleibler divergence of q from p is defined to be\u000a\u000ain words it is the expectation of the logarithmic difference between the probabilities p and q where the expectation is taken using the probabilities p the kullbackleibler divergence is defined only if qi0 implies pi0 for all i absolute continuity whenever pi is zero the contribution of the ith term is interpreted as zero because \u000afor distributions p and q of a continuous random variable the kullbackleibler divergence is defined to be the integral\u000a\u000awhere p and q denote the densities of p and q\u000amore generally if p and q are probability measures over a set x and p is absolutely continuous with respect to q then the kullbackleibler divergence from p to q is defined as\u000a\u000awhere  is the radonnikodym derivative of p with respect to q and provided the expression on the righthand side exists equivalently this can be written as\u000a\u000awhich we recognize as the entropy of p relative to q continuing in this case if  is any measure on x for which  and  exist meaning that p and q are absolutely continuous with respect to  then the kullbackleibler divergence from p to q is given as\u000a\u000athe logarithms in these formulae are taken to base 2 if information is measured in units of bits or to base e if information is measured in nats most formulas involving the kullbackleibler divergence hold regardless of the base of the logarithm\u000avarious conventions exist for referring to dklpq in words often it is referred to as the divergence between p and q however this fails to convey the fundamental asymmetry in the relation sometimes it may be found described as the divergence of p from or with respect to q often in the context of relative entropy or information gain however in the present article the divergence of q from p will be the language used as this best relates to the idea that it is p that is considered the underlying true or best guess distribution that expectations will be calculated with reference to while q is some divergent less good approximate distribution\u000a\u000a\u000a characterization \u000aarthur hobson proved that kullbackleibler divergence is the only measure of difference between probability distributions that satisfies some desiderata which are the canonical extension to those for the characterization of entropy consequently mutual information is the only measure of mutual dependence that satisfies an induced criteria since it is defined in terms of kullbackleibler divergence\u000a\u000a\u000a motivation \u000a\u000ain information theory the kraftmcmillan theorem establishes that any directly decodable coding scheme for coding a message to identify one value  out of a set of possibilities  can be seen as representing an implicit probability distribution  over  where  is the length of the code for  in bits therefore kullbackleibler divergence can be interpreted as the expected extra messagelength per datum that must be communicated if a code that is optimal for a given wrong distribution  is used compared to using a code based on the true distribution \u000a\u000awhere hpq is the cross entropy of p and q and hp is the entropy of p\u000anote also that there is a relation between the kullbackleibler divergence and the rate function in the theory of large deviations\u000akullback brings together all notions of information in his historic text information theory and statistics for instance he shows that the mean discriminating information between two hypotheses is the basis for all of the various measures of information from shannon to fisher shannons rate is the mean information between the hypotheses of dependence and independence of processes fishers information is second order term and dominant in the taylor approximation of the discriminating information between two models of the same parametric family\u000a\u000a\u000a properties \u000athe kullbackleibler divergence is always nonnegative\u000a\u000aa result known as gibbs inequality with dklpq zero if and only if p  q almost everywhere the entropy hp thus sets a minimum value for the crossentropy hpq the expected number of bits required when using a code based on q rather than p and the kullbackleibler divergence therefore represents the expected number of extra bits that must be transmitted to identify a value x drawn from x if a code is used corresponding to the probability distribution q rather than the true distribution p\u000athe kullbackleibler divergence remains welldefined for continuous distributions and furthermore is invariant under parameter transformations for example if a transformation is made from variable x to variable yx then since px dx  py dy and qx dx  qy dy the kullbackleibler divergence may be rewritten\u000a\u000awhere  and  although it was assumed that the transformation was continuous this need not be the case this also shows that the kullbackleibler divergence produces a dimensionally consistent quantity since if x is a dimensioned variable px and qx are also dimensioned since eg px dx is dimensionless the argument of the logarithmic term is and remains dimensionless as it must it can therefore be seen as in some ways a more fundamental quantity than some other properties in information theory such as selfinformation or shannon entropy which can become undefined or negative for nondiscrete probabilities\u000athe kullbackleibler divergence is additive for independent distributions in much the same way as shannon entropy if  are independent distributions with the joint distribution  and  likewise then\u000a\u000a\u000a kullbackleibler divergence for multivariate normal distributions \u000asuppose that we have two multivariate normal distributions with means  and with nonsingular covariance matrices  if the two distributions have the same dimension k then the kullbackleibler divergence between the distributions is as follows\u000a\u000athe logarithm in the last term must be taken to base e since all terms apart from the last are basee logarithms of expressions that are either factors of the density function or otherwise arise naturally the equation therefore gives a result measured in nats dividing the entire expression above by loge 2 yields the divergence in bits\u000a\u000a\u000a relation to metrics \u000aone might be tempted to call it a distance metric on the space of probability distributions but this would not be correct as the kullbackleibler divergence is not symmetric  that is   nor does it satisfy the triangle inequality even so being a premetric it generates a topology on the space of probability distributions more concretely if  is a sequence of distributions such that\u000a\u000athen it is said that  pinskers inequality entails that  where the latter stands for the usual convergence in total variation\u000afollowing rnyi 1970 1961 the term is sometimes also called the information gain about x achieved if p can be used instead of q it is also called the relative entropy of p with respect to q and written hpq\u000a\u000a\u000a fisher information metric \u000ahowever the kullbackleibler divergence is rather directly related to a metric specifically the fisher information metric this can be made explicit as follows assume that the probability distributions p and q are both parameterized by some possibly multidimensional parameter  consider then two close by values of  and  so that the parameter  differs by only a small amount from the parameter value  specifically up to first order one has using the einstein summation convention\u000a\u000awith  a small change of  in the j direction and  the corresponding rate of change in the probability distribution since the kullbackleibler divergence has an absolute minimum 0 for p  q ie  it changes only to second order in the small parameters  more formally as for any minimum the first derivatives of the divergence vanish\u000a\u000aand by the taylor expansion one has up to second order\u000a\u000awhere the hessian matrix of the divergence\u000a\u000amust be positive semidefinite letting  vary and dropping the subindex 0 the hessian  defines a possibly degenerate riemannian metric on the  parameter space called the fisher information metric\u000a\u000a\u000a relation to other quantities of information theory \u000amany of the other quantities of information theory can be interpreted as applications of the kullbackleibler divergence to specific cases\u000athe selfinformation\u000a\u000ais the kullbackleibler divergence of the probability distribution pi from a kronecker delta representing certainty that i  m  ie the number of extra bits that must be transmitted to identify i if only the probability distribution pi is available to the receiver not the fact that i  m\u000athe mutual information\u000a\u000ais the kullbackleibler divergence of the product pxpy of the two marginal probability distributions from the joint probability distribution pxy  ie the expected number of extra bits that must be transmitted to identify x and y if they are coded using only their marginal distributions instead of the joint distribution equivalently if the joint probability pxy is known it is the expected number of extra bits that must on average be sent to identify y if the value of x is not already known to the receiver\u000athe shannon entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the uniform distribution pux from the true distribution px  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the true distribution px\u000athe conditional entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the product distribution pux py from the true joint distribution pxy  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the conditional distribution px  y of x given y\u000athe cross entropy between two probability distributions measures the average number of bits needed to identify an event from a set of possibilities if a coding scheme is used based on a given probability distribution  rather than the true distribution  the cross entropy for two distributions  and  over the same probability space is thus defined as follows\u000a\u000a\u000a kullbackleibler divergence and bayesian updating \u000ain bayesian statistics the kullbackleibler divergence can be used as a measure of the information gain in moving from a prior distribution to a posterior distribution if some new fact y  y is discovered it can be used to update the probability distribution for x from px  i to a new posterior probability distribution px  yi using bayes theorem\u000a\u000athis distribution has a new entropy\u000a\u000awhich may be less than or greater than the original entropy hp  i however from the standpoint of the new probability distribution one can estimate that to have used the original code based on px  i instead of a new code based on px  yi would have added an expected number of bits\u000a\u000ato the message length this therefore represents the amount of useful information or information gain about x that we can estimate has been learned by discovering y  y\u000aif a further piece of data y2  y2 subsequently comes in the probability distribution for x can be updated further to give a new best guess pxy1y2i if one reinvestigates the information gain for using pxy1i rather than pxi it turns out that it may be either greater or less than previously estimated\u000a may be  or  than \u000aand so the combined information gain does not obey the triangle inequality\u000a may be   or  than \u000aall one can say is that on average averaging using py2  y1xi the two sides will average out\u000a\u000a\u000a bayesian experimental design \u000aa common goal in bayesian experimental design is to maximise the expected kullbackleibler divergence between the prior and the posterior when posteriors are approximated to be gaussian distributions a design maximising the expected kullbackleibler divergence is called bayes doptimal\u000a\u000a\u000a discrimination information \u000athe kullbackleibler divergence dkl pxh1  pxh0  can also be interpreted as the expected discrimination information for h1 over h0 the mean information per sample for discriminating in favor of a hypothesis h1 against a hypothesis h0 when hypothesis h1 is true another name for this quantity given to it by ij good is the expected weight of evidence for h1 over h0 to be expected from each sample\u000athe expected weight of evidence for h1 over h0 is not the same as the information gain expected per sample about the probability distribution ph of the hypotheses\u000a\u000aeither of the two quantities can be used as a utility function in bayesian experimental design to choose an optimal next question to investigate but they will in general lead to rather different experimental strategies\u000aon the entropy scale of information gain there is very little difference between near certainty and absolute certaintycoding according to a near certainty requires hardly any more bits than coding according to an absolute certainty on the other hand on the logit scale implied by weight of evidence the difference between the two is enormous  infinite perhaps this might reflect the difference between being almost sure on a probabilistic level that say the riemann hypothesis is correct compared to being certain that it is correct because one has a mathematical proof these two different scales of loss function for uncertainty are both useful according to how well each reflects the particular circumstances of the problem in question\u000a\u000a\u000a principle of minimum discrimination information \u000athe idea of kullbackleibler divergence as discrimination information led kullback to propose the principle of minimum discrimination information mdi given new facts a new distribution f should be chosen which is as hard to discriminate from the original distribution f0 as possible so that the new data produces as small an information gain dkl f  f0  as possible\u000afor example if one had a prior distribution pxa over x and a and subsequently learnt the true distribution of a was ua the kullbackleibler divergence between the new joint distribution for x and a qxa ua and the earlier prior distribution would be\u000a\u000aie the sum of the kullbackleibler divergence of pa the prior distribution for a from the updated distribution ua plus the expected value using the probability distribution ua of the kullbackleibler divergence of the prior conditional distribution pxa from the new conditional distribution qxa note that often the later expected value is called the conditional kullbackleibler divergence or conditional relative entropy and denoted by dklqxapxa this is minimized if qxa  pxa over the whole support of ua and we note that this result incorporates bayes theorem if the new distribution ua is in fact a  function representing certainty that a has one particular value\u000amdi can be seen as an extension of laplaces principle of insufficient reason and the principle of maximum entropy of et jaynes in particular it is the natural extension of the principle of maximum entropy from discrete to continuous distributions for which shannon entropy ceases to be so useful see differential entropy but the kullbackleibler divergence continues to be just as relevant\u000ain the engineering literature mdi is sometimes called the principle of minimum crossentropy mce or minxent for short minimising the kullbackleibler divergence of m from p with respect to m is equivalent to minimizing the crossentropy of p and m since\u000a\u000awhich is appropriate if one is trying to choose an adequate approximation to p however this is just as often not the task one is trying to achieve instead just as often it is m that is some fixed prior reference measure and p that one is attempting to optimise by minimising dklpm subject to some constraint this has led to some ambiguity in the literature with some authors attempting to resolve the inconsistency by redefining crossentropy to be dklpm rather than hpm\u000a\u000a\u000a relationship to available work \u000a\u000asurprisals add where probabilities multiply the surprisal for an event of probability  is defined as  if  is  then surprisal is in nats bits or  so that for instance there are  bits of surprisal for landing all heads on a toss of  coins\u000abestguess states eg for atoms in a gas are inferred by maximizing the average surprisal  entropy for a given set of control parameters like pressure  or volume  this constrained entropy maximization both classically and quantum mechanically minimizes gibbs availability in entropy units  where  is a constrained multiplicity or partition function\u000awhen temperature  is fixed free energy  is also minimized thus if  and number of molecules  are constant the helmholtz free energy  where  is energy is minimized as a system equilibrates if  and  are held constant say during processes in your body the gibbs free energy  is minimized instead the change in free energy under these conditions is a measure of available work that might be done in the process thus available work for an ideal gas at constant temperature  and pressure  is  where  and  see also gibbs inequality\u000amore generally the work available relative to some ambient is obtained by multiplying ambient temperature  by kullbackleibler divergence or net surprisal  defined as the average value of  where  is the probability of a given state under ambient conditions for instance the work available in equilibrating a monatomic ideal gas to ambient values of  and  is thus  where kullbackleibler divergence  the resulting contours of constant kullbackleibler divergence shown at right for a mole of argon at standard temperature and pressure for example put limits on the conversion of hot to cold as in flamepowered airconditioning or in the unpowered device to convert boilingwater to icewater discussed here thus kullbackleibler divergence measures thermodynamic availability in bits\u000a\u000a\u000a quantum information theory \u000afor density matrices p and q on a hilbert space the kl divergence or quantum relative entropy as it is often called in this case from p to q is defined to be\u000a\u000ain quantum information science the minimum of  over all separable states q can also be used as a measure of entanglement in the state p\u000a\u000a\u000a relationship between models and reality \u000ajust as kullbackleibler divergence of ambient from actual measures thermodynamic availability kullbackleibler divergence of model from reality is also useful even if the only clues we have about reality are some experimental measurements in the former case kullbackleibler divergence describes distance to equilibrium or when multiplied by ambient temperature the amount of available work while in the latter case it tells you about surprises that reality has up its sleeve or in other words how much the model has yet to learn\u000aalthough this tool for evaluating models against systems that are accessible experimentally may be applied in any field its application to selecting a statistical model via akaike information criterion are particularly well described in papers and a book by burnham and anderson in a nutshell the kullbackleibler divergence of a model from reality may be estimated to within a constant additive term by a function like the squares summed of the deviations observed between data and the models predictions estimates of such divergence for models that share the same additive term can in turn be used to select among models\u000awhen trying to fit parametrized models to data there are various estimators which attempt to minimize kullbackleibler divergence such as maximum likelihood and maximum spacing estimators\u000a\u000a\u000a symmetrised divergence \u000akullback and leibler themselves actually defined the divergence as\u000a\u000awhich is symmetric and nonnegative this quantity has sometimes been used for feature selection in classification problems where p and q are the conditional pdfs of a feature under two different classes\u000aan alternative is given via the  divergence\u000a\u000awhich can be interpreted as the expected information gain about x from discovering which probability distribution x is drawn from p or q if they currently have probabilities  and 1   respectively\u000athe value   05 gives the jensenshannon divergence defined by\u000a\u000awhere m is the average of the two distributions\u000a\u000adjs can also be interpreted as the capacity of a noisy information channel with two inputs giving the output distributions p and q the jensenshannon divergence like all fdivergences is locally proportional to the fisher information metric it is similar to the hellinger metric in the sense that induces the same affine connection on a statistical manifold and equal to onehalf the socalled jeffreys divergence\u000a\u000a\u000a relationship to other probabilitydistance measures \u000athere are many other important measures of probability distance some of these are particularly connected with the kullbackleibler divergence for example\u000athe total variation distance this is connected to the divergence through pinskers inequality \u000athe family of rnyi divergences provide generalizations of the kullbackleibler divergence depending on the value of a certain parameter  various inequalities may be deduced\u000aother notable measures of distance include the hellinger distance histogram intersection chisquared statistic quadratic form distance match distance kolmogorovsmirnov distance and earth movers distance\u000a\u000a\u000a data differencing \u000a\u000ajust as absolute entropy serves as theoretical background for data compression relative entropy serves as theoretical background for data differencing  the absolute entropy of a set of data in this sense being the data required to reconstruct it minimum compressed size while the relative entropy of a target set of data given a source set of data is the data required to reconstruct the target given the source minimum size of a patch\u000a\u000a\u000a see also \u000aakaike information criterion\u000abayesian information criterion\u000abregman divergence\u000adeviance information criterion\u000aentropic value at risk\u000aentropy power inequality\u000ainformation gain in decision trees\u000ainformation gain ratio\u000ainformation theory and measure theory\u000ajensenshannon divergence\u000aquantum relative entropy\u000asolomon kullback and richard leibler\u000a\u000a\u000a
p56
sg14
g17
sg18
Vin probability theory and information theory the kullbackleibler divergence also information divergence information gain relative entropy klic or kl divergence is a nonsymmetric measure of the difference between two probability distributions p and q specifically the kullbackleibler divergence of q from p denoted dklpq is a measure of the information lost when q is used to approximate p the kullbackleibler divergence measures the expected number of extra bits so intuitively it is non negative this can be verified by jensens inequality required to code samples from p when using a code optimized for q rather than using the true code optimized for p typically p represents the true distribution of data observations or a precisely calculated theoretical distribution the measure q typically represents a theory model description or approximation of p\u000aalthough it is often intuited as a metric or distance the kullbackleibler divergence is not a true metric  for example it is not symmetric the kullbackleibler divergence from p to q is generally not the same as that from q to p however its infinitesimal form specifically its hessian is a metric tensor it is the fisher information metric\u000akullbackleibler divergence is a special case of a broader class of divergences called fdivergences it was originally introduced by solomon kullback and richard leibler in 1951 as the directed divergence between two distributions it can be derived from a bregman divergence\u000a\u000a\u000a definition \u000afor discrete probability distributions p and q the kullbackleibler divergence of q from p is defined to be\u000a\u000ain words it is the expectation of the logarithmic difference between the probabilities p and q where the expectation is taken using the probabilities p the kullbackleibler divergence is defined only if qi0 implies pi0 for all i absolute continuity whenever pi is zero the contribution of the ith term is interpreted as zero because \u000afor distributions p and q of a continuous random variable the kullbackleibler divergence is defined to be the integral\u000a\u000awhere p and q denote the densities of p and q\u000amore generally if p and q are probability measures over a set x and p is absolutely continuous with respect to q then the kullbackleibler divergence from p to q is defined as\u000a\u000awhere  is the radonnikodym derivative of p with respect to q and provided the expression on the righthand side exists equivalently this can be written as\u000a\u000awhich we recognize as the entropy of p relative to q continuing in this case if  is any measure on x for which  and  exist meaning that p and q are absolutely continuous with respect to  then the kullbackleibler divergence from p to q is given as\u000a\u000athe logarithms in these formulae are taken to base 2 if information is measured in units of bits or to base e if information is measured in nats most formulas involving the kullbackleibler divergence hold regardless of the base of the logarithm\u000avarious conventions exist for referring to dklpq in words often it is referred to as the divergence between p and q however this fails to convey the fundamental asymmetry in the relation sometimes it may be found described as the divergence of p from or with respect to q often in the context of relative entropy or information gain however in the present article the divergence of q from p will be the language used as this best relates to the idea that it is p that is considered the underlying true or best guess distribution that expectations will be calculated with reference to while q is some divergent less good approximate distribution\u000a\u000a\u000a characterization \u000aarthur hobson proved that kullbackleibler divergence is the only measure of difference between probability distributions that satisfies some desiderata which are the canonical extension to those for the characterization of entropy consequently mutual information is the only measure of mutual dependence that satisfies an induced criteria since it is defined in terms of kullbackleibler divergence\u000a\u000a\u000a motivation \u000a\u000ain information theory the kraftmcmillan theorem establishes that any directly decodable coding scheme for coding a message to identify one value  out of a set of possibilities  can be seen as representing an implicit probability distribution  over  where  is the length of the code for  in bits therefore kullbackleibler divergence can be interpreted as the expected extra messagelength per datum that must be communicated if a code that is optimal for a given wrong distribution  is used compared to using a code based on the true distribution \u000a\u000awhere hpq is the cross entropy of p and q and hp is the entropy of p\u000anote also that there is a relation between the kullbackleibler divergence and the rate function in the theory of large deviations\u000akullback brings together all notions of information in his historic text information theory and statistics for instance he shows that the mean discriminating information between two hypotheses is the basis for all of the various measures of information from shannon to fisher shannons rate is the mean information between the hypotheses of dependence and independence of processes fishers information is second order term and dominant in the taylor approximation of the discriminating information between two models of the same parametric family\u000a\u000a\u000a properties \u000athe kullbackleibler divergence is always nonnegative\u000a\u000aa result known as gibbs inequality with dklpq zero if and only if p  q almost everywhere the entropy hp thus sets a minimum value for the crossentropy hpq the expected number of bits required when using a code based on q rather than p and the kullbackleibler divergence therefore represents the expected number of extra bits that must be transmitted to identify a value x drawn from x if a code is used corresponding to the probability distribution q rather than the true distribution p\u000athe kullbackleibler divergence remains welldefined for continuous distributions and furthermore is invariant under parameter transformations for example if a transformation is made from variable x to variable yx then since px dx  py dy and qx dx  qy dy the kullbackleibler divergence may be rewritten\u000a\u000awhere  and  although it was assumed that the transformation was continuous this need not be the case this also shows that the kullbackleibler divergence produces a dimensionally consistent quantity since if x is a dimensioned variable px and qx are also dimensioned since eg px dx is dimensionless the argument of the logarithmic term is and remains dimensionless as it must it can therefore be seen as in some ways a more fundamental quantity than some other properties in information theory such as selfinformation or shannon entropy which can become undefined or negative for nondiscrete probabilities\u000athe kullbackleibler divergence is additive for independent distributions in much the same way as shannon entropy if  are independent distributions with the joint distribution  and  likewise then\u000a\u000a\u000a kullbackleibler divergence for multivariate normal distributions \u000asuppose that we have two multivariate normal distributions with means  and with nonsingular covariance matrices  if the two distributions have the same dimension k then the kullbackleibler divergence between the distributions is as follows\u000a\u000athe logarithm in the last term must be taken to base e since all terms apart from the last are basee logarithms of expressions that are either factors of the density function or otherwise arise naturally the equation therefore gives a result measured in nats dividing the entire expression above by loge 2 yields the divergence in bits\u000a\u000a\u000a relation to metrics \u000aone might be tempted to call it a distance metric on the space of probability distributions but this would not be correct as the kullbackleibler divergence is not symmetric  that is   nor does it satisfy the triangle inequality even so being a premetric it generates a topology on the space of probability distributions more concretely if  is a sequence of distributions such that\u000a\u000athen it is said that  pinskers inequality entails that  where the latter stands for the usual convergence in total variation\u000afollowing rnyi 1970 1961 the term is sometimes also called the information gain about x achieved if p can be used instead of q it is also called the relative entropy of p with respect to q and written hpq\u000a\u000a\u000a fisher information metric \u000ahowever the kullbackleibler divergence is rather directly related to a metric specifically the fisher information metric this can be made explicit as follows assume that the probability distributions p and q are both parameterized by some possibly multidimensional parameter  consider then two close by values of  and  so that the parameter  differs by only a small amount from the parameter value  specifically up to first order one has using the einstein summation convention\u000a\u000awith  a small change of  in the j direction and  the corresponding rate of change in the probability distribution since the kullbackleibler divergence has an absolute minimum 0 for p  q ie  it changes only to second order in the small parameters  more formally as for any minimum the first derivatives of the divergence vanish\u000a\u000aand by the taylor expansion one has up to second order\u000a\u000awhere the hessian matrix of the divergence\u000a\u000amust be positive semidefinite letting  vary and dropping the subindex 0 the hessian  defines a possibly degenerate riemannian metric on the  parameter space called the fisher information metric\u000a\u000a\u000a relation to other quantities of information theory \u000amany of the other quantities of information theory can be interpreted as applications of the kullbackleibler divergence to specific cases\u000athe selfinformation\u000a\u000ais the kullbackleibler divergence of the probability distribution pi from a kronecker delta representing certainty that i  m  ie the number of extra bits that must be transmitted to identify i if only the probability distribution pi is available to the receiver not the fact that i  m\u000athe mutual information\u000a\u000ais the kullbackleibler divergence of the product pxpy of the two marginal probability distributions from the joint probability distribution pxy  ie the expected number of extra bits that must be transmitted to identify x and y if they are coded using only their marginal distributions instead of the joint distribution equivalently if the joint probability pxy is known it is the expected number of extra bits that must on average be sent to identify y if the value of x is not already known to the receiver\u000athe shannon entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the uniform distribution pux from the true distribution px  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the true distribution px\u000athe conditional entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the product distribution pux py from the true joint distribution pxy  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the conditional distribution px  y of x given y\u000athe cross entropy between two probability distributions measures the average number of bits needed to identify an event from a set of possibilities if a coding scheme is used based on a given probability distribution  rather than the true distribution  the cross entropy for two distributions  and  over the same probability space is thus defined as follows\u000a\u000a\u000a kullbackleibler divergence and bayesian updating \u000ain bayesian statistics the kullbackleibler divergence can be used as a measure of the information gain in moving from a prior distribution to a posterior distribution if some new fact y  y is discovered it can be used to update the probability distribution for x from px  i to a new posterior probability distribution px  yi using bayes theorem\u000a\u000athis distribution has a new entropy\u000a\u000awhich may be less than or greater than the original entropy hp  i however from the standpoint of the new probability distribution one can estimate that to have used the original code based on px  i instead of a new code based on px  yi would have added an expected number of bits\u000a\u000ato the message length this therefore represents the amount of useful information or information gain about x that we can estimate has been learned by discovering y  y\u000aif a further piece of data y2  y2 subsequently comes in the probability distribution for x can be updated further to give a new best guess pxy1y2i if one reinvestigates the information gain for using pxy1i rather than pxi it turns out that it may be either greater or less than previously estimated\u000a may be  or  than \u000aand so the combined information gain does not obey the triangle inequality\u000a may be   or  than \u000aall one can say is that on average averaging using py2  y1xi the two sides will average out\u000a\u000a\u000a bayesian experimental design \u000aa common goal in bayesian experimental design is to maximise the expected kullbackleibler divergence between the prior and the posterior when posteriors are approximated to be gaussian distributions a design maximising the expected kullbackleibler divergence is called bayes doptimal\u000a\u000a\u000a discrimination information \u000athe kullbackleibler divergence dkl pxh1  pxh0  can also be interpreted as the expected discrimination information for h1 over h0 the mean information per sample for discriminating in favor of a hypothesis h1 against a hypothesis h0 when hypothesis h1 is true another name for this quantity given to it by ij good is the expected weight of evidence for h1 over h0 to be expected from each sample\u000athe expected weight of evidence for h1 over h0 is not the same as the information gain expected per sample about the probability distribution ph of the hypotheses\u000a\u000aeither of the two quantities can be used as a utility function in bayesian experimental design to choose an optimal next question to investigate but they will in general lead to rather different experimental strategies\u000aon the entropy scale of information gain there is very little difference between near certainty and absolute certaintycoding according to a near certainty requires hardly any more bits than coding according to an absolute certainty on the other hand on the logit scale implied by weight of evidence the difference between the two is enormous  infinite perhaps this might reflect the difference between being almost sure on a probabilistic level that say the riemann hypothesis is correct compared to being certain that it is correct because one has a mathematical proof these two different scales of loss function for uncertainty are both useful according to how well each reflects the particular circumstances of the problem in question\u000a\u000a\u000a principle of minimum discrimination information \u000athe idea of kullbackleibler divergence as discrimination information led kullback to propose the principle of minimum discrimination information mdi given new facts a new distribution f should be chosen which is as hard to discriminate from the original distribution f0 as possible so that the new data produces as small an information gain dkl f  f0  as possible\u000afor example if one had a prior distribution pxa over x and a and subsequently learnt the true distribution of a was ua the kullbackleibler divergence between the new joint distribution for x and a qxa ua and the earlier prior distribution would be\u000a\u000aie the sum of the kullbackleibler divergence of pa the prior distribution for a from the updated distribution ua plus the expected value using the probability distribution ua of the kullbackleibler divergence of the prior conditional distribution pxa from the new conditional distribution qxa note that often the later expected value is called the conditional kullbackleibler divergence or conditional relative entropy and denoted by dklqxapxa this is minimized if qxa  pxa over the whole support of ua and we note that this result incorporates bayes theorem if the new distribution ua is in fact a  function representing certainty that a has one particular value\u000amdi can be seen as an extension of laplaces principle of insufficient reason and the principle of maximum entropy of et jaynes in particular it is the natural extension of the principle of maximum entropy from discrete to continuous distributions for which shannon entropy ceases to be so useful see differential entropy but the kullbackleibler divergence continues to be just as relevant\u000ain the engineering literature mdi is sometimes called the principle of minimum crossentropy mce or minxent for short minimising the kullbackleibler divergence of m from p with respect to m is equivalent to minimizing the crossentropy of p and m since\u000a\u000awhich is appropriate if one is trying to choose an adequate approximation to p however this is just as often not the task one is trying to achieve instead just as often it is m that is some fixed prior reference measure and p that one is attempting to optimise by minimising dklpm subject to some constraint this has led to some ambiguity in the literature with some authors attempting to resolve the inconsistency by redefining crossentropy to be dklpm rather than hpm\u000a\u000a\u000a relationship to available work \u000a\u000asurprisals add where probabilities multiply the surprisal for an event of probability  is defined as  if  is  then surprisal is in nats bits or  so that for instance there are  bits of surprisal for landing all heads on a toss of  coins\u000abestguess states eg for atoms in a gas are inferred by maximizing the average surprisal  entropy for a given set of control parameters like pressure  or volume  this constrained entropy maximization both classically and quantum mechanically minimizes gibbs availability in entropy units  where  is a constrained multiplicity or partition function\u000awhen temperature  is fixed free energy  is also minimized thus if  and number of molecules  are constant the helmholtz free energy  where  is energy is minimized as a system equilibrates if  and  are held constant say during processes in your body the gibbs free energy  is minimized instead the change in free energy under these conditions is a measure of available work that might be done in the process thus available work for an ideal gas at constant temperature  and pressure  is  where  and  see also gibbs inequality\u000amore generally the work available relative to some ambient is obtained by multiplying ambient temperature  by kullbackleibler divergence or net surprisal  defined as the average value of  where  is the probability of a given state under ambient conditions for instance the work available in equilibrating a monatomic ideal gas to ambient values of  and  is thus  where kullbackleibler divergence  the resulting contours of constant kullbackleibler divergence shown at right for a mole of argon at standard temperature and pressure for example put limits on the conversion of hot to cold as in flamepowered airconditioning or in the unpowered device to convert boilingwater to icewater discussed here thus kullbackleibler divergence measures thermodynamic availability in bits\u000a\u000a\u000a quantum information theory \u000afor density matrices p and q on a hilbert space the kl divergence or quantum relative entropy as it is often called in this case from p to q is defined to be\u000a\u000ain quantum information science the minimum of  over all separable states q can also be used as a measure of entanglement in the state p\u000a\u000a\u000a relationship between models and reality \u000ajust as kullbackleibler divergence of ambient from actual measures thermodynamic availability kullbackleibler divergence of model from reality is also useful even if the only clues we have about reality are some experimental measurements in the former case kullbackleibler divergence describes distance to equilibrium or when multiplied by ambient temperature the amount of available work while in the latter case it tells you about surprises that reality has up its sleeve or in other words how much the model has yet to learn\u000aalthough this tool for evaluating models against systems that are accessible experimentally may be applied in any field its application to selecting a statistical model via akaike information criterion are particularly well described in papers and a book by burnham and anderson in a nutshell the kullbackleibler divergence of a model from reality may be estimated to within a constant additive term by a function like the squares summed of the deviations observed between data and the models predictions estimates of such divergence for models that share the same additive term can in turn be used to select among models\u000awhen trying to fit parametrized models to data there are various estimators which attempt to minimize kullbackleibler divergence such as maximum likelihood and maximum spacing estimators\u000a\u000a\u000a symmetrised divergence \u000akullback and leibler themselves actually defined the divergence as\u000a\u000awhich is symmetric and nonnegative this quantity has sometimes been used for feature selection in classification problems where p and q are the conditional pdfs of a feature under two different classes\u000aan alternative is given via the  divergence\u000a\u000awhich can be interpreted as the expected information gain about x from discovering which probability distribution x is drawn from p or q if they currently have probabilities  and 1   respectively\u000athe value   05 gives the jensenshannon divergence defined by\u000a\u000awhere m is the average of the two distributions\u000a\u000adjs can also be interpreted as the capacity of a noisy information channel with two inputs giving the output distributions p and q the jensenshannon divergence like all fdivergences is locally proportional to the fisher information metric it is similar to the hellinger metric in the sense that induces the same affine connection on a statistical manifold and equal to onehalf the socalled jeffreys divergence\u000a\u000a\u000a relationship to other probabilitydistance measures \u000athere are many other important measures of probability distance some of these are particularly connected with the kullbackleibler divergence for example\u000athe total variation distance this is connected to the divergence through pinskers inequality \u000athe family of rnyi divergences provide generalizations of the kullbackleibler divergence depending on the value of a certain parameter  various inequalities may be deduced\u000aother notable measures of distance include the hellinger distance histogram intersection chisquared statistic quadratic form distance match distance kolmogorovsmirnov distance and earth movers distance\u000a\u000a\u000a data differencing \u000a\u000ajust as absolute entropy serves as theoretical background for data compression relative entropy serves as theoretical background for data differencing  the absolute entropy of a set of data in this sense being the data required to reconstruct it minimum compressed size while the relative entropy of a target set of data given a source set of data is the data required to reconstruct the target given the source minimum size of a patch\u000a\u000a\u000a see also \u000aakaike information criterion\u000abayesian information criterion\u000abregman divergence\u000adeviance information criterion\u000aentropic value at risk\u000aentropy power inequality\u000ainformation gain in decision trees\u000ainformation gain ratio\u000ainformation theory and measure theory\u000ajensenshannon divergence\u000aquantum relative entropy\u000asolomon kullback and richard leibler
p57
sg20
g23
sg24
g27
sg30
Vin probability theory and information theory the kullbackleibler divergence also information divergence information gain relative entropy klic or kl divergence is a nonsymmetric measure of the difference between two probability distributions p and q specifically the kullbackleibler divergence of q from p denoted dklpq is a measure of the information lost when q is used to approximate p the kullbackleibler divergence measures the expected number of extra bits so intuitively it is non negative this can be verified by jensens inequality required to code samples from p when using a code optimized for q rather than using the true code optimized for p typically p represents the true distribution of data observations or a precisely calculated theoretical distribution the measure q typically represents a theory model description or approximation of p\u000aalthough it is often intuited as a metric or distance the kullbackleibler divergence is not a true metric  for example it is not symmetric the kullbackleibler divergence from p to q is generally not the same as that from q to p however its infinitesimal form specifically its hessian is a metric tensor it is the fisher information metric\u000akullbackleibler divergence is a special case of a broader class of divergences called fdivergences it was originally introduced by solomon kullback and richard leibler in 1951 as the directed divergence between two distributions it can be derived from a bregman divergence\u000a\u000a\u000a definition \u000afor discrete probability distributions p and q the kullbackleibler divergence of q from p is defined to be\u000a\u000ain words it is the expectation of the logarithmic difference between the probabilities p and q where the expectation is taken using the probabilities p the kullbackleibler divergence is defined only if qi0 implies pi0 for all i absolute continuity whenever pi is zero the contribution of the ith term is interpreted as zero because \u000afor distributions p and q of a continuous random variable the kullbackleibler divergence is defined to be the integral\u000a\u000awhere p and q denote the densities of p and q\u000amore generally if p and q are probability measures over a set x and p is absolutely continuous with respect to q then the kullbackleibler divergence from p to q is defined as\u000a\u000awhere  is the radonnikodym derivative of p with respect to q and provided the expression on the righthand side exists equivalently this can be written as\u000a\u000awhich we recognize as the entropy of p relative to q continuing in this case if  is any measure on x for which  and  exist meaning that p and q are absolutely continuous with respect to  then the kullbackleibler divergence from p to q is given as\u000a\u000athe logarithms in these formulae are taken to base 2 if information is measured in units of bits or to base e if information is measured in nats most formulas involving the kullbackleibler divergence hold regardless of the base of the logarithm\u000avarious conventions exist for referring to dklpq in words often it is referred to as the divergence between p and q however this fails to convey the fundamental asymmetry in the relation sometimes it may be found described as the divergence of p from or with respect to q often in the context of relative entropy or information gain however in the present article the divergence of q from p will be the language used as this best relates to the idea that it is p that is considered the underlying true or best guess distribution that expectations will be calculated with reference to while q is some divergent less good approximate distribution\u000a\u000a\u000a characterization \u000aarthur hobson proved that kullbackleibler divergence is the only measure of difference between probability distributions that satisfies some desiderata which are the canonical extension to those for the characterization of entropy consequently mutual information is the only measure of mutual dependence that satisfies an induced criteria since it is defined in terms of kullbackleibler divergence\u000a\u000a\u000a motivation \u000a\u000ain information theory the kraftmcmillan theorem establishes that any directly decodable coding scheme for coding a message to identify one value  out of a set of possibilities  can be seen as representing an implicit probability distribution  over  where  is the length of the code for  in bits therefore kullbackleibler divergence can be interpreted as the expected extra messagelength per datum that must be communicated if a code that is optimal for a given wrong distribution  is used compared to using a code based on the true distribution \u000a\u000awhere hpq is the cross entropy of p and q and hp is the entropy of p\u000anote also that there is a relation between the kullbackleibler divergence and the rate function in the theory of large deviations\u000akullback brings together all notions of information in his historic text information theory and statistics for instance he shows that the mean discriminating information between two hypotheses is the basis for all of the various measures of information from shannon to fisher shannons rate is the mean information between the hypotheses of dependence and independence of processes fishers information is second order term and dominant in the taylor approximation of the discriminating information between two models of the same parametric family\u000a\u000a\u000a properties \u000athe kullbackleibler divergence is always nonnegative\u000a\u000aa result known as gibbs inequality with dklpq zero if and only if p  q almost everywhere the entropy hp thus sets a minimum value for the crossentropy hpq the expected number of bits required when using a code based on q rather than p and the kullbackleibler divergence therefore represents the expected number of extra bits that must be transmitted to identify a value x drawn from x if a code is used corresponding to the probability distribution q rather than the true distribution p\u000athe kullbackleibler divergence remains welldefined for continuous distributions and furthermore is invariant under parameter transformations for example if a transformation is made from variable x to variable yx then since px dx  py dy and qx dx  qy dy the kullbackleibler divergence may be rewritten\u000a\u000awhere  and  although it was assumed that the transformation was continuous this need not be the case this also shows that the kullbackleibler divergence produces a dimensionally consistent quantity since if x is a dimensioned variable px and qx are also dimensioned since eg px dx is dimensionless the argument of the logarithmic term is and remains dimensionless as it must it can therefore be seen as in some ways a more fundamental quantity than some other properties in information theory such as selfinformation or shannon entropy which can become undefined or negative for nondiscrete probabilities\u000athe kullbackleibler divergence is additive for independent distributions in much the same way as shannon entropy if  are independent distributions with the joint distribution  and  likewise then\u000a\u000a\u000a kullbackleibler divergence for multivariate normal distributions \u000asuppose that we have two multivariate normal distributions with means  and with nonsingular covariance matrices  if the two distributions have the same dimension k then the kullbackleibler divergence between the distributions is as follows\u000a\u000athe logarithm in the last term must be taken to base e since all terms apart from the last are basee logarithms of expressions that are either factors of the density function or otherwise arise naturally the equation therefore gives a result measured in nats dividing the entire expression above by loge 2 yields the divergence in bits\u000a\u000a\u000a relation to metrics \u000aone might be tempted to call it a distance metric on the space of probability distributions but this would not be correct as the kullbackleibler divergence is not symmetric  that is   nor does it satisfy the triangle inequality even so being a premetric it generates a topology on the space of probability distributions more concretely if  is a sequence of distributions such that\u000a\u000athen it is said that  pinskers inequality entails that  where the latter stands for the usual convergence in total variation\u000afollowing rnyi 1970 1961 the term is sometimes also called the information gain about x achieved if p can be used instead of q it is also called the relative entropy of p with respect to q and written hpq\u000a\u000a\u000a fisher information metric \u000ahowever the kullbackleibler divergence is rather directly related to a metric specifically the fisher information metric this can be made explicit as follows assume that the probability distributions p and q are both parameterized by some possibly multidimensional parameter  consider then two close by values of  and  so that the parameter  differs by only a small amount from the parameter value  specifically up to first order one has using the einstein summation convention\u000a\u000awith  a small change of  in the j direction and  the corresponding rate of change in the probability distribution since the kullbackleibler divergence has an absolute minimum 0 for p  q ie  it changes only to second order in the small parameters  more formally as for any minimum the first derivatives of the divergence vanish\u000a\u000aand by the taylor expansion one has up to second order\u000a\u000awhere the hessian matrix of the divergence\u000a\u000amust be positive semidefinite letting  vary and dropping the subindex 0 the hessian  defines a possibly degenerate riemannian metric on the  parameter space called the fisher information metric\u000a\u000a\u000a relation to other quantities of information theory \u000amany of the other quantities of information theory can be interpreted as applications of the kullbackleibler divergence to specific cases\u000athe selfinformation\u000a\u000ais the kullbackleibler divergence of the probability distribution pi from a kronecker delta representing certainty that i  m  ie the number of extra bits that must be transmitted to identify i if only the probability distribution pi is available to the receiver not the fact that i  m\u000athe mutual information\u000a\u000ais the kullbackleibler divergence of the product pxpy of the two marginal probability distributions from the joint probability distribution pxy  ie the expected number of extra bits that must be transmitted to identify x and y if they are coded using only their marginal distributions instead of the joint distribution equivalently if the joint probability pxy is known it is the expected number of extra bits that must on average be sent to identify y if the value of x is not already known to the receiver\u000athe shannon entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the uniform distribution pux from the true distribution px  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the true distribution px\u000athe conditional entropy\u000a\u000ais the number of bits which would have to be transmitted to identify x from n equally likely possibilities less the kullbackleibler divergence of the product distribution pux py from the true joint distribution pxy  ie less the expected number of bits saved which would have had to be sent if the value of x were coded according to the uniform distribution pux rather than the conditional distribution px  y of x given y\u000athe cross entropy between two probability distributions measures the average number of bits needed to identify an event from a set of possibilities if a coding scheme is used based on a given probability distribution  rather than the true distribution  the cross entropy for two distributions  and  over the same probability space is thus defined as follows\u000a\u000a\u000a kullbackleibler divergence and bayesian updating \u000ain bayesian statistics the kullbackleibler divergence can be used as a measure of the information gain in moving from a prior distribution to a posterior distribution if some new fact y  y is discovered it can be used to update the probability distribution for x from px  i to a new posterior probability distribution px  yi using bayes theorem\u000a\u000athis distribution has a new entropy\u000a\u000awhich may be less than or greater than the original entropy hp  i however from the standpoint of the new probability distribution one can estimate that to have used the original code based on px  i instead of a new code based on px  yi would have added an expected number of bits\u000a\u000ato the message length this therefore represents the amount of useful information or information gain about x that we can estimate has been learned by discovering y  y\u000aif a further piece of data y2  y2 subsequently comes in the probability distribution for x can be updated further to give a new best guess pxy1y2i if one reinvestigates the information gain for using pxy1i rather than pxi it turns out that it may be either greater or less than previously estimated\u000a may be  or  than \u000aand so the combined information gain does not obey the triangle inequality\u000a may be   or  than \u000aall one can say is that on average averaging using py2  y1xi the two sides will average out\u000a\u000a\u000a bayesian experimental design \u000aa common goal in bayesian experimental design is to maximise the expected kullbackleibler divergence between the prior and the posterior when posteriors are approximated to be gaussian distributions a design maximising the expected kullbackleibler divergence is called bayes doptimal\u000a\u000a\u000a discrimination information \u000athe kullbackleibler divergence dkl pxh1  pxh0  can also be interpreted as the expected discrimination information for h1 over h0 the mean information per sample for discriminating in favor of a hypothesis h1 against a hypothesis h0 when hypothesis h1 is true another name for this quantity given to it by ij good is the expected weight of evidence for h1 over h0 to be expected from each sample\u000athe expected weight of evidence for h1 over h0 is not the same as the information gain expected per sample about the probability distribution ph of the hypotheses\u000a\u000aeither of the two quantities can be used as a utility function in bayesian experimental design to choose an optimal next question to investigate but they will in general lead to rather different experimental strategies\u000aon the entropy scale of information gain there is very little difference between near certainty and absolute certaintycoding according to a near certainty requires hardly any more bits than coding according to an absolute certainty on the other hand on the logit scale implied by weight of evidence the difference between the two is enormous  infinite perhaps this might reflect the difference between being almost sure on a probabilistic level that say the riemann hypothesis is correct compared to being certain that it is correct because one has a mathematical proof these two different scales of loss function for uncertainty are both useful according to how well each reflects the particular circumstances of the problem in question\u000a\u000a\u000a principle of minimum discrimination information \u000athe idea of kullbackleibler divergence as discrimination information led kullback to propose the principle of minimum discrimination information mdi given new facts a new distribution f should be chosen which is as hard to discriminate from the original distribution f0 as possible so that the new data produces as small an information gain dkl f  f0  as possible\u000afor example if one had a prior distribution pxa over x and a and subsequently learnt the true distribution of a was ua the kullbackleibler divergence between the new joint distribution for x and a qxa ua and the earlier prior distribution would be\u000a\u000aie the sum of the kullbackleibler divergence of pa the prior distribution for a from the updated distribution ua plus the expected value using the probability distribution ua of the kullbackleibler divergence of the prior conditional distribution pxa from the new conditional distribution qxa note that often the later expected value is called the conditional kullbackleibler divergence or conditional relative entropy and denoted by dklqxapxa this is minimized if qxa  pxa over the whole support of ua and we note that this result incorporates bayes theorem if the new distribution ua is in fact a  function representing certainty that a has one particular value\u000amdi can be seen as an extension of laplaces principle of insufficient reason and the principle of maximum entropy of et jaynes in particular it is the natural extension of the principle of maximum entropy from discrete to continuous distributions for which shannon entropy ceases to be so useful see differential entropy but the kullbackleibler divergence continues to be just as relevant\u000ain the engineering literature mdi is sometimes called the principle of minimum crossentropy mce or minxent for short minimising the kullbackleibler divergence of m from p with respect to m is equivalent to minimizing the crossentropy of p and m since\u000a\u000awhich is appropriate if one is trying to choose an adequate approximation to p however this is just as often not the task one is trying to achieve instead just as often it is m that is some fixed prior reference measure and p that one is attempting to optimise by minimising dklpm subject to some constraint this has led to some ambiguity in the literature with some authors attempting to resolve the inconsistency by redefining crossentropy to be dklpm rather than hpm\u000a\u000a\u000a relationship to available work \u000a\u000asurprisals add where probabilities multiply the surprisal for an event of probability  is defined as  if  is  then surprisal is in nats bits or  so that for instance there are  bits of surprisal for landing all heads on a toss of  coins\u000abestguess states eg for atoms in a gas are inferred by maximizing the average surprisal  entropy for a given set of control parameters like pressure  or volume  this constrained entropy maximization both classically and quantum mechanically minimizes gibbs availability in entropy units  where  is a constrained multiplicity or partition function\u000awhen temperature  is fixed free energy  is also minimized thus if  and number of molecules  are constant the helmholtz free energy  where  is energy is minimized as a system equilibrates if  and  are held constant say during processes in your body the gibbs free energy  is minimized instead the change in free energy under these conditions is a measure of available work that might be done in the process thus available work for an ideal gas at constant temperature  and pressure  is  where  and  see also gibbs inequality\u000amore generally the work available relative to some ambient is obtained by multiplying ambient temperature  by kullbackleibler divergence or net surprisal  defined as the average value of  where  is the probability of a given state under ambient conditions for instance the work available in equilibrating a monatomic ideal gas to ambient values of  and  is thus  where kullbackleibler divergence  the resulting contours of constant kullbackleibler divergence shown at right for a mole of argon at standard temperature and pressure for example put limits on the conversion of hot to cold as in flamepowered airconditioning or in the unpowered device to convert boilingwater to icewater discussed here thus kullbackleibler divergence measures thermodynamic availability in bits\u000a\u000a\u000a quantum information theory \u000afor density matrices p and q on a hilbert space the kl divergence or quantum relative entropy as it is often called in this case from p to q is defined to be\u000a\u000ain quantum information science the minimum of  over all separable states q can also be used as a measure of entanglement in the state p\u000a\u000a\u000a relationship between models and reality \u000ajust as kullbackleibler divergence of ambient from actual measures thermodynamic availability kullbackleibler divergence of model from reality is also useful even if the only clues we have about reality are some experimental measurements in the former case kullbackleibler divergence describes distance to equilibrium or when multiplied by ambient temperature the amount of available work while in the latter case it tells you about surprises that reality has up its sleeve or in other words how much the model has yet to learn\u000aalthough this tool for evaluating models against systems that are accessible experimentally may be applied in any field its application to selecting a statistical model via akaike information criterion are particularly well described in papers and a book by burnham and anderson in a nutshell the kullbackleibler divergence of a model from reality may be estimated to within a constant additive term by a function like the squares summed of the deviations observed between data and the models predictions estimates of such divergence for models that share the same additive term can in turn be used to select among models\u000awhen trying to fit parametrized models to data there are various estimators which attempt to minimize kullbackleibler divergence such as maximum likelihood and maximum spacing estimators\u000a\u000a\u000a symmetrised divergence \u000akullback and leibler themselves actually defined the divergence as\u000a\u000awhich is symmetric and nonnegative this quantity has sometimes been used for feature selection in classification problems where p and q are the conditional pdfs of a feature under two different classes\u000aan alternative is given via the  divergence\u000a\u000awhich can be interpreted as the expected information gain about x from discovering which probability distribution x is drawn from p or q if they currently have probabilities  and 1   respectively\u000athe value   05 gives the jensenshannon divergence defined by\u000a\u000awhere m is the average of the two distributions\u000a\u000adjs can also be interpreted as the capacity of a noisy information channel with two inputs giving the output distributions p and q the jensenshannon divergence like all fdivergences is locally proportional to the fisher information metric it is similar to the hellinger metric in the sense that induces the same affine connection on a statistical manifold and equal to onehalf the socalled jeffreys divergence\u000a\u000a\u000a relationship to other probabilitydistance measures \u000athere are many other important measures of probability distance some of these are particularly connected with the kullbackleibler divergence for example\u000athe total variation distance this is connected to the divergence through pinskers inequality \u000athe family of rnyi divergences provide generalizations of the kullbackleibler divergence depending on the value of a certain parameter  various inequalities may be deduced\u000aother notable measures of distance include the hellinger distance histogram intersection chisquared statistic quadratic form distance match distance kolmogorovsmirnov distance and earth movers distance\u000a\u000a\u000a data differencing \u000a\u000ajust as absolute entropy serves as theoretical background for data compression relative entropy serves as theoretical background for data differencing  the absolute entropy of a set of data in this sense being the data required to reconstruct it minimum compressed size while the relative entropy of a target set of data given a source set of data is the data required to reconstruct the target given the source minimum size of a patch\u000a\u000a\u000a see also \u000aakaike information criterion\u000abayesian information criterion\u000abregman divergence\u000adeviance information criterion\u000aentropic value at risk\u000aentropy power inequality\u000ainformation gain in decision trees\u000ainformation gain ratio\u000ainformation theory and measure theory\u000ajensenshannon divergence\u000aquantum relative entropy\u000asolomon kullback and richard leibler\u000a\u000a\u000a
p58
sg32
g35
sg37
NsbsS'analytic_and_enumerative_statistical_studies.txt'
p59
g2
(g3
g4
Ntp60
Rp61
(dp62
g8
g11
sg12
Vanalytic and enumerative statistical studies are two types of scientific studies\u000ain any statistical study the ultimate aim is to provide a rational basis for action enumerative and analytic studies differ by where the action is taken deming summarized the distinction between enumerative and analytic studies as follows\u000a\u000aenumerative study a statistical study in which action will be taken on the material in the frame being studied\u000a\u000aanalytic study a statistical study in which action will be taken on the process or causesystem that produced the frame being studied the aim being to improve practice in the future\u000a\u000ain a statistical study the frame is the set from which the sample statistics is taken\u000athese terms were introduced in some theory of sampling 1950 chapter 7 by w edwards deming\u000ain other words an enumerative study is a statistical study in which the focus is on judgment of results and an analytic study is one in which the focus is on improvement of the process or system which created the results being evaluated and which will continue creating results in the future a statistical study can be enumerative or analytic but it cannot be both\u000athis distinction between enumerative and analytic studies is the theory behind the fourteen points for management dr demings philosophy is that management should be analytic instead of enumerative in other words management should focus on improvement of processes for the future instead of on judgment of current results\u000a\u000ause of data requires knowledge about the different sources of uncertainty\u000ameasurement is a process is the system of measurement stable or unstable use\u000aof data requires also understanding of the distinction between enumerative studies and analytic problems\u000a\u000athe interpretation of results of a test or experiment is something else it is prediction that a specific change in a process or procedure will be a wise choice or that no change would be better either way the choice is prediction this is known as an analytic problem or a problem of inference prediction\u000a\u000astatistician dr mike tveite has pointed out the dangers of attempting to use an enumerative study for prediction\u000a\u000a\u000a notes \u000a\u000a\u000a external links \u000aon the distinction between enumerative and analytic surveys by w edwards deming\u000astatistics and reality by david and sarah kerridge at the wayback machine archived september 20 2010
p63
sg14
g17
sg18
Vanalytic and enumerative statistical studies are two types of scientific studies\u000ain any statistical study the ultimate aim is to provide a rational basis for action enumerative and analytic studies differ by where the action is taken deming summarized the distinction between enumerative and analytic studies as follows\u000a\u000aenumerative study a statistical study in which action will be taken on the material in the frame being studied\u000a\u000aanalytic study a statistical study in which action will be taken on the process or causesystem that produced the frame being studied the aim being to improve practice in the future\u000a\u000ain a statistical study the frame is the set from which the sample statistics is taken\u000athese terms were introduced in some theory of sampling 1950 chapter 7 by w edwards deming\u000ain other words an enumerative study is a statistical study in which the focus is on judgment of results and an analytic study is one in which the focus is on improvement of the process or system which created the results being evaluated and which will continue creating results in the future a statistical study can be enumerative or analytic but it cannot be both\u000athis distinction between enumerative and analytic studies is the theory behind the fourteen points for management dr demings philosophy is that management should be analytic instead of enumerative in other words management should focus on improvement of processes for the future instead of on judgment of current results\u000a\u000ause of data requires knowledge about the different sources of uncertainty\u000ameasurement is a process is the system of measurement stable or unstable use\u000aof data requires also understanding of the distinction between enumerative studies and analytic problems\u000a\u000athe interpretation of results of a test or experiment is something else it is prediction that a specific change in a process or procedure will be a wise choice or that no change would be better either way the choice is prediction this is known as an analytic problem or a problem of inference prediction\u000a\u000astatistician dr mike tveite has pointed out the dangers of attempting to use an enumerative study for prediction\u000a\u000a\u000a notes \u000a\u000a\u000a external links \u000aon the distinction between enumerative and analytic surveys by w edwards deming\u000astatistics and reality by david and sarah kerridge at the wayback machine archived september 20 2010
p64
sg20
g23
sg24
g27
sg30
Vanalytic and enumerative statistical studies are two types of scientific studies\u000ain any statistical study the ultimate aim is to provide a rational basis for action enumerative and analytic studies differ by where the action is taken deming summarized the distinction between enumerative and analytic studies as follows\u000a\u000aenumerative study a statistical study in which action will be taken on the material in the frame being studied\u000a\u000aanalytic study a statistical study in which action will be taken on the process or causesystem that produced the frame being studied the aim being to improve practice in the future\u000a\u000ain a statistical study the frame is the set from which the sample statistics is taken\u000athese terms were introduced in some theory of sampling 1950 chapter 7 by w edwards deming\u000ain other words an enumerative study is a statistical study in which the focus is on judgment of results and an analytic study is one in which the focus is on improvement of the process or system which created the results being evaluated and which will continue creating results in the future a statistical study can be enumerative or analytic but it cannot be both\u000athis distinction between enumerative and analytic studies is the theory behind the fourteen points for management dr demings philosophy is that management should be analytic instead of enumerative in other words management should focus on improvement of processes for the future instead of on judgment of current results\u000a\u000ause of data requires knowledge about the different sources of uncertainty\u000ameasurement is a process is the system of measurement stable or unstable use\u000aof data requires also understanding of the distinction between enumerative studies and analytic problems\u000a\u000athe interpretation of results of a test or experiment is something else it is prediction that a specific change in a process or procedure will be a wise choice or that no change would be better either way the choice is prediction this is known as an analytic problem or a problem of inference prediction\u000a\u000astatistician dr mike tveite has pointed out the dangers of attempting to use an enumerative study for prediction\u000a\u000a\u000a notes \u000a\u000a\u000a external links \u000aon the distinction between enumerative and analytic surveys by w edwards deming\u000astatistics and reality by david and sarah kerridge at the wayback machine archived september 20 2010
p65
sg32
g35
sg37
NsbsS'random_variable.txt'
p66
g2
(g3
g4
Ntp67
Rp68
(dp69
g8
g11
sg12
Vin probability and statistics a random variable aleatory variable or stochastic variable is a variable whose value is subject to variations due to chance ie randomness in a mathematical sense a random variable can take on a set of possible different values similarly to other mathematical variables each with an associated probability in contrast to other mathematical variables\u000aa random variables possible values might represent the possible outcomes of a yettobeperformed experiment or the possible outcomes of a past experiment whose alreadyexisting value is uncertain for example due to imprecise measurements or quantum uncertainty they may also conceptually represent either the results of an objectively random process such as rolling a die or the subjective randomness that results from incomplete knowledge of a quantity the meaning of the probabilities assigned to the potential values of a random variable is not part of probability theory itself but is instead related to philosophical arguments over the interpretation of probability the mathematics works the same regardless of the particular interpretation in use\u000athe mathematical function describing the possible values of a random variable and their associated probabilities is known as a probability distribution random variables can be discrete that is taking any of a specified finite or countable list of values endowed with a probability mass function characteristic of a probability distribution or continuous taking any numerical value in an interval or collection of intervals via a probability density function that is characteristic of a probability distribution or a mixture of both types the realizations of a random variable that is the results of randomly choosing values according to the variables probability distribution function are called random variates\u000athe formal mathematical treatment of random variables is a topic in probability theory in that context a random variable is understood as a function defined on a sample space whose outputs are numerical values\u000a\u000a\u000a definitionedit \u000aa random variable  is a measurable function from the set of possible outcomes  to some set  the technical axiomatic definition requires  to be a probability space and  to be a measurable space see measuretheoretic definition\u000anote that although  is usually a realvalued function  it does not return a probability the probabilities of different outcomes or sets of outcomes events are already given by the probability measure  with which  is equipped rather  describes some numerical property that outcomes in  may have eg the number of heads in a random collection of coin flips the height of a random person the probability that  takes value  is the measure of the set of outcomes  denoted \u000a\u000a\u000a standard caseedit \u000ausually   otherwise the term random element is used see extensions\u000awhen the image or range of  is finite or countably infinite the random variable is called a discrete random variable and its distribution can be described by a probability mass function which assigns a probability to each value in the image of  if the image is uncountably infinite then  is called a continuous random variable in the special case that it is absolutely continuous its distribution can be described by a probability density function which assigns probabilities to intervals in particular each individual point must necessarily have probability zero for an absolutely continuous random variable not all continuous random variables are absolutely continuous for example a mixture distribution such random variables cannot be described by a probability density or a probability mass function\u000aany random variable can be described by its cumulative distribution function which describes the probability that the random variable will be less than or equal to a certain value\u000a\u000a\u000a extensionsedit \u000athe term random variable in statistics is traditionally limited to the realvalued case  this ensures that it is possible to define quantities such as the expected value and variance of a random variable its cumulative distribution function and the moments of its distribution\u000ahowever the definition above is valid for any measurable space  of values thus one can consider random elements of other sets  such as random boolean values categorical values complex numbers vectors matrices sequences trees sets shapes manifolds and functions one may then specifically refer to a random variable of type  or an valued random variable\u000athis more general concept of a random element is particularly useful in disciplines such as graph theory machine learning natural language processing and other fields in discrete mathematics and computer science where one is often interested in modeling the random variation of nonnumerical data structures in some cases it is nonetheless convenient to represent each element of  using one or more real numbers in this case a random element may optionally be represented as a vector of realvalued random variables all defined on the same underlying probability space  which allows the different random variables to covary for example\u000aa random word may be represented as a random integer that serves as an index into the vocabulary of possible words alternatively it can be represented as a random indicator vector whose length equals the size of the vocabulary where the only values of positive probability are 1 0 0 0  0 1 0 0  0 0 1 0  and the position of the 1 indicates the word\u000aa random sentence of given length  may be represented as a vector of  random words\u000aa random graph on  given vertices may be represented as a  matrix of random variables whose values specify the adjacency matrix of the random graph\u000aa random function  may be represented as a collection of random variables  giving the functions values at the various points  in the functions domain the  are ordinary realvalued random variables provided that the function is realvalued for example a stochastic process is a random function of time a random vector is a random function of some index set such as  and random field is a random function on any set typically time space or a discrete set\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete random variableedit \u000ain an experiment a person may be chosen at random and one random variable may be the persons height mathematically the random variable is interpreted as a function which maps the person to the persons height associated with the random variable is a probability distribution that allows the computation of the probability that the height is in any subset of possible values such as the probability that the height is between 180 and 190 cm or the probability that the height is either less than 150 or more than 200 cm\u000aanother random variable may be the persons number of children this is a discrete random variable with nonnegative integer values it allows the computation of probabilities for individual integer values  the probability mass function pmf  or for sets of values including infinite sets for example the event of interest may be an even number of children for both finite and infinite event sets their probabilities can be found by adding up the pmfs of the elements that is the probability of an even number of children is the infinite sum pmf0  pmf2  pmf4  \u000ain examples such as these the sample space the set of all possible persons is often suppressed since it is mathematically hard to describe and the possible values of the random variables are then treated as a sample space but when two random variables are measured on the same sample space of outcomes such as the height and number of children being computed on the same random persons it is easier to track their relationship if it is acknowledged that both height and number of children come from the same random person for example so that questions of whether such random variables are correlated or not can be posed\u000a\u000a\u000a coin tossedit \u000athe possible outcomes for one coin toss can be described by the sample space  we can introduce a realvalued random variable  that models a 1 payoff for a successful bet on heads as follows\u000a\u000aif the coin is a fair coin y has a probability mass function  given by\u000a\u000a\u000a die rolledit \u000a\u000aa random variable can also be used to describe the process of rolling dice and the possible outcomes the most obvious representation for the twodice case is to take the set of pairs of numbers n1 and n2 from 1 2 3 4 5 6 representing the numbers on the two dice as the sample space the total number rolled the sum of the numbers in each pair is then a random variable x given by the function that maps the pair to the sum\u000a\u000aand if the dice are fair has a probability mass function x given by\u000a\u000a\u000a continuous random variableedit \u000aan example of a continuous random variable would be one based on a spinner that can choose a horizontal direction then the values taken by the random variable are directions we could represent these directions by north west east south southeast etc however it is commonly more convenient to map the sample space to a random variable which takes values which are real numbers this can be done for example by mapping a direction to a bearing in degrees clockwise from north the random variable then takes values which are real numbers from the interval 0 360 with all parts of the range being equally likely in this case x  the angle spun any real number has probability zero of being selected but a positive probability can be assigned to any range of values for example the probability of choosing a number in 0 180 is 12 instead of speaking of a probability mass function we say that the probability density of x is 1360 the probability of a subset of 0 360 can be calculated by multiplying the measure of the set by 1360 in general the probability of a set for a given continuous random variable can be calculated by integrating the density over the given set\u000a\u000a\u000a mixed typeedit \u000aan example of a random variable of mixed type would be based on an experiment where a coin is flipped and the spinner is spun only if the result of the coin toss is heads if the result is tails x  1 otherwise x  the value of the spinner as in the preceding example there is a probability of 12 that this random variable will have the value 1 other ranges of values would have half the probability of the last example\u000a\u000a\u000a measuretheoretic definitionedit \u000athe most formal axiomatic definition of a random variable involves measure theory continuous random variables are defined in terms of sets of numbers along with functions that map such sets to probabilities because of various difficulties eg the banachtarski paradox that arise if such sets are insufficiently constrained it is necessary to introduce what is termed a sigmaalgebra to constrain the possible sets over which probabilities can be defined normally a particular such sigmaalgebra is used the borel algebra which allows for probabilities to be defined over any sets that can be derived either directly from continuous intervals of numbers or by a finite or countably infinite number of unions andor intersections of such intervals\u000athe measuretheoretic definition is as follows\u000alet  be a probability space and  a measurable space then an valued random variable is a function  which is measurable the latter means that for every subset  its preimage  where  this definition enables us to measure any subset  in the target space by looking at its preimage which by assumption is measurable\u000awhen  is a topological space then the most common choice for the algebra  is the borel algebra  which is the algebra generated by the collection of all open sets in  in such case the valued random variable is called the valued random variable moreover when space  is the real line  then such a realvalued random variable is called simply the random variable\u000a\u000a\u000a realvalued random variablesedit \u000ain this case the observation space is the set of real numbers recall  is the probability space for real observation space the function  is a realvalued random variable if\u000a\u000athis definition is a special case of the above because the set  generates the borel algebra on the set of real numbers and it suffices to check measurability on any generating set here we can prove measurability on this generating set by using the fact that \u000a\u000a\u000a distribution functions of random variablesedit \u000aif a random variable  defined on the probability space  is given we can ask questions like how likely is it that the value of  is equal to 2 this is the same as the probability of the event  which is often written as  or  for short\u000arecording all these probabilities of output ranges of a realvalued random variable  yields the probability distribution of  the probability distribution forgets about the particular probability space used to define  and only records the probabilities of various values of  such a probability distribution can always be captured by its cumulative distribution function\u000a\u000aand sometimes also using a probability density function  in measuretheoretic terms we use the random variable  to pushforward the measure  on  to a measure  on  the underlying probability space  is a technical device used to guarantee the existence of random variables sometimes to construct them and to define notions such as correlation and dependence or independence based on a joint distribution of two or more random variables on the same probability space in practice one often disposes of the space  altogether and just puts a measure on  that assigns measure 1 to the whole real line ie one works with probability distributions instead of random variables\u000a\u000a\u000a momentsedit \u000athe probability distribution of a random variable is often characterised by a small number of parameters which also have a practical interpretation for example it is often enough to know what its average value is this is captured by the mathematical concept of expected value of a random variable denoted ex and also called the first moment in general efx is not equal to fex once the average value is known one could then ask how far from this average value the values of x typically are a question that is answered by the variance and standard deviation of a random variable ex can be viewed intuitively as an average obtained from an infinite population the members of which are particular evaluations of x\u000amathematically this is known as the generalised problem of moments for a given class of random variables x find a collection fi of functions such that the expectation values efix fully characterise the distribution of the random variable x\u000amoments can only be defined for realvalued functions of random variables or complexvalued etc if the random variable is itself realvalued then moments of the variable itself can be taken which are equivalent to moments of the identity function  of the random variable however even for nonrealvalued random variables moments can be taken of realvalued functions of those variables for example for a categorical random variable x that can take on the nominal values red blue or green the realvalued function  can be constructed this uses the iverson bracket and has the value 1 if x has the value green 0 otherwise then the expected value and other moments of this function can be determined\u000a\u000a\u000a functions of random variablesedit \u000aa new random variable y can be defined by applying a real borel measurable function  to the outcomes of a realvalued random variable x the cumulative distribution function of  is\u000a\u000aif function g is invertible ie g1 exists and is either increasing or decreasing then the previous relation can be extended to obtain\u000a\u000aand again with the same hypotheses of invertibility of g assuming also differentiability we can find the relation between the probability density functions by differentiating both sides with respect to y in order to obtain\u000a\u000aif there is no invertibility of g but each y admits at most a countable number of roots ie a finite or countably infinite number of xi such that y  gxi then the previous relation between the probability density functions can be generalized with\u000a\u000awhere xi  gi1y the formulas for densities do not demand g to be increasing\u000ain the measuretheoretic axiomatic approach to probability if we have a random variable  on  and a borel measurable function  then  will also be a random variable on  since the composition of measurable functions is also measurable however this is not true if  is lebesgue measurable the same procedure that allowed one to go from a probability space  to  can be used to obtain the distribution of \u000a\u000a\u000a example 1edit \u000alet x be a realvalued continuous random variable and let y  x2\u000a\u000aif y  0 then px2  y  0 so\u000a\u000aif y  0 then\u000a\u000aso\u000a\u000a\u000a example 2edit \u000asuppose  is a random variable with a cumulative distribution\u000a\u000awhere  is a fixed parameter consider the random variable  then\u000a\u000athe last expression can be calculated in terms of the cumulative distribution of  so\u000a\u000awhich is the cdf of an exponential distribution\u000a\u000a\u000a example 3edit \u000asuppose  is a random variable with a standard normal distribution whose density is\u000a\u000aconsider the random variable  we can find the density using the above formula for a change of variables\u000a\u000ain this case the change is not monotonic because every value of  has two corresponding values of  one positive and negative however because of symmetry both halves will transform identically ie\u000a\u000athe inverse transformation is\u000a\u000aand its derivative is\u000a\u000athen\u000a\u000athis is a chisquared distribution with one degree of freedom\u000a\u000a\u000a equivalence of random variablesedit \u000athere are several different senses in which random variables can be considered to be equivalent two random variables can be equal equal almost surely or equal in distribution\u000ain increasing order of strength the precise definition of these notions of equivalence is given below\u000a\u000a\u000a equality in distributionedit \u000aif the sample space is a subset of the real line random variables x and y are equal in distribution denoted  if they have the same distribution functions\u000a\u000atwo random variables having equal moment generating functions have the same distribution this provides for example a useful method of checking equality of certain functions of iid random variables however the moment generating function exists only for distributions that have a defined laplace transform\u000a\u000a\u000a almost sure equalityedit \u000atwo random variables x and y are equal almost surely if and only if the probability that they are different is zero\u000a\u000afor all practical purposes in probability theory this notion of equivalence is as strong as actual equality it is associated to the following distance\u000a\u000awhere ess sup represents the essential supremum in the sense of measure theory\u000a\u000a\u000a equalityedit \u000afinally the two random variables x and y are equal if they are equal as functions on their measurable space\u000a\u000a\u000a convergenceedit \u000aa significant theme in mathematical statistics consists of obtaining convergence results for certain sequences of random variables for instance the law of large numbers and the central limit theorem\u000athere are various senses in which a sequence xn of random variables can converge to a random variable x these are explained in the article on convergence of random variables\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a literatureedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 random variable encyclopedia of mathematics springer isbn 9781556080104 \u000azukerman moshe 2014 introduction to queueing theory and stochastic teletraffic models pdf \u000azukerman moshe 2014 basic probability topics pdf
p70
sg14
g17
sg18
Vin probability and statistics a random variable aleatory variable or stochastic variable is a variable whose value is subject to variations due to chance ie randomness in a mathematical sense a random variable can take on a set of possible different values similarly to other mathematical variables each with an associated probability in contrast to other mathematical variables\u000aa random variables possible values might represent the possible outcomes of a yettobeperformed experiment or the possible outcomes of a past experiment whose alreadyexisting value is uncertain for example due to imprecise measurements or quantum uncertainty they may also conceptually represent either the results of an objectively random process such as rolling a die or the subjective randomness that results from incomplete knowledge of a quantity the meaning of the probabilities assigned to the potential values of a random variable is not part of probability theory itself but is instead related to philosophical arguments over the interpretation of probability the mathematics works the same regardless of the particular interpretation in use\u000athe mathematical function describing the possible values of a random variable and their associated probabilities is known as a probability distribution random variables can be discrete that is taking any of a specified finite or countable list of values endowed with a probability mass function characteristic of a probability distribution or continuous taking any numerical value in an interval or collection of intervals via a probability density function that is characteristic of a probability distribution or a mixture of both types the realizations of a random variable that is the results of randomly choosing values according to the variables probability distribution function are called random variates\u000athe formal mathematical treatment of random variables is a topic in probability theory in that context a random variable is understood as a function defined on a sample space whose outputs are numerical values\u000a\u000a\u000a definitionedit \u000aa random variable  is a measurable function from the set of possible outcomes  to some set  the technical axiomatic definition requires  to be a probability space and  to be a measurable space see measuretheoretic definition\u000anote that although  is usually a realvalued function  it does not return a probability the probabilities of different outcomes or sets of outcomes events are already given by the probability measure  with which  is equipped rather  describes some numerical property that outcomes in  may have eg the number of heads in a random collection of coin flips the height of a random person the probability that  takes value  is the measure of the set of outcomes  denoted \u000a\u000a\u000a standard caseedit \u000ausually   otherwise the term random element is used see extensions\u000awhen the image or range of  is finite or countably infinite the random variable is called a discrete random variable and its distribution can be described by a probability mass function which assigns a probability to each value in the image of  if the image is uncountably infinite then  is called a continuous random variable in the special case that it is absolutely continuous its distribution can be described by a probability density function which assigns probabilities to intervals in particular each individual point must necessarily have probability zero for an absolutely continuous random variable not all continuous random variables are absolutely continuous for example a mixture distribution such random variables cannot be described by a probability density or a probability mass function\u000aany random variable can be described by its cumulative distribution function which describes the probability that the random variable will be less than or equal to a certain value\u000a\u000a\u000a extensionsedit \u000athe term random variable in statistics is traditionally limited to the realvalued case  this ensures that it is possible to define quantities such as the expected value and variance of a random variable its cumulative distribution function and the moments of its distribution\u000ahowever the definition above is valid for any measurable space  of values thus one can consider random elements of other sets  such as random boolean values categorical values complex numbers vectors matrices sequences trees sets shapes manifolds and functions one may then specifically refer to a random variable of type  or an valued random variable\u000athis more general concept of a random element is particularly useful in disciplines such as graph theory machine learning natural language processing and other fields in discrete mathematics and computer science where one is often interested in modeling the random variation of nonnumerical data structures in some cases it is nonetheless convenient to represent each element of  using one or more real numbers in this case a random element may optionally be represented as a vector of realvalued random variables all defined on the same underlying probability space  which allows the different random variables to covary for example\u000aa random word may be represented as a random integer that serves as an index into the vocabulary of possible words alternatively it can be represented as a random indicator vector whose length equals the size of the vocabulary where the only values of positive probability are 1 0 0 0  0 1 0 0  0 0 1 0  and the position of the 1 indicates the word\u000aa random sentence of given length  may be represented as a vector of  random words\u000aa random graph on  given vertices may be represented as a  matrix of random variables whose values specify the adjacency matrix of the random graph\u000aa random function  may be represented as a collection of random variables  giving the functions values at the various points  in the functions domain the  are ordinary realvalued random variables provided that the function is realvalued for example a stochastic process is a random function of time a random vector is a random function of some index set such as  and random field is a random function on any set typically time space or a discrete set\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete random variableedit \u000ain an experiment a person may be chosen at random and one random variable may be the persons height mathematically the random variable is interpreted as a function which maps the person to the persons height associated with the random variable is a probability distribution that allows the computation of the probability that the height is in any subset of possible values such as the probability that the height is between 180 and 190 cm or the probability that the height is either less than 150 or more than 200 cm\u000aanother random variable may be the persons number of children this is a discrete random variable with nonnegative integer values it allows the computation of probabilities for individual integer values  the probability mass function pmf  or for sets of values including infinite sets for example the event of interest may be an even number of children for both finite and infinite event sets their probabilities can be found by adding up the pmfs of the elements that is the probability of an even number of children is the infinite sum pmf0  pmf2  pmf4  \u000ain examples such as these the sample space the set of all possible persons is often suppressed since it is mathematically hard to describe and the possible values of the random variables are then treated as a sample space but when two random variables are measured on the same sample space of outcomes such as the height and number of children being computed on the same random persons it is easier to track their relationship if it is acknowledged that both height and number of children come from the same random person for example so that questions of whether such random variables are correlated or not can be posed\u000a\u000a\u000a coin tossedit \u000athe possible outcomes for one coin toss can be described by the sample space  we can introduce a realvalued random variable  that models a 1 payoff for a successful bet on heads as follows\u000a\u000aif the coin is a fair coin y has a probability mass function  given by\u000a\u000a\u000a die rolledit \u000a\u000aa random variable can also be used to describe the process of rolling dice and the possible outcomes the most obvious representation for the twodice case is to take the set of pairs of numbers n1 and n2 from 1 2 3 4 5 6 representing the numbers on the two dice as the sample space the total number rolled the sum of the numbers in each pair is then a random variable x given by the function that maps the pair to the sum\u000a\u000aand if the dice are fair has a probability mass function x given by\u000a\u000a\u000a continuous random variableedit \u000aan example of a continuous random variable would be one based on a spinner that can choose a horizontal direction then the values taken by the random variable are directions we could represent these directions by north west east south southeast etc however it is commonly more convenient to map the sample space to a random variable which takes values which are real numbers this can be done for example by mapping a direction to a bearing in degrees clockwise from north the random variable then takes values which are real numbers from the interval 0 360 with all parts of the range being equally likely in this case x  the angle spun any real number has probability zero of being selected but a positive probability can be assigned to any range of values for example the probability of choosing a number in 0 180 is 12 instead of speaking of a probability mass function we say that the probability density of x is 1360 the probability of a subset of 0 360 can be calculated by multiplying the measure of the set by 1360 in general the probability of a set for a given continuous random variable can be calculated by integrating the density over the given set\u000a\u000a\u000a mixed typeedit \u000aan example of a random variable of mixed type would be based on an experiment where a coin is flipped and the spinner is spun only if the result of the coin toss is heads if the result is tails x  1 otherwise x  the value of the spinner as in the preceding example there is a probability of 12 that this random variable will have the value 1 other ranges of values would have half the probability of the last example\u000a\u000a\u000a measuretheoretic definitionedit \u000athe most formal axiomatic definition of a random variable involves measure theory continuous random variables are defined in terms of sets of numbers along with functions that map such sets to probabilities because of various difficulties eg the banachtarski paradox that arise if such sets are insufficiently constrained it is necessary to introduce what is termed a sigmaalgebra to constrain the possible sets over which probabilities can be defined normally a particular such sigmaalgebra is used the borel algebra which allows for probabilities to be defined over any sets that can be derived either directly from continuous intervals of numbers or by a finite or countably infinite number of unions andor intersections of such intervals\u000athe measuretheoretic definition is as follows\u000alet  be a probability space and  a measurable space then an valued random variable is a function  which is measurable the latter means that for every subset  its preimage  where  this definition enables us to measure any subset  in the target space by looking at its preimage which by assumption is measurable\u000awhen  is a topological space then the most common choice for the algebra  is the borel algebra  which is the algebra generated by the collection of all open sets in  in such case the valued random variable is called the valued random variable moreover when space  is the real line  then such a realvalued random variable is called simply the random variable\u000a\u000a\u000a realvalued random variablesedit \u000ain this case the observation space is the set of real numbers recall  is the probability space for real observation space the function  is a realvalued random variable if\u000a\u000athis definition is a special case of the above because the set  generates the borel algebra on the set of real numbers and it suffices to check measurability on any generating set here we can prove measurability on this generating set by using the fact that \u000a\u000a\u000a distribution functions of random variablesedit \u000aif a random variable  defined on the probability space  is given we can ask questions like how likely is it that the value of  is equal to 2 this is the same as the probability of the event  which is often written as  or  for short\u000arecording all these probabilities of output ranges of a realvalued random variable  yields the probability distribution of  the probability distribution forgets about the particular probability space used to define  and only records the probabilities of various values of  such a probability distribution can always be captured by its cumulative distribution function\u000a\u000aand sometimes also using a probability density function  in measuretheoretic terms we use the random variable  to pushforward the measure  on  to a measure  on  the underlying probability space  is a technical device used to guarantee the existence of random variables sometimes to construct them and to define notions such as correlation and dependence or independence based on a joint distribution of two or more random variables on the same probability space in practice one often disposes of the space  altogether and just puts a measure on  that assigns measure 1 to the whole real line ie one works with probability distributions instead of random variables\u000a\u000a\u000a momentsedit \u000athe probability distribution of a random variable is often characterised by a small number of parameters which also have a practical interpretation for example it is often enough to know what its average value is this is captured by the mathematical concept of expected value of a random variable denoted ex and also called the first moment in general efx is not equal to fex once the average value is known one could then ask how far from this average value the values of x typically are a question that is answered by the variance and standard deviation of a random variable ex can be viewed intuitively as an average obtained from an infinite population the members of which are particular evaluations of x\u000amathematically this is known as the generalised problem of moments for a given class of random variables x find a collection fi of functions such that the expectation values efix fully characterise the distribution of the random variable x\u000amoments can only be defined for realvalued functions of random variables or complexvalued etc if the random variable is itself realvalued then moments of the variable itself can be taken which are equivalent to moments of the identity function  of the random variable however even for nonrealvalued random variables moments can be taken of realvalued functions of those variables for example for a categorical random variable x that can take on the nominal values red blue or green the realvalued function  can be constructed this uses the iverson bracket and has the value 1 if x has the value green 0 otherwise then the expected value and other moments of this function can be determined\u000a\u000a\u000a functions of random variablesedit \u000aa new random variable y can be defined by applying a real borel measurable function  to the outcomes of a realvalued random variable x the cumulative distribution function of  is\u000a\u000aif function g is invertible ie g1 exists and is either increasing or decreasing then the previous relation can be extended to obtain\u000a\u000aand again with the same hypotheses of invertibility of g assuming also differentiability we can find the relation between the probability density functions by differentiating both sides with respect to y in order to obtain\u000a\u000aif there is no invertibility of g but each y admits at most a countable number of roots ie a finite or countably infinite number of xi such that y  gxi then the previous relation between the probability density functions can be generalized with\u000a\u000awhere xi  gi1y the formulas for densities do not demand g to be increasing\u000ain the measuretheoretic axiomatic approach to probability if we have a random variable  on  and a borel measurable function  then  will also be a random variable on  since the composition of measurable functions is also measurable however this is not true if  is lebesgue measurable the same procedure that allowed one to go from a probability space  to  can be used to obtain the distribution of \u000a\u000a\u000a example 1edit \u000alet x be a realvalued continuous random variable and let y  x2\u000a\u000aif y  0 then px2  y  0 so\u000a\u000aif y  0 then\u000a\u000aso\u000a\u000a\u000a example 2edit \u000asuppose  is a random variable with a cumulative distribution\u000a\u000awhere  is a fixed parameter consider the random variable  then\u000a\u000athe last expression can be calculated in terms of the cumulative distribution of  so\u000a\u000awhich is the cdf of an exponential distribution\u000a\u000a\u000a example 3edit \u000asuppose  is a random variable with a standard normal distribution whose density is\u000a\u000aconsider the random variable  we can find the density using the above formula for a change of variables\u000a\u000ain this case the change is not monotonic because every value of  has two corresponding values of  one positive and negative however because of symmetry both halves will transform identically ie\u000a\u000athe inverse transformation is\u000a\u000aand its derivative is\u000a\u000athen\u000a\u000athis is a chisquared distribution with one degree of freedom\u000a\u000a\u000a equivalence of random variablesedit \u000athere are several different senses in which random variables can be considered to be equivalent two random variables can be equal equal almost surely or equal in distribution\u000ain increasing order of strength the precise definition of these notions of equivalence is given below\u000a\u000a\u000a equality in distributionedit \u000aif the sample space is a subset of the real line random variables x and y are equal in distribution denoted  if they have the same distribution functions\u000a\u000atwo random variables having equal moment generating functions have the same distribution this provides for example a useful method of checking equality of certain functions of iid random variables however the moment generating function exists only for distributions that have a defined laplace transform\u000a\u000a\u000a almost sure equalityedit \u000atwo random variables x and y are equal almost surely if and only if the probability that they are different is zero\u000a\u000afor all practical purposes in probability theory this notion of equivalence is as strong as actual equality it is associated to the following distance\u000a\u000awhere ess sup represents the essential supremum in the sense of measure theory\u000a\u000a\u000a equalityedit \u000afinally the two random variables x and y are equal if they are equal as functions on their measurable space\u000a\u000a\u000a convergenceedit \u000aa significant theme in mathematical statistics consists of obtaining convergence results for certain sequences of random variables for instance the law of large numbers and the central limit theorem\u000athere are various senses in which a sequence xn of random variables can converge to a random variable x these are explained in the article on convergence of random variables\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a literatureedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 random variable encyclopedia of mathematics springer isbn 9781556080104 \u000azukerman moshe 2014 introduction to queueing theory and stochastic teletraffic models pdf \u000azukerman moshe 2014 basic probability topics pdf
p71
sg20
g23
sg24
g27
sg30
Vin probability and statistics a random variable aleatory variable or stochastic variable is a variable whose value is subject to variations due to chance ie randomness in a mathematical sense a random variable can take on a set of possible different values similarly to other mathematical variables each with an associated probability in contrast to other mathematical variables\u000aa random variables possible values might represent the possible outcomes of a yettobeperformed experiment or the possible outcomes of a past experiment whose alreadyexisting value is uncertain for example due to imprecise measurements or quantum uncertainty they may also conceptually represent either the results of an objectively random process such as rolling a die or the subjective randomness that results from incomplete knowledge of a quantity the meaning of the probabilities assigned to the potential values of a random variable is not part of probability theory itself but is instead related to philosophical arguments over the interpretation of probability the mathematics works the same regardless of the particular interpretation in use\u000athe mathematical function describing the possible values of a random variable and their associated probabilities is known as a probability distribution random variables can be discrete that is taking any of a specified finite or countable list of values endowed with a probability mass function characteristic of a probability distribution or continuous taking any numerical value in an interval or collection of intervals via a probability density function that is characteristic of a probability distribution or a mixture of both types the realizations of a random variable that is the results of randomly choosing values according to the variables probability distribution function are called random variates\u000athe formal mathematical treatment of random variables is a topic in probability theory in that context a random variable is understood as a function defined on a sample space whose outputs are numerical values\u000a\u000a\u000a definitionedit \u000aa random variable  is a measurable function from the set of possible outcomes  to some set  the technical axiomatic definition requires  to be a probability space and  to be a measurable space see measuretheoretic definition\u000anote that although  is usually a realvalued function  it does not return a probability the probabilities of different outcomes or sets of outcomes events are already given by the probability measure  with which  is equipped rather  describes some numerical property that outcomes in  may have eg the number of heads in a random collection of coin flips the height of a random person the probability that  takes value  is the measure of the set of outcomes  denoted \u000a\u000a\u000a standard caseedit \u000ausually   otherwise the term random element is used see extensions\u000awhen the image or range of  is finite or countably infinite the random variable is called a discrete random variable and its distribution can be described by a probability mass function which assigns a probability to each value in the image of  if the image is uncountably infinite then  is called a continuous random variable in the special case that it is absolutely continuous its distribution can be described by a probability density function which assigns probabilities to intervals in particular each individual point must necessarily have probability zero for an absolutely continuous random variable not all continuous random variables are absolutely continuous for example a mixture distribution such random variables cannot be described by a probability density or a probability mass function\u000aany random variable can be described by its cumulative distribution function which describes the probability that the random variable will be less than or equal to a certain value\u000a\u000a\u000a extensionsedit \u000athe term random variable in statistics is traditionally limited to the realvalued case  this ensures that it is possible to define quantities such as the expected value and variance of a random variable its cumulative distribution function and the moments of its distribution\u000ahowever the definition above is valid for any measurable space  of values thus one can consider random elements of other sets  such as random boolean values categorical values complex numbers vectors matrices sequences trees sets shapes manifolds and functions one may then specifically refer to a random variable of type  or an valued random variable\u000athis more general concept of a random element is particularly useful in disciplines such as graph theory machine learning natural language processing and other fields in discrete mathematics and computer science where one is often interested in modeling the random variation of nonnumerical data structures in some cases it is nonetheless convenient to represent each element of  using one or more real numbers in this case a random element may optionally be represented as a vector of realvalued random variables all defined on the same underlying probability space  which allows the different random variables to covary for example\u000aa random word may be represented as a random integer that serves as an index into the vocabulary of possible words alternatively it can be represented as a random indicator vector whose length equals the size of the vocabulary where the only values of positive probability are 1 0 0 0  0 1 0 0  0 0 1 0  and the position of the 1 indicates the word\u000aa random sentence of given length  may be represented as a vector of  random words\u000aa random graph on  given vertices may be represented as a  matrix of random variables whose values specify the adjacency matrix of the random graph\u000aa random function  may be represented as a collection of random variables  giving the functions values at the various points  in the functions domain the  are ordinary realvalued random variables provided that the function is realvalued for example a stochastic process is a random function of time a random vector is a random function of some index set such as  and random field is a random function on any set typically time space or a discrete set\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete random variableedit \u000ain an experiment a person may be chosen at random and one random variable may be the persons height mathematically the random variable is interpreted as a function which maps the person to the persons height associated with the random variable is a probability distribution that allows the computation of the probability that the height is in any subset of possible values such as the probability that the height is between 180 and 190 cm or the probability that the height is either less than 150 or more than 200 cm\u000aanother random variable may be the persons number of children this is a discrete random variable with nonnegative integer values it allows the computation of probabilities for individual integer values  the probability mass function pmf  or for sets of values including infinite sets for example the event of interest may be an even number of children for both finite and infinite event sets their probabilities can be found by adding up the pmfs of the elements that is the probability of an even number of children is the infinite sum pmf0  pmf2  pmf4  \u000ain examples such as these the sample space the set of all possible persons is often suppressed since it is mathematically hard to describe and the possible values of the random variables are then treated as a sample space but when two random variables are measured on the same sample space of outcomes such as the height and number of children being computed on the same random persons it is easier to track their relationship if it is acknowledged that both height and number of children come from the same random person for example so that questions of whether such random variables are correlated or not can be posed\u000a\u000a\u000a coin tossedit \u000athe possible outcomes for one coin toss can be described by the sample space  we can introduce a realvalued random variable  that models a 1 payoff for a successful bet on heads as follows\u000a\u000aif the coin is a fair coin y has a probability mass function  given by\u000a\u000a\u000a die rolledit \u000a\u000aa random variable can also be used to describe the process of rolling dice and the possible outcomes the most obvious representation for the twodice case is to take the set of pairs of numbers n1 and n2 from 1 2 3 4 5 6 representing the numbers on the two dice as the sample space the total number rolled the sum of the numbers in each pair is then a random variable x given by the function that maps the pair to the sum\u000a\u000aand if the dice are fair has a probability mass function x given by\u000a\u000a\u000a continuous random variableedit \u000aan example of a continuous random variable would be one based on a spinner that can choose a horizontal direction then the values taken by the random variable are directions we could represent these directions by north west east south southeast etc however it is commonly more convenient to map the sample space to a random variable which takes values which are real numbers this can be done for example by mapping a direction to a bearing in degrees clockwise from north the random variable then takes values which are real numbers from the interval 0 360 with all parts of the range being equally likely in this case x  the angle spun any real number has probability zero of being selected but a positive probability can be assigned to any range of values for example the probability of choosing a number in 0 180 is 12 instead of speaking of a probability mass function we say that the probability density of x is 1360 the probability of a subset of 0 360 can be calculated by multiplying the measure of the set by 1360 in general the probability of a set for a given continuous random variable can be calculated by integrating the density over the given set\u000a\u000a\u000a mixed typeedit \u000aan example of a random variable of mixed type would be based on an experiment where a coin is flipped and the spinner is spun only if the result of the coin toss is heads if the result is tails x  1 otherwise x  the value of the spinner as in the preceding example there is a probability of 12 that this random variable will have the value 1 other ranges of values would have half the probability of the last example\u000a\u000a\u000a measuretheoretic definitionedit \u000athe most formal axiomatic definition of a random variable involves measure theory continuous random variables are defined in terms of sets of numbers along with functions that map such sets to probabilities because of various difficulties eg the banachtarski paradox that arise if such sets are insufficiently constrained it is necessary to introduce what is termed a sigmaalgebra to constrain the possible sets over which probabilities can be defined normally a particular such sigmaalgebra is used the borel algebra which allows for probabilities to be defined over any sets that can be derived either directly from continuous intervals of numbers or by a finite or countably infinite number of unions andor intersections of such intervals\u000athe measuretheoretic definition is as follows\u000alet  be a probability space and  a measurable space then an valued random variable is a function  which is measurable the latter means that for every subset  its preimage  where  this definition enables us to measure any subset  in the target space by looking at its preimage which by assumption is measurable\u000awhen  is a topological space then the most common choice for the algebra  is the borel algebra  which is the algebra generated by the collection of all open sets in  in such case the valued random variable is called the valued random variable moreover when space  is the real line  then such a realvalued random variable is called simply the random variable\u000a\u000a\u000a realvalued random variablesedit \u000ain this case the observation space is the set of real numbers recall  is the probability space for real observation space the function  is a realvalued random variable if\u000a\u000athis definition is a special case of the above because the set  generates the borel algebra on the set of real numbers and it suffices to check measurability on any generating set here we can prove measurability on this generating set by using the fact that \u000a\u000a\u000a distribution functions of random variablesedit \u000aif a random variable  defined on the probability space  is given we can ask questions like how likely is it that the value of  is equal to 2 this is the same as the probability of the event  which is often written as  or  for short\u000arecording all these probabilities of output ranges of a realvalued random variable  yields the probability distribution of  the probability distribution forgets about the particular probability space used to define  and only records the probabilities of various values of  such a probability distribution can always be captured by its cumulative distribution function\u000a\u000aand sometimes also using a probability density function  in measuretheoretic terms we use the random variable  to pushforward the measure  on  to a measure  on  the underlying probability space  is a technical device used to guarantee the existence of random variables sometimes to construct them and to define notions such as correlation and dependence or independence based on a joint distribution of two or more random variables on the same probability space in practice one often disposes of the space  altogether and just puts a measure on  that assigns measure 1 to the whole real line ie one works with probability distributions instead of random variables\u000a\u000a\u000a momentsedit \u000athe probability distribution of a random variable is often characterised by a small number of parameters which also have a practical interpretation for example it is often enough to know what its average value is this is captured by the mathematical concept of expected value of a random variable denoted ex and also called the first moment in general efx is not equal to fex once the average value is known one could then ask how far from this average value the values of x typically are a question that is answered by the variance and standard deviation of a random variable ex can be viewed intuitively as an average obtained from an infinite population the members of which are particular evaluations of x\u000amathematically this is known as the generalised problem of moments for a given class of random variables x find a collection fi of functions such that the expectation values efix fully characterise the distribution of the random variable x\u000amoments can only be defined for realvalued functions of random variables or complexvalued etc if the random variable is itself realvalued then moments of the variable itself can be taken which are equivalent to moments of the identity function  of the random variable however even for nonrealvalued random variables moments can be taken of realvalued functions of those variables for example for a categorical random variable x that can take on the nominal values red blue or green the realvalued function  can be constructed this uses the iverson bracket and has the value 1 if x has the value green 0 otherwise then the expected value and other moments of this function can be determined\u000a\u000a\u000a functions of random variablesedit \u000aa new random variable y can be defined by applying a real borel measurable function  to the outcomes of a realvalued random variable x the cumulative distribution function of  is\u000a\u000aif function g is invertible ie g1 exists and is either increasing or decreasing then the previous relation can be extended to obtain\u000a\u000aand again with the same hypotheses of invertibility of g assuming also differentiability we can find the relation between the probability density functions by differentiating both sides with respect to y in order to obtain\u000a\u000aif there is no invertibility of g but each y admits at most a countable number of roots ie a finite or countably infinite number of xi such that y  gxi then the previous relation between the probability density functions can be generalized with\u000a\u000awhere xi  gi1y the formulas for densities do not demand g to be increasing\u000ain the measuretheoretic axiomatic approach to probability if we have a random variable  on  and a borel measurable function  then  will also be a random variable on  since the composition of measurable functions is also measurable however this is not true if  is lebesgue measurable the same procedure that allowed one to go from a probability space  to  can be used to obtain the distribution of \u000a\u000a\u000a example 1edit \u000alet x be a realvalued continuous random variable and let y  x2\u000a\u000aif y  0 then px2  y  0 so\u000a\u000aif y  0 then\u000a\u000aso\u000a\u000a\u000a example 2edit \u000asuppose  is a random variable with a cumulative distribution\u000a\u000awhere  is a fixed parameter consider the random variable  then\u000a\u000athe last expression can be calculated in terms of the cumulative distribution of  so\u000a\u000awhich is the cdf of an exponential distribution\u000a\u000a\u000a example 3edit \u000asuppose  is a random variable with a standard normal distribution whose density is\u000a\u000aconsider the random variable  we can find the density using the above formula for a change of variables\u000a\u000ain this case the change is not monotonic because every value of  has two corresponding values of  one positive and negative however because of symmetry both halves will transform identically ie\u000a\u000athe inverse transformation is\u000a\u000aand its derivative is\u000a\u000athen\u000a\u000athis is a chisquared distribution with one degree of freedom\u000a\u000a\u000a equivalence of random variablesedit \u000athere are several different senses in which random variables can be considered to be equivalent two random variables can be equal equal almost surely or equal in distribution\u000ain increasing order of strength the precise definition of these notions of equivalence is given below\u000a\u000a\u000a equality in distributionedit \u000aif the sample space is a subset of the real line random variables x and y are equal in distribution denoted  if they have the same distribution functions\u000a\u000atwo random variables having equal moment generating functions have the same distribution this provides for example a useful method of checking equality of certain functions of iid random variables however the moment generating function exists only for distributions that have a defined laplace transform\u000a\u000a\u000a almost sure equalityedit \u000atwo random variables x and y are equal almost surely if and only if the probability that they are different is zero\u000a\u000afor all practical purposes in probability theory this notion of equivalence is as strong as actual equality it is associated to the following distance\u000a\u000awhere ess sup represents the essential supremum in the sense of measure theory\u000a\u000a\u000a equalityedit \u000afinally the two random variables x and y are equal if they are equal as functions on their measurable space\u000a\u000a\u000a convergenceedit \u000aa significant theme in mathematical statistics consists of obtaining convergence results for certain sequences of random variables for instance the law of large numbers and the central limit theorem\u000athere are various senses in which a sequence xn of random variables can converge to a random variable x these are explained in the article on convergence of random variables\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a literatureedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 random variable encyclopedia of mathematics springer isbn 9781556080104 \u000azukerman moshe 2014 introduction to queueing theory and stochastic teletraffic models pdf \u000azukerman moshe 2014 basic probability topics pdf
p72
sg32
g35
sg37
NsbsS'statistical_inference.txt'
p73
g2
(g3
g4
Ntp74
Rp75
(dp76
g8
g11
sg12
Vstatistical inference is the process of deducing properties of an underlying distribution by analysis of data inferential statistical analysis infers properties about a population this includes testing hypotheses and deriving estimates the population is assumed to be larger than the observed data set in other words the observed data is assumed to be sampled from a larger population\u000ainferential statistics can be contrasted with descriptive statistics descriptive statistics is solely concerned with properties of the observed data and does not assume that the data came from a larger population\u000a\u000a\u000a introduction \u000astatistical inference makes propositions about a population using data drawn from the population with some form of sampling given a hypothesis about a population for which we wish to draw inferences statistical inference consists of firstly selecting a statistical model of the process that generates the data and secondly deducing propositions from the model\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000athe conclusion of a statistical inference is a statistical proposition some common forms of statistical proposition are the following\u000aa point estimate ie a particular value that best approximates some parameter of interest\u000aan interval estimate eg a confidence interval or set estimate ie an interval constructed using a dataset drawn from a population so that under repeated sampling of such datasets such intervals would contain the true parameter value with the probability at the stated confidence level\u000aa credible interval ie a set of values containing for example 95 of posterior belief\u000arejection of a hypothesis\u000aclustering or classification of data points into groups\u000a\u000a\u000a models and assumptions \u000a\u000aany statistical inference requires some assumptions a statistical model is a set of assumptions concerning the generation of the observed data and similar data descriptions of statistical models usually emphasize the role of population quantities of interest about which we wish to draw inference descriptive statistics are typically used as a preliminary step before more formal inferences are drawn\u000a\u000a\u000a degree of modelsassumptions \u000astatisticians distinguish between three levels of modeling assumptions\u000afully parametric the probability distributions describing the datageneration process are assumed to be fully described by a family of probability distributions involving only a finite number of unknown parameters for example one may assume that the distribution of population values is truly normal with unknown mean and variance and that datasets are generated by simple random sampling the family of generalized linear models is a widely used and flexible class of parametric models\u000anonparametric the assumptions made about the process generating the data are much less than in parametric statistics and may be minimal for example every continuous probability distribution has a median which may be estimated using the sample median or the hodgeslehmannsen estimator which has good properties when the data arise from simple random sampling\u000asemiparametric this term typically implies assumptions in between fully and nonparametric approaches for example one may assume that a population distribution has a finite mean furthermore one may assume that the mean response level in the population depends in a truly linear manner on some covariate a parametric assumption but not make any parametric assumption describing the variance around that mean ie about the presence or possible form of any heteroscedasticity more generally semiparametric models can often be separated into structural and random variation components one component is treated parametrically and the other nonparametrically the wellknown cox model is a set of semiparametric assumptions\u000a\u000a\u000a importance of valid modelsassumptions \u000awhatever level of assumption is made correctly calibrated inference in general requires these assumptions to be correct ie that the datagenerating mechanisms really have been correctly specified\u000aincorrect assumptions of simple random sampling can invalidate statistical inference more complex semi and fully parametric assumptions are also cause for concern for example incorrectly assuming the cox model can in some cases lead to faulty conclusions incorrect assumptions of normality in the population also invalidates some forms of regressionbased inference the use of any parametric model is viewed skeptically by most experts in sampling human populations most sampling statisticians when they deal with confidence intervals at all limit themselves to statements about estimators based on very large samples where the central limit theorem ensures that these estimators will have distributions that are nearly normal in particular a normal distribution would be a totally unrealistic and catastrophically unwise assumption to make if we were dealing with any kind of economic population here the central limit theorem states that the distribution of the sample mean for very large samples is approximately normally distributed if the distribution is not heavy tailed\u000a\u000a\u000a approximate distributions \u000a\u000agiven the difficulty in specifying exact distributions of sample statistics many methods have been developed for approximating these\u000awith finite samples approximation results measure how close a limiting distribution approaches the statistics sample distribution for example with 10000 independent samples the normal distribution approximates to two digits of accuracy the distribution of the sample mean for many population distributions by the berryesseen theorem yet for many practical purposes the normal approximation provides a good approximation to the samplemeans distribution when there are 10 or more independent samples according to simulation studies and statisticians experience following kolmogorovs work in the 1950s advanced statistics uses approximation theory and functional analysis to quantify the error of approximation in this approach the metric geometry of probability distributions is studied this approach quantifies approximation error with for example the kullbackleibler divergence bregman divergence and the hellinger distance\u000awith indefinitely large samples limiting results like the central limit theorem describe the sample statistics limiting distribution if one exists limiting results are not statements about finite samples and indeed are irrelevant to finite samples however the asymptotic theory of limiting distributions is often invoked for work with finite samples for example limiting results are often invoked to justify the generalized method of moments and the use of generalized estimating equations which are popular in econometrics and biostatistics the magnitude of the difference between the limiting distribution and the true distribution formally the error of the approximation can be assessed using simulation the heuristic application of limiting results to finite samples is common practice in many applications especially with lowdimensional models with logconcave likelihoods such as with oneparameter exponential families\u000a\u000a\u000a randomizationbased models \u000a\u000afor a given dataset that was produced by a randomization design the randomization distribution of a statistic under the nullhypothesis is defined by evaluating the test statistic for all of the plans that could have been generated by the randomization design in frequentist inference randomization allows inferences to be based on the randomization distribution rather than a subjective model and this is important especially in survey sampling and design of experiments statistical inference from randomized studies is also more straightforward than many other situations in bayesian inference randomization is also of importance in survey sampling use of sampling without replacement ensures the exchangeability of the sample with the population in randomized experiments randomization warrants a missing at random assumption for covariate information\u000aobjective randomization allows properly inductive procedures many statisticians prefer randomizationbased analysis of data that was generated by welldefined randomization procedures however it is true that in fields of science with developed theoretical knowledge and experimental control randomized experiments may increase the costs of experimentation without improving the quality of inferences similarly results from randomized experiments are recommended by leading statistical authorities as allowing inferences with greater reliability than do observational studies of the same phenomena however a good observational study may be better than a bad randomized experiment\u000athe statistical analysis of a randomized experiment may be based on the randomization scheme stated in the experimental protocol and does not need a subjective model\u000ahowever at any time some hypotheses cannot be tested using objective statistical models which accurately describe randomized experiments or random samples in some cases such randomized studies are uneconomical or unethical\u000a\u000a\u000a modelbased analysis of randomized experiments \u000ait is standard practice to refer to a statistical model often a linear model when analyzing data from randomized experiments however the randomization scheme guides the choice of a statistical model it is not possible to choose an appropriate model without knowing the randomization scheme seriously misleading results can be obtained analyzing data from randomized experiments while ignoring the experimental protocol common mistakes include forgetting the blocking used in an experiment and confusing repeated measurements on the same experimental unit with independent replicates of the treatment applied to different experimental units\u000a\u000a\u000a paradigms for inference \u000adifferent schools of statistical inference have become established these schoolsor paradigmsare not mutually exclusive and methods that work well under one paradigm often have attractive interpretations under other paradigms\u000abandyopadhyay  forster describe four paradigms i classical statistics or error statistics ii bayesian statistics iii likelihoodbased statistics and iv the akaikeaninformation criterionbased statistics the classical or frequentist paradigm the bayesian paradigm and the aicbased paradigm are summarized below the likelihoodbased paradigm is essentially a subparadigm of the aicbased paradigm\u000a\u000a\u000a frequentist inference \u000a\u000athis paradigm calibrates the production of propositions by considering notional repeated sampling of datasets similar to the one at hand by considering its characteristics under repeated sample the frequentist properties of any statistical inference procedure can be described  although in practice this quantification may be challenging\u000a\u000a\u000a examples of frequentist inference \u000apvalue\u000aconfidence interval\u000a\u000a\u000a frequentist inference objectivity and decision theory \u000aone interpretation of frequentist inference or classical inference is that it is applicable only in terms of frequency probability that is in terms of repeated sampling from a population however the approach of neyman develops these procedures in terms of preexperiment probabilities that is before undertaking an experiment one decides on a rule for coming to a conclusion such that the probability of being correct is controlled in a suitable way such a probability need not have a frequentist or repeated sampling interpretation in contrast bayesian inference works in terms of conditional probabilities ie probabilities conditional on the observed data compared to the marginal but conditioned on unknown parameters probabilities used in the frequentist approach\u000athe frequentist procedures of significance testing and confidence intervals can be constructed without regard to utility functions however some elements of frequentist statistics such as statistical decision theory do incorporate utility functions in particular frequentist developments of optimal inference such as minimumvariance unbiased estimators or uniformly most powerful testing make use of loss functions which play the role of negative utility functions loss functions need not be explicitly stated for statistical theorists to prove that a statistical procedure has an optimality property however lossfunctions are often useful for stating optimality properties for example medianunbiased estimators are optimal under absolute value loss functions in that they minimize expected loss and least squares estimators are optimal under squared error loss functions in that they minimize expected loss\u000awhile statisticians using frequentist inference must choose for themselves the parameters of interest and the estimatorstest statistic to be used the absence of obviously explicit utilities and prior distributions has helped frequentist procedures to become widely viewed as objective\u000a\u000a\u000a bayesian inference \u000a\u000athe bayesian calculus describes degrees of belief using the language of probability beliefs are positive integrate to one and obey probability axioms bayesian inference uses the available posterior beliefs as the basis for making statistical propositions there are several different justifications for using the bayesian approach\u000a\u000a\u000a examples of bayesian inference \u000acredible interval for interval estimation\u000abayes factors for model comparison\u000a\u000a\u000a bayesian inference subjectivity and decision theory \u000amany informal bayesian inferences are based on intuitively reasonable summaries of the posterior for example the posterior mean median and mode highest posterior density intervals and bayes factors can all be motivated in this way while a users utility function need not be stated for this sort of inference these summaries do all depend to some extent on stated prior beliefs and are generally viewed as subjective conclusions methods of prior construction which do not require external input have been proposed but not yet fully developed\u000aformally bayesian inference is calibrated with reference to an explicitly stated utility or loss function the bayes rule is the one which maximizes expected utility averaged over the posterior uncertainty formal bayesian inference therefore automatically provides optimal decisions in a decision theoretic sense given assumptions data and utility bayesian inference can be made for essentially any problem although not every statistical inference need have a bayesian interpretation analyses which are not formally bayesian can be logically incoherent a feature of bayesian procedures which use proper priors ie those integrable to one is that they are guaranteed to be coherent some advocates of bayesian inference assert that inference must take place in this decisiontheoretic framework and that bayesian inference should not conclude with the evaluation and summarization of posterior beliefs\u000a\u000a\u000a aicbased inference \u000a\u000a\u000a other paradigms for inference \u000a\u000a\u000a minimum description length \u000a\u000athe minimum description length mdl principle has been developed from ideas in information theory and the theory of kolmogorov complexity the mdl principle selects statistical models that maximally compress the data inference proceeds without assuming counterfactual or nonfalsifiable datagenerating mechanisms or probability models for the data as might be done in frequentist or bayesian approaches\u000ahowever if a data generating mechanism does exist in reality then according to shannons source coding theorem it provides the mdl description of the data on average and asymptotically in minimizing description length or descriptive complexity mdl estimation is similar to maximum likelihood estimation and maximum a posteriori estimation using maximumentropy bayesian priors however mdl avoids assuming that the underlying probability model is known the mdl principle can also be applied without assumptions that eg the data arose from independent sampling\u000athe mdl principle has been applied in communicationcoding theory in information theory in linear regression and in data mining\u000athe evaluation of mdlbased inferential procedures often uses techniques or criteria from computational complexity theory\u000a\u000a\u000a fiducial inference \u000a\u000afiducial inference was an approach to statistical inference based on fiducial probability also known as a fiducial distribution in subsequent work this approach has been called illdefined extremely limited in applicability and even fallacious however this argument is the same as that which shows that a socalled confidence distribution is not a valid probability distribution and since this has not invalidated the application of confidence intervals it does not necessarily invalidate conclusions drawn from fiducial arguments\u000a\u000a\u000a structural inference \u000adeveloping ideas of fisher and of pitman from 1938 to 1939 george a barnard developed structural inference or pivotal inference an approach using invariant probabilities on group families barnard reformulated the arguments behind fiducial inference on a restricted class of models on which fiducial procedures would be welldefined and useful\u000a\u000a\u000a inference topics \u000athe topics below are usually included in the area of statistical inference\u000astatistical assumptions\u000astatistical decision theory\u000aestimation theory\u000astatistical hypothesis testing\u000arevising opinions in statistics\u000adesign of experiments the analysis of variance and regression\u000asurvey sampling\u000asummarizing statistical data\u000a\u000a\u000a see also \u000aalgorithmic inference\u000ainduction philosophy\u000aphilosophy of statistics\u000apredictive inference\u000a\u000a\u000a notes \u000a\u000a\u000a
p77
sg14
g17
sg18
Vstatistical inference is the process of deducing properties of an underlying distribution by analysis of data inferential statistical analysis infers properties about a population this includes testing hypotheses and deriving estimates the population is assumed to be larger than the observed data set in other words the observed data is assumed to be sampled from a larger population\u000ainferential statistics can be contrasted with descriptive statistics descriptive statistics is solely concerned with properties of the observed data and does not assume that the data came from a larger population\u000a\u000a\u000a introduction \u000astatistical inference makes propositions about a population using data drawn from the population with some form of sampling given a hypothesis about a population for which we wish to draw inferences statistical inference consists of firstly selecting a statistical model of the process that generates the data and secondly deducing propositions from the model\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000athe conclusion of a statistical inference is a statistical proposition some common forms of statistical proposition are the following\u000aa point estimate ie a particular value that best approximates some parameter of interest\u000aan interval estimate eg a confidence interval or set estimate ie an interval constructed using a dataset drawn from a population so that under repeated sampling of such datasets such intervals would contain the true parameter value with the probability at the stated confidence level\u000aa credible interval ie a set of values containing for example 95 of posterior belief\u000arejection of a hypothesis\u000aclustering or classification of data points into groups\u000a\u000a\u000a models and assumptions \u000a\u000aany statistical inference requires some assumptions a statistical model is a set of assumptions concerning the generation of the observed data and similar data descriptions of statistical models usually emphasize the role of population quantities of interest about which we wish to draw inference descriptive statistics are typically used as a preliminary step before more formal inferences are drawn\u000a\u000a\u000a degree of modelsassumptions \u000astatisticians distinguish between three levels of modeling assumptions\u000afully parametric the probability distributions describing the datageneration process are assumed to be fully described by a family of probability distributions involving only a finite number of unknown parameters for example one may assume that the distribution of population values is truly normal with unknown mean and variance and that datasets are generated by simple random sampling the family of generalized linear models is a widely used and flexible class of parametric models\u000anonparametric the assumptions made about the process generating the data are much less than in parametric statistics and may be minimal for example every continuous probability distribution has a median which may be estimated using the sample median or the hodgeslehmannsen estimator which has good properties when the data arise from simple random sampling\u000asemiparametric this term typically implies assumptions in between fully and nonparametric approaches for example one may assume that a population distribution has a finite mean furthermore one may assume that the mean response level in the population depends in a truly linear manner on some covariate a parametric assumption but not make any parametric assumption describing the variance around that mean ie about the presence or possible form of any heteroscedasticity more generally semiparametric models can often be separated into structural and random variation components one component is treated parametrically and the other nonparametrically the wellknown cox model is a set of semiparametric assumptions\u000a\u000a\u000a importance of valid modelsassumptions \u000awhatever level of assumption is made correctly calibrated inference in general requires these assumptions to be correct ie that the datagenerating mechanisms really have been correctly specified\u000aincorrect assumptions of simple random sampling can invalidate statistical inference more complex semi and fully parametric assumptions are also cause for concern for example incorrectly assuming the cox model can in some cases lead to faulty conclusions incorrect assumptions of normality in the population also invalidates some forms of regressionbased inference the use of any parametric model is viewed skeptically by most experts in sampling human populations most sampling statisticians when they deal with confidence intervals at all limit themselves to statements about estimators based on very large samples where the central limit theorem ensures that these estimators will have distributions that are nearly normal in particular a normal distribution would be a totally unrealistic and catastrophically unwise assumption to make if we were dealing with any kind of economic population here the central limit theorem states that the distribution of the sample mean for very large samples is approximately normally distributed if the distribution is not heavy tailed\u000a\u000a\u000a approximate distributions \u000a\u000agiven the difficulty in specifying exact distributions of sample statistics many methods have been developed for approximating these\u000awith finite samples approximation results measure how close a limiting distribution approaches the statistics sample distribution for example with 10000 independent samples the normal distribution approximates to two digits of accuracy the distribution of the sample mean for many population distributions by the berryesseen theorem yet for many practical purposes the normal approximation provides a good approximation to the samplemeans distribution when there are 10 or more independent samples according to simulation studies and statisticians experience following kolmogorovs work in the 1950s advanced statistics uses approximation theory and functional analysis to quantify the error of approximation in this approach the metric geometry of probability distributions is studied this approach quantifies approximation error with for example the kullbackleibler divergence bregman divergence and the hellinger distance\u000awith indefinitely large samples limiting results like the central limit theorem describe the sample statistics limiting distribution if one exists limiting results are not statements about finite samples and indeed are irrelevant to finite samples however the asymptotic theory of limiting distributions is often invoked for work with finite samples for example limiting results are often invoked to justify the generalized method of moments and the use of generalized estimating equations which are popular in econometrics and biostatistics the magnitude of the difference between the limiting distribution and the true distribution formally the error of the approximation can be assessed using simulation the heuristic application of limiting results to finite samples is common practice in many applications especially with lowdimensional models with logconcave likelihoods such as with oneparameter exponential families\u000a\u000a\u000a randomizationbased models \u000a\u000afor a given dataset that was produced by a randomization design the randomization distribution of a statistic under the nullhypothesis is defined by evaluating the test statistic for all of the plans that could have been generated by the randomization design in frequentist inference randomization allows inferences to be based on the randomization distribution rather than a subjective model and this is important especially in survey sampling and design of experiments statistical inference from randomized studies is also more straightforward than many other situations in bayesian inference randomization is also of importance in survey sampling use of sampling without replacement ensures the exchangeability of the sample with the population in randomized experiments randomization warrants a missing at random assumption for covariate information\u000aobjective randomization allows properly inductive procedures many statisticians prefer randomizationbased analysis of data that was generated by welldefined randomization procedures however it is true that in fields of science with developed theoretical knowledge and experimental control randomized experiments may increase the costs of experimentation without improving the quality of inferences similarly results from randomized experiments are recommended by leading statistical authorities as allowing inferences with greater reliability than do observational studies of the same phenomena however a good observational study may be better than a bad randomized experiment\u000athe statistical analysis of a randomized experiment may be based on the randomization scheme stated in the experimental protocol and does not need a subjective model\u000ahowever at any time some hypotheses cannot be tested using objective statistical models which accurately describe randomized experiments or random samples in some cases such randomized studies are uneconomical or unethical\u000a\u000a\u000a modelbased analysis of randomized experiments \u000ait is standard practice to refer to a statistical model often a linear model when analyzing data from randomized experiments however the randomization scheme guides the choice of a statistical model it is not possible to choose an appropriate model without knowing the randomization scheme seriously misleading results can be obtained analyzing data from randomized experiments while ignoring the experimental protocol common mistakes include forgetting the blocking used in an experiment and confusing repeated measurements on the same experimental unit with independent replicates of the treatment applied to different experimental units\u000a\u000a\u000a paradigms for inference \u000adifferent schools of statistical inference have become established these schoolsor paradigmsare not mutually exclusive and methods that work well under one paradigm often have attractive interpretations under other paradigms\u000abandyopadhyay  forster describe four paradigms i classical statistics or error statistics ii bayesian statistics iii likelihoodbased statistics and iv the akaikeaninformation criterionbased statistics the classical or frequentist paradigm the bayesian paradigm and the aicbased paradigm are summarized below the likelihoodbased paradigm is essentially a subparadigm of the aicbased paradigm\u000a\u000a\u000a frequentist inference \u000a\u000athis paradigm calibrates the production of propositions by considering notional repeated sampling of datasets similar to the one at hand by considering its characteristics under repeated sample the frequentist properties of any statistical inference procedure can be described  although in practice this quantification may be challenging\u000a\u000a\u000a examples of frequentist inference \u000apvalue\u000aconfidence interval\u000a\u000a\u000a frequentist inference objectivity and decision theory \u000aone interpretation of frequentist inference or classical inference is that it is applicable only in terms of frequency probability that is in terms of repeated sampling from a population however the approach of neyman develops these procedures in terms of preexperiment probabilities that is before undertaking an experiment one decides on a rule for coming to a conclusion such that the probability of being correct is controlled in a suitable way such a probability need not have a frequentist or repeated sampling interpretation in contrast bayesian inference works in terms of conditional probabilities ie probabilities conditional on the observed data compared to the marginal but conditioned on unknown parameters probabilities used in the frequentist approach\u000athe frequentist procedures of significance testing and confidence intervals can be constructed without regard to utility functions however some elements of frequentist statistics such as statistical decision theory do incorporate utility functions in particular frequentist developments of optimal inference such as minimumvariance unbiased estimators or uniformly most powerful testing make use of loss functions which play the role of negative utility functions loss functions need not be explicitly stated for statistical theorists to prove that a statistical procedure has an optimality property however lossfunctions are often useful for stating optimality properties for example medianunbiased estimators are optimal under absolute value loss functions in that they minimize expected loss and least squares estimators are optimal under squared error loss functions in that they minimize expected loss\u000awhile statisticians using frequentist inference must choose for themselves the parameters of interest and the estimatorstest statistic to be used the absence of obviously explicit utilities and prior distributions has helped frequentist procedures to become widely viewed as objective\u000a\u000a\u000a bayesian inference \u000a\u000athe bayesian calculus describes degrees of belief using the language of probability beliefs are positive integrate to one and obey probability axioms bayesian inference uses the available posterior beliefs as the basis for making statistical propositions there are several different justifications for using the bayesian approach\u000a\u000a\u000a examples of bayesian inference \u000acredible interval for interval estimation\u000abayes factors for model comparison\u000a\u000a\u000a bayesian inference subjectivity and decision theory \u000amany informal bayesian inferences are based on intuitively reasonable summaries of the posterior for example the posterior mean median and mode highest posterior density intervals and bayes factors can all be motivated in this way while a users utility function need not be stated for this sort of inference these summaries do all depend to some extent on stated prior beliefs and are generally viewed as subjective conclusions methods of prior construction which do not require external input have been proposed but not yet fully developed\u000aformally bayesian inference is calibrated with reference to an explicitly stated utility or loss function the bayes rule is the one which maximizes expected utility averaged over the posterior uncertainty formal bayesian inference therefore automatically provides optimal decisions in a decision theoretic sense given assumptions data and utility bayesian inference can be made for essentially any problem although not every statistical inference need have a bayesian interpretation analyses which are not formally bayesian can be logically incoherent a feature of bayesian procedures which use proper priors ie those integrable to one is that they are guaranteed to be coherent some advocates of bayesian inference assert that inference must take place in this decisiontheoretic framework and that bayesian inference should not conclude with the evaluation and summarization of posterior beliefs\u000a\u000a\u000a aicbased inference \u000a\u000a\u000a other paradigms for inference \u000a\u000a\u000a minimum description length \u000a\u000athe minimum description length mdl principle has been developed from ideas in information theory and the theory of kolmogorov complexity the mdl principle selects statistical models that maximally compress the data inference proceeds without assuming counterfactual or nonfalsifiable datagenerating mechanisms or probability models for the data as might be done in frequentist or bayesian approaches\u000ahowever if a data generating mechanism does exist in reality then according to shannons source coding theorem it provides the mdl description of the data on average and asymptotically in minimizing description length or descriptive complexity mdl estimation is similar to maximum likelihood estimation and maximum a posteriori estimation using maximumentropy bayesian priors however mdl avoids assuming that the underlying probability model is known the mdl principle can also be applied without assumptions that eg the data arose from independent sampling\u000athe mdl principle has been applied in communicationcoding theory in information theory in linear regression and in data mining\u000athe evaluation of mdlbased inferential procedures often uses techniques or criteria from computational complexity theory\u000a\u000a\u000a fiducial inference \u000a\u000afiducial inference was an approach to statistical inference based on fiducial probability also known as a fiducial distribution in subsequent work this approach has been called illdefined extremely limited in applicability and even fallacious however this argument is the same as that which shows that a socalled confidence distribution is not a valid probability distribution and since this has not invalidated the application of confidence intervals it does not necessarily invalidate conclusions drawn from fiducial arguments\u000a\u000a\u000a structural inference \u000adeveloping ideas of fisher and of pitman from 1938 to 1939 george a barnard developed structural inference or pivotal inference an approach using invariant probabilities on group families barnard reformulated the arguments behind fiducial inference on a restricted class of models on which fiducial procedures would be welldefined and useful\u000a\u000a\u000a inference topics \u000athe topics below are usually included in the area of statistical inference\u000astatistical assumptions\u000astatistical decision theory\u000aestimation theory\u000astatistical hypothesis testing\u000arevising opinions in statistics\u000adesign of experiments the analysis of variance and regression\u000asurvey sampling\u000asummarizing statistical data\u000a\u000a\u000a see also \u000aalgorithmic inference\u000ainduction philosophy\u000aphilosophy of statistics\u000apredictive inference\u000a\u000a\u000a notes
p78
sg20
g23
sg24
g27
sg30
Vstatistical inference is the process of deducing properties of an underlying distribution by analysis of data inferential statistical analysis infers properties about a population this includes testing hypotheses and deriving estimates the population is assumed to be larger than the observed data set in other words the observed data is assumed to be sampled from a larger population\u000ainferential statistics can be contrasted with descriptive statistics descriptive statistics is solely concerned with properties of the observed data and does not assume that the data came from a larger population\u000a\u000a\u000a introduction \u000astatistical inference makes propositions about a population using data drawn from the population with some form of sampling given a hypothesis about a population for which we wish to draw inferences statistical inference consists of firstly selecting a statistical model of the process that generates the data and secondly deducing propositions from the model\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000athe conclusion of a statistical inference is a statistical proposition some common forms of statistical proposition are the following\u000aa point estimate ie a particular value that best approximates some parameter of interest\u000aan interval estimate eg a confidence interval or set estimate ie an interval constructed using a dataset drawn from a population so that under repeated sampling of such datasets such intervals would contain the true parameter value with the probability at the stated confidence level\u000aa credible interval ie a set of values containing for example 95 of posterior belief\u000arejection of a hypothesis\u000aclustering or classification of data points into groups\u000a\u000a\u000a models and assumptions \u000a\u000aany statistical inference requires some assumptions a statistical model is a set of assumptions concerning the generation of the observed data and similar data descriptions of statistical models usually emphasize the role of population quantities of interest about which we wish to draw inference descriptive statistics are typically used as a preliminary step before more formal inferences are drawn\u000a\u000a\u000a degree of modelsassumptions \u000astatisticians distinguish between three levels of modeling assumptions\u000afully parametric the probability distributions describing the datageneration process are assumed to be fully described by a family of probability distributions involving only a finite number of unknown parameters for example one may assume that the distribution of population values is truly normal with unknown mean and variance and that datasets are generated by simple random sampling the family of generalized linear models is a widely used and flexible class of parametric models\u000anonparametric the assumptions made about the process generating the data are much less than in parametric statistics and may be minimal for example every continuous probability distribution has a median which may be estimated using the sample median or the hodgeslehmannsen estimator which has good properties when the data arise from simple random sampling\u000asemiparametric this term typically implies assumptions in between fully and nonparametric approaches for example one may assume that a population distribution has a finite mean furthermore one may assume that the mean response level in the population depends in a truly linear manner on some covariate a parametric assumption but not make any parametric assumption describing the variance around that mean ie about the presence or possible form of any heteroscedasticity more generally semiparametric models can often be separated into structural and random variation components one component is treated parametrically and the other nonparametrically the wellknown cox model is a set of semiparametric assumptions\u000a\u000a\u000a importance of valid modelsassumptions \u000awhatever level of assumption is made correctly calibrated inference in general requires these assumptions to be correct ie that the datagenerating mechanisms really have been correctly specified\u000aincorrect assumptions of simple random sampling can invalidate statistical inference more complex semi and fully parametric assumptions are also cause for concern for example incorrectly assuming the cox model can in some cases lead to faulty conclusions incorrect assumptions of normality in the population also invalidates some forms of regressionbased inference the use of any parametric model is viewed skeptically by most experts in sampling human populations most sampling statisticians when they deal with confidence intervals at all limit themselves to statements about estimators based on very large samples where the central limit theorem ensures that these estimators will have distributions that are nearly normal in particular a normal distribution would be a totally unrealistic and catastrophically unwise assumption to make if we were dealing with any kind of economic population here the central limit theorem states that the distribution of the sample mean for very large samples is approximately normally distributed if the distribution is not heavy tailed\u000a\u000a\u000a approximate distributions \u000a\u000agiven the difficulty in specifying exact distributions of sample statistics many methods have been developed for approximating these\u000awith finite samples approximation results measure how close a limiting distribution approaches the statistics sample distribution for example with 10000 independent samples the normal distribution approximates to two digits of accuracy the distribution of the sample mean for many population distributions by the berryesseen theorem yet for many practical purposes the normal approximation provides a good approximation to the samplemeans distribution when there are 10 or more independent samples according to simulation studies and statisticians experience following kolmogorovs work in the 1950s advanced statistics uses approximation theory and functional analysis to quantify the error of approximation in this approach the metric geometry of probability distributions is studied this approach quantifies approximation error with for example the kullbackleibler divergence bregman divergence and the hellinger distance\u000awith indefinitely large samples limiting results like the central limit theorem describe the sample statistics limiting distribution if one exists limiting results are not statements about finite samples and indeed are irrelevant to finite samples however the asymptotic theory of limiting distributions is often invoked for work with finite samples for example limiting results are often invoked to justify the generalized method of moments and the use of generalized estimating equations which are popular in econometrics and biostatistics the magnitude of the difference between the limiting distribution and the true distribution formally the error of the approximation can be assessed using simulation the heuristic application of limiting results to finite samples is common practice in many applications especially with lowdimensional models with logconcave likelihoods such as with oneparameter exponential families\u000a\u000a\u000a randomizationbased models \u000a\u000afor a given dataset that was produced by a randomization design the randomization distribution of a statistic under the nullhypothesis is defined by evaluating the test statistic for all of the plans that could have been generated by the randomization design in frequentist inference randomization allows inferences to be based on the randomization distribution rather than a subjective model and this is important especially in survey sampling and design of experiments statistical inference from randomized studies is also more straightforward than many other situations in bayesian inference randomization is also of importance in survey sampling use of sampling without replacement ensures the exchangeability of the sample with the population in randomized experiments randomization warrants a missing at random assumption for covariate information\u000aobjective randomization allows properly inductive procedures many statisticians prefer randomizationbased analysis of data that was generated by welldefined randomization procedures however it is true that in fields of science with developed theoretical knowledge and experimental control randomized experiments may increase the costs of experimentation without improving the quality of inferences similarly results from randomized experiments are recommended by leading statistical authorities as allowing inferences with greater reliability than do observational studies of the same phenomena however a good observational study may be better than a bad randomized experiment\u000athe statistical analysis of a randomized experiment may be based on the randomization scheme stated in the experimental protocol and does not need a subjective model\u000ahowever at any time some hypotheses cannot be tested using objective statistical models which accurately describe randomized experiments or random samples in some cases such randomized studies are uneconomical or unethical\u000a\u000a\u000a modelbased analysis of randomized experiments \u000ait is standard practice to refer to a statistical model often a linear model when analyzing data from randomized experiments however the randomization scheme guides the choice of a statistical model it is not possible to choose an appropriate model without knowing the randomization scheme seriously misleading results can be obtained analyzing data from randomized experiments while ignoring the experimental protocol common mistakes include forgetting the blocking used in an experiment and confusing repeated measurements on the same experimental unit with independent replicates of the treatment applied to different experimental units\u000a\u000a\u000a paradigms for inference \u000adifferent schools of statistical inference have become established these schoolsor paradigmsare not mutually exclusive and methods that work well under one paradigm often have attractive interpretations under other paradigms\u000abandyopadhyay  forster describe four paradigms i classical statistics or error statistics ii bayesian statistics iii likelihoodbased statistics and iv the akaikeaninformation criterionbased statistics the classical or frequentist paradigm the bayesian paradigm and the aicbased paradigm are summarized below the likelihoodbased paradigm is essentially a subparadigm of the aicbased paradigm\u000a\u000a\u000a frequentist inference \u000a\u000athis paradigm calibrates the production of propositions by considering notional repeated sampling of datasets similar to the one at hand by considering its characteristics under repeated sample the frequentist properties of any statistical inference procedure can be described  although in practice this quantification may be challenging\u000a\u000a\u000a examples of frequentist inference \u000apvalue\u000aconfidence interval\u000a\u000a\u000a frequentist inference objectivity and decision theory \u000aone interpretation of frequentist inference or classical inference is that it is applicable only in terms of frequency probability that is in terms of repeated sampling from a population however the approach of neyman develops these procedures in terms of preexperiment probabilities that is before undertaking an experiment one decides on a rule for coming to a conclusion such that the probability of being correct is controlled in a suitable way such a probability need not have a frequentist or repeated sampling interpretation in contrast bayesian inference works in terms of conditional probabilities ie probabilities conditional on the observed data compared to the marginal but conditioned on unknown parameters probabilities used in the frequentist approach\u000athe frequentist procedures of significance testing and confidence intervals can be constructed without regard to utility functions however some elements of frequentist statistics such as statistical decision theory do incorporate utility functions in particular frequentist developments of optimal inference such as minimumvariance unbiased estimators or uniformly most powerful testing make use of loss functions which play the role of negative utility functions loss functions need not be explicitly stated for statistical theorists to prove that a statistical procedure has an optimality property however lossfunctions are often useful for stating optimality properties for example medianunbiased estimators are optimal under absolute value loss functions in that they minimize expected loss and least squares estimators are optimal under squared error loss functions in that they minimize expected loss\u000awhile statisticians using frequentist inference must choose for themselves the parameters of interest and the estimatorstest statistic to be used the absence of obviously explicit utilities and prior distributions has helped frequentist procedures to become widely viewed as objective\u000a\u000a\u000a bayesian inference \u000a\u000athe bayesian calculus describes degrees of belief using the language of probability beliefs are positive integrate to one and obey probability axioms bayesian inference uses the available posterior beliefs as the basis for making statistical propositions there are several different justifications for using the bayesian approach\u000a\u000a\u000a examples of bayesian inference \u000acredible interval for interval estimation\u000abayes factors for model comparison\u000a\u000a\u000a bayesian inference subjectivity and decision theory \u000amany informal bayesian inferences are based on intuitively reasonable summaries of the posterior for example the posterior mean median and mode highest posterior density intervals and bayes factors can all be motivated in this way while a users utility function need not be stated for this sort of inference these summaries do all depend to some extent on stated prior beliefs and are generally viewed as subjective conclusions methods of prior construction which do not require external input have been proposed but not yet fully developed\u000aformally bayesian inference is calibrated with reference to an explicitly stated utility or loss function the bayes rule is the one which maximizes expected utility averaged over the posterior uncertainty formal bayesian inference therefore automatically provides optimal decisions in a decision theoretic sense given assumptions data and utility bayesian inference can be made for essentially any problem although not every statistical inference need have a bayesian interpretation analyses which are not formally bayesian can be logically incoherent a feature of bayesian procedures which use proper priors ie those integrable to one is that they are guaranteed to be coherent some advocates of bayesian inference assert that inference must take place in this decisiontheoretic framework and that bayesian inference should not conclude with the evaluation and summarization of posterior beliefs\u000a\u000a\u000a aicbased inference \u000a\u000a\u000a other paradigms for inference \u000a\u000a\u000a minimum description length \u000a\u000athe minimum description length mdl principle has been developed from ideas in information theory and the theory of kolmogorov complexity the mdl principle selects statistical models that maximally compress the data inference proceeds without assuming counterfactual or nonfalsifiable datagenerating mechanisms or probability models for the data as might be done in frequentist or bayesian approaches\u000ahowever if a data generating mechanism does exist in reality then according to shannons source coding theorem it provides the mdl description of the data on average and asymptotically in minimizing description length or descriptive complexity mdl estimation is similar to maximum likelihood estimation and maximum a posteriori estimation using maximumentropy bayesian priors however mdl avoids assuming that the underlying probability model is known the mdl principle can also be applied without assumptions that eg the data arose from independent sampling\u000athe mdl principle has been applied in communicationcoding theory in information theory in linear regression and in data mining\u000athe evaluation of mdlbased inferential procedures often uses techniques or criteria from computational complexity theory\u000a\u000a\u000a fiducial inference \u000a\u000afiducial inference was an approach to statistical inference based on fiducial probability also known as a fiducial distribution in subsequent work this approach has been called illdefined extremely limited in applicability and even fallacious however this argument is the same as that which shows that a socalled confidence distribution is not a valid probability distribution and since this has not invalidated the application of confidence intervals it does not necessarily invalidate conclusions drawn from fiducial arguments\u000a\u000a\u000a structural inference \u000adeveloping ideas of fisher and of pitman from 1938 to 1939 george a barnard developed structural inference or pivotal inference an approach using invariant probabilities on group families barnard reformulated the arguments behind fiducial inference on a restricted class of models on which fiducial procedures would be welldefined and useful\u000a\u000a\u000a inference topics \u000athe topics below are usually included in the area of statistical inference\u000astatistical assumptions\u000astatistical decision theory\u000aestimation theory\u000astatistical hypothesis testing\u000arevising opinions in statistics\u000adesign of experiments the analysis of variance and regression\u000asurvey sampling\u000asummarizing statistical data\u000a\u000a\u000a see also \u000aalgorithmic inference\u000ainduction philosophy\u000aphilosophy of statistics\u000apredictive inference\u000a\u000a\u000a notes \u000a\u000a\u000a
p79
sg32
g35
sg37
NsbsS'design_of_experiments.txt'
p80
g2
(g3
g4
Ntp81
Rp82
(dp83
g8
g11
sg12
Vthe design of experiments or experimental design is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation the term is generally associated with true experiments in which the design introduces conditions that directly affect the variation but may also refer to the design of quasiexperiments in which natural conditions that influence the variation are selected for observation\u000ain its simplest form an experiment aims at predicting the outcome by introducing a change of the preconditions which is reflected in a variable called the predictor the change in the predictor is generally hypothesized to result in a change in the second variable hence called the outcome variable experimental design involves not only the selection of suitable predictors and outcomes but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources\u000amain concerns in experimental design include the establishment of validity reliability and replicability for example these concerns can be partially addressed by carefully choosing the predictor reducing the risk of measurement error and ensuring that the documentation of the method is sufficiently detailed related concerns include achieving appropriate levels of statistical power and sensitivity\u000acorrectly designed experiments advance knowledge in the natural and social sciences and engineering other applications include marketing and policy making\u000a\u000a\u000a history \u000a\u000a\u000a systematic clinical trials \u000ain 1747 while serving as surgeon on hms salisbury james lind carried out a systematic clinical trial to compare remedies for scurvy this systematic clinical trial constitutes a type of doe\u000alind selected 12 men from the ship all suffering from scurvy lind limited his subjects to men who were as similar as i could have them that is he provided strict entry requirements to reduce extraneous variation he divided them into six pairs giving each pair different supplements to their basic diet for two weeks the treatments were all remedies that had been proposed\u000aa quart of cider every day\u000atwenty five gutts drops of vitriol sulphuric acid three times a day upon an empty stomach\u000aone halfpint of seawater every day\u000aa mixture of garlic mustard and horseradish in a lump the size of a nutmeg\u000atwo spoonfuls of vinegar three times a day\u000atwo oranges and one lemon every day\u000athe citrus treatment stopped after six days when they ran out of fruit but by that time one sailor was fit for duty while the other had almost recovered apart from that only group one cider showed some effect of its treatment the remainder of the crew presumably served as a control but lind did not report results from any control untreated group\u000a\u000a\u000a statistical experiments following charles s peirce \u000a\u000aa theory of statistical inference was developed by charles s peirce in illustrations of the logic of science 18771878 and a theory of probable inference 1883 two publications that emphasized the importance of randomizationbased inference in statistics\u000a\u000a\u000a randomized experiments \u000a\u000acharles s peirce randomly assigned volunteers to a blinded repeatedmeasures design to evaluate their ability to discriminate weights peirces experiment inspired other researchers in psychology and education which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s\u000a\u000a\u000a optimal designs for regression models \u000a\u000acharles s peirce also contributed the first englishlanguage publication on an optimal design for regression models in 1876 a pioneering optimal design for polynomial regression was suggested by gergonne in 1815 in 1918 kirstine smith published optimal designs for polynomials of degree six and less\u000a\u000a\u000a sequences of experiments \u000a\u000athe use of a sequence of experiments where the design of each may depend on the results of previous experiments including the possible decision to stop experimenting is within the scope of sequential analysis a field that was pioneered by abraham wald in the context of sequential tests of statistical hypotheses herman chernoff wrote an overview of optimal sequential designs while adaptive designs have been surveyed by s zacks one specific type of sequential design is the twoarmed bandit generalized to the multiarmed bandit on which early work was done by herbert robbins in 1952\u000a\u000a\u000a fishers principles \u000a\u000aa methodology for designing experiments was proposed by ronald fisher in his innovative books the arrangement of field experiments 1926 and the design of experiments 1935 much of his pioneering work dealt with agricultural applications of statistical methods as a mundane example he described how to test the lady tasting tea hypothesis that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup these methods have been broadly adapted in the physical and social sciences are still used in agricultural engineering and differ from the design and analysis of computer experiments\u000acomparison\u000ain some fields of study it is not possible to have independent measurements to a traceable metrology standard comparisons between treatments are much more valuable and are usually preferable and often compared against a scientific control or traditional treatment that acts as baseline\u000arandomization\u000arandom assignment is the process of assigning individuals at random to groups or to different groups in an experiment so that each individual of the population has the same chance of becoming a participant in the study the random assignment of individuals to groups or conditions within a group distinguishes a rigorous true experiment from an observational study or quasiexperiment there is an extensive body of mathematical theory that explores the consequences of making the allocation of units to treatments by means of some random mechanism such as tables of random numbers or the use of randomization devices such as playing cards or dice assigning units to treatments at random tends to mitigate confounding which makes effects due to factors other than the treatment to appear to result from the treatment the risks associated with random allocation such as having a serious imbalance in a key characteristic between a treatment group and a control group are calculable and hence can be managed down to an acceptable level by using enough experimental units however if the population is divided into several subpopulations that somehow differ and the research requires each subpopulation to be equal in size stratified sampling can be used in that way the units in each subpopulation are randomized but not the whole sample the results of an experiment can be generalized reliably from the experimental units to a larger statistical population of units only if the experimental units are a random sample from the larger population the probable error of such an extrapolation depends on the sample size among other things\u000astatistical replication\u000ameasurements are usually subject to variation and measurement uncertainty thus they are repeated and full experiments are replicated to help identify the sources of variation to better estimate the true effects of treatments to further strengthen the experiments reliability and validity and to add to the existing knowledge of the topic however certain conditions must be met before the replication of the experiment is commenced the original research question has been published in a peerreviewed journal or widely cited the researcher is independent of the original experiment the researcher must first try to replicate the original findings using the original data and the writeup should state that the study conducted is a replication study that tried to follow the original study as strictly as possible\u000ablocking\u000ablocking is the arrangement of experimental units into groups blockslots consisting of units that are similar to one another blocking reduces known but irrelevant sources of variation between units and thus allows greater precision in the estimation of the source of variation under study\u000aorthogonality\u000a\u000aorthogonality concerns the forms of comparison contrasts that can be legitimately and efficiently carried out contrasts can be represented by vectors and sets of orthogonal contrasts are uncorrelated and independently distributed if the data are normal because of this independence each orthogonal treatment provides different information to the others if there are t treatments and t  1 orthogonal contrasts all the information that can be captured from the experiment is obtainable from the set of contrasts\u000afactorial experiments\u000ause of factorial experiments instead of the onefactoratatime method these are efficient at evaluating the effects and possible interactions of several factors independent variables analysis of experiment design is built on the foundation of the analysis of variance a collection of models that partition the observed variance into components according to what factors the experiment must estimate or test\u000a\u000a\u000a example \u000a\u000athis example is attributed to harold hotelling it conveys some of the flavor of those aspects of the subject that involve combinatorial designs\u000aweights of eight objects are measured using a pan balance and set of standard weights each weighing measures the weight difference between objects in the left pan vs any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium each measurement has a random error the average error is zero the standard deviations of the probability distribution of the errors is the same number  on different weighings and errors on different weighings are independent denote the true weights by\u000a\u000awe consider two different experiments\u000aweigh each object in one pan with the other pan empty let xi be the measured weight of the ith object for i  1  8\u000ado the eight weighings according to the following schedule and let yi be the measured difference for i  1  8\u000a\u000athen the estimated value of the weight 1 is\u000a\u000asimilar estimates can be found for the weights of the other items for example\u000a\u000athe question of design of experiments is which experiment is better\u000athe variance of the estimate x1 of 1 is 2 if we use the first experiment but if we use the second experiment the variance of the estimate given above is 28 thus the second experiment gives us 8 times as much precision for the estimate of a single item and estimates all items simultaneously with the same precision what the second experiment achieves with eight would require 64 weighings if the items are weighed separately however note that the estimates for the items obtained in the second experiment have errors that correlate with each other\u000amany problems of the design of experiments involve combinatorial designs as in this example and others\u000a\u000a\u000a avoiding false positives \u000afalse positive conclusions often resulting from the pressure to publish or the authors own confirmation bias are an inherent hazard in many fields a good way to prevent biases potentially leading to false positives in the data collection phase is to use a doubleblind design when a doubleblind design is used participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group therefore the researcher can not affect the participants response to the intervention experimental designs with undisclosed degrees of freedom are a problem this can lead to conscious or unconscious phacking trying multiple things until you get the desired result it typically involves the manipulation  perhaps unconsciously  of the process of statistical analysis and the degrees of freedom until they return a figure below the p05 level of statistical significance so the design of the experiment should include a clear statement proposing the analyses to be undertaken phacking can be prevented by preregistering researches in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection so no data mining is possible httpsosfio another way to prevent this is taking the doubleblind design to the dataanalysis phase where the data are sent to a dataanalyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers\u000aclear and complete documentation of the experimental methodology is also important in order to support replication of results\u000a\u000a\u000a discussion topics when setting up an experimental design \u000aan experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment an experimental design is the laying out of a detailed experimental plan in advance of doing the experiment some of the following topics have already been discussed in the principles of experimental design section\u000ahow many factors does the design have and are the levels of these factors fixed or random\u000aare control conditions needed and what should they be\u000amanipulation checks did the manipulation really work\u000awhat are the background variables\u000awhat is the sample size how many units must be collected for the experiment to be generalisable and have enough power\u000awhat is the relevance of interactions between factors\u000awhat is the influence of delayed effects of substantive factors on outcomes\u000ahow do response shifts affect selfreport measures\u000ahow feasible is repeated administration of the same measurement instruments to the same units at different occasions with a posttest and followup tests\u000awhat about using a proxy pretest\u000aare there lurking variables\u000ashould the clientpatient researcher or even the analyst of the data be blind to conditions\u000awhat is the feasibility of subsequent application of different conditions to the same units\u000ahow many of each control and noise factors should be taken into account\u000athe independent variable of a study often has many levels or different groups in a true experiment researchers can have an experimental group which is where their intervention testing the hypothesis is implemented and a control group which has all the same element as the experimental group without the interventional element thus when everything else except for one intervention is held constant researchers can certify with some certainty that this one element is what caused the observed change in some instances having a control group is not ethical this is sometimes solved using two different experimental groups in some cases independent variables are not manipulable for example when testing the difference between two groups who have a different disease or testing the difference between genders obviously variables that would be hard or unethical to assign participants to in these cases a quasiexperimental design may be used\u000a\u000a\u000a causal attributions \u000ain the pure experimental design the independent predictor variable is manipulated by the researcher  that is  every participant of the research is chosen randomly from the population and each participant chosen is assigned randomly to conditions of the independent variable only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions therefore researchers should choose the experimental design over other design types whenever possible however the nature of the independent variable does not always allow for manipulation in those cases researchers must be aware of not certifying about causal attribution when their design doesnt allow for it for example in observational designs participants are not assigned randomly to conditions and so if there are differences found in outcome variables between conditions it is likely that there is something other than the differences between the conditions that causes the differences in outcomes that is  a third variable the same goes for studies with correlational design adr  mellenbergh 2008\u000a\u000a\u000a statistical control \u000ait is best that a process be in reasonable statistical control prior to conducting designed experiments when this is not possible proper blocking replication and randomization allow for the careful conduct of designed experiments to control for nuisance variables researchers institute control checks as additional measures investigators should ensure that uncontrolled influences eg source credibility perception do not skew the findings of the study a manipulation check is one example of a control check manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned\u000aone of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious intervening and antecedent variables in the most basic model cause x leads to effect y but there could be a third variable z that influences y and x might not be the true cause at all z is said to be a spurious variable and must be controlled for the same is true for intervening variables a variable in between the supposed cause x and the effect y and anteceding variables a variable prior to the supposed cause x that is the true cause when a third variable is involved and has not been controlled for the relation is said to be a zero order relationship in most practical applications of experimental research designs there are several causes x1 x2 x3 in most designs only one of these causes is manipulated at a time\u000a\u000a\u000a experimental designs after fisher \u000asome efficient designs for estimating several main effects were found independently and in near succession by raj chandra bose and k kishen in 1940 at the indian statistical institute but remained little known until the plackettburman designs were published in biometrika in 1946 about the same time c r rao introduced the concepts of orthogonal arrays as experimental designs this concept played a central role in the development of taguchi methods by genichi taguchi which took place during his visit to indian statistical institute in early 1950s his methods were successfully applied and adopted by japanese and indian industries and subsequently were also embraced by us industry albeit with some reservations\u000ain 1950 gertrude mary cox and william gemmell cochran published the book experimental designs which became the major reference work on the design of experiments for statisticians for years afterwards\u000adevelopments of the theory of linear models have encompassed and surpassed the cases that concerned early writers today the theory rests on advanced topics in linear algebra algebra and combinatorics\u000aas with other branches of statistics experimental design is pursued using both frequentist and bayesian approaches in evaluating statistical procedures like experimental designs frequentist statistics studies the sampling distribution while bayesian statistics updates a probability distribution on the parameter space\u000asome important contributors to the field of experimental designs are c s peirce r a fisher f yates c r rao r c bose j n srivastava shrikhande s s d raghavarao w g cochran o kempthorne w t federer v v fedorov a s hedayat j a nelder r a bailey j kiefer w j studden a pzman f pukelsheim d r cox h p wynn a c atkinson g e p box and g taguchi the textbooks of d montgomery and r myers have reached generations of students and practitioners  \u000a\u000a\u000a human participant experimental design constraints \u000alaws and ethical considerations preclude some carefully designed experiments with human subjects legal constraints are dependent on jurisdiction constraints may involve institutional review boards informed consent and confidentiality affecting both clinical medical trials and behavioral and social science experiments in the field of toxicology for example experimentation is performed on laboratory animals with the goal of defining safe exposure limits for humans balancing the constraints are views from the medical field regarding the randomization of patients  if no one knows which therapy is better there is no ethical imperative to use one therapy or another p 380 regarding experimental design it is clearly not ethical to place subjects at risk to collect data in a poorly designed study when this situation can be easily avoided p 393\u000a\u000a\u000a see also \u000a\u000a\u000a notes \u000a\u000a\u000a
p84
sg14
g17
sg18
Vthe design of experiments or experimental design is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation the term is generally associated with true experiments in which the design introduces conditions that directly affect the variation but may also refer to the design of quasiexperiments in which natural conditions that influence the variation are selected for observation\u000ain its simplest form an experiment aims at predicting the outcome by introducing a change of the preconditions which is reflected in a variable called the predictor the change in the predictor is generally hypothesized to result in a change in the second variable hence called the outcome variable experimental design involves not only the selection of suitable predictors and outcomes but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources\u000amain concerns in experimental design include the establishment of validity reliability and replicability for example these concerns can be partially addressed by carefully choosing the predictor reducing the risk of measurement error and ensuring that the documentation of the method is sufficiently detailed related concerns include achieving appropriate levels of statistical power and sensitivity\u000acorrectly designed experiments advance knowledge in the natural and social sciences and engineering other applications include marketing and policy making\u000a\u000a\u000a history \u000a\u000a\u000a systematic clinical trials \u000ain 1747 while serving as surgeon on hms salisbury james lind carried out a systematic clinical trial to compare remedies for scurvy this systematic clinical trial constitutes a type of doe\u000alind selected 12 men from the ship all suffering from scurvy lind limited his subjects to men who were as similar as i could have them that is he provided strict entry requirements to reduce extraneous variation he divided them into six pairs giving each pair different supplements to their basic diet for two weeks the treatments were all remedies that had been proposed\u000aa quart of cider every day\u000atwenty five gutts drops of vitriol sulphuric acid three times a day upon an empty stomach\u000aone halfpint of seawater every day\u000aa mixture of garlic mustard and horseradish in a lump the size of a nutmeg\u000atwo spoonfuls of vinegar three times a day\u000atwo oranges and one lemon every day\u000athe citrus treatment stopped after six days when they ran out of fruit but by that time one sailor was fit for duty while the other had almost recovered apart from that only group one cider showed some effect of its treatment the remainder of the crew presumably served as a control but lind did not report results from any control untreated group\u000a\u000a\u000a statistical experiments following charles s peirce \u000a\u000aa theory of statistical inference was developed by charles s peirce in illustrations of the logic of science 18771878 and a theory of probable inference 1883 two publications that emphasized the importance of randomizationbased inference in statistics\u000a\u000a\u000a randomized experiments \u000a\u000acharles s peirce randomly assigned volunteers to a blinded repeatedmeasures design to evaluate their ability to discriminate weights peirces experiment inspired other researchers in psychology and education which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s\u000a\u000a\u000a optimal designs for regression models \u000a\u000acharles s peirce also contributed the first englishlanguage publication on an optimal design for regression models in 1876 a pioneering optimal design for polynomial regression was suggested by gergonne in 1815 in 1918 kirstine smith published optimal designs for polynomials of degree six and less\u000a\u000a\u000a sequences of experiments \u000a\u000athe use of a sequence of experiments where the design of each may depend on the results of previous experiments including the possible decision to stop experimenting is within the scope of sequential analysis a field that was pioneered by abraham wald in the context of sequential tests of statistical hypotheses herman chernoff wrote an overview of optimal sequential designs while adaptive designs have been surveyed by s zacks one specific type of sequential design is the twoarmed bandit generalized to the multiarmed bandit on which early work was done by herbert robbins in 1952\u000a\u000a\u000a fishers principles \u000a\u000aa methodology for designing experiments was proposed by ronald fisher in his innovative books the arrangement of field experiments 1926 and the design of experiments 1935 much of his pioneering work dealt with agricultural applications of statistical methods as a mundane example he described how to test the lady tasting tea hypothesis that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup these methods have been broadly adapted in the physical and social sciences are still used in agricultural engineering and differ from the design and analysis of computer experiments\u000acomparison\u000ain some fields of study it is not possible to have independent measurements to a traceable metrology standard comparisons between treatments are much more valuable and are usually preferable and often compared against a scientific control or traditional treatment that acts as baseline\u000arandomization\u000arandom assignment is the process of assigning individuals at random to groups or to different groups in an experiment so that each individual of the population has the same chance of becoming a participant in the study the random assignment of individuals to groups or conditions within a group distinguishes a rigorous true experiment from an observational study or quasiexperiment there is an extensive body of mathematical theory that explores the consequences of making the allocation of units to treatments by means of some random mechanism such as tables of random numbers or the use of randomization devices such as playing cards or dice assigning units to treatments at random tends to mitigate confounding which makes effects due to factors other than the treatment to appear to result from the treatment the risks associated with random allocation such as having a serious imbalance in a key characteristic between a treatment group and a control group are calculable and hence can be managed down to an acceptable level by using enough experimental units however if the population is divided into several subpopulations that somehow differ and the research requires each subpopulation to be equal in size stratified sampling can be used in that way the units in each subpopulation are randomized but not the whole sample the results of an experiment can be generalized reliably from the experimental units to a larger statistical population of units only if the experimental units are a random sample from the larger population the probable error of such an extrapolation depends on the sample size among other things\u000astatistical replication\u000ameasurements are usually subject to variation and measurement uncertainty thus they are repeated and full experiments are replicated to help identify the sources of variation to better estimate the true effects of treatments to further strengthen the experiments reliability and validity and to add to the existing knowledge of the topic however certain conditions must be met before the replication of the experiment is commenced the original research question has been published in a peerreviewed journal or widely cited the researcher is independent of the original experiment the researcher must first try to replicate the original findings using the original data and the writeup should state that the study conducted is a replication study that tried to follow the original study as strictly as possible\u000ablocking\u000ablocking is the arrangement of experimental units into groups blockslots consisting of units that are similar to one another blocking reduces known but irrelevant sources of variation between units and thus allows greater precision in the estimation of the source of variation under study\u000aorthogonality\u000a\u000aorthogonality concerns the forms of comparison contrasts that can be legitimately and efficiently carried out contrasts can be represented by vectors and sets of orthogonal contrasts are uncorrelated and independently distributed if the data are normal because of this independence each orthogonal treatment provides different information to the others if there are t treatments and t  1 orthogonal contrasts all the information that can be captured from the experiment is obtainable from the set of contrasts\u000afactorial experiments\u000ause of factorial experiments instead of the onefactoratatime method these are efficient at evaluating the effects and possible interactions of several factors independent variables analysis of experiment design is built on the foundation of the analysis of variance a collection of models that partition the observed variance into components according to what factors the experiment must estimate or test\u000a\u000a\u000a example \u000a\u000athis example is attributed to harold hotelling it conveys some of the flavor of those aspects of the subject that involve combinatorial designs\u000aweights of eight objects are measured using a pan balance and set of standard weights each weighing measures the weight difference between objects in the left pan vs any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium each measurement has a random error the average error is zero the standard deviations of the probability distribution of the errors is the same number  on different weighings and errors on different weighings are independent denote the true weights by\u000a\u000awe consider two different experiments\u000aweigh each object in one pan with the other pan empty let xi be the measured weight of the ith object for i  1  8\u000ado the eight weighings according to the following schedule and let yi be the measured difference for i  1  8\u000a\u000athen the estimated value of the weight 1 is\u000a\u000asimilar estimates can be found for the weights of the other items for example\u000a\u000athe question of design of experiments is which experiment is better\u000athe variance of the estimate x1 of 1 is 2 if we use the first experiment but if we use the second experiment the variance of the estimate given above is 28 thus the second experiment gives us 8 times as much precision for the estimate of a single item and estimates all items simultaneously with the same precision what the second experiment achieves with eight would require 64 weighings if the items are weighed separately however note that the estimates for the items obtained in the second experiment have errors that correlate with each other\u000amany problems of the design of experiments involve combinatorial designs as in this example and others\u000a\u000a\u000a avoiding false positives \u000afalse positive conclusions often resulting from the pressure to publish or the authors own confirmation bias are an inherent hazard in many fields a good way to prevent biases potentially leading to false positives in the data collection phase is to use a doubleblind design when a doubleblind design is used participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group therefore the researcher can not affect the participants response to the intervention experimental designs with undisclosed degrees of freedom are a problem this can lead to conscious or unconscious phacking trying multiple things until you get the desired result it typically involves the manipulation  perhaps unconsciously  of the process of statistical analysis and the degrees of freedom until they return a figure below the p05 level of statistical significance so the design of the experiment should include a clear statement proposing the analyses to be undertaken phacking can be prevented by preregistering researches in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection so no data mining is possible httpsosfio another way to prevent this is taking the doubleblind design to the dataanalysis phase where the data are sent to a dataanalyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers\u000aclear and complete documentation of the experimental methodology is also important in order to support replication of results\u000a\u000a\u000a discussion topics when setting up an experimental design \u000aan experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment an experimental design is the laying out of a detailed experimental plan in advance of doing the experiment some of the following topics have already been discussed in the principles of experimental design section\u000ahow many factors does the design have and are the levels of these factors fixed or random\u000aare control conditions needed and what should they be\u000amanipulation checks did the manipulation really work\u000awhat are the background variables\u000awhat is the sample size how many units must be collected for the experiment to be generalisable and have enough power\u000awhat is the relevance of interactions between factors\u000awhat is the influence of delayed effects of substantive factors on outcomes\u000ahow do response shifts affect selfreport measures\u000ahow feasible is repeated administration of the same measurement instruments to the same units at different occasions with a posttest and followup tests\u000awhat about using a proxy pretest\u000aare there lurking variables\u000ashould the clientpatient researcher or even the analyst of the data be blind to conditions\u000awhat is the feasibility of subsequent application of different conditions to the same units\u000ahow many of each control and noise factors should be taken into account\u000athe independent variable of a study often has many levels or different groups in a true experiment researchers can have an experimental group which is where their intervention testing the hypothesis is implemented and a control group which has all the same element as the experimental group without the interventional element thus when everything else except for one intervention is held constant researchers can certify with some certainty that this one element is what caused the observed change in some instances having a control group is not ethical this is sometimes solved using two different experimental groups in some cases independent variables are not manipulable for example when testing the difference between two groups who have a different disease or testing the difference between genders obviously variables that would be hard or unethical to assign participants to in these cases a quasiexperimental design may be used\u000a\u000a\u000a causal attributions \u000ain the pure experimental design the independent predictor variable is manipulated by the researcher  that is  every participant of the research is chosen randomly from the population and each participant chosen is assigned randomly to conditions of the independent variable only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions therefore researchers should choose the experimental design over other design types whenever possible however the nature of the independent variable does not always allow for manipulation in those cases researchers must be aware of not certifying about causal attribution when their design doesnt allow for it for example in observational designs participants are not assigned randomly to conditions and so if there are differences found in outcome variables between conditions it is likely that there is something other than the differences between the conditions that causes the differences in outcomes that is  a third variable the same goes for studies with correlational design adr  mellenbergh 2008\u000a\u000a\u000a statistical control \u000ait is best that a process be in reasonable statistical control prior to conducting designed experiments when this is not possible proper blocking replication and randomization allow for the careful conduct of designed experiments to control for nuisance variables researchers institute control checks as additional measures investigators should ensure that uncontrolled influences eg source credibility perception do not skew the findings of the study a manipulation check is one example of a control check manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned\u000aone of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious intervening and antecedent variables in the most basic model cause x leads to effect y but there could be a third variable z that influences y and x might not be the true cause at all z is said to be a spurious variable and must be controlled for the same is true for intervening variables a variable in between the supposed cause x and the effect y and anteceding variables a variable prior to the supposed cause x that is the true cause when a third variable is involved and has not been controlled for the relation is said to be a zero order relationship in most practical applications of experimental research designs there are several causes x1 x2 x3 in most designs only one of these causes is manipulated at a time\u000a\u000a\u000a experimental designs after fisher \u000asome efficient designs for estimating several main effects were found independently and in near succession by raj chandra bose and k kishen in 1940 at the indian statistical institute but remained little known until the plackettburman designs were published in biometrika in 1946 about the same time c r rao introduced the concepts of orthogonal arrays as experimental designs this concept played a central role in the development of taguchi methods by genichi taguchi which took place during his visit to indian statistical institute in early 1950s his methods were successfully applied and adopted by japanese and indian industries and subsequently were also embraced by us industry albeit with some reservations\u000ain 1950 gertrude mary cox and william gemmell cochran published the book experimental designs which became the major reference work on the design of experiments for statisticians for years afterwards\u000adevelopments of the theory of linear models have encompassed and surpassed the cases that concerned early writers today the theory rests on advanced topics in linear algebra algebra and combinatorics\u000aas with other branches of statistics experimental design is pursued using both frequentist and bayesian approaches in evaluating statistical procedures like experimental designs frequentist statistics studies the sampling distribution while bayesian statistics updates a probability distribution on the parameter space\u000asome important contributors to the field of experimental designs are c s peirce r a fisher f yates c r rao r c bose j n srivastava shrikhande s s d raghavarao w g cochran o kempthorne w t federer v v fedorov a s hedayat j a nelder r a bailey j kiefer w j studden a pzman f pukelsheim d r cox h p wynn a c atkinson g e p box and g taguchi the textbooks of d montgomery and r myers have reached generations of students and practitioners  \u000a\u000a\u000a human participant experimental design constraints \u000alaws and ethical considerations preclude some carefully designed experiments with human subjects legal constraints are dependent on jurisdiction constraints may involve institutional review boards informed consent and confidentiality affecting both clinical medical trials and behavioral and social science experiments in the field of toxicology for example experimentation is performed on laboratory animals with the goal of defining safe exposure limits for humans balancing the constraints are views from the medical field regarding the randomization of patients  if no one knows which therapy is better there is no ethical imperative to use one therapy or another p 380 regarding experimental design it is clearly not ethical to place subjects at risk to collect data in a poorly designed study when this situation can be easily avoided p 393\u000a\u000a\u000a see also \u000a\u000a\u000a notes
p85
sg20
g23
sg24
g27
sg30
Vthe design of experiments or experimental design is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation the term is generally associated with true experiments in which the design introduces conditions that directly affect the variation but may also refer to the design of quasiexperiments in which natural conditions that influence the variation are selected for observation\u000ain its simplest form an experiment aims at predicting the outcome by introducing a change of the preconditions which is reflected in a variable called the predictor the change in the predictor is generally hypothesized to result in a change in the second variable hence called the outcome variable experimental design involves not only the selection of suitable predictors and outcomes but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources\u000amain concerns in experimental design include the establishment of validity reliability and replicability for example these concerns can be partially addressed by carefully choosing the predictor reducing the risk of measurement error and ensuring that the documentation of the method is sufficiently detailed related concerns include achieving appropriate levels of statistical power and sensitivity\u000acorrectly designed experiments advance knowledge in the natural and social sciences and engineering other applications include marketing and policy making\u000a\u000a\u000a history \u000a\u000a\u000a systematic clinical trials \u000ain 1747 while serving as surgeon on hms salisbury james lind carried out a systematic clinical trial to compare remedies for scurvy this systematic clinical trial constitutes a type of doe\u000alind selected 12 men from the ship all suffering from scurvy lind limited his subjects to men who were as similar as i could have them that is he provided strict entry requirements to reduce extraneous variation he divided them into six pairs giving each pair different supplements to their basic diet for two weeks the treatments were all remedies that had been proposed\u000aa quart of cider every day\u000atwenty five gutts drops of vitriol sulphuric acid three times a day upon an empty stomach\u000aone halfpint of seawater every day\u000aa mixture of garlic mustard and horseradish in a lump the size of a nutmeg\u000atwo spoonfuls of vinegar three times a day\u000atwo oranges and one lemon every day\u000athe citrus treatment stopped after six days when they ran out of fruit but by that time one sailor was fit for duty while the other had almost recovered apart from that only group one cider showed some effect of its treatment the remainder of the crew presumably served as a control but lind did not report results from any control untreated group\u000a\u000a\u000a statistical experiments following charles s peirce \u000a\u000aa theory of statistical inference was developed by charles s peirce in illustrations of the logic of science 18771878 and a theory of probable inference 1883 two publications that emphasized the importance of randomizationbased inference in statistics\u000a\u000a\u000a randomized experiments \u000a\u000acharles s peirce randomly assigned volunteers to a blinded repeatedmeasures design to evaluate their ability to discriminate weights peirces experiment inspired other researchers in psychology and education which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s\u000a\u000a\u000a optimal designs for regression models \u000a\u000acharles s peirce also contributed the first englishlanguage publication on an optimal design for regression models in 1876 a pioneering optimal design for polynomial regression was suggested by gergonne in 1815 in 1918 kirstine smith published optimal designs for polynomials of degree six and less\u000a\u000a\u000a sequences of experiments \u000a\u000athe use of a sequence of experiments where the design of each may depend on the results of previous experiments including the possible decision to stop experimenting is within the scope of sequential analysis a field that was pioneered by abraham wald in the context of sequential tests of statistical hypotheses herman chernoff wrote an overview of optimal sequential designs while adaptive designs have been surveyed by s zacks one specific type of sequential design is the twoarmed bandit generalized to the multiarmed bandit on which early work was done by herbert robbins in 1952\u000a\u000a\u000a fishers principles \u000a\u000aa methodology for designing experiments was proposed by ronald fisher in his innovative books the arrangement of field experiments 1926 and the design of experiments 1935 much of his pioneering work dealt with agricultural applications of statistical methods as a mundane example he described how to test the lady tasting tea hypothesis that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup these methods have been broadly adapted in the physical and social sciences are still used in agricultural engineering and differ from the design and analysis of computer experiments\u000acomparison\u000ain some fields of study it is not possible to have independent measurements to a traceable metrology standard comparisons between treatments are much more valuable and are usually preferable and often compared against a scientific control or traditional treatment that acts as baseline\u000arandomization\u000arandom assignment is the process of assigning individuals at random to groups or to different groups in an experiment so that each individual of the population has the same chance of becoming a participant in the study the random assignment of individuals to groups or conditions within a group distinguishes a rigorous true experiment from an observational study or quasiexperiment there is an extensive body of mathematical theory that explores the consequences of making the allocation of units to treatments by means of some random mechanism such as tables of random numbers or the use of randomization devices such as playing cards or dice assigning units to treatments at random tends to mitigate confounding which makes effects due to factors other than the treatment to appear to result from the treatment the risks associated with random allocation such as having a serious imbalance in a key characteristic between a treatment group and a control group are calculable and hence can be managed down to an acceptable level by using enough experimental units however if the population is divided into several subpopulations that somehow differ and the research requires each subpopulation to be equal in size stratified sampling can be used in that way the units in each subpopulation are randomized but not the whole sample the results of an experiment can be generalized reliably from the experimental units to a larger statistical population of units only if the experimental units are a random sample from the larger population the probable error of such an extrapolation depends on the sample size among other things\u000astatistical replication\u000ameasurements are usually subject to variation and measurement uncertainty thus they are repeated and full experiments are replicated to help identify the sources of variation to better estimate the true effects of treatments to further strengthen the experiments reliability and validity and to add to the existing knowledge of the topic however certain conditions must be met before the replication of the experiment is commenced the original research question has been published in a peerreviewed journal or widely cited the researcher is independent of the original experiment the researcher must first try to replicate the original findings using the original data and the writeup should state that the study conducted is a replication study that tried to follow the original study as strictly as possible\u000ablocking\u000ablocking is the arrangement of experimental units into groups blockslots consisting of units that are similar to one another blocking reduces known but irrelevant sources of variation between units and thus allows greater precision in the estimation of the source of variation under study\u000aorthogonality\u000a\u000aorthogonality concerns the forms of comparison contrasts that can be legitimately and efficiently carried out contrasts can be represented by vectors and sets of orthogonal contrasts are uncorrelated and independently distributed if the data are normal because of this independence each orthogonal treatment provides different information to the others if there are t treatments and t  1 orthogonal contrasts all the information that can be captured from the experiment is obtainable from the set of contrasts\u000afactorial experiments\u000ause of factorial experiments instead of the onefactoratatime method these are efficient at evaluating the effects and possible interactions of several factors independent variables analysis of experiment design is built on the foundation of the analysis of variance a collection of models that partition the observed variance into components according to what factors the experiment must estimate or test\u000a\u000a\u000a example \u000a\u000athis example is attributed to harold hotelling it conveys some of the flavor of those aspects of the subject that involve combinatorial designs\u000aweights of eight objects are measured using a pan balance and set of standard weights each weighing measures the weight difference between objects in the left pan vs any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium each measurement has a random error the average error is zero the standard deviations of the probability distribution of the errors is the same number  on different weighings and errors on different weighings are independent denote the true weights by\u000a\u000awe consider two different experiments\u000aweigh each object in one pan with the other pan empty let xi be the measured weight of the ith object for i  1  8\u000ado the eight weighings according to the following schedule and let yi be the measured difference for i  1  8\u000a\u000athen the estimated value of the weight 1 is\u000a\u000asimilar estimates can be found for the weights of the other items for example\u000a\u000athe question of design of experiments is which experiment is better\u000athe variance of the estimate x1 of 1 is 2 if we use the first experiment but if we use the second experiment the variance of the estimate given above is 28 thus the second experiment gives us 8 times as much precision for the estimate of a single item and estimates all items simultaneously with the same precision what the second experiment achieves with eight would require 64 weighings if the items are weighed separately however note that the estimates for the items obtained in the second experiment have errors that correlate with each other\u000amany problems of the design of experiments involve combinatorial designs as in this example and others\u000a\u000a\u000a avoiding false positives \u000afalse positive conclusions often resulting from the pressure to publish or the authors own confirmation bias are an inherent hazard in many fields a good way to prevent biases potentially leading to false positives in the data collection phase is to use a doubleblind design when a doubleblind design is used participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group therefore the researcher can not affect the participants response to the intervention experimental designs with undisclosed degrees of freedom are a problem this can lead to conscious or unconscious phacking trying multiple things until you get the desired result it typically involves the manipulation  perhaps unconsciously  of the process of statistical analysis and the degrees of freedom until they return a figure below the p05 level of statistical significance so the design of the experiment should include a clear statement proposing the analyses to be undertaken phacking can be prevented by preregistering researches in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection so no data mining is possible httpsosfio another way to prevent this is taking the doubleblind design to the dataanalysis phase where the data are sent to a dataanalyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers\u000aclear and complete documentation of the experimental methodology is also important in order to support replication of results\u000a\u000a\u000a discussion topics when setting up an experimental design \u000aan experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment an experimental design is the laying out of a detailed experimental plan in advance of doing the experiment some of the following topics have already been discussed in the principles of experimental design section\u000ahow many factors does the design have and are the levels of these factors fixed or random\u000aare control conditions needed and what should they be\u000amanipulation checks did the manipulation really work\u000awhat are the background variables\u000awhat is the sample size how many units must be collected for the experiment to be generalisable and have enough power\u000awhat is the relevance of interactions between factors\u000awhat is the influence of delayed effects of substantive factors on outcomes\u000ahow do response shifts affect selfreport measures\u000ahow feasible is repeated administration of the same measurement instruments to the same units at different occasions with a posttest and followup tests\u000awhat about using a proxy pretest\u000aare there lurking variables\u000ashould the clientpatient researcher or even the analyst of the data be blind to conditions\u000awhat is the feasibility of subsequent application of different conditions to the same units\u000ahow many of each control and noise factors should be taken into account\u000athe independent variable of a study often has many levels or different groups in a true experiment researchers can have an experimental group which is where their intervention testing the hypothesis is implemented and a control group which has all the same element as the experimental group without the interventional element thus when everything else except for one intervention is held constant researchers can certify with some certainty that this one element is what caused the observed change in some instances having a control group is not ethical this is sometimes solved using two different experimental groups in some cases independent variables are not manipulable for example when testing the difference between two groups who have a different disease or testing the difference between genders obviously variables that would be hard or unethical to assign participants to in these cases a quasiexperimental design may be used\u000a\u000a\u000a causal attributions \u000ain the pure experimental design the independent predictor variable is manipulated by the researcher  that is  every participant of the research is chosen randomly from the population and each participant chosen is assigned randomly to conditions of the independent variable only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions therefore researchers should choose the experimental design over other design types whenever possible however the nature of the independent variable does not always allow for manipulation in those cases researchers must be aware of not certifying about causal attribution when their design doesnt allow for it for example in observational designs participants are not assigned randomly to conditions and so if there are differences found in outcome variables between conditions it is likely that there is something other than the differences between the conditions that causes the differences in outcomes that is  a third variable the same goes for studies with correlational design adr  mellenbergh 2008\u000a\u000a\u000a statistical control \u000ait is best that a process be in reasonable statistical control prior to conducting designed experiments when this is not possible proper blocking replication and randomization allow for the careful conduct of designed experiments to control for nuisance variables researchers institute control checks as additional measures investigators should ensure that uncontrolled influences eg source credibility perception do not skew the findings of the study a manipulation check is one example of a control check manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned\u000aone of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious intervening and antecedent variables in the most basic model cause x leads to effect y but there could be a third variable z that influences y and x might not be the true cause at all z is said to be a spurious variable and must be controlled for the same is true for intervening variables a variable in between the supposed cause x and the effect y and anteceding variables a variable prior to the supposed cause x that is the true cause when a third variable is involved and has not been controlled for the relation is said to be a zero order relationship in most practical applications of experimental research designs there are several causes x1 x2 x3 in most designs only one of these causes is manipulated at a time\u000a\u000a\u000a experimental designs after fisher \u000asome efficient designs for estimating several main effects were found independently and in near succession by raj chandra bose and k kishen in 1940 at the indian statistical institute but remained little known until the plackettburman designs were published in biometrika in 1946 about the same time c r rao introduced the concepts of orthogonal arrays as experimental designs this concept played a central role in the development of taguchi methods by genichi taguchi which took place during his visit to indian statistical institute in early 1950s his methods were successfully applied and adopted by japanese and indian industries and subsequently were also embraced by us industry albeit with some reservations\u000ain 1950 gertrude mary cox and william gemmell cochran published the book experimental designs which became the major reference work on the design of experiments for statisticians for years afterwards\u000adevelopments of the theory of linear models have encompassed and surpassed the cases that concerned early writers today the theory rests on advanced topics in linear algebra algebra and combinatorics\u000aas with other branches of statistics experimental design is pursued using both frequentist and bayesian approaches in evaluating statistical procedures like experimental designs frequentist statistics studies the sampling distribution while bayesian statistics updates a probability distribution on the parameter space\u000asome important contributors to the field of experimental designs are c s peirce r a fisher f yates c r rao r c bose j n srivastava shrikhande s s d raghavarao w g cochran o kempthorne w t federer v v fedorov a s hedayat j a nelder r a bailey j kiefer w j studden a pzman f pukelsheim d r cox h p wynn a c atkinson g e p box and g taguchi the textbooks of d montgomery and r myers have reached generations of students and practitioners  \u000a\u000a\u000a human participant experimental design constraints \u000alaws and ethical considerations preclude some carefully designed experiments with human subjects legal constraints are dependent on jurisdiction constraints may involve institutional review boards informed consent and confidentiality affecting both clinical medical trials and behavioral and social science experiments in the field of toxicology for example experimentation is performed on laboratory animals with the goal of defining safe exposure limits for humans balancing the constraints are views from the medical field regarding the randomization of patients  if no one knows which therapy is better there is no ethical imperative to use one therapy or another p 380 regarding experimental design it is clearly not ethical to place subjects at risk to collect data in a poorly designed study when this situation can be easily avoided p 393\u000a\u000a\u000a see also \u000a\u000a\u000a notes \u000a\u000a\u000a
p86
sg32
g35
sg37
NsbsS'maximum_likelihood.txt'
p87
g2
(g3
g4
Ntp88
Rp89
(dp90
g8
g11
sg12
Vin statistics maximumlikelihood estimation mle is a method of estimating the parameters of a statistical model given data\u000athe method of maximum likelihood corresponds to many wellknown estimation methods in statistics for example one may be interested in the heights of adult female penguins but be unable to measure the height of every single penguin in a population due to cost or time constraints assuming that the heights are normally distributed with some unknown mean and variance the mean and variance can be estimated with mle while only knowing the heights of some sample of the overall population mle would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model\u000ain general for a fixed set of data and underlying statistical model the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function intuitively this maximizes the agreement of the selected model with the observed data and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution maximumlikelihood estimation gives a unified approach to estimation which is welldefined in the case of the normal distribution and many other problems however in some complicated problems difficulties do occur in such problems maximumlikelihood estimators are unsuitable or do not exist\u000a\u000a\u000a historyedit \u000a\u000amaximumlikelihood estimation was recommended analyzed with fruitless attempts at proofs and vastly popularized by ronald fisher between 1912 and 1922 although it had been used earlier by carl friedrich gauss pierresimon laplace thorvald n thiele and francis ysidro edgeworth reviews of the development of maximum likelihood have been provided by a number of authors\u000asome of the theory behind maximumlikelihood estimation was developed for bayesian statistics\u000a\u000a\u000a principlesedit \u000asuppose there is a sample x1 x2  xn of n independent and identically distributed observations coming from a distribution with an unknown probability density function f0 it is however surmised that the function f0 belongs to a certain family of distributions f    where  is a vector of parameters for this family called the parametric model so that f0  f0 the value 0 is unknown and is referred to as the true value of the parameter vector it is desirable to find an estimator  which would be as close to the true value 0 as possible either or both the observed variables xi and the parameter  can be vectors\u000ato use the method of maximum likelihood one first specifies the joint density function for all observations for an independent and identically distributed sample this joint density function is\u000a\u000anow we look at this function from a different perspective by considering the observed values x1 x2  xn to be fixed parameters of this function whereas  will be the functions variable and allowed to vary freely this function will be called the likelihood\u000a\u000anote that    denotes a separation between the two input arguments  and the observations \u000ain practice it is often more convenient to work with the logarithm of the likelihood function called the loglikelihood\u000a\u000aor the average loglikelihood\u000a\u000athe hat over  indicates that it is akin to some estimator indeed  estimates the expected loglikelihood of a single observation in the model\u000athe method of maximum likelihood estimates 0 by finding a value of  that maximizes  this method of estimation defines a maximumlikelihood estimator mle of 0\u000a\u000aif a maximum exists an mle estimate is the same regardless of whether we maximize the likelihood or the loglikelihood function since log is a strictly monotonically increasing function\u000afor many models a maximum likelihood estimator can be found as an explicit function of the observed data x1  xn for many other models however no closedform solution to the maximization problem is known or available and an mle has to be found numerically using optimization methods for some problems there may be multiple estimates that maximize the likelihood for other problems no maximum likelihood estimate exists meaning that the loglikelihood function increases without attaining the supremum value\u000ain the exposition above it is assumed that the data are independent and identically distributed the method can be applied however to a broader setting as long as it is possible to write the joint density function fx1  xn   and its parameter  has a finite dimension which does not depend on the sample size n in a simpler extension an allowance can be made for data heterogeneity so that the joint density is equal to f1x1    f2x2    fnxn   put another way we are now assuming that each observation xi comes from a random variable that has its own distribution function fi in the more complicated case of time series models the independence assumption may have to be dropped as well\u000aa maximum likelihood estimator coincides with the most probable bayesian estimator given a uniform prior distribution on the parameters indeed the maximum a posteriori estimate is the parameter  that maximizes the probability of  given the data given by bayes theorem\u000a\u000awhere  is the prior distribution for the parameter  and where  is the probability of the data averaged over all parameters since the denominator is independent of  the bayesian estimator is obtained by maximizing  with respect to  if we further assume that the prior  is a uniform distribution the bayesian estimator is obtained by maximizing the likelihood function  thus the bayesian estimator coincides with the maximumlikelihood estimator for a uniform prior distribution \u000a\u000a\u000a propertiesedit \u000aa maximumlikelihood estimator is an extremum estimator obtained by maximizing as a function of  the objective function cf the loss function\u000a\u000athis being the sample analogue of the expected loglikelihood  where this expectation is taken with respect to the true density \u000amaximumlikelihood estimators have no optimum properties for finite samples in the sense that when evaluated on finite samples other estimators may have greater concentration around the true parametervalue however like other estimation methods maximumlikelihood estimation possesses a number of attractive limiting properties as the sample size increases to infinity sequences of maximumlikelihood estimators have these properties\u000aconsistency the sequence of mles converges in probability to the value being estimated\u000aasymptotic normality as the sample size increases the distribution of the mle tends to the gaussian distribution with mean  and covariance matrix equal to the inverse of the fisher information matrix\u000aefficiency ie it achieves the cramrrao lower bound when the sample size tends to infinity this means that no consistent estimator has lower asymptotic mean squared error than the mle or other estimators attaining this bound\u000asecondorder efficiency after correction for bias\u000a\u000a\u000a consistencyedit \u000aunder the conditions outlined below the maximum likelihood estimator is consistent the consistency means that having a sufficiently large number of observations n it is possible to find the value of 0 with arbitrary precision in mathematical terms this means that as n goes to infinity the estimator  converges in probability to its true value\u000a\u000aunder slightly stronger conditions the estimator converges almost surely or strongly to\u000a\u000ato establish consistency the following conditions are sufficient\u000a\u000athe dominance condition can be employed in the case of iid observations in the noniid case the uniform convergence in probability can be checked by showing that the sequence  is stochastically equicontinuous\u000aif one wants to demonstrate that the ml estimator  converges to 0 almost surely then a stronger condition of uniform convergence almost surely has to be imposed\u000a\u000a\u000a asymptotic normalityedit \u000ain a wide range of situations maximum likelihood parameter estimates exhibit asymptotic normality  that is they are equal to the true parameters plus a random error that is approximately normal given sufficient data and the errors variance decays as 1n for this property to hold it is necessary that the estimator does not suffer from the following issues\u000aestimate on boundary sometimes the maximum likelihood estimate lies on the boundary of the set of possible parameters or if the boundary is not strictly speaking allowed the likelihood gets larger and larger as the parameter approaches the boundary standard asymptotic theory needs the assumption that the true parameter value lies away from the boundary if we have enough data the maximum likelihood estimate will keep away from the boundary too but with smaller samples the estimate can lie on the boundary in such cases the asymptotic theory clearly does not give a practically useful approximation examples here would be variancecomponent models where each component of variance 2 must satisfy the constraint 2  0\u000adata boundary parameterdependent for the theory to apply in a simple way the set of data values which has positive probability or positive probability density should not depend on the unknown parameter a simple example where such parameterdependence does hold is the case of estimating  from a set of independent identically distributed when the common distribution is uniform on the range 0 for estimation purposes the relevant range of  is such that  cannot be less than the largest observation because the interval 0 is not compact there exists no maximum for the likelihood function for any estimate of theta there exists a greater estimate that also has greater likelihood in contrast the interval 0 includes the endpoint  and is compact in which case the maximumlikelihood estimator exists however in this case the maximumlikelihood estimator is biased asymptotically this maximumlikelihood estimator is not normally distributed\u000anuisance parameters for maximum likelihood estimations a model may have a number of nuisance parameters for the asymptotic behaviour outlined to hold the number of nuisance parameters should not increase with the number of observations the sample size a wellknown example of this case is where observations occur as pairs where the observations in each pair have a different unknown mean but otherwise the observations are independent and normally distributed with a common variance here for 2n observations there are n  1 parameters it is well known that the maximum likelihood estimate for the variance does not converge to the true value of the variance\u000aincreasing information for the asymptotics to hold in cases where the assumption of independent identically distributed observations does not hold a basic requirement is that the amount of information in the data increases indefinitely as the sample size increases such a requirement may not be met if either there is too much dependence in the data for example if new observations are essentially identical to existing observations or if new independent observations are subject to an increasing observation error\u000asome regularity conditions which ensure this behavior are\u000athe first and second derivatives of the loglikelihood function must be defined\u000athe fisher information matrix must not be zero and must be continuous as a function of the parameter\u000athe maximum likelihood estimator is consistent\u000asuppose that conditions for consistency of maximum likelihood estimator are satisfied and\u000a0  interior\u000afx    0 and is twice continuously differentiable in  in some neighborhood n of 0\u000a supnfx  dx   and  supnfx  dx  \u000ai  elnfx  0 lnfx  0 exists and is nonsingular\u000ae supnlnfx    \u000athen the maximum likelihood estimator has asymptotically normal distribution\u000a\u000aproof skipping the technicalities\u000asince the loglikelihood function is differentiable and 0 lies in the interior of the parameter set in the maximum the firstorder condition will be satisfied\u000a\u000awhen the loglikelihood is twice differentiable this expression can be expanded into a taylor series around the point   0\u000a\u000awhere  is some point intermediate between 0 and  from this expression we can derive that\u000a\u000ahere the expression in square brackets converges in probability to h  elnfx  0 by the law of large numbers the continuous mapping theorem ensures that the inverse of this expression also converges in probability to h1 the second sum by the central limit theorem converges in distribution to a multivariate normal with mean zero and variance matrix equal to the fisher information i thus applying slutskys theorem to the whole expression we obtain that\u000a\u000afinally the information equality guarantees that when the model is correctly specified matrix h will be equal to the fisher information i so that the variance expression simplifies to just i1\u000a\u000a\u000a functional invarianceedit \u000athe maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability or probability density in the continuous case if the parameter consists of a number of components then we define their separate maximum likelihood estimators as the corresponding component of the mle of the complete parameter consistent with this if  is the mle for  and if g is any transformation of  then the mle for   g is by definition\u000a\u000ait maximizes the socalled profile likelihood\u000a\u000athe mle is also invariant with respect to certain transformations of the data if y  gx where g is one to one and does not depend on the parameters to be estimated then the density functions satisfy\u000a\u000aand hence the likelihood functions for x and y differ only by a factor that does not depend on the model parameters\u000afor example the mle parameters of the lognormal distribution are the same as those of the normal distribution fitted to the logarithm of the data\u000a\u000a\u000a higherorder propertiesedit \u000athe standard asymptotics tells that the maximumlikelihood estimator is nconsistent and asymptotically efficient meaning that it reaches the cramrrao bound\u000a\u000awhere i is the fisher information matrix\u000a\u000ain particular it means that the bias of the maximumlikelihood estimator is equal to zero up to the order n12 however when we consider the higherorder terms in the expansion of the distribution of this estimator it turns out that mle has bias of order n1 this bias is equal to componentwise\u000a\u000awhere einsteins summation convention over the repeating indices has been adopted ijk denotes the jkth component of the inverse fisher information matrix i1 and\u000a\u000ausing these formulas it is possible to estimate the secondorder bias of the maximum likelihood estimator and correct for that bias by subtracting it\u000a\u000athis estimator is unbiased up to the terms of order n1 and is called the biascorrected maximum likelihood estimator\u000athis biascorrected estimator is secondorder efficient at least within the curved exponential family meaning that it has minimal mean squared error among all secondorder biascorrected estimators up to the terms of the order n2 it is possible to continue this process that is to derive the thirdorder biascorrection term and so on however as was shown by kano 1996 the maximumlikelihood estimator is not thirdorder efficient\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete uniform distributionedit \u000a\u000aconsider a case where n tickets numbered from 1 to n are placed in a box and one is selected at random see uniform distribution thus the sample size is 1 if n is unknown then the maximumlikelihood estimator  of n is the number m on the drawn ticket the likelihood is 0 for n  m 1n for n  m and this is greatest when n  m note that the maximum likelihood estimate of n occurs at the lower extreme of possible values m m  1  rather than somewhere in the middle of the range of possible values which would result in less bias the expected value of the number m on the drawn ticket and therefore the expected value of  is n  12 as a result with a sample size of 1 the maximum likelihood estimator for n will systematically underestimate n by n  12\u000a\u000a\u000a discrete distribution finite parameter spaceedit \u000asuppose one wishes to determine just how biased an unfair coin is call the probability of tossing a head p the goal then becomes to determine p\u000asuppose the coin is tossed 80 times ie the sample might be something like x1  h x2  t  x80  t and the count of the number of heads h is observed\u000athe probability of tossing tails is 1  p so here p is  above suppose the outcome is 49 heads and 31 tails and suppose the coin was taken from a box containing three coins one which gives heads with probability p  13 one which gives heads with probability p  12 and another which gives heads with probability p  23 the coins have lost their labels so which one it was is unknown using maximum likelihood estimation the coin that has the largest likelihood can be found given the data that were observed by using the probability mass function of the binomial distribution with sample size equal to 80 number successes equal to 49 but different values of p the probability of success the likelihood function defined below takes one of three values\u000a\u000athe likelihood is maximized when p  23 and so this is the maximum likelihood estimate for p\u000a\u000a\u000a discrete distribution continuous parameter spaceedit \u000anow suppose that there was only one coin but its p could have been any value 0  p  1 the likelihood function to be maximised is\u000a\u000aand the maximisation is over all possible values 0  p  1\u000a\u000aone way to maximize this function is by differentiating with respect to p and setting to zero\u000a\u000awhich has solutions p  0 p  1 and p  4980 the solution which maximizes the likelihood is clearly p  4980 since p  0 and p  1 result in a likelihood of zero thus the maximum likelihood estimator for p is 4980\u000athis result is easily generalized by substituting a letter such as t in the place of 49 to represent the observed number of successes of our bernoulli trials and a letter such as n in the place of 80 to represent the number of bernoulli trials exactly the same calculation yields the maximum likelihood estimator t  n for any sequence of n bernoulli trials resulting in t successes\u000a\u000a\u000a continuous distribution continuous parameter spaceedit \u000afor the normal distribution  which has probability density function\u000a\u000athe corresponding probability density function for a sample of n independent identically distributed normal random variables the likelihood is\u000a\u000aor more conveniently\u000a\u000awhere  is the sample mean\u000athis family of distributions has two parameters     so we maximize the likelihood  over both parameters simultaneously or if possible individually\u000asince the logarithm is a continuous strictly increasing function over the range of the likelihood the values which maximize the likelihood will also maximize its logarithm this log likelihood can be written as follows\u000a\u000anote the loglikelihood is closely related to information entropy and fisher information\u000awe now compute the derivatives of this log likelihood as follows\u000a\u000athis is solved by\u000a\u000athis is indeed the maximum of the function since it is the only turning point in  and the second derivative is strictly less than zero its expectation value is equal to the parameter  of the given distribution\u000a\u000awhich means that the maximumlikelihood estimator  is unbiased\u000asimilarly we differentiate the log likelihood with respect to  and equate to zero\u000a\u000awhich is solved by\u000a\u000ainserting the estimate  we obtain\u000a\u000ato calculate its expected value it is convenient to rewrite the expression in terms of zeromean random variables statistical error  expressing the estimate in these variables yields\u000a\u000asimplifying the expression above utilizing the facts that  and  allows us to obtain\u000a\u000athis means that the estimator  is biased however  is consistent\u000aformally we say that the maximum likelihood estimator for  is\u000a\u000ain this case the mles could be obtained individually in general this may not be the case and the mles would have to be obtained simultaneously\u000athe normal log likelihood at its maximum takes a particularly simple form\u000a\u000athis maximum log likelihood can be shown to be the same for more general least squares even for nonlinear least squares this is often used in determining likelihoodbased approximate confidence intervals and confidence regions which are generally more accurate than those using the asymptotic normality discussed above\u000a\u000a\u000a nonindependent variablesedit \u000ait may be the case that variables are correlated that is not independent two random variables x and y are independent only if their joint probability density function is the product of the individual probability density functions ie\u000a\u000asuppose one constructs an ordern gaussian vector out of random variables  where each variable has means given by  furthermore let the covariance matrix be denoted by \u000athe joint probability density function of these n random variables is then given by\u000a\u000ain the two variable case the joint probability density function is given by\u000a\u000ain this and other cases where a joint density function exists the likelihood function is defined as above in the section principles using this density\u000a\u000a\u000a iterative proceduresedit \u000aconsider problems where both states  and parameters such as  require to be estimated iterative procedures such as expectationmaximization algorithms may be used to solve joint stateparameter estimation problems\u000afor example suppose that n samples of state estimates  together with a sample mean  have been calculated by either a minimumvariance kalman filter or a minimumvariance smoother using a previous variance estimate  then the next variance iterate may be obtained from the maximum likelihood estimate calculation\u000a\u000athe convergence of mles within filtering and smoothing em algorithms are studied in  \u000a\u000a\u000a applicationsedit \u000amaximum likelihood estimation is used for a wide range of statistical models including\u000alinear models and generalized linear models\u000aexploratory and confirmatory factor analysis\u000astructural equation modeling\u000amany situations in the context of hypothesis testing and confidence interval \u000adiscrete choice models\u000athese uses arise across applications in widespread set of fields including\u000acommunication systems\u000apsychometrics\u000aeconometrics\u000atimedelay of arrival tdoa in acoustic or electromagnetic detection\u000adata modeling in nuclear and particle physics\u000amagnetic resonance imaging\u000acomputational phylogenetics\u000aorigindestination and pathchoice modeling in transport networks\u000ageographical satelliteimage classification\u000apower system state estimation\u000a\u000a\u000a see alsoedit \u000a\u000aother estimation methods\u000ageneralized method of moments are methods related to the likelihood equation in maximum likelihood estimation\u000amestimator an approach used in robust statistics\u000amaximum a posteriori map estimator for a contrast in the way to calculate estimators when prior knowledge is postulated\u000amaximum spacing estimation a related method that is more robust in many situations\u000amethod of moments statistics another popular method for finding parameters of distributions\u000amethod of support a variation of the maximum likelihood technique\u000aminimum distance estimation\u000aquasimaximum likelihood estimator an mle estimator that is misspecified but still consistent\u000arestricted maximum likelihood a variation using a likelihood function calculated from a transformed set of data\u000a\u000arelated concepts\u000athe bhhh algorithm is a nonlinear optimization algorithm that is popular for maximum likelihood estimations\u000aextremum estimator a more general class of estimators to which mle belongs\u000afisher information information matrix its relationship to covariance matrix of ml estimates\u000alikelihood function a description on what likelihood functions are\u000amean squared error a measure of how good an estimator of a distributional parameter is be it the maximum likelihood estimator or some other estimator\u000athe raoblackwell theorem a result which yields a process for finding the best possible unbiased estimator in the sense of having minimal mean squared error the mle is often a good starting place for the process\u000asufficient statistic a function of the data through which the mle if it exists and is unique will depend on the data\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 maximumlikelihood method encyclopedia of mathematics springer isbn 9781556080104 \u000amaximum likelihood estimation primer an excellent tutorial\u000aimplementing mle for your own likelihood function using r\u000aa selection of likelihood functions in r\u000atutorial on maximum likelihood estimation journal of mathematical psychology citeseerx 101174671
p91
sg14
g17
sg18
Vin statistics maximumlikelihood estimation mle is a method of estimating the parameters of a statistical model given data\u000athe method of maximum likelihood corresponds to many wellknown estimation methods in statistics for example one may be interested in the heights of adult female penguins but be unable to measure the height of every single penguin in a population due to cost or time constraints assuming that the heights are normally distributed with some unknown mean and variance the mean and variance can be estimated with mle while only knowing the heights of some sample of the overall population mle would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model\u000ain general for a fixed set of data and underlying statistical model the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function intuitively this maximizes the agreement of the selected model with the observed data and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution maximumlikelihood estimation gives a unified approach to estimation which is welldefined in the case of the normal distribution and many other problems however in some complicated problems difficulties do occur in such problems maximumlikelihood estimators are unsuitable or do not exist\u000a\u000a\u000a historyedit \u000a\u000amaximumlikelihood estimation was recommended analyzed with fruitless attempts at proofs and vastly popularized by ronald fisher between 1912 and 1922 although it had been used earlier by carl friedrich gauss pierresimon laplace thorvald n thiele and francis ysidro edgeworth reviews of the development of maximum likelihood have been provided by a number of authors\u000asome of the theory behind maximumlikelihood estimation was developed for bayesian statistics\u000a\u000a\u000a principlesedit \u000asuppose there is a sample x1 x2  xn of n independent and identically distributed observations coming from a distribution with an unknown probability density function f0 it is however surmised that the function f0 belongs to a certain family of distributions f    where  is a vector of parameters for this family called the parametric model so that f0  f0 the value 0 is unknown and is referred to as the true value of the parameter vector it is desirable to find an estimator  which would be as close to the true value 0 as possible either or both the observed variables xi and the parameter  can be vectors\u000ato use the method of maximum likelihood one first specifies the joint density function for all observations for an independent and identically distributed sample this joint density function is\u000a\u000anow we look at this function from a different perspective by considering the observed values x1 x2  xn to be fixed parameters of this function whereas  will be the functions variable and allowed to vary freely this function will be called the likelihood\u000a\u000anote that    denotes a separation between the two input arguments  and the observations \u000ain practice it is often more convenient to work with the logarithm of the likelihood function called the loglikelihood\u000a\u000aor the average loglikelihood\u000a\u000athe hat over  indicates that it is akin to some estimator indeed  estimates the expected loglikelihood of a single observation in the model\u000athe method of maximum likelihood estimates 0 by finding a value of  that maximizes  this method of estimation defines a maximumlikelihood estimator mle of 0\u000a\u000aif a maximum exists an mle estimate is the same regardless of whether we maximize the likelihood or the loglikelihood function since log is a strictly monotonically increasing function\u000afor many models a maximum likelihood estimator can be found as an explicit function of the observed data x1  xn for many other models however no closedform solution to the maximization problem is known or available and an mle has to be found numerically using optimization methods for some problems there may be multiple estimates that maximize the likelihood for other problems no maximum likelihood estimate exists meaning that the loglikelihood function increases without attaining the supremum value\u000ain the exposition above it is assumed that the data are independent and identically distributed the method can be applied however to a broader setting as long as it is possible to write the joint density function fx1  xn   and its parameter  has a finite dimension which does not depend on the sample size n in a simpler extension an allowance can be made for data heterogeneity so that the joint density is equal to f1x1    f2x2    fnxn   put another way we are now assuming that each observation xi comes from a random variable that has its own distribution function fi in the more complicated case of time series models the independence assumption may have to be dropped as well\u000aa maximum likelihood estimator coincides with the most probable bayesian estimator given a uniform prior distribution on the parameters indeed the maximum a posteriori estimate is the parameter  that maximizes the probability of  given the data given by bayes theorem\u000a\u000awhere  is the prior distribution for the parameter  and where  is the probability of the data averaged over all parameters since the denominator is independent of  the bayesian estimator is obtained by maximizing  with respect to  if we further assume that the prior  is a uniform distribution the bayesian estimator is obtained by maximizing the likelihood function  thus the bayesian estimator coincides with the maximumlikelihood estimator for a uniform prior distribution \u000a\u000a\u000a propertiesedit \u000aa maximumlikelihood estimator is an extremum estimator obtained by maximizing as a function of  the objective function cf the loss function\u000a\u000athis being the sample analogue of the expected loglikelihood  where this expectation is taken with respect to the true density \u000amaximumlikelihood estimators have no optimum properties for finite samples in the sense that when evaluated on finite samples other estimators may have greater concentration around the true parametervalue however like other estimation methods maximumlikelihood estimation possesses a number of attractive limiting properties as the sample size increases to infinity sequences of maximumlikelihood estimators have these properties\u000aconsistency the sequence of mles converges in probability to the value being estimated\u000aasymptotic normality as the sample size increases the distribution of the mle tends to the gaussian distribution with mean  and covariance matrix equal to the inverse of the fisher information matrix\u000aefficiency ie it achieves the cramrrao lower bound when the sample size tends to infinity this means that no consistent estimator has lower asymptotic mean squared error than the mle or other estimators attaining this bound\u000asecondorder efficiency after correction for bias\u000a\u000a\u000a consistencyedit \u000aunder the conditions outlined below the maximum likelihood estimator is consistent the consistency means that having a sufficiently large number of observations n it is possible to find the value of 0 with arbitrary precision in mathematical terms this means that as n goes to infinity the estimator  converges in probability to its true value\u000a\u000aunder slightly stronger conditions the estimator converges almost surely or strongly to\u000a\u000ato establish consistency the following conditions are sufficient\u000a\u000athe dominance condition can be employed in the case of iid observations in the noniid case the uniform convergence in probability can be checked by showing that the sequence  is stochastically equicontinuous\u000aif one wants to demonstrate that the ml estimator  converges to 0 almost surely then a stronger condition of uniform convergence almost surely has to be imposed\u000a\u000a\u000a asymptotic normalityedit \u000ain a wide range of situations maximum likelihood parameter estimates exhibit asymptotic normality  that is they are equal to the true parameters plus a random error that is approximately normal given sufficient data and the errors variance decays as 1n for this property to hold it is necessary that the estimator does not suffer from the following issues\u000aestimate on boundary sometimes the maximum likelihood estimate lies on the boundary of the set of possible parameters or if the boundary is not strictly speaking allowed the likelihood gets larger and larger as the parameter approaches the boundary standard asymptotic theory needs the assumption that the true parameter value lies away from the boundary if we have enough data the maximum likelihood estimate will keep away from the boundary too but with smaller samples the estimate can lie on the boundary in such cases the asymptotic theory clearly does not give a practically useful approximation examples here would be variancecomponent models where each component of variance 2 must satisfy the constraint 2  0\u000adata boundary parameterdependent for the theory to apply in a simple way the set of data values which has positive probability or positive probability density should not depend on the unknown parameter a simple example where such parameterdependence does hold is the case of estimating  from a set of independent identically distributed when the common distribution is uniform on the range 0 for estimation purposes the relevant range of  is such that  cannot be less than the largest observation because the interval 0 is not compact there exists no maximum for the likelihood function for any estimate of theta there exists a greater estimate that also has greater likelihood in contrast the interval 0 includes the endpoint  and is compact in which case the maximumlikelihood estimator exists however in this case the maximumlikelihood estimator is biased asymptotically this maximumlikelihood estimator is not normally distributed\u000anuisance parameters for maximum likelihood estimations a model may have a number of nuisance parameters for the asymptotic behaviour outlined to hold the number of nuisance parameters should not increase with the number of observations the sample size a wellknown example of this case is where observations occur as pairs where the observations in each pair have a different unknown mean but otherwise the observations are independent and normally distributed with a common variance here for 2n observations there are n  1 parameters it is well known that the maximum likelihood estimate for the variance does not converge to the true value of the variance\u000aincreasing information for the asymptotics to hold in cases where the assumption of independent identically distributed observations does not hold a basic requirement is that the amount of information in the data increases indefinitely as the sample size increases such a requirement may not be met if either there is too much dependence in the data for example if new observations are essentially identical to existing observations or if new independent observations are subject to an increasing observation error\u000asome regularity conditions which ensure this behavior are\u000athe first and second derivatives of the loglikelihood function must be defined\u000athe fisher information matrix must not be zero and must be continuous as a function of the parameter\u000athe maximum likelihood estimator is consistent\u000asuppose that conditions for consistency of maximum likelihood estimator are satisfied and\u000a0  interior\u000afx    0 and is twice continuously differentiable in  in some neighborhood n of 0\u000a supnfx  dx   and  supnfx  dx  \u000ai  elnfx  0 lnfx  0 exists and is nonsingular\u000ae supnlnfx    \u000athen the maximum likelihood estimator has asymptotically normal distribution\u000a\u000aproof skipping the technicalities\u000asince the loglikelihood function is differentiable and 0 lies in the interior of the parameter set in the maximum the firstorder condition will be satisfied\u000a\u000awhen the loglikelihood is twice differentiable this expression can be expanded into a taylor series around the point   0\u000a\u000awhere  is some point intermediate between 0 and  from this expression we can derive that\u000a\u000ahere the expression in square brackets converges in probability to h  elnfx  0 by the law of large numbers the continuous mapping theorem ensures that the inverse of this expression also converges in probability to h1 the second sum by the central limit theorem converges in distribution to a multivariate normal with mean zero and variance matrix equal to the fisher information i thus applying slutskys theorem to the whole expression we obtain that\u000a\u000afinally the information equality guarantees that when the model is correctly specified matrix h will be equal to the fisher information i so that the variance expression simplifies to just i1\u000a\u000a\u000a functional invarianceedit \u000athe maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability or probability density in the continuous case if the parameter consists of a number of components then we define their separate maximum likelihood estimators as the corresponding component of the mle of the complete parameter consistent with this if  is the mle for  and if g is any transformation of  then the mle for   g is by definition\u000a\u000ait maximizes the socalled profile likelihood\u000a\u000athe mle is also invariant with respect to certain transformations of the data if y  gx where g is one to one and does not depend on the parameters to be estimated then the density functions satisfy\u000a\u000aand hence the likelihood functions for x and y differ only by a factor that does not depend on the model parameters\u000afor example the mle parameters of the lognormal distribution are the same as those of the normal distribution fitted to the logarithm of the data\u000a\u000a\u000a higherorder propertiesedit \u000athe standard asymptotics tells that the maximumlikelihood estimator is nconsistent and asymptotically efficient meaning that it reaches the cramrrao bound\u000a\u000awhere i is the fisher information matrix\u000a\u000ain particular it means that the bias of the maximumlikelihood estimator is equal to zero up to the order n12 however when we consider the higherorder terms in the expansion of the distribution of this estimator it turns out that mle has bias of order n1 this bias is equal to componentwise\u000a\u000awhere einsteins summation convention over the repeating indices has been adopted ijk denotes the jkth component of the inverse fisher information matrix i1 and\u000a\u000ausing these formulas it is possible to estimate the secondorder bias of the maximum likelihood estimator and correct for that bias by subtracting it\u000a\u000athis estimator is unbiased up to the terms of order n1 and is called the biascorrected maximum likelihood estimator\u000athis biascorrected estimator is secondorder efficient at least within the curved exponential family meaning that it has minimal mean squared error among all secondorder biascorrected estimators up to the terms of the order n2 it is possible to continue this process that is to derive the thirdorder biascorrection term and so on however as was shown by kano 1996 the maximumlikelihood estimator is not thirdorder efficient\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete uniform distributionedit \u000a\u000aconsider a case where n tickets numbered from 1 to n are placed in a box and one is selected at random see uniform distribution thus the sample size is 1 if n is unknown then the maximumlikelihood estimator  of n is the number m on the drawn ticket the likelihood is 0 for n  m 1n for n  m and this is greatest when n  m note that the maximum likelihood estimate of n occurs at the lower extreme of possible values m m  1  rather than somewhere in the middle of the range of possible values which would result in less bias the expected value of the number m on the drawn ticket and therefore the expected value of  is n  12 as a result with a sample size of 1 the maximum likelihood estimator for n will systematically underestimate n by n  12\u000a\u000a\u000a discrete distribution finite parameter spaceedit \u000asuppose one wishes to determine just how biased an unfair coin is call the probability of tossing a head p the goal then becomes to determine p\u000asuppose the coin is tossed 80 times ie the sample might be something like x1  h x2  t  x80  t and the count of the number of heads h is observed\u000athe probability of tossing tails is 1  p so here p is  above suppose the outcome is 49 heads and 31 tails and suppose the coin was taken from a box containing three coins one which gives heads with probability p  13 one which gives heads with probability p  12 and another which gives heads with probability p  23 the coins have lost their labels so which one it was is unknown using maximum likelihood estimation the coin that has the largest likelihood can be found given the data that were observed by using the probability mass function of the binomial distribution with sample size equal to 80 number successes equal to 49 but different values of p the probability of success the likelihood function defined below takes one of three values\u000a\u000athe likelihood is maximized when p  23 and so this is the maximum likelihood estimate for p\u000a\u000a\u000a discrete distribution continuous parameter spaceedit \u000anow suppose that there was only one coin but its p could have been any value 0  p  1 the likelihood function to be maximised is\u000a\u000aand the maximisation is over all possible values 0  p  1\u000a\u000aone way to maximize this function is by differentiating with respect to p and setting to zero\u000a\u000awhich has solutions p  0 p  1 and p  4980 the solution which maximizes the likelihood is clearly p  4980 since p  0 and p  1 result in a likelihood of zero thus the maximum likelihood estimator for p is 4980\u000athis result is easily generalized by substituting a letter such as t in the place of 49 to represent the observed number of successes of our bernoulli trials and a letter such as n in the place of 80 to represent the number of bernoulli trials exactly the same calculation yields the maximum likelihood estimator t  n for any sequence of n bernoulli trials resulting in t successes\u000a\u000a\u000a continuous distribution continuous parameter spaceedit \u000afor the normal distribution  which has probability density function\u000a\u000athe corresponding probability density function for a sample of n independent identically distributed normal random variables the likelihood is\u000a\u000aor more conveniently\u000a\u000awhere  is the sample mean\u000athis family of distributions has two parameters     so we maximize the likelihood  over both parameters simultaneously or if possible individually\u000asince the logarithm is a continuous strictly increasing function over the range of the likelihood the values which maximize the likelihood will also maximize its logarithm this log likelihood can be written as follows\u000a\u000anote the loglikelihood is closely related to information entropy and fisher information\u000awe now compute the derivatives of this log likelihood as follows\u000a\u000athis is solved by\u000a\u000athis is indeed the maximum of the function since it is the only turning point in  and the second derivative is strictly less than zero its expectation value is equal to the parameter  of the given distribution\u000a\u000awhich means that the maximumlikelihood estimator  is unbiased\u000asimilarly we differentiate the log likelihood with respect to  and equate to zero\u000a\u000awhich is solved by\u000a\u000ainserting the estimate  we obtain\u000a\u000ato calculate its expected value it is convenient to rewrite the expression in terms of zeromean random variables statistical error  expressing the estimate in these variables yields\u000a\u000asimplifying the expression above utilizing the facts that  and  allows us to obtain\u000a\u000athis means that the estimator  is biased however  is consistent\u000aformally we say that the maximum likelihood estimator for  is\u000a\u000ain this case the mles could be obtained individually in general this may not be the case and the mles would have to be obtained simultaneously\u000athe normal log likelihood at its maximum takes a particularly simple form\u000a\u000athis maximum log likelihood can be shown to be the same for more general least squares even for nonlinear least squares this is often used in determining likelihoodbased approximate confidence intervals and confidence regions which are generally more accurate than those using the asymptotic normality discussed above\u000a\u000a\u000a nonindependent variablesedit \u000ait may be the case that variables are correlated that is not independent two random variables x and y are independent only if their joint probability density function is the product of the individual probability density functions ie\u000a\u000asuppose one constructs an ordern gaussian vector out of random variables  where each variable has means given by  furthermore let the covariance matrix be denoted by \u000athe joint probability density function of these n random variables is then given by\u000a\u000ain the two variable case the joint probability density function is given by\u000a\u000ain this and other cases where a joint density function exists the likelihood function is defined as above in the section principles using this density\u000a\u000a\u000a iterative proceduresedit \u000aconsider problems where both states  and parameters such as  require to be estimated iterative procedures such as expectationmaximization algorithms may be used to solve joint stateparameter estimation problems\u000afor example suppose that n samples of state estimates  together with a sample mean  have been calculated by either a minimumvariance kalman filter or a minimumvariance smoother using a previous variance estimate  then the next variance iterate may be obtained from the maximum likelihood estimate calculation\u000a\u000athe convergence of mles within filtering and smoothing em algorithms are studied in  \u000a\u000a\u000a applicationsedit \u000amaximum likelihood estimation is used for a wide range of statistical models including\u000alinear models and generalized linear models\u000aexploratory and confirmatory factor analysis\u000astructural equation modeling\u000amany situations in the context of hypothesis testing and confidence interval \u000adiscrete choice models\u000athese uses arise across applications in widespread set of fields including\u000acommunication systems\u000apsychometrics\u000aeconometrics\u000atimedelay of arrival tdoa in acoustic or electromagnetic detection\u000adata modeling in nuclear and particle physics\u000amagnetic resonance imaging\u000acomputational phylogenetics\u000aorigindestination and pathchoice modeling in transport networks\u000ageographical satelliteimage classification\u000apower system state estimation\u000a\u000a\u000a see alsoedit \u000a\u000aother estimation methods\u000ageneralized method of moments are methods related to the likelihood equation in maximum likelihood estimation\u000amestimator an approach used in robust statistics\u000amaximum a posteriori map estimator for a contrast in the way to calculate estimators when prior knowledge is postulated\u000amaximum spacing estimation a related method that is more robust in many situations\u000amethod of moments statistics another popular method for finding parameters of distributions\u000amethod of support a variation of the maximum likelihood technique\u000aminimum distance estimation\u000aquasimaximum likelihood estimator an mle estimator that is misspecified but still consistent\u000arestricted maximum likelihood a variation using a likelihood function calculated from a transformed set of data\u000a\u000arelated concepts\u000athe bhhh algorithm is a nonlinear optimization algorithm that is popular for maximum likelihood estimations\u000aextremum estimator a more general class of estimators to which mle belongs\u000afisher information information matrix its relationship to covariance matrix of ml estimates\u000alikelihood function a description on what likelihood functions are\u000amean squared error a measure of how good an estimator of a distributional parameter is be it the maximum likelihood estimator or some other estimator\u000athe raoblackwell theorem a result which yields a process for finding the best possible unbiased estimator in the sense of having minimal mean squared error the mle is often a good starting place for the process\u000asufficient statistic a function of the data through which the mle if it exists and is unique will depend on the data\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 maximumlikelihood method encyclopedia of mathematics springer isbn 9781556080104 \u000amaximum likelihood estimation primer an excellent tutorial\u000aimplementing mle for your own likelihood function using r\u000aa selection of likelihood functions in r\u000atutorial on maximum likelihood estimation journal of mathematical psychology citeseerx 101174671
p92
sg20
g23
sg24
g27
sg30
Vin statistics maximumlikelihood estimation mle is a method of estimating the parameters of a statistical model given data\u000athe method of maximum likelihood corresponds to many wellknown estimation methods in statistics for example one may be interested in the heights of adult female penguins but be unable to measure the height of every single penguin in a population due to cost or time constraints assuming that the heights are normally distributed with some unknown mean and variance the mean and variance can be estimated with mle while only knowing the heights of some sample of the overall population mle would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model\u000ain general for a fixed set of data and underlying statistical model the method of maximum likelihood selects the set of values of the model parameters that maximizes the likelihood function intuitively this maximizes the agreement of the selected model with the observed data and for discrete random variables it indeed maximizes the probability of the observed data under the resulting distribution maximumlikelihood estimation gives a unified approach to estimation which is welldefined in the case of the normal distribution and many other problems however in some complicated problems difficulties do occur in such problems maximumlikelihood estimators are unsuitable or do not exist\u000a\u000a\u000a historyedit \u000a\u000amaximumlikelihood estimation was recommended analyzed with fruitless attempts at proofs and vastly popularized by ronald fisher between 1912 and 1922 although it had been used earlier by carl friedrich gauss pierresimon laplace thorvald n thiele and francis ysidro edgeworth reviews of the development of maximum likelihood have been provided by a number of authors\u000asome of the theory behind maximumlikelihood estimation was developed for bayesian statistics\u000a\u000a\u000a principlesedit \u000asuppose there is a sample x1 x2  xn of n independent and identically distributed observations coming from a distribution with an unknown probability density function f0 it is however surmised that the function f0 belongs to a certain family of distributions f    where  is a vector of parameters for this family called the parametric model so that f0  f0 the value 0 is unknown and is referred to as the true value of the parameter vector it is desirable to find an estimator  which would be as close to the true value 0 as possible either or both the observed variables xi and the parameter  can be vectors\u000ato use the method of maximum likelihood one first specifies the joint density function for all observations for an independent and identically distributed sample this joint density function is\u000a\u000anow we look at this function from a different perspective by considering the observed values x1 x2  xn to be fixed parameters of this function whereas  will be the functions variable and allowed to vary freely this function will be called the likelihood\u000a\u000anote that    denotes a separation between the two input arguments  and the observations \u000ain practice it is often more convenient to work with the logarithm of the likelihood function called the loglikelihood\u000a\u000aor the average loglikelihood\u000a\u000athe hat over  indicates that it is akin to some estimator indeed  estimates the expected loglikelihood of a single observation in the model\u000athe method of maximum likelihood estimates 0 by finding a value of  that maximizes  this method of estimation defines a maximumlikelihood estimator mle of 0\u000a\u000aif a maximum exists an mle estimate is the same regardless of whether we maximize the likelihood or the loglikelihood function since log is a strictly monotonically increasing function\u000afor many models a maximum likelihood estimator can be found as an explicit function of the observed data x1  xn for many other models however no closedform solution to the maximization problem is known or available and an mle has to be found numerically using optimization methods for some problems there may be multiple estimates that maximize the likelihood for other problems no maximum likelihood estimate exists meaning that the loglikelihood function increases without attaining the supremum value\u000ain the exposition above it is assumed that the data are independent and identically distributed the method can be applied however to a broader setting as long as it is possible to write the joint density function fx1  xn   and its parameter  has a finite dimension which does not depend on the sample size n in a simpler extension an allowance can be made for data heterogeneity so that the joint density is equal to f1x1    f2x2    fnxn   put another way we are now assuming that each observation xi comes from a random variable that has its own distribution function fi in the more complicated case of time series models the independence assumption may have to be dropped as well\u000aa maximum likelihood estimator coincides with the most probable bayesian estimator given a uniform prior distribution on the parameters indeed the maximum a posteriori estimate is the parameter  that maximizes the probability of  given the data given by bayes theorem\u000a\u000awhere  is the prior distribution for the parameter  and where  is the probability of the data averaged over all parameters since the denominator is independent of  the bayesian estimator is obtained by maximizing  with respect to  if we further assume that the prior  is a uniform distribution the bayesian estimator is obtained by maximizing the likelihood function  thus the bayesian estimator coincides with the maximumlikelihood estimator for a uniform prior distribution \u000a\u000a\u000a propertiesedit \u000aa maximumlikelihood estimator is an extremum estimator obtained by maximizing as a function of  the objective function cf the loss function\u000a\u000athis being the sample analogue of the expected loglikelihood  where this expectation is taken with respect to the true density \u000amaximumlikelihood estimators have no optimum properties for finite samples in the sense that when evaluated on finite samples other estimators may have greater concentration around the true parametervalue however like other estimation methods maximumlikelihood estimation possesses a number of attractive limiting properties as the sample size increases to infinity sequences of maximumlikelihood estimators have these properties\u000aconsistency the sequence of mles converges in probability to the value being estimated\u000aasymptotic normality as the sample size increases the distribution of the mle tends to the gaussian distribution with mean  and covariance matrix equal to the inverse of the fisher information matrix\u000aefficiency ie it achieves the cramrrao lower bound when the sample size tends to infinity this means that no consistent estimator has lower asymptotic mean squared error than the mle or other estimators attaining this bound\u000asecondorder efficiency after correction for bias\u000a\u000a\u000a consistencyedit \u000aunder the conditions outlined below the maximum likelihood estimator is consistent the consistency means that having a sufficiently large number of observations n it is possible to find the value of 0 with arbitrary precision in mathematical terms this means that as n goes to infinity the estimator  converges in probability to its true value\u000a\u000aunder slightly stronger conditions the estimator converges almost surely or strongly to\u000a\u000ato establish consistency the following conditions are sufficient\u000a\u000athe dominance condition can be employed in the case of iid observations in the noniid case the uniform convergence in probability can be checked by showing that the sequence  is stochastically equicontinuous\u000aif one wants to demonstrate that the ml estimator  converges to 0 almost surely then a stronger condition of uniform convergence almost surely has to be imposed\u000a\u000a\u000a asymptotic normalityedit \u000ain a wide range of situations maximum likelihood parameter estimates exhibit asymptotic normality  that is they are equal to the true parameters plus a random error that is approximately normal given sufficient data and the errors variance decays as 1n for this property to hold it is necessary that the estimator does not suffer from the following issues\u000aestimate on boundary sometimes the maximum likelihood estimate lies on the boundary of the set of possible parameters or if the boundary is not strictly speaking allowed the likelihood gets larger and larger as the parameter approaches the boundary standard asymptotic theory needs the assumption that the true parameter value lies away from the boundary if we have enough data the maximum likelihood estimate will keep away from the boundary too but with smaller samples the estimate can lie on the boundary in such cases the asymptotic theory clearly does not give a practically useful approximation examples here would be variancecomponent models where each component of variance 2 must satisfy the constraint 2  0\u000adata boundary parameterdependent for the theory to apply in a simple way the set of data values which has positive probability or positive probability density should not depend on the unknown parameter a simple example where such parameterdependence does hold is the case of estimating  from a set of independent identically distributed when the common distribution is uniform on the range 0 for estimation purposes the relevant range of  is such that  cannot be less than the largest observation because the interval 0 is not compact there exists no maximum for the likelihood function for any estimate of theta there exists a greater estimate that also has greater likelihood in contrast the interval 0 includes the endpoint  and is compact in which case the maximumlikelihood estimator exists however in this case the maximumlikelihood estimator is biased asymptotically this maximumlikelihood estimator is not normally distributed\u000anuisance parameters for maximum likelihood estimations a model may have a number of nuisance parameters for the asymptotic behaviour outlined to hold the number of nuisance parameters should not increase with the number of observations the sample size a wellknown example of this case is where observations occur as pairs where the observations in each pair have a different unknown mean but otherwise the observations are independent and normally distributed with a common variance here for 2n observations there are n  1 parameters it is well known that the maximum likelihood estimate for the variance does not converge to the true value of the variance\u000aincreasing information for the asymptotics to hold in cases where the assumption of independent identically distributed observations does not hold a basic requirement is that the amount of information in the data increases indefinitely as the sample size increases such a requirement may not be met if either there is too much dependence in the data for example if new observations are essentially identical to existing observations or if new independent observations are subject to an increasing observation error\u000asome regularity conditions which ensure this behavior are\u000athe first and second derivatives of the loglikelihood function must be defined\u000athe fisher information matrix must not be zero and must be continuous as a function of the parameter\u000athe maximum likelihood estimator is consistent\u000asuppose that conditions for consistency of maximum likelihood estimator are satisfied and\u000a0  interior\u000afx    0 and is twice continuously differentiable in  in some neighborhood n of 0\u000a supnfx  dx   and  supnfx  dx  \u000ai  elnfx  0 lnfx  0 exists and is nonsingular\u000ae supnlnfx    \u000athen the maximum likelihood estimator has asymptotically normal distribution\u000a\u000aproof skipping the technicalities\u000asince the loglikelihood function is differentiable and 0 lies in the interior of the parameter set in the maximum the firstorder condition will be satisfied\u000a\u000awhen the loglikelihood is twice differentiable this expression can be expanded into a taylor series around the point   0\u000a\u000awhere  is some point intermediate between 0 and  from this expression we can derive that\u000a\u000ahere the expression in square brackets converges in probability to h  elnfx  0 by the law of large numbers the continuous mapping theorem ensures that the inverse of this expression also converges in probability to h1 the second sum by the central limit theorem converges in distribution to a multivariate normal with mean zero and variance matrix equal to the fisher information i thus applying slutskys theorem to the whole expression we obtain that\u000a\u000afinally the information equality guarantees that when the model is correctly specified matrix h will be equal to the fisher information i so that the variance expression simplifies to just i1\u000a\u000a\u000a functional invarianceedit \u000athe maximum likelihood estimator selects the parameter value which gives the observed data the largest possible probability or probability density in the continuous case if the parameter consists of a number of components then we define their separate maximum likelihood estimators as the corresponding component of the mle of the complete parameter consistent with this if  is the mle for  and if g is any transformation of  then the mle for   g is by definition\u000a\u000ait maximizes the socalled profile likelihood\u000a\u000athe mle is also invariant with respect to certain transformations of the data if y  gx where g is one to one and does not depend on the parameters to be estimated then the density functions satisfy\u000a\u000aand hence the likelihood functions for x and y differ only by a factor that does not depend on the model parameters\u000afor example the mle parameters of the lognormal distribution are the same as those of the normal distribution fitted to the logarithm of the data\u000a\u000a\u000a higherorder propertiesedit \u000athe standard asymptotics tells that the maximumlikelihood estimator is nconsistent and asymptotically efficient meaning that it reaches the cramrrao bound\u000a\u000awhere i is the fisher information matrix\u000a\u000ain particular it means that the bias of the maximumlikelihood estimator is equal to zero up to the order n12 however when we consider the higherorder terms in the expansion of the distribution of this estimator it turns out that mle has bias of order n1 this bias is equal to componentwise\u000a\u000awhere einsteins summation convention over the repeating indices has been adopted ijk denotes the jkth component of the inverse fisher information matrix i1 and\u000a\u000ausing these formulas it is possible to estimate the secondorder bias of the maximum likelihood estimator and correct for that bias by subtracting it\u000a\u000athis estimator is unbiased up to the terms of order n1 and is called the biascorrected maximum likelihood estimator\u000athis biascorrected estimator is secondorder efficient at least within the curved exponential family meaning that it has minimal mean squared error among all secondorder biascorrected estimators up to the terms of the order n2 it is possible to continue this process that is to derive the thirdorder biascorrection term and so on however as was shown by kano 1996 the maximumlikelihood estimator is not thirdorder efficient\u000a\u000a\u000a examplesedit \u000a\u000a\u000a discrete uniform distributionedit \u000a\u000aconsider a case where n tickets numbered from 1 to n are placed in a box and one is selected at random see uniform distribution thus the sample size is 1 if n is unknown then the maximumlikelihood estimator  of n is the number m on the drawn ticket the likelihood is 0 for n  m 1n for n  m and this is greatest when n  m note that the maximum likelihood estimate of n occurs at the lower extreme of possible values m m  1  rather than somewhere in the middle of the range of possible values which would result in less bias the expected value of the number m on the drawn ticket and therefore the expected value of  is n  12 as a result with a sample size of 1 the maximum likelihood estimator for n will systematically underestimate n by n  12\u000a\u000a\u000a discrete distribution finite parameter spaceedit \u000asuppose one wishes to determine just how biased an unfair coin is call the probability of tossing a head p the goal then becomes to determine p\u000asuppose the coin is tossed 80 times ie the sample might be something like x1  h x2  t  x80  t and the count of the number of heads h is observed\u000athe probability of tossing tails is 1  p so here p is  above suppose the outcome is 49 heads and 31 tails and suppose the coin was taken from a box containing three coins one which gives heads with probability p  13 one which gives heads with probability p  12 and another which gives heads with probability p  23 the coins have lost their labels so which one it was is unknown using maximum likelihood estimation the coin that has the largest likelihood can be found given the data that were observed by using the probability mass function of the binomial distribution with sample size equal to 80 number successes equal to 49 but different values of p the probability of success the likelihood function defined below takes one of three values\u000a\u000athe likelihood is maximized when p  23 and so this is the maximum likelihood estimate for p\u000a\u000a\u000a discrete distribution continuous parameter spaceedit \u000anow suppose that there was only one coin but its p could have been any value 0  p  1 the likelihood function to be maximised is\u000a\u000aand the maximisation is over all possible values 0  p  1\u000a\u000aone way to maximize this function is by differentiating with respect to p and setting to zero\u000a\u000awhich has solutions p  0 p  1 and p  4980 the solution which maximizes the likelihood is clearly p  4980 since p  0 and p  1 result in a likelihood of zero thus the maximum likelihood estimator for p is 4980\u000athis result is easily generalized by substituting a letter such as t in the place of 49 to represent the observed number of successes of our bernoulli trials and a letter such as n in the place of 80 to represent the number of bernoulli trials exactly the same calculation yields the maximum likelihood estimator t  n for any sequence of n bernoulli trials resulting in t successes\u000a\u000a\u000a continuous distribution continuous parameter spaceedit \u000afor the normal distribution  which has probability density function\u000a\u000athe corresponding probability density function for a sample of n independent identically distributed normal random variables the likelihood is\u000a\u000aor more conveniently\u000a\u000awhere  is the sample mean\u000athis family of distributions has two parameters     so we maximize the likelihood  over both parameters simultaneously or if possible individually\u000asince the logarithm is a continuous strictly increasing function over the range of the likelihood the values which maximize the likelihood will also maximize its logarithm this log likelihood can be written as follows\u000a\u000anote the loglikelihood is closely related to information entropy and fisher information\u000awe now compute the derivatives of this log likelihood as follows\u000a\u000athis is solved by\u000a\u000athis is indeed the maximum of the function since it is the only turning point in  and the second derivative is strictly less than zero its expectation value is equal to the parameter  of the given distribution\u000a\u000awhich means that the maximumlikelihood estimator  is unbiased\u000asimilarly we differentiate the log likelihood with respect to  and equate to zero\u000a\u000awhich is solved by\u000a\u000ainserting the estimate  we obtain\u000a\u000ato calculate its expected value it is convenient to rewrite the expression in terms of zeromean random variables statistical error  expressing the estimate in these variables yields\u000a\u000asimplifying the expression above utilizing the facts that  and  allows us to obtain\u000a\u000athis means that the estimator  is biased however  is consistent\u000aformally we say that the maximum likelihood estimator for  is\u000a\u000ain this case the mles could be obtained individually in general this may not be the case and the mles would have to be obtained simultaneously\u000athe normal log likelihood at its maximum takes a particularly simple form\u000a\u000athis maximum log likelihood can be shown to be the same for more general least squares even for nonlinear least squares this is often used in determining likelihoodbased approximate confidence intervals and confidence regions which are generally more accurate than those using the asymptotic normality discussed above\u000a\u000a\u000a nonindependent variablesedit \u000ait may be the case that variables are correlated that is not independent two random variables x and y are independent only if their joint probability density function is the product of the individual probability density functions ie\u000a\u000asuppose one constructs an ordern gaussian vector out of random variables  where each variable has means given by  furthermore let the covariance matrix be denoted by \u000athe joint probability density function of these n random variables is then given by\u000a\u000ain the two variable case the joint probability density function is given by\u000a\u000ain this and other cases where a joint density function exists the likelihood function is defined as above in the section principles using this density\u000a\u000a\u000a iterative proceduresedit \u000aconsider problems where both states  and parameters such as  require to be estimated iterative procedures such as expectationmaximization algorithms may be used to solve joint stateparameter estimation problems\u000afor example suppose that n samples of state estimates  together with a sample mean  have been calculated by either a minimumvariance kalman filter or a minimumvariance smoother using a previous variance estimate  then the next variance iterate may be obtained from the maximum likelihood estimate calculation\u000a\u000athe convergence of mles within filtering and smoothing em algorithms are studied in  \u000a\u000a\u000a applicationsedit \u000amaximum likelihood estimation is used for a wide range of statistical models including\u000alinear models and generalized linear models\u000aexploratory and confirmatory factor analysis\u000astructural equation modeling\u000amany situations in the context of hypothesis testing and confidence interval \u000adiscrete choice models\u000athese uses arise across applications in widespread set of fields including\u000acommunication systems\u000apsychometrics\u000aeconometrics\u000atimedelay of arrival tdoa in acoustic or electromagnetic detection\u000adata modeling in nuclear and particle physics\u000amagnetic resonance imaging\u000acomputational phylogenetics\u000aorigindestination and pathchoice modeling in transport networks\u000ageographical satelliteimage classification\u000apower system state estimation\u000a\u000a\u000a see alsoedit \u000a\u000aother estimation methods\u000ageneralized method of moments are methods related to the likelihood equation in maximum likelihood estimation\u000amestimator an approach used in robust statistics\u000amaximum a posteriori map estimator for a contrast in the way to calculate estimators when prior knowledge is postulated\u000amaximum spacing estimation a related method that is more robust in many situations\u000amethod of moments statistics another popular method for finding parameters of distributions\u000amethod of support a variation of the maximum likelihood technique\u000aminimum distance estimation\u000aquasimaximum likelihood estimator an mle estimator that is misspecified but still consistent\u000arestricted maximum likelihood a variation using a likelihood function calculated from a transformed set of data\u000a\u000arelated concepts\u000athe bhhh algorithm is a nonlinear optimization algorithm that is popular for maximum likelihood estimations\u000aextremum estimator a more general class of estimators to which mle belongs\u000afisher information information matrix its relationship to covariance matrix of ml estimates\u000alikelihood function a description on what likelihood functions are\u000amean squared error a measure of how good an estimator of a distributional parameter is be it the maximum likelihood estimator or some other estimator\u000athe raoblackwell theorem a result which yields a process for finding the best possible unbiased estimator in the sense of having minimal mean squared error the mle is often a good starting place for the process\u000asufficient statistic a function of the data through which the mle if it exists and is unique will depend on the data\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 maximumlikelihood method encyclopedia of mathematics springer isbn 9781556080104 \u000amaximum likelihood estimation primer an excellent tutorial\u000aimplementing mle for your own likelihood function using r\u000aa selection of likelihood functions in r\u000atutorial on maximum likelihood estimation journal of mathematical psychology citeseerx 101174671
p93
sg32
g35
sg37
NsbsS'consistent_estimator.txt'
p94
g2
(g3
g4
Ntp95
Rp96
(dp97
g8
g11
sg12
Vin statistics a consistent estimator or asymptotically consistent estimator is an estimatora rule for computing estimates of a parameter 0having the property that as the number of data points used increases indefinitely the resulting sequence of estimates converges in probability to 0 this means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated so that the probability of the estimator being arbitrarily close to 0 converges to one\u000ain practice one constructs an estimator as a function of an available sample of size n and then imagines being able to keep collecting data and expanding the sample ad infinitum in this way one would obtain a sequence of estimates indexed by n and consistency is a property of what occurs as the sample size grows to infinity if the sequence of estimates can be mathematically shown to converge in probability to the true value 0 it is called a consistent estimator otherwise the estimator is said to be inconsistent\u000aconsistency as defined here is sometimes referred to as weak consistency when we replace convergence in probability with almost sure convergence then the estimator is said to be strongly consistent consistency is related to bias see bias versus consistency\u000a\u000a\u000a definitionedit \u000aloosely speaking an estimator tn of parameter  is said to be consistent if it converges in probability to the true value of the parameter\u000a\u000aa more rigorous definition takes into account the fact that  is actually unknown and thus the convergence in probability must take place for every possible value of this parameter suppose p  is a family of distributions the parametric model and x  x1 x2   xi  p is an infinite sample from the distribution p let tnx be a sequence of estimators for some parameter g usually tn will be based on the first n observations of a sample then this sequence tn is said to be weakly consistent if \u000a\u000athis definition uses g instead of simply  because often one is interested in estimating a certain function or a subvector of the underlying parameter in the next example we estimate the location parameter of the model but not the scale\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample mean of a normal random variableedit \u000asuppose one has a sequence of observations x1 x2  from a normal n2 distribution to estimate  based on the first n observations one can use the sample mean tn  x1    xnn this defines a sequence of estimators indexed by the sample size n\u000afrom the properties of the normal distribution we know the sampling distribution of this statistic tn is itself normally distributed with mean  and variance 2n equivalently  has a standard normal distribution\u000a\u000aas n tends to infinity for any fixed   0 therefore the sequence tn of sample means is consistent for the population mean \u000a\u000a\u000a establishing consistencyedit \u000athe notion of asymptotic consistency is very close almost synonymous to the notion of convergence in probability as such any theorem lemma or property which establishes convergence in probability may be used to prove the consistency many such tools exist\u000ain order to demonstrate consistency directly from the definition one can use the inequality \u000a\u000athe most common choice for function h being either the absolute value in which case it is known as markov inequality or the quadratic function respectively chebychevs inequality\u000aanother useful result is the continuous mapping theorem if tn is consistent for  and g is a realvalued function continuous at point  then gtn will be consistent for g\u000a\u000aslutskys theorem can be used to combine several different estimators or an estimator with a nonrandom convergent sequence if tn p and sn p then \u000a\u000aif estimator tn is given by an explicit formula then most likely the formula will employ sums of random variables and then the law of large numbers can be used for a sequence xn of random variables and under suitable conditions\u000a\u000aif estimator tn is defined implicitly for example as a value that maximizes certain objective function see extremum estimator then a more complicated argument involving stochastic equicontinuity has to be used\u000a\u000a\u000a bias versus consistencyedit \u000abias is related to consistency as follows a sequence of estimators is consistent if and only if it converges to a value and the bias converges to zero consistent estimators are convergent and asymptotically unbiased hence converge to the correct value individual estimators in the sequence may be biased but the overall sequence still consistent if the bias converges to zero conversely if the sequence does not converge to a value then it is not consistent regardless of whether the estimators in the sequence are biased or not\u000a\u000a\u000a unbiased but not consistentedit \u000aan estimator can be unbiased but not consistent for example for an iid sample x\u000a1 x\u000an one can use tx  x\u000a1 as the estimator of the mean ex note that here the sampling distribution of t is the same as the underlying distribution for any n as it ignores all points but the first so etx  ex and it is unbiased but it does not converge to any value\u000ahowever if a sequence of estimators is unbiased and converges to a value then it is consistent as it must converge to the correct value\u000a\u000a\u000a biased but consistentedit \u000aalternatively an estimator can be biased but consistent for example if the mean is estimated by  it is biased but as  it approaches the correct value and so it is consistent\u000aimportant examples include the sample variance and sample standard deviation without bessels correction using the sample size n instead of the degrees of freedom n  1 these are both negatively biased but consistent estimators with the correction the unbiased sample variance is unbiased while the corrected sample standard deviation is still biased but less so and both are still consistent the correction factor converges to 1 as sample size grows\u000a\u000a\u000a see alsoedit \u000aefficient estimator\u000afisher consistency  alternative although rarely used concept of consistency for the estimators\u000aregression dilution\u000astatistical hypothesis testing\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aamemiya takeshi 1985 advanced econometrics harvard university press isbn 0674005600 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer isbn 0387985026 \u000anewey w mcfadden d 1994 large sample estimation and hypothesis testing in handbook of econometrics vol 4 ch 36 elsevier science isbn 0444887660 \u000anikulin ms 2001 consistent estimator in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000a\u000a\u000a external linksedit \u000aeconometrics lecture topic unbiased vs consistent on youtube by mark thoma
p98
sg14
g17
sg18
Vin statistics a consistent estimator or asymptotically consistent estimator is an estimatora rule for computing estimates of a parameter 0having the property that as the number of data points used increases indefinitely the resulting sequence of estimates converges in probability to 0 this means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated so that the probability of the estimator being arbitrarily close to 0 converges to one\u000ain practice one constructs an estimator as a function of an available sample of size n and then imagines being able to keep collecting data and expanding the sample ad infinitum in this way one would obtain a sequence of estimates indexed by n and consistency is a property of what occurs as the sample size grows to infinity if the sequence of estimates can be mathematically shown to converge in probability to the true value 0 it is called a consistent estimator otherwise the estimator is said to be inconsistent\u000aconsistency as defined here is sometimes referred to as weak consistency when we replace convergence in probability with almost sure convergence then the estimator is said to be strongly consistent consistency is related to bias see bias versus consistency\u000a\u000a\u000a definitionedit \u000aloosely speaking an estimator tn of parameter  is said to be consistent if it converges in probability to the true value of the parameter\u000a\u000aa more rigorous definition takes into account the fact that  is actually unknown and thus the convergence in probability must take place for every possible value of this parameter suppose p  is a family of distributions the parametric model and x  x1 x2   xi  p is an infinite sample from the distribution p let tnx be a sequence of estimators for some parameter g usually tn will be based on the first n observations of a sample then this sequence tn is said to be weakly consistent if \u000a\u000athis definition uses g instead of simply  because often one is interested in estimating a certain function or a subvector of the underlying parameter in the next example we estimate the location parameter of the model but not the scale\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample mean of a normal random variableedit \u000asuppose one has a sequence of observations x1 x2  from a normal n2 distribution to estimate  based on the first n observations one can use the sample mean tn  x1    xnn this defines a sequence of estimators indexed by the sample size n\u000afrom the properties of the normal distribution we know the sampling distribution of this statistic tn is itself normally distributed with mean  and variance 2n equivalently  has a standard normal distribution\u000a\u000aas n tends to infinity for any fixed   0 therefore the sequence tn of sample means is consistent for the population mean \u000a\u000a\u000a establishing consistencyedit \u000athe notion of asymptotic consistency is very close almost synonymous to the notion of convergence in probability as such any theorem lemma or property which establishes convergence in probability may be used to prove the consistency many such tools exist\u000ain order to demonstrate consistency directly from the definition one can use the inequality \u000a\u000athe most common choice for function h being either the absolute value in which case it is known as markov inequality or the quadratic function respectively chebychevs inequality\u000aanother useful result is the continuous mapping theorem if tn is consistent for  and g is a realvalued function continuous at point  then gtn will be consistent for g\u000a\u000aslutskys theorem can be used to combine several different estimators or an estimator with a nonrandom convergent sequence if tn p and sn p then \u000a\u000aif estimator tn is given by an explicit formula then most likely the formula will employ sums of random variables and then the law of large numbers can be used for a sequence xn of random variables and under suitable conditions\u000a\u000aif estimator tn is defined implicitly for example as a value that maximizes certain objective function see extremum estimator then a more complicated argument involving stochastic equicontinuity has to be used\u000a\u000a\u000a bias versus consistencyedit \u000abias is related to consistency as follows a sequence of estimators is consistent if and only if it converges to a value and the bias converges to zero consistent estimators are convergent and asymptotically unbiased hence converge to the correct value individual estimators in the sequence may be biased but the overall sequence still consistent if the bias converges to zero conversely if the sequence does not converge to a value then it is not consistent regardless of whether the estimators in the sequence are biased or not\u000a\u000a\u000a unbiased but not consistentedit \u000aan estimator can be unbiased but not consistent for example for an iid sample x\u000a1 x\u000an one can use tx  x\u000a1 as the estimator of the mean ex note that here the sampling distribution of t is the same as the underlying distribution for any n as it ignores all points but the first so etx  ex and it is unbiased but it does not converge to any value\u000ahowever if a sequence of estimators is unbiased and converges to a value then it is consistent as it must converge to the correct value\u000a\u000a\u000a biased but consistentedit \u000aalternatively an estimator can be biased but consistent for example if the mean is estimated by  it is biased but as  it approaches the correct value and so it is consistent\u000aimportant examples include the sample variance and sample standard deviation without bessels correction using the sample size n instead of the degrees of freedom n  1 these are both negatively biased but consistent estimators with the correction the unbiased sample variance is unbiased while the corrected sample standard deviation is still biased but less so and both are still consistent the correction factor converges to 1 as sample size grows\u000a\u000a\u000a see alsoedit \u000aefficient estimator\u000afisher consistency  alternative although rarely used concept of consistency for the estimators\u000aregression dilution\u000astatistical hypothesis testing\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aamemiya takeshi 1985 advanced econometrics harvard university press isbn 0674005600 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer isbn 0387985026 \u000anewey w mcfadden d 1994 large sample estimation and hypothesis testing in handbook of econometrics vol 4 ch 36 elsevier science isbn 0444887660 \u000anikulin ms 2001 consistent estimator in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000a\u000a\u000a external linksedit \u000aeconometrics lecture topic unbiased vs consistent on youtube by mark thoma
p99
sg20
g23
sg24
g27
sg30
Vin statistics a consistent estimator or asymptotically consistent estimator is an estimatora rule for computing estimates of a parameter 0having the property that as the number of data points used increases indefinitely the resulting sequence of estimates converges in probability to 0 this means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated so that the probability of the estimator being arbitrarily close to 0 converges to one\u000ain practice one constructs an estimator as a function of an available sample of size n and then imagines being able to keep collecting data and expanding the sample ad infinitum in this way one would obtain a sequence of estimates indexed by n and consistency is a property of what occurs as the sample size grows to infinity if the sequence of estimates can be mathematically shown to converge in probability to the true value 0 it is called a consistent estimator otherwise the estimator is said to be inconsistent\u000aconsistency as defined here is sometimes referred to as weak consistency when we replace convergence in probability with almost sure convergence then the estimator is said to be strongly consistent consistency is related to bias see bias versus consistency\u000a\u000a\u000a definitionedit \u000aloosely speaking an estimator tn of parameter  is said to be consistent if it converges in probability to the true value of the parameter\u000a\u000aa more rigorous definition takes into account the fact that  is actually unknown and thus the convergence in probability must take place for every possible value of this parameter suppose p  is a family of distributions the parametric model and x  x1 x2   xi  p is an infinite sample from the distribution p let tnx be a sequence of estimators for some parameter g usually tn will be based on the first n observations of a sample then this sequence tn is said to be weakly consistent if \u000a\u000athis definition uses g instead of simply  because often one is interested in estimating a certain function or a subvector of the underlying parameter in the next example we estimate the location parameter of the model but not the scale\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample mean of a normal random variableedit \u000asuppose one has a sequence of observations x1 x2  from a normal n2 distribution to estimate  based on the first n observations one can use the sample mean tn  x1    xnn this defines a sequence of estimators indexed by the sample size n\u000afrom the properties of the normal distribution we know the sampling distribution of this statistic tn is itself normally distributed with mean  and variance 2n equivalently  has a standard normal distribution\u000a\u000aas n tends to infinity for any fixed   0 therefore the sequence tn of sample means is consistent for the population mean \u000a\u000a\u000a establishing consistencyedit \u000athe notion of asymptotic consistency is very close almost synonymous to the notion of convergence in probability as such any theorem lemma or property which establishes convergence in probability may be used to prove the consistency many such tools exist\u000ain order to demonstrate consistency directly from the definition one can use the inequality \u000a\u000athe most common choice for function h being either the absolute value in which case it is known as markov inequality or the quadratic function respectively chebychevs inequality\u000aanother useful result is the continuous mapping theorem if tn is consistent for  and g is a realvalued function continuous at point  then gtn will be consistent for g\u000a\u000aslutskys theorem can be used to combine several different estimators or an estimator with a nonrandom convergent sequence if tn p and sn p then \u000a\u000aif estimator tn is given by an explicit formula then most likely the formula will employ sums of random variables and then the law of large numbers can be used for a sequence xn of random variables and under suitable conditions\u000a\u000aif estimator tn is defined implicitly for example as a value that maximizes certain objective function see extremum estimator then a more complicated argument involving stochastic equicontinuity has to be used\u000a\u000a\u000a bias versus consistencyedit \u000abias is related to consistency as follows a sequence of estimators is consistent if and only if it converges to a value and the bias converges to zero consistent estimators are convergent and asymptotically unbiased hence converge to the correct value individual estimators in the sequence may be biased but the overall sequence still consistent if the bias converges to zero conversely if the sequence does not converge to a value then it is not consistent regardless of whether the estimators in the sequence are biased or not\u000a\u000a\u000a unbiased but not consistentedit \u000aan estimator can be unbiased but not consistent for example for an iid sample x\u000a1 x\u000an one can use tx  x\u000a1 as the estimator of the mean ex note that here the sampling distribution of t is the same as the underlying distribution for any n as it ignores all points but the first so etx  ex and it is unbiased but it does not converge to any value\u000ahowever if a sequence of estimators is unbiased and converges to a value then it is consistent as it must converge to the correct value\u000a\u000a\u000a biased but consistentedit \u000aalternatively an estimator can be biased but consistent for example if the mean is estimated by  it is biased but as  it approaches the correct value and so it is consistent\u000aimportant examples include the sample variance and sample standard deviation without bessels correction using the sample size n instead of the degrees of freedom n  1 these are both negatively biased but consistent estimators with the correction the unbiased sample variance is unbiased while the corrected sample standard deviation is still biased but less so and both are still consistent the correction factor converges to 1 as sample size grows\u000a\u000a\u000a see alsoedit \u000aefficient estimator\u000afisher consistency  alternative although rarely used concept of consistency for the estimators\u000aregression dilution\u000astatistical hypothesis testing\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aamemiya takeshi 1985 advanced econometrics harvard university press isbn 0674005600 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer isbn 0387985026 \u000anewey w mcfadden d 1994 large sample estimation and hypothesis testing in handbook of econometrics vol 4 ch 36 elsevier science isbn 0444887660 \u000anikulin ms 2001 consistent estimator in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000a\u000a\u000a external linksedit \u000aeconometrics lecture topic unbiased vs consistent on youtube by mark thoma
p100
sg32
g35
sg37
NsbsS'window_function.txt'
p101
g2
(g3
g4
Ntp102
Rp103
(dp104
g8
g11
sg12
Vin signal processing a window function also known as an apodization function or tapering function is a mathematical function that is zerovalued outside of some chosen interval for instance a function that is constant inside the interval and zero elsewhere is called a rectangular window which describes the shape of its graphical representation when another function or waveformdatasequence is multiplied by a window function the product is also zerovalued outside the interval all that is left is the part where they overlap the view through the window\u000aapplications of window functions include spectral analysis filter design and beamforming in typical applications the window functions used are nonnegative smooth bellshaped curves though rectangle triangle and other functions can be used\u000aa more general definition of window functions does not require them to be identically zero outside an interval as long as the product of the window multiplied by its argument is square integrable and more specifically that the function goes sufficiently rapidly toward zero\u000a\u000a\u000a applications \u000aapplications of window functions include spectral analysis and the design of finite impulse response filters\u000a\u000a\u000a spectral analysis \u000athe fourier transform of the function cos t is zero except at frequency  however many other functions and waveforms do not have convenient closed form transforms alternatively one might be interested in their spectral content only during a certain time period\u000ain either case the fourier transform or something similar can be applied on one or more finite intervals of the waveform in general the transform is applied to the product of the waveform and a window function any window including rectangular affects the spectral estimate computed by this method\u000a\u000a\u000a windowing \u000awindowing of a simple waveform like cos t causes its fourier transform to develop nonzero values commonly called spectral leakage at frequencies other than  the leakage tends to be worst highest near  and least at frequencies farthest from \u000aif the waveform under analysis comprises two sinusoids of different frequencies leakage can interfere with the ability to distinguish them spectrally if their frequencies are dissimilar and one component is weaker then leakage from the stronger component can obscure the weaker ones presence but if the frequencies are similar leakage can render them unresolvable even when the sinusoids are of equal strength the rectangular window has excellent resolution characteristics for sinusoids of comparable strength but it is a poor choice for sinusoids of disparate amplitudes this characteristic is sometimes described as lowdynamicrange\u000aat the other extreme of dynamic range are the windows with the poorest resolution and sensitivity which is the ability to reveal relatively weak sinusoids in the presence of additive random noise that is because the noise produces a stronger response with highdynamicrange windows than with highresolution windows therefore highdynamicrange windows are most often justified in wideband applications where the spectrum being analyzed is expected to contain many different components of various amplitudes\u000ain between the extremes are moderate windows such as hamming and hann they are commonly used in narrowband applications such as the spectrum of a telephone channel in summary spectral analysis involves a tradeoff between resolving comparable strength components with similar frequencies and resolving disparate strength components with dissimilar frequencies that tradeoff occurs when the window function is chosen\u000a\u000a\u000a discretetime signals \u000awhen the input waveform is timesampled instead of continuous the analysis is usually done by applying a window function and then a discrete fourier transform dft but the dft provides only a coarse sampling of the actual discretetime fourier transform dtft spectrum figure 1 shows a portion of the dtft for a rectangularly windowed sinusoid the actual frequency of the sinusoid is indicated as 0 on the horizontal axis everything else is leakage exaggerated by the use of a logarithmic presentation the unit of frequency is dft bins that is the integer values on the frequency axis correspond to the frequencies sampled by the dft so the figure depicts a case where the actual frequency of the sinusoid happens to coincide with a dft sample and the maximum value of the spectrum is accurately measured by that sample when it misses the maximum value by some amount up to 12 bin the measurement error is referred to as scalloping loss inspired by the shape of the peak but the most interesting thing about this case is that all the other samples coincide with nulls in the true spectrum the nulls are actually zerocrossings which cannot be shown on a logarithmic scale such as this so in this case the dft creates the illusion of no leakage despite the unlikely conditions of this example it is a common misconception that visible leakage is some sort of artifact of the dft but since any window function causes leakage its apparent absence in this contrived example is actually the dft artifact\u000a\u000a\u000a noise bandwidth \u000athe concepts of resolution and dynamic range tend to be somewhat subjective depending on what the user is actually trying to do but they also tend to be highly correlated with the total leakage which is quantifiable it is usually expressed as an equivalent bandwidth b it can be thought of as redistributing the dtft into a rectangular shape with height equal to the spectral maximum and width b the more the leakage the greater the bandwidth it is sometimes called noise equivalent bandwidth or equivalent noise bandwidth because it is proportional to the average power that will be registered by each dft bin when the input signal contains a random noise component or is just random noise a graph of the power spectrum averaged over time typically reveals a flat noise floor caused by this effect the height of the noise floor is proportional to b so two different window functions can produce different noise floors\u000a\u000a\u000a processing gain and losses \u000ain signal processing operations are chosen to improve some aspect of quality of a signal by exploiting the differences between the signal and the corrupting influences when the signal is a sinusoid corrupted by additive random noise spectral analysis distributes the signal and noise components differently often making it easier to detect the signals presence or measure certain characteristics such as amplitude and frequency effectively the signal to noise ratio snr is improved by distributing the noise uniformly while concentrating most of the sinusoids energy around one frequency processing gain is a term often used to describe an snr improvement the processing gain of spectral analysis depends on the window function both its noise bandwidth b and its potential scalloping loss these effects partially offset because windows with the least scalloping naturally have the most leakage\u000athe figure at right depicts the effects of three different window functions on the same data set comprising two equal strength sinusoids in additive noise the frequencies of the sinusoids are chosen such that one encounters no scalloping and the other encounters maximum scalloping both sinusoids suffer less snr loss under the hann window than under the blackmanharris window in general as mentioned earlier this is a deterrent to using highdynamicrange windows in lowdynamicrange applications\u000a\u000a\u000a filter design \u000a\u000awindows are sometimes used in the design of digital filters in particular to convert an ideal impulse response of infinite duration such as a sinc function to a finite impulse response fir filter design that is called the window method\u000a\u000a\u000a symmetry and asymmetry \u000awindow functions generated for digital filter design are symmetrical sequences usually an odd length with a single maximum at the center windows for dftfft usage such as in spectral analysis are often created by deleting the rightmost coefficient of an oddlength symmetrical window such truncated sequences are known as periodic the deleted coefficient is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended which is the timedomain equivalent of sampling the dtft a different way of saying the same thing is that the dft samples the dtft of the window at the exact points that are not affected by spectral leakage from the discontinuity the advantage of this trick is that a 512 length window for example enjoys the slightly better performance metrics of a 513 length design such a window is generated by the matlab function hann512periodic for instance to generate it with the formula in this article below the window length n is 513 and the 513th coefficient of the generated sequence is discarded\u000aanother type of asymmetric window called dfteven is limited to even length sequences the generated sequence is offset cyclically from its zerophase counterpart by exactly half the sequence length in the frequency domain that corresponds to a multiplication by the trivial sequence 1k which can have implementation advantages for windows defined by their frequency domain form compared to a symmetrical window the dfteven sequence has an offset of  sample as illustrated in the figure at right that means the asymmetry is limited to just one missing coefficient therefore as in the periodic case it is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended\u000a\u000a\u000a applications for which windows should not be used \u000ain some applications it is preferable not to use a window function for example\u000ain impact modal testing when analyzing transient signals such as an excitation signal from hammer blow see impulse excitation technique where most of the energy is located at the beginning of the recording using a nonrectangular window would attenuate most of the energy and spread the frequency response unnecessarily\u000aa generalization of above when measuring a selfwindowing signal such as an impulse a shock response a sine burst a chirp burst noise burst such signals are used in modal analysis applying a window function in this case would just deteriorate the signaltonoise ratio\u000awhen measuring a pseudorandom noise prn excitation signal with period t and using the same recording period t a prn signal is periodic and therefore all spectral components of the signal will coincide with fft bin centers with no leakage\u000awhen measuring a repetitive signal lockedin to the sampling frequency for example measuring the vibration spectrum analysis during shaft alignment fault diagnosis of bearings engines gearboxes etc since the signal is repetitive all spectral energy is confined to multiples of the base repetition frequency\u000ain an ofdm receiver the input signal is directly multiplied by fft without a window function the frequency subcarriers aka symbols are designed to align exactly to the fft frequency bins a cyclic prefix is usually added to the transmitted signal allowing frequencyselective fading due to multipath to be modeled as circular convolution thus avoiding intersymbol interference which in ofdm is equivalent to spectral leakage\u000a\u000a\u000a a list of window functions \u000aterminology\u000an represents the width in samples of a discretetime symmetrical window function    when n is an odd number the nonflat windows have a singular maximum point when n is even they have a double maximum\u000ait is sometimes useful to express    as a sequence of samples of the lagged version of a zerophase function\u000a  \u000afor instance for even values of n we can describe the related dfteven window as    as discussed in the previous section the dft of such a sequence in terms of the dft of the    sequence is  \u000aeach figure label includes the corresponding noise equivalent bandwidth metric b in units of dft bins\u000a\u000a\u000a bspline windows \u000abspline windows can be obtained as kfold convolutions of the rectangular window they include the rectangular window itself k  1 the triangular window k  2 and the parzen window k  4 alternative definitions sample the appropriate normalized bspline basis functions instead of convolving discretetime windows a kth order bspline basis function is a piecewise polynomial function of degree k1 that is obtained by kfold selfconvolution of the rectangular function\u000a\u000a\u000a rectangular window \u000a\u000athe rectangular window sometimes known as the boxcar or dirichlet window is the simplest window equivalent to replacing all but n values of a data sequence by zeros making it appear as though the waveform suddenly turns on and off\u000a\u000aother windows are designed to moderate these sudden changes because discontinuities have undesirable effects on the discretetime fourier transform dtft andor the algorithms that produce samples of the dtft\u000athe rectangular window is the 1st order bspline window as well as the 0th power cosine window\u000a\u000a\u000a triangular window \u000atriangular windows are given by\u000a\u000awhere l can be n n1 or n1 the last one is also known as bartlett window or fejr window all three definitions converge at large n\u000athe triangular window is the 2nd order bspline window and can be seen as the convolution of two n2 width rectangular windows the fourier transform of the result is the squared values of the transform of the halfwidth rectangular window\u000a\u000a\u000a parzen window \u000a\u000athe parzen window also known as the de la valle poussin window is the 4th order bspline window given by\u000a\u000a\u000a other polynomial windows \u000a\u000a\u000a welch window \u000a\u000athe welch window consists of a single parabolic section\u000a\u000athe defining quadratic polynomial reaches a value of zero at the samples just outside the span of the window\u000a\u000a\u000a generalized hamming windows \u000ageneralized hamming windows are of the form\u000a\u000athey have only three nonzero dft coefficients and share the benefits of a sparse frequency domain representation with higherorder generalized cosine windows\u000a\u000a\u000a hann hanning window \u000a\u000athe hann window named after julius von hann and also known as the hanning for being similar in name and form to the hamming window von hann and the raised cosine window is defined by with hav for the haversine function\u000a\u000athe ends of the cosine just touch zero so the sidelobes roll off at about 18 db per octave\u000a\u000a\u000a hamming window \u000a\u000athe window with these particular coefficients was proposed by richard w hamming the window is optimized to minimize the maximum nearest side lobe giving it a height of about onefifth that of the hann window\u000a\u000awith\u000a\u000ainstead of both constants being equal to 12 in the hann window the constants are approximations of values   2546 and   2146 which cancel the first sidelobe of the hann window by placing a zero at frequency 5n  1 approximation of the constants to two decimal places substantially lowers the level of sidelobes to a nearly equiripple condition in the equiripple sense the optimal values for the coefficients are   053836 and   046164\u000azerophase version\u000a\u000a\u000a higherorder generalized cosine windows \u000awindows of the form\u000a\u000ahave only 2k  1 nonzero dft coefficients which makes them good choices for applications that require windowing by convolution in the frequencydomain in those applications the dft of the unwindowed data vector is needed for a different purpose than spectral analysis see overlapsave method generalized cosine windows with just two terms k  1 belong in the subfamily generalized hamming windows\u000a\u000a\u000a blackman windows \u000a\u000ablackman windows are defined as\u000a\u000aby common convention the unqualified term blackman window refers to   016 as this most closely approximates the exact blackman with a0  793818608  042659 a1  924018608  049656 and a2  143018608  0076849 these exact values place zeros at the third and fourth sidelobes\u000a\u000a\u000a nuttall window continuous first derivative \u000a\u000aconsidering n as a real number the nuttall window function and its first derivative are continuous everywhere that is the function goes to 0 at n  0 unlike the blackmannuttall and blackmanharris windows which have a small positive value at zero at step from the zero outside the window like the hamming window the blackman window defined via  is also continuous with continuous derivative at the edge but the described exact blackman window is not\u000a\u000a\u000a blackmannuttall window \u000a\u000a\u000a blackmanharris window \u000a\u000aa generalization of the hamming family produced by adding more shifted sinc functions meant to minimize sidelobe levels\u000a\u000a\u000a flat top window \u000a\u000aa flat top window is a partially negativevalued window that has a flat top in the frequency domain such windows have been made available in spectrum analyzers for the measurement of amplitudes of sinusoidal frequency components they have a low amplitude measurement error suitable for this purpose achieved by the spreading of the energy of a sine wave over multiple bins in the spectrum this ensures that the unattenuated amplitude of the sinusoid can be found on at least one of the neighboring bins the drawback of the broad bandwidth is poor frequency resolution to compensate a longer window length may be chosen\u000aflat top windows can be designed using lowpass filter design methods or they may be of the usual sumofcosineterms variety an example of the latter is the flat top window available in the stanford research systems srs sr785 spectrum analyzer\u000a\u000a \u000a\u000a\u000a rifevincent window \u000arife and vincent define three classes of windows constructed as sums of cosines the classes are generalizations of the hanning window their orderp windows are of the form normalized to have unity average as opposed to unity max as the windows above are\u000a\u000afor order 1 this formula can match the hanning window for a1  1 this is the rifevincent classi window defined by minimizing the highorder sidelobe amplitude the classi order2 rifevincent window has a1  43 and a2  13 coefficients for orders up to 4 are tabulated for orders greater than 1 the rifevincent window coefficients can be optimized for class ii meaning minimized mainlobe width for a given maximum sidelobe or for class iii a compromise for which order 2 resembles blackmanns window given the wide variety of rifevincent windows plots are not given here\u000a\u000a\u000a powerofcosine windows \u000awindow functions in the powerofcosine family are of the form\u000a\u000athe rectangular window   0 the cosine window   1 and the hann window   2 are members of this family\u000a\u000a\u000a cosine window \u000a\u000athe cosine window is also known as the sine window cosine window describes the shape of \u000aa cosine window convolved by itself is known as the bohman window\u000a\u000a\u000a adjustable windows \u000a\u000a\u000a gaussian window \u000a\u000athe fourier transform of a gaussian is also a gaussian it is an eigenfunction of the fourier transform since the gaussian function extends to infinity it must either be truncated at the ends of the window or itself windowed with another zeroended window\u000asince the log of a gaussian produces a parabola this can be used for nearly exact quadratic interpolation in frequency estimation\u000a\u000athe standard deviation of the gaussian function is n12 sampling periods\u000a\u000a\u000a confined gaussian window \u000athe confined gaussian window yields the smallest possible root mean square frequency width  for a given temporal width t these windows optimize the rms timefrequency bandwidth products they are computed as the minimum eigenvectors of a parameterdependent matrix the confined gaussian window family contains the cosine window and the gaussian window in the limiting cases of large and small t respectively\u000a\u000a\u000a approximate confined gaussian window \u000aa confined gaussian window of temporal width t is well approximated by\u000a\u000awith the gaussian\u000a\u000athe temporal width of the approximate window is asymptotically equal to t for t  014 n\u000a\u000a\u000a generalized normal window \u000aa more generalized version of the gaussian window is the generalized normal window retaining the notation from the gaussian window above we can represent this window as\u000a\u000afor any even  at  this is a gaussian window and as  approaches  this approximates to a rectangular window the fourier transform of this window does not exist in a closed form for a general  however it demonstrates the other benefits of being smooth adjustable bandwidth like the tukey window discussed later this window naturally offers a flat top to control the amplitude attenuation of a timeseries on which we dont have a control with gaussian window in essence it offers a good controllable compromise in terms of spectral leakage frequency resolution and amplitude attenuation between the gaussian window and the rectangular window see also  for a study on timefrequency representation of this window or function\u000a\u000a\u000a tukey window \u000a\u000athe tukey window also known as the tapered cosine window can be regarded as a cosine lobe of width n2 that is convolved with a rectangular window of width 1  2n\u000a\u000aor expressed with the havercosine hvc function\u000a\u000aat   0 it becomes rectangular and at   1 it becomes a hann window\u000a\u000a\u000a plancktaper window \u000a\u000athe socalled plancktaper window is a bump function that has been widely used in the theory of partitions of unity in manifolds it is a  function everywhere but is exactly zero outside of a compact region exactly one over an interval within that region and varies smoothly and monotonically between those limits its use as a window function in signal processing was first suggested in the context of gravitationalwave astronomy inspired by the planck distribution it is defined as a piecewise function\u000a\u000awhere\u000a\u000athe amount of tapering the region over which the function is exactly 1 is controlled by the parameter  with smaller values giving sharper transitions\u000a\u000a\u000a dpss or slepian window \u000a\u000athe dpss discrete prolate spheroidal sequence or slepian window is used to maximize the energy concentration in the main lobe\u000athe main lobe ends at a bin given by the parameter \u000a\u000a\u000a kaiser window \u000a\u000athe kaiser or kaiserbessel window is a simple approximation of the dpss window using bessel functions discovered by jim kaiser\u000a\u000awhere i0 is the zeroth order modified bessel function of the first kind variable parameter  determines the tradeoff between main lobe width and side lobe levels of the spectral leakage pattern the main lobe width in between the nulls is given by    in units of dft bins  and a typical value of  is 3\u000asometimes the formula for wn is written in terms of a parameter \u000azerophase version\u000a\u000a\u000a dolphchebyshev window \u000a\u000aminimizes the chebyshev norm of the sidelobes for a given main lobe width\u000athe zerophase dolphchebyshev window function w0n is usually defined in terms of its realvalued discrete fourier transform w0k\u000a\u000awhere the parameter  sets the chebyshev norm of the sidelobes to 20 decibels\u000athe window function can be calculated from w0k by an inverse discrete fourier transform dft\u000a\u000athe lagged version of the window with 0  n  n1 can be obtained by\u000a\u000awhich for even values of n must be computed as follows\u000a\u000awhich is an inverse dft of  \u000avariations\u000athe dfteven sequence for even values of n is given by    which is the inverse dft of   \u000adue to the equiripple condition the timedomain window has discontinuities at the edges an approximation that avoids them by allowing the equiripples to drop off at the edges is a taylor window\u000aan alternative to the inverse dft definition is also available2 it isnt clear if it is the symmetric   or dfteven   definition but for typical values of n found in practice the difference is negligible\u000a\u000a\u000a ultraspherical window \u000a\u000athe ultraspherical window was introduced in 1984 by roy streit and has application in antenna array design nonrecursive filter design and spectrum analysis\u000alike other adjustable windows the ultraspherical window has parameters that can be used to control its fourier transform mainlobe width and relative sidelobe amplitude uncommon to other windows it has an additional parameter which can be used to set the rate at which sidelobes decrease or increase in amplitude\u000athe window can be expressed in the timedomain as follows\u000a\u000awhere  is the ultraspherical polynomial of degree n and  and  control the sidelobe patterns\u000acertain specific values of  yield other wellknown windows  and  give the dolphchebyshev and saramki windows respectively see here for illustration of ultraspherical windows with varied parametrization\u000a\u000a\u000a exponential or poisson window \u000a\u000athe poisson window or more generically the exponential window increases exponentially towards the center of the window and decreases exponentially in the second half since the exponential function never reaches zero the values of the window at its limits are nonzero it can be seen as the multiplication of an exponential function by a rectangular window  it is defined by\u000a\u000awhere  is the time constant of the function the exponential function decays as e  271828 or approximately 869 db per time constant this means that for a targeted decay of d db over half of the window length the time constant  is given by\u000a\u000a\u000a hybrid windows \u000awindow functions have also been constructed as multiplicative or additive combinations of other windows\u000a\u000a\u000a bartletthann window \u000a\u000a\u000a planckbessel window \u000a\u000aa plancktaper window multiplied by a kaiser window which is defined in terms of a modified bessel function this hybrid window function was introduced to decrease the peak sidelobe level of the plancktaper window while still exploiting its good asymptotic decay it has two tunable parameters  from the plancktaper and  from the kaiser window so it can be adjusted to fit the requirements of a given signal\u000a\u000a\u000a hannpoisson window \u000a\u000aa hann window multiplied by a poisson window which has no sidelobes in the sense that its fourier transform drops off forever away from the main lobe it can thus be used in hill climbing algorithms like newtons method the hannpoisson window is defined by\u000a\u000awhere  is a parameter that controls the slope of the exponential\u000a\u000a\u000a other windows \u000a\u000a\u000a lanczos window \u000a\u000aused in lanczos resampling\u000afor the lanczos window sincx is defined as sinxx\u000aalso known as a sinc window because\u000a\u000a is the main lobe of a normalized sinc function\u000a\u000a\u000a comparison of windows \u000a\u000awhen selecting an appropriate window function for an application this comparison graph may be useful the frequency axis has units of fft bins when the window of length n is applied to data and a transform of length n is computed for instance the value at frequency  bin third tick mark is the response that would be measured in bins k and k1 to a sinusoidal signal at frequency k it is relative to the maximum possible response which occurs when the signal frequency is an integer number of bins the value at frequency  is referred to as the maximum scalloping loss of the window which is one metric used to compare windows the rectangular window is noticeably worse than the others in terms of that metric\u000aother metrics that can be seen are the width of the main lobe and the peak level of the sidelobes which respectively determine the ability to resolve comparable strength signals and disparate strength signals the rectangular window for instance is the best choice for the former and the worst choice for the latter what cannot be seen from the graphs is that the rectangular window has the best noise bandwidth which makes it a good candidate for detecting lowlevel sinusoids in an otherwise white noise environment interpolation techniques such as zeropadding and frequencyshifting are available to mitigate its potential scalloping loss\u000a\u000a\u000a overlapping windows \u000awhen the length of a data set to be transformed is larger than necessary to provide the desired frequency resolution a common practice is to subdivide it into smaller sets and window them individually to mitigate the loss at the edges of the window the individual sets may overlap in time see welch method of power spectral analysis and the modified discrete cosine transform\u000a\u000a\u000a twodimensional windows \u000atwodimensional windows are used in eg image processing they can be constructed from onedimensional windows in either of two forms\u000athe separable form  is trivial to compute the radial form  which involves the radius  is isotropic independent on the orientation of the coordinate axes only the gaussian function is both separable and isotropic the separable forms of all other window functions have corners that depend on the choice of the coordinate axes the isotropyanisotropy of a twodimensional window function is shared by its twodimensional fourier transform the difference between the separable and radial forms is akin to the result of diffraction from rectangular vs circular appertures which can be visualized in terms of the product of two sinc functions vs an airy function respectively\u000a\u000a\u000a see also \u000aspectral leakage\u000amultitaper\u000aapodization\u000awelch method\u000ashorttime fourier transform\u000awindow design method\u000akolmogorovzurbenko filter\u000a\u000a\u000a notes \u000a\u000a\u000a
p105
sg14
g17
sg18
Vin signal processing a window function also known as an apodization function or tapering function is a mathematical function that is zerovalued outside of some chosen interval for instance a function that is constant inside the interval and zero elsewhere is called a rectangular window which describes the shape of its graphical representation when another function or waveformdatasequence is multiplied by a window function the product is also zerovalued outside the interval all that is left is the part where they overlap the view through the window\u000aapplications of window functions include spectral analysis filter design and beamforming in typical applications the window functions used are nonnegative smooth bellshaped curves though rectangle triangle and other functions can be used\u000aa more general definition of window functions does not require them to be identically zero outside an interval as long as the product of the window multiplied by its argument is square integrable and more specifically that the function goes sufficiently rapidly toward zero\u000a\u000a\u000a applications \u000aapplications of window functions include spectral analysis and the design of finite impulse response filters\u000a\u000a\u000a spectral analysis \u000athe fourier transform of the function cos t is zero except at frequency  however many other functions and waveforms do not have convenient closed form transforms alternatively one might be interested in their spectral content only during a certain time period\u000ain either case the fourier transform or something similar can be applied on one or more finite intervals of the waveform in general the transform is applied to the product of the waveform and a window function any window including rectangular affects the spectral estimate computed by this method\u000a\u000a\u000a windowing \u000awindowing of a simple waveform like cos t causes its fourier transform to develop nonzero values commonly called spectral leakage at frequencies other than  the leakage tends to be worst highest near  and least at frequencies farthest from \u000aif the waveform under analysis comprises two sinusoids of different frequencies leakage can interfere with the ability to distinguish them spectrally if their frequencies are dissimilar and one component is weaker then leakage from the stronger component can obscure the weaker ones presence but if the frequencies are similar leakage can render them unresolvable even when the sinusoids are of equal strength the rectangular window has excellent resolution characteristics for sinusoids of comparable strength but it is a poor choice for sinusoids of disparate amplitudes this characteristic is sometimes described as lowdynamicrange\u000aat the other extreme of dynamic range are the windows with the poorest resolution and sensitivity which is the ability to reveal relatively weak sinusoids in the presence of additive random noise that is because the noise produces a stronger response with highdynamicrange windows than with highresolution windows therefore highdynamicrange windows are most often justified in wideband applications where the spectrum being analyzed is expected to contain many different components of various amplitudes\u000ain between the extremes are moderate windows such as hamming and hann they are commonly used in narrowband applications such as the spectrum of a telephone channel in summary spectral analysis involves a tradeoff between resolving comparable strength components with similar frequencies and resolving disparate strength components with dissimilar frequencies that tradeoff occurs when the window function is chosen\u000a\u000a\u000a discretetime signals \u000awhen the input waveform is timesampled instead of continuous the analysis is usually done by applying a window function and then a discrete fourier transform dft but the dft provides only a coarse sampling of the actual discretetime fourier transform dtft spectrum figure 1 shows a portion of the dtft for a rectangularly windowed sinusoid the actual frequency of the sinusoid is indicated as 0 on the horizontal axis everything else is leakage exaggerated by the use of a logarithmic presentation the unit of frequency is dft bins that is the integer values on the frequency axis correspond to the frequencies sampled by the dft so the figure depicts a case where the actual frequency of the sinusoid happens to coincide with a dft sample and the maximum value of the spectrum is accurately measured by that sample when it misses the maximum value by some amount up to 12 bin the measurement error is referred to as scalloping loss inspired by the shape of the peak but the most interesting thing about this case is that all the other samples coincide with nulls in the true spectrum the nulls are actually zerocrossings which cannot be shown on a logarithmic scale such as this so in this case the dft creates the illusion of no leakage despite the unlikely conditions of this example it is a common misconception that visible leakage is some sort of artifact of the dft but since any window function causes leakage its apparent absence in this contrived example is actually the dft artifact\u000a\u000a\u000a noise bandwidth \u000athe concepts of resolution and dynamic range tend to be somewhat subjective depending on what the user is actually trying to do but they also tend to be highly correlated with the total leakage which is quantifiable it is usually expressed as an equivalent bandwidth b it can be thought of as redistributing the dtft into a rectangular shape with height equal to the spectral maximum and width b the more the leakage the greater the bandwidth it is sometimes called noise equivalent bandwidth or equivalent noise bandwidth because it is proportional to the average power that will be registered by each dft bin when the input signal contains a random noise component or is just random noise a graph of the power spectrum averaged over time typically reveals a flat noise floor caused by this effect the height of the noise floor is proportional to b so two different window functions can produce different noise floors\u000a\u000a\u000a processing gain and losses \u000ain signal processing operations are chosen to improve some aspect of quality of a signal by exploiting the differences between the signal and the corrupting influences when the signal is a sinusoid corrupted by additive random noise spectral analysis distributes the signal and noise components differently often making it easier to detect the signals presence or measure certain characteristics such as amplitude and frequency effectively the signal to noise ratio snr is improved by distributing the noise uniformly while concentrating most of the sinusoids energy around one frequency processing gain is a term often used to describe an snr improvement the processing gain of spectral analysis depends on the window function both its noise bandwidth b and its potential scalloping loss these effects partially offset because windows with the least scalloping naturally have the most leakage\u000athe figure at right depicts the effects of three different window functions on the same data set comprising two equal strength sinusoids in additive noise the frequencies of the sinusoids are chosen such that one encounters no scalloping and the other encounters maximum scalloping both sinusoids suffer less snr loss under the hann window than under the blackmanharris window in general as mentioned earlier this is a deterrent to using highdynamicrange windows in lowdynamicrange applications\u000a\u000a\u000a filter design \u000a\u000awindows are sometimes used in the design of digital filters in particular to convert an ideal impulse response of infinite duration such as a sinc function to a finite impulse response fir filter design that is called the window method\u000a\u000a\u000a symmetry and asymmetry \u000awindow functions generated for digital filter design are symmetrical sequences usually an odd length with a single maximum at the center windows for dftfft usage such as in spectral analysis are often created by deleting the rightmost coefficient of an oddlength symmetrical window such truncated sequences are known as periodic the deleted coefficient is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended which is the timedomain equivalent of sampling the dtft a different way of saying the same thing is that the dft samples the dtft of the window at the exact points that are not affected by spectral leakage from the discontinuity the advantage of this trick is that a 512 length window for example enjoys the slightly better performance metrics of a 513 length design such a window is generated by the matlab function hann512periodic for instance to generate it with the formula in this article below the window length n is 513 and the 513th coefficient of the generated sequence is discarded\u000aanother type of asymmetric window called dfteven is limited to even length sequences the generated sequence is offset cyclically from its zerophase counterpart by exactly half the sequence length in the frequency domain that corresponds to a multiplication by the trivial sequence 1k which can have implementation advantages for windows defined by their frequency domain form compared to a symmetrical window the dfteven sequence has an offset of  sample as illustrated in the figure at right that means the asymmetry is limited to just one missing coefficient therefore as in the periodic case it is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended\u000a\u000a\u000a applications for which windows should not be used \u000ain some applications it is preferable not to use a window function for example\u000ain impact modal testing when analyzing transient signals such as an excitation signal from hammer blow see impulse excitation technique where most of the energy is located at the beginning of the recording using a nonrectangular window would attenuate most of the energy and spread the frequency response unnecessarily\u000aa generalization of above when measuring a selfwindowing signal such as an impulse a shock response a sine burst a chirp burst noise burst such signals are used in modal analysis applying a window function in this case would just deteriorate the signaltonoise ratio\u000awhen measuring a pseudorandom noise prn excitation signal with period t and using the same recording period t a prn signal is periodic and therefore all spectral components of the signal will coincide with fft bin centers with no leakage\u000awhen measuring a repetitive signal lockedin to the sampling frequency for example measuring the vibration spectrum analysis during shaft alignment fault diagnosis of bearings engines gearboxes etc since the signal is repetitive all spectral energy is confined to multiples of the base repetition frequency\u000ain an ofdm receiver the input signal is directly multiplied by fft without a window function the frequency subcarriers aka symbols are designed to align exactly to the fft frequency bins a cyclic prefix is usually added to the transmitted signal allowing frequencyselective fading due to multipath to be modeled as circular convolution thus avoiding intersymbol interference which in ofdm is equivalent to spectral leakage\u000a\u000a\u000a a list of window functions \u000aterminology\u000an represents the width in samples of a discretetime symmetrical window function    when n is an odd number the nonflat windows have a singular maximum point when n is even they have a double maximum\u000ait is sometimes useful to express    as a sequence of samples of the lagged version of a zerophase function\u000a  \u000afor instance for even values of n we can describe the related dfteven window as    as discussed in the previous section the dft of such a sequence in terms of the dft of the    sequence is  \u000aeach figure label includes the corresponding noise equivalent bandwidth metric b in units of dft bins\u000a\u000a\u000a bspline windows \u000abspline windows can be obtained as kfold convolutions of the rectangular window they include the rectangular window itself k  1 the triangular window k  2 and the parzen window k  4 alternative definitions sample the appropriate normalized bspline basis functions instead of convolving discretetime windows a kth order bspline basis function is a piecewise polynomial function of degree k1 that is obtained by kfold selfconvolution of the rectangular function\u000a\u000a\u000a rectangular window \u000a\u000athe rectangular window sometimes known as the boxcar or dirichlet window is the simplest window equivalent to replacing all but n values of a data sequence by zeros making it appear as though the waveform suddenly turns on and off\u000a\u000aother windows are designed to moderate these sudden changes because discontinuities have undesirable effects on the discretetime fourier transform dtft andor the algorithms that produce samples of the dtft\u000athe rectangular window is the 1st order bspline window as well as the 0th power cosine window\u000a\u000a\u000a triangular window \u000atriangular windows are given by\u000a\u000awhere l can be n n1 or n1 the last one is also known as bartlett window or fejr window all three definitions converge at large n\u000athe triangular window is the 2nd order bspline window and can be seen as the convolution of two n2 width rectangular windows the fourier transform of the result is the squared values of the transform of the halfwidth rectangular window\u000a\u000a\u000a parzen window \u000a\u000athe parzen window also known as the de la valle poussin window is the 4th order bspline window given by\u000a\u000a\u000a other polynomial windows \u000a\u000a\u000a welch window \u000a\u000athe welch window consists of a single parabolic section\u000a\u000athe defining quadratic polynomial reaches a value of zero at the samples just outside the span of the window\u000a\u000a\u000a generalized hamming windows \u000ageneralized hamming windows are of the form\u000a\u000athey have only three nonzero dft coefficients and share the benefits of a sparse frequency domain representation with higherorder generalized cosine windows\u000a\u000a\u000a hann hanning window \u000a\u000athe hann window named after julius von hann and also known as the hanning for being similar in name and form to the hamming window von hann and the raised cosine window is defined by with hav for the haversine function\u000a\u000athe ends of the cosine just touch zero so the sidelobes roll off at about 18 db per octave\u000a\u000a\u000a hamming window \u000a\u000athe window with these particular coefficients was proposed by richard w hamming the window is optimized to minimize the maximum nearest side lobe giving it a height of about onefifth that of the hann window\u000a\u000awith\u000a\u000ainstead of both constants being equal to 12 in the hann window the constants are approximations of values   2546 and   2146 which cancel the first sidelobe of the hann window by placing a zero at frequency 5n  1 approximation of the constants to two decimal places substantially lowers the level of sidelobes to a nearly equiripple condition in the equiripple sense the optimal values for the coefficients are   053836 and   046164\u000azerophase version\u000a\u000a\u000a higherorder generalized cosine windows \u000awindows of the form\u000a\u000ahave only 2k  1 nonzero dft coefficients which makes them good choices for applications that require windowing by convolution in the frequencydomain in those applications the dft of the unwindowed data vector is needed for a different purpose than spectral analysis see overlapsave method generalized cosine windows with just two terms k  1 belong in the subfamily generalized hamming windows\u000a\u000a\u000a blackman windows \u000a\u000ablackman windows are defined as\u000a\u000aby common convention the unqualified term blackman window refers to   016 as this most closely approximates the exact blackman with a0  793818608  042659 a1  924018608  049656 and a2  143018608  0076849 these exact values place zeros at the third and fourth sidelobes\u000a\u000a\u000a nuttall window continuous first derivative \u000a\u000aconsidering n as a real number the nuttall window function and its first derivative are continuous everywhere that is the function goes to 0 at n  0 unlike the blackmannuttall and blackmanharris windows which have a small positive value at zero at step from the zero outside the window like the hamming window the blackman window defined via  is also continuous with continuous derivative at the edge but the described exact blackman window is not\u000a\u000a\u000a blackmannuttall window \u000a\u000a\u000a blackmanharris window \u000a\u000aa generalization of the hamming family produced by adding more shifted sinc functions meant to minimize sidelobe levels\u000a\u000a\u000a flat top window \u000a\u000aa flat top window is a partially negativevalued window that has a flat top in the frequency domain such windows have been made available in spectrum analyzers for the measurement of amplitudes of sinusoidal frequency components they have a low amplitude measurement error suitable for this purpose achieved by the spreading of the energy of a sine wave over multiple bins in the spectrum this ensures that the unattenuated amplitude of the sinusoid can be found on at least one of the neighboring bins the drawback of the broad bandwidth is poor frequency resolution to compensate a longer window length may be chosen\u000aflat top windows can be designed using lowpass filter design methods or they may be of the usual sumofcosineterms variety an example of the latter is the flat top window available in the stanford research systems srs sr785 spectrum analyzer\u000a\u000a \u000a\u000a\u000a rifevincent window \u000arife and vincent define three classes of windows constructed as sums of cosines the classes are generalizations of the hanning window their orderp windows are of the form normalized to have unity average as opposed to unity max as the windows above are\u000a\u000afor order 1 this formula can match the hanning window for a1  1 this is the rifevincent classi window defined by minimizing the highorder sidelobe amplitude the classi order2 rifevincent window has a1  43 and a2  13 coefficients for orders up to 4 are tabulated for orders greater than 1 the rifevincent window coefficients can be optimized for class ii meaning minimized mainlobe width for a given maximum sidelobe or for class iii a compromise for which order 2 resembles blackmanns window given the wide variety of rifevincent windows plots are not given here\u000a\u000a\u000a powerofcosine windows \u000awindow functions in the powerofcosine family are of the form\u000a\u000athe rectangular window   0 the cosine window   1 and the hann window   2 are members of this family\u000a\u000a\u000a cosine window \u000a\u000athe cosine window is also known as the sine window cosine window describes the shape of \u000aa cosine window convolved by itself is known as the bohman window\u000a\u000a\u000a adjustable windows \u000a\u000a\u000a gaussian window \u000a\u000athe fourier transform of a gaussian is also a gaussian it is an eigenfunction of the fourier transform since the gaussian function extends to infinity it must either be truncated at the ends of the window or itself windowed with another zeroended window\u000asince the log of a gaussian produces a parabola this can be used for nearly exact quadratic interpolation in frequency estimation\u000a\u000athe standard deviation of the gaussian function is n12 sampling periods\u000a\u000a\u000a confined gaussian window \u000athe confined gaussian window yields the smallest possible root mean square frequency width  for a given temporal width t these windows optimize the rms timefrequency bandwidth products they are computed as the minimum eigenvectors of a parameterdependent matrix the confined gaussian window family contains the cosine window and the gaussian window in the limiting cases of large and small t respectively\u000a\u000a\u000a approximate confined gaussian window \u000aa confined gaussian window of temporal width t is well approximated by\u000a\u000awith the gaussian\u000a\u000athe temporal width of the approximate window is asymptotically equal to t for t  014 n\u000a\u000a\u000a generalized normal window \u000aa more generalized version of the gaussian window is the generalized normal window retaining the notation from the gaussian window above we can represent this window as\u000a\u000afor any even  at  this is a gaussian window and as  approaches  this approximates to a rectangular window the fourier transform of this window does not exist in a closed form for a general  however it demonstrates the other benefits of being smooth adjustable bandwidth like the tukey window discussed later this window naturally offers a flat top to control the amplitude attenuation of a timeseries on which we dont have a control with gaussian window in essence it offers a good controllable compromise in terms of spectral leakage frequency resolution and amplitude attenuation between the gaussian window and the rectangular window see also  for a study on timefrequency representation of this window or function\u000a\u000a\u000a tukey window \u000a\u000athe tukey window also known as the tapered cosine window can be regarded as a cosine lobe of width n2 that is convolved with a rectangular window of width 1  2n\u000a\u000aor expressed with the havercosine hvc function\u000a\u000aat   0 it becomes rectangular and at   1 it becomes a hann window\u000a\u000a\u000a plancktaper window \u000a\u000athe socalled plancktaper window is a bump function that has been widely used in the theory of partitions of unity in manifolds it is a  function everywhere but is exactly zero outside of a compact region exactly one over an interval within that region and varies smoothly and monotonically between those limits its use as a window function in signal processing was first suggested in the context of gravitationalwave astronomy inspired by the planck distribution it is defined as a piecewise function\u000a\u000awhere\u000a\u000athe amount of tapering the region over which the function is exactly 1 is controlled by the parameter  with smaller values giving sharper transitions\u000a\u000a\u000a dpss or slepian window \u000a\u000athe dpss discrete prolate spheroidal sequence or slepian window is used to maximize the energy concentration in the main lobe\u000athe main lobe ends at a bin given by the parameter \u000a\u000a\u000a kaiser window \u000a\u000athe kaiser or kaiserbessel window is a simple approximation of the dpss window using bessel functions discovered by jim kaiser\u000a\u000awhere i0 is the zeroth order modified bessel function of the first kind variable parameter  determines the tradeoff between main lobe width and side lobe levels of the spectral leakage pattern the main lobe width in between the nulls is given by    in units of dft bins  and a typical value of  is 3\u000asometimes the formula for wn is written in terms of a parameter \u000azerophase version\u000a\u000a\u000a dolphchebyshev window \u000a\u000aminimizes the chebyshev norm of the sidelobes for a given main lobe width\u000athe zerophase dolphchebyshev window function w0n is usually defined in terms of its realvalued discrete fourier transform w0k\u000a\u000awhere the parameter  sets the chebyshev norm of the sidelobes to 20 decibels\u000athe window function can be calculated from w0k by an inverse discrete fourier transform dft\u000a\u000athe lagged version of the window with 0  n  n1 can be obtained by\u000a\u000awhich for even values of n must be computed as follows\u000a\u000awhich is an inverse dft of  \u000avariations\u000athe dfteven sequence for even values of n is given by    which is the inverse dft of   \u000adue to the equiripple condition the timedomain window has discontinuities at the edges an approximation that avoids them by allowing the equiripples to drop off at the edges is a taylor window\u000aan alternative to the inverse dft definition is also available2 it isnt clear if it is the symmetric   or dfteven   definition but for typical values of n found in practice the difference is negligible\u000a\u000a\u000a ultraspherical window \u000a\u000athe ultraspherical window was introduced in 1984 by roy streit and has application in antenna array design nonrecursive filter design and spectrum analysis\u000alike other adjustable windows the ultraspherical window has parameters that can be used to control its fourier transform mainlobe width and relative sidelobe amplitude uncommon to other windows it has an additional parameter which can be used to set the rate at which sidelobes decrease or increase in amplitude\u000athe window can be expressed in the timedomain as follows\u000a\u000awhere  is the ultraspherical polynomial of degree n and  and  control the sidelobe patterns\u000acertain specific values of  yield other wellknown windows  and  give the dolphchebyshev and saramki windows respectively see here for illustration of ultraspherical windows with varied parametrization\u000a\u000a\u000a exponential or poisson window \u000a\u000athe poisson window or more generically the exponential window increases exponentially towards the center of the window and decreases exponentially in the second half since the exponential function never reaches zero the values of the window at its limits are nonzero it can be seen as the multiplication of an exponential function by a rectangular window  it is defined by\u000a\u000awhere  is the time constant of the function the exponential function decays as e  271828 or approximately 869 db per time constant this means that for a targeted decay of d db over half of the window length the time constant  is given by\u000a\u000a\u000a hybrid windows \u000awindow functions have also been constructed as multiplicative or additive combinations of other windows\u000a\u000a\u000a bartletthann window \u000a\u000a\u000a planckbessel window \u000a\u000aa plancktaper window multiplied by a kaiser window which is defined in terms of a modified bessel function this hybrid window function was introduced to decrease the peak sidelobe level of the plancktaper window while still exploiting its good asymptotic decay it has two tunable parameters  from the plancktaper and  from the kaiser window so it can be adjusted to fit the requirements of a given signal\u000a\u000a\u000a hannpoisson window \u000a\u000aa hann window multiplied by a poisson window which has no sidelobes in the sense that its fourier transform drops off forever away from the main lobe it can thus be used in hill climbing algorithms like newtons method the hannpoisson window is defined by\u000a\u000awhere  is a parameter that controls the slope of the exponential\u000a\u000a\u000a other windows \u000a\u000a\u000a lanczos window \u000a\u000aused in lanczos resampling\u000afor the lanczos window sincx is defined as sinxx\u000aalso known as a sinc window because\u000a\u000a is the main lobe of a normalized sinc function\u000a\u000a\u000a comparison of windows \u000a\u000awhen selecting an appropriate window function for an application this comparison graph may be useful the frequency axis has units of fft bins when the window of length n is applied to data and a transform of length n is computed for instance the value at frequency  bin third tick mark is the response that would be measured in bins k and k1 to a sinusoidal signal at frequency k it is relative to the maximum possible response which occurs when the signal frequency is an integer number of bins the value at frequency  is referred to as the maximum scalloping loss of the window which is one metric used to compare windows the rectangular window is noticeably worse than the others in terms of that metric\u000aother metrics that can be seen are the width of the main lobe and the peak level of the sidelobes which respectively determine the ability to resolve comparable strength signals and disparate strength signals the rectangular window for instance is the best choice for the former and the worst choice for the latter what cannot be seen from the graphs is that the rectangular window has the best noise bandwidth which makes it a good candidate for detecting lowlevel sinusoids in an otherwise white noise environment interpolation techniques such as zeropadding and frequencyshifting are available to mitigate its potential scalloping loss\u000a\u000a\u000a overlapping windows \u000awhen the length of a data set to be transformed is larger than necessary to provide the desired frequency resolution a common practice is to subdivide it into smaller sets and window them individually to mitigate the loss at the edges of the window the individual sets may overlap in time see welch method of power spectral analysis and the modified discrete cosine transform\u000a\u000a\u000a twodimensional windows \u000atwodimensional windows are used in eg image processing they can be constructed from onedimensional windows in either of two forms\u000athe separable form  is trivial to compute the radial form  which involves the radius  is isotropic independent on the orientation of the coordinate axes only the gaussian function is both separable and isotropic the separable forms of all other window functions have corners that depend on the choice of the coordinate axes the isotropyanisotropy of a twodimensional window function is shared by its twodimensional fourier transform the difference between the separable and radial forms is akin to the result of diffraction from rectangular vs circular appertures which can be visualized in terms of the product of two sinc functions vs an airy function respectively\u000a\u000a\u000a see also \u000aspectral leakage\u000amultitaper\u000aapodization\u000awelch method\u000ashorttime fourier transform\u000awindow design method\u000akolmogorovzurbenko filter\u000a\u000a\u000a notes
p106
sg20
g23
sg24
g27
sg30
Vin signal processing a window function also known as an apodization function or tapering function is a mathematical function that is zerovalued outside of some chosen interval for instance a function that is constant inside the interval and zero elsewhere is called a rectangular window which describes the shape of its graphical representation when another function or waveformdatasequence is multiplied by a window function the product is also zerovalued outside the interval all that is left is the part where they overlap the view through the window\u000aapplications of window functions include spectral analysis filter design and beamforming in typical applications the window functions used are nonnegative smooth bellshaped curves though rectangle triangle and other functions can be used\u000aa more general definition of window functions does not require them to be identically zero outside an interval as long as the product of the window multiplied by its argument is square integrable and more specifically that the function goes sufficiently rapidly toward zero\u000a\u000a\u000a applications \u000aapplications of window functions include spectral analysis and the design of finite impulse response filters\u000a\u000a\u000a spectral analysis \u000athe fourier transform of the function cos t is zero except at frequency  however many other functions and waveforms do not have convenient closed form transforms alternatively one might be interested in their spectral content only during a certain time period\u000ain either case the fourier transform or something similar can be applied on one or more finite intervals of the waveform in general the transform is applied to the product of the waveform and a window function any window including rectangular affects the spectral estimate computed by this method\u000a\u000a\u000a windowing \u000awindowing of a simple waveform like cos t causes its fourier transform to develop nonzero values commonly called spectral leakage at frequencies other than  the leakage tends to be worst highest near  and least at frequencies farthest from \u000aif the waveform under analysis comprises two sinusoids of different frequencies leakage can interfere with the ability to distinguish them spectrally if their frequencies are dissimilar and one component is weaker then leakage from the stronger component can obscure the weaker ones presence but if the frequencies are similar leakage can render them unresolvable even when the sinusoids are of equal strength the rectangular window has excellent resolution characteristics for sinusoids of comparable strength but it is a poor choice for sinusoids of disparate amplitudes this characteristic is sometimes described as lowdynamicrange\u000aat the other extreme of dynamic range are the windows with the poorest resolution and sensitivity which is the ability to reveal relatively weak sinusoids in the presence of additive random noise that is because the noise produces a stronger response with highdynamicrange windows than with highresolution windows therefore highdynamicrange windows are most often justified in wideband applications where the spectrum being analyzed is expected to contain many different components of various amplitudes\u000ain between the extremes are moderate windows such as hamming and hann they are commonly used in narrowband applications such as the spectrum of a telephone channel in summary spectral analysis involves a tradeoff between resolving comparable strength components with similar frequencies and resolving disparate strength components with dissimilar frequencies that tradeoff occurs when the window function is chosen\u000a\u000a\u000a discretetime signals \u000awhen the input waveform is timesampled instead of continuous the analysis is usually done by applying a window function and then a discrete fourier transform dft but the dft provides only a coarse sampling of the actual discretetime fourier transform dtft spectrum figure 1 shows a portion of the dtft for a rectangularly windowed sinusoid the actual frequency of the sinusoid is indicated as 0 on the horizontal axis everything else is leakage exaggerated by the use of a logarithmic presentation the unit of frequency is dft bins that is the integer values on the frequency axis correspond to the frequencies sampled by the dft so the figure depicts a case where the actual frequency of the sinusoid happens to coincide with a dft sample and the maximum value of the spectrum is accurately measured by that sample when it misses the maximum value by some amount up to 12 bin the measurement error is referred to as scalloping loss inspired by the shape of the peak but the most interesting thing about this case is that all the other samples coincide with nulls in the true spectrum the nulls are actually zerocrossings which cannot be shown on a logarithmic scale such as this so in this case the dft creates the illusion of no leakage despite the unlikely conditions of this example it is a common misconception that visible leakage is some sort of artifact of the dft but since any window function causes leakage its apparent absence in this contrived example is actually the dft artifact\u000a\u000a\u000a noise bandwidth \u000athe concepts of resolution and dynamic range tend to be somewhat subjective depending on what the user is actually trying to do but they also tend to be highly correlated with the total leakage which is quantifiable it is usually expressed as an equivalent bandwidth b it can be thought of as redistributing the dtft into a rectangular shape with height equal to the spectral maximum and width b the more the leakage the greater the bandwidth it is sometimes called noise equivalent bandwidth or equivalent noise bandwidth because it is proportional to the average power that will be registered by each dft bin when the input signal contains a random noise component or is just random noise a graph of the power spectrum averaged over time typically reveals a flat noise floor caused by this effect the height of the noise floor is proportional to b so two different window functions can produce different noise floors\u000a\u000a\u000a processing gain and losses \u000ain signal processing operations are chosen to improve some aspect of quality of a signal by exploiting the differences between the signal and the corrupting influences when the signal is a sinusoid corrupted by additive random noise spectral analysis distributes the signal and noise components differently often making it easier to detect the signals presence or measure certain characteristics such as amplitude and frequency effectively the signal to noise ratio snr is improved by distributing the noise uniformly while concentrating most of the sinusoids energy around one frequency processing gain is a term often used to describe an snr improvement the processing gain of spectral analysis depends on the window function both its noise bandwidth b and its potential scalloping loss these effects partially offset because windows with the least scalloping naturally have the most leakage\u000athe figure at right depicts the effects of three different window functions on the same data set comprising two equal strength sinusoids in additive noise the frequencies of the sinusoids are chosen such that one encounters no scalloping and the other encounters maximum scalloping both sinusoids suffer less snr loss under the hann window than under the blackmanharris window in general as mentioned earlier this is a deterrent to using highdynamicrange windows in lowdynamicrange applications\u000a\u000a\u000a filter design \u000a\u000awindows are sometimes used in the design of digital filters in particular to convert an ideal impulse response of infinite duration such as a sinc function to a finite impulse response fir filter design that is called the window method\u000a\u000a\u000a symmetry and asymmetry \u000awindow functions generated for digital filter design are symmetrical sequences usually an odd length with a single maximum at the center windows for dftfft usage such as in spectral analysis are often created by deleting the rightmost coefficient of an oddlength symmetrical window such truncated sequences are known as periodic the deleted coefficient is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended which is the timedomain equivalent of sampling the dtft a different way of saying the same thing is that the dft samples the dtft of the window at the exact points that are not affected by spectral leakage from the discontinuity the advantage of this trick is that a 512 length window for example enjoys the slightly better performance metrics of a 513 length design such a window is generated by the matlab function hann512periodic for instance to generate it with the formula in this article below the window length n is 513 and the 513th coefficient of the generated sequence is discarded\u000aanother type of asymmetric window called dfteven is limited to even length sequences the generated sequence is offset cyclically from its zerophase counterpart by exactly half the sequence length in the frequency domain that corresponds to a multiplication by the trivial sequence 1k which can have implementation advantages for windows defined by their frequency domain form compared to a symmetrical window the dfteven sequence has an offset of  sample as illustrated in the figure at right that means the asymmetry is limited to just one missing coefficient therefore as in the periodic case it is effectively restored by a virtual copy of the symmetrical leftmost coefficient when the truncated sequence is periodically extended\u000a\u000a\u000a applications for which windows should not be used \u000ain some applications it is preferable not to use a window function for example\u000ain impact modal testing when analyzing transient signals such as an excitation signal from hammer blow see impulse excitation technique where most of the energy is located at the beginning of the recording using a nonrectangular window would attenuate most of the energy and spread the frequency response unnecessarily\u000aa generalization of above when measuring a selfwindowing signal such as an impulse a shock response a sine burst a chirp burst noise burst such signals are used in modal analysis applying a window function in this case would just deteriorate the signaltonoise ratio\u000awhen measuring a pseudorandom noise prn excitation signal with period t and using the same recording period t a prn signal is periodic and therefore all spectral components of the signal will coincide with fft bin centers with no leakage\u000awhen measuring a repetitive signal lockedin to the sampling frequency for example measuring the vibration spectrum analysis during shaft alignment fault diagnosis of bearings engines gearboxes etc since the signal is repetitive all spectral energy is confined to multiples of the base repetition frequency\u000ain an ofdm receiver the input signal is directly multiplied by fft without a window function the frequency subcarriers aka symbols are designed to align exactly to the fft frequency bins a cyclic prefix is usually added to the transmitted signal allowing frequencyselective fading due to multipath to be modeled as circular convolution thus avoiding intersymbol interference which in ofdm is equivalent to spectral leakage\u000a\u000a\u000a a list of window functions \u000aterminology\u000an represents the width in samples of a discretetime symmetrical window function    when n is an odd number the nonflat windows have a singular maximum point when n is even they have a double maximum\u000ait is sometimes useful to express    as a sequence of samples of the lagged version of a zerophase function\u000a  \u000afor instance for even values of n we can describe the related dfteven window as    as discussed in the previous section the dft of such a sequence in terms of the dft of the    sequence is  \u000aeach figure label includes the corresponding noise equivalent bandwidth metric b in units of dft bins\u000a\u000a\u000a bspline windows \u000abspline windows can be obtained as kfold convolutions of the rectangular window they include the rectangular window itself k  1 the triangular window k  2 and the parzen window k  4 alternative definitions sample the appropriate normalized bspline basis functions instead of convolving discretetime windows a kth order bspline basis function is a piecewise polynomial function of degree k1 that is obtained by kfold selfconvolution of the rectangular function\u000a\u000a\u000a rectangular window \u000a\u000athe rectangular window sometimes known as the boxcar or dirichlet window is the simplest window equivalent to replacing all but n values of a data sequence by zeros making it appear as though the waveform suddenly turns on and off\u000a\u000aother windows are designed to moderate these sudden changes because discontinuities have undesirable effects on the discretetime fourier transform dtft andor the algorithms that produce samples of the dtft\u000athe rectangular window is the 1st order bspline window as well as the 0th power cosine window\u000a\u000a\u000a triangular window \u000atriangular windows are given by\u000a\u000awhere l can be n n1 or n1 the last one is also known as bartlett window or fejr window all three definitions converge at large n\u000athe triangular window is the 2nd order bspline window and can be seen as the convolution of two n2 width rectangular windows the fourier transform of the result is the squared values of the transform of the halfwidth rectangular window\u000a\u000a\u000a parzen window \u000a\u000athe parzen window also known as the de la valle poussin window is the 4th order bspline window given by\u000a\u000a\u000a other polynomial windows \u000a\u000a\u000a welch window \u000a\u000athe welch window consists of a single parabolic section\u000a\u000athe defining quadratic polynomial reaches a value of zero at the samples just outside the span of the window\u000a\u000a\u000a generalized hamming windows \u000ageneralized hamming windows are of the form\u000a\u000athey have only three nonzero dft coefficients and share the benefits of a sparse frequency domain representation with higherorder generalized cosine windows\u000a\u000a\u000a hann hanning window \u000a\u000athe hann window named after julius von hann and also known as the hanning for being similar in name and form to the hamming window von hann and the raised cosine window is defined by with hav for the haversine function\u000a\u000athe ends of the cosine just touch zero so the sidelobes roll off at about 18 db per octave\u000a\u000a\u000a hamming window \u000a\u000athe window with these particular coefficients was proposed by richard w hamming the window is optimized to minimize the maximum nearest side lobe giving it a height of about onefifth that of the hann window\u000a\u000awith\u000a\u000ainstead of both constants being equal to 12 in the hann window the constants are approximations of values   2546 and   2146 which cancel the first sidelobe of the hann window by placing a zero at frequency 5n  1 approximation of the constants to two decimal places substantially lowers the level of sidelobes to a nearly equiripple condition in the equiripple sense the optimal values for the coefficients are   053836 and   046164\u000azerophase version\u000a\u000a\u000a higherorder generalized cosine windows \u000awindows of the form\u000a\u000ahave only 2k  1 nonzero dft coefficients which makes them good choices for applications that require windowing by convolution in the frequencydomain in those applications the dft of the unwindowed data vector is needed for a different purpose than spectral analysis see overlapsave method generalized cosine windows with just two terms k  1 belong in the subfamily generalized hamming windows\u000a\u000a\u000a blackman windows \u000a\u000ablackman windows are defined as\u000a\u000aby common convention the unqualified term blackman window refers to   016 as this most closely approximates the exact blackman with a0  793818608  042659 a1  924018608  049656 and a2  143018608  0076849 these exact values place zeros at the third and fourth sidelobes\u000a\u000a\u000a nuttall window continuous first derivative \u000a\u000aconsidering n as a real number the nuttall window function and its first derivative are continuous everywhere that is the function goes to 0 at n  0 unlike the blackmannuttall and blackmanharris windows which have a small positive value at zero at step from the zero outside the window like the hamming window the blackman window defined via  is also continuous with continuous derivative at the edge but the described exact blackman window is not\u000a\u000a\u000a blackmannuttall window \u000a\u000a\u000a blackmanharris window \u000a\u000aa generalization of the hamming family produced by adding more shifted sinc functions meant to minimize sidelobe levels\u000a\u000a\u000a flat top window \u000a\u000aa flat top window is a partially negativevalued window that has a flat top in the frequency domain such windows have been made available in spectrum analyzers for the measurement of amplitudes of sinusoidal frequency components they have a low amplitude measurement error suitable for this purpose achieved by the spreading of the energy of a sine wave over multiple bins in the spectrum this ensures that the unattenuated amplitude of the sinusoid can be found on at least one of the neighboring bins the drawback of the broad bandwidth is poor frequency resolution to compensate a longer window length may be chosen\u000aflat top windows can be designed using lowpass filter design methods or they may be of the usual sumofcosineterms variety an example of the latter is the flat top window available in the stanford research systems srs sr785 spectrum analyzer\u000a\u000a \u000a\u000a\u000a rifevincent window \u000arife and vincent define three classes of windows constructed as sums of cosines the classes are generalizations of the hanning window their orderp windows are of the form normalized to have unity average as opposed to unity max as the windows above are\u000a\u000afor order 1 this formula can match the hanning window for a1  1 this is the rifevincent classi window defined by minimizing the highorder sidelobe amplitude the classi order2 rifevincent window has a1  43 and a2  13 coefficients for orders up to 4 are tabulated for orders greater than 1 the rifevincent window coefficients can be optimized for class ii meaning minimized mainlobe width for a given maximum sidelobe or for class iii a compromise for which order 2 resembles blackmanns window given the wide variety of rifevincent windows plots are not given here\u000a\u000a\u000a powerofcosine windows \u000awindow functions in the powerofcosine family are of the form\u000a\u000athe rectangular window   0 the cosine window   1 and the hann window   2 are members of this family\u000a\u000a\u000a cosine window \u000a\u000athe cosine window is also known as the sine window cosine window describes the shape of \u000aa cosine window convolved by itself is known as the bohman window\u000a\u000a\u000a adjustable windows \u000a\u000a\u000a gaussian window \u000a\u000athe fourier transform of a gaussian is also a gaussian it is an eigenfunction of the fourier transform since the gaussian function extends to infinity it must either be truncated at the ends of the window or itself windowed with another zeroended window\u000asince the log of a gaussian produces a parabola this can be used for nearly exact quadratic interpolation in frequency estimation\u000a\u000athe standard deviation of the gaussian function is n12 sampling periods\u000a\u000a\u000a confined gaussian window \u000athe confined gaussian window yields the smallest possible root mean square frequency width  for a given temporal width t these windows optimize the rms timefrequency bandwidth products they are computed as the minimum eigenvectors of a parameterdependent matrix the confined gaussian window family contains the cosine window and the gaussian window in the limiting cases of large and small t respectively\u000a\u000a\u000a approximate confined gaussian window \u000aa confined gaussian window of temporal width t is well approximated by\u000a\u000awith the gaussian\u000a\u000athe temporal width of the approximate window is asymptotically equal to t for t  014 n\u000a\u000a\u000a generalized normal window \u000aa more generalized version of the gaussian window is the generalized normal window retaining the notation from the gaussian window above we can represent this window as\u000a\u000afor any even  at  this is a gaussian window and as  approaches  this approximates to a rectangular window the fourier transform of this window does not exist in a closed form for a general  however it demonstrates the other benefits of being smooth adjustable bandwidth like the tukey window discussed later this window naturally offers a flat top to control the amplitude attenuation of a timeseries on which we dont have a control with gaussian window in essence it offers a good controllable compromise in terms of spectral leakage frequency resolution and amplitude attenuation between the gaussian window and the rectangular window see also  for a study on timefrequency representation of this window or function\u000a\u000a\u000a tukey window \u000a\u000athe tukey window also known as the tapered cosine window can be regarded as a cosine lobe of width n2 that is convolved with a rectangular window of width 1  2n\u000a\u000aor expressed with the havercosine hvc function\u000a\u000aat   0 it becomes rectangular and at   1 it becomes a hann window\u000a\u000a\u000a plancktaper window \u000a\u000athe socalled plancktaper window is a bump function that has been widely used in the theory of partitions of unity in manifolds it is a  function everywhere but is exactly zero outside of a compact region exactly one over an interval within that region and varies smoothly and monotonically between those limits its use as a window function in signal processing was first suggested in the context of gravitationalwave astronomy inspired by the planck distribution it is defined as a piecewise function\u000a\u000awhere\u000a\u000athe amount of tapering the region over which the function is exactly 1 is controlled by the parameter  with smaller values giving sharper transitions\u000a\u000a\u000a dpss or slepian window \u000a\u000athe dpss discrete prolate spheroidal sequence or slepian window is used to maximize the energy concentration in the main lobe\u000athe main lobe ends at a bin given by the parameter \u000a\u000a\u000a kaiser window \u000a\u000athe kaiser or kaiserbessel window is a simple approximation of the dpss window using bessel functions discovered by jim kaiser\u000a\u000awhere i0 is the zeroth order modified bessel function of the first kind variable parameter  determines the tradeoff between main lobe width and side lobe levels of the spectral leakage pattern the main lobe width in between the nulls is given by    in units of dft bins  and a typical value of  is 3\u000asometimes the formula for wn is written in terms of a parameter \u000azerophase version\u000a\u000a\u000a dolphchebyshev window \u000a\u000aminimizes the chebyshev norm of the sidelobes for a given main lobe width\u000athe zerophase dolphchebyshev window function w0n is usually defined in terms of its realvalued discrete fourier transform w0k\u000a\u000awhere the parameter  sets the chebyshev norm of the sidelobes to 20 decibels\u000athe window function can be calculated from w0k by an inverse discrete fourier transform dft\u000a\u000athe lagged version of the window with 0  n  n1 can be obtained by\u000a\u000awhich for even values of n must be computed as follows\u000a\u000awhich is an inverse dft of  \u000avariations\u000athe dfteven sequence for even values of n is given by    which is the inverse dft of   \u000adue to the equiripple condition the timedomain window has discontinuities at the edges an approximation that avoids them by allowing the equiripples to drop off at the edges is a taylor window\u000aan alternative to the inverse dft definition is also available2 it isnt clear if it is the symmetric   or dfteven   definition but for typical values of n found in practice the difference is negligible\u000a\u000a\u000a ultraspherical window \u000a\u000athe ultraspherical window was introduced in 1984 by roy streit and has application in antenna array design nonrecursive filter design and spectrum analysis\u000alike other adjustable windows the ultraspherical window has parameters that can be used to control its fourier transform mainlobe width and relative sidelobe amplitude uncommon to other windows it has an additional parameter which can be used to set the rate at which sidelobes decrease or increase in amplitude\u000athe window can be expressed in the timedomain as follows\u000a\u000awhere  is the ultraspherical polynomial of degree n and  and  control the sidelobe patterns\u000acertain specific values of  yield other wellknown windows  and  give the dolphchebyshev and saramki windows respectively see here for illustration of ultraspherical windows with varied parametrization\u000a\u000a\u000a exponential or poisson window \u000a\u000athe poisson window or more generically the exponential window increases exponentially towards the center of the window and decreases exponentially in the second half since the exponential function never reaches zero the values of the window at its limits are nonzero it can be seen as the multiplication of an exponential function by a rectangular window  it is defined by\u000a\u000awhere  is the time constant of the function the exponential function decays as e  271828 or approximately 869 db per time constant this means that for a targeted decay of d db over half of the window length the time constant  is given by\u000a\u000a\u000a hybrid windows \u000awindow functions have also been constructed as multiplicative or additive combinations of other windows\u000a\u000a\u000a bartletthann window \u000a\u000a\u000a planckbessel window \u000a\u000aa plancktaper window multiplied by a kaiser window which is defined in terms of a modified bessel function this hybrid window function was introduced to decrease the peak sidelobe level of the plancktaper window while still exploiting its good asymptotic decay it has two tunable parameters  from the plancktaper and  from the kaiser window so it can be adjusted to fit the requirements of a given signal\u000a\u000a\u000a hannpoisson window \u000a\u000aa hann window multiplied by a poisson window which has no sidelobes in the sense that its fourier transform drops off forever away from the main lobe it can thus be used in hill climbing algorithms like newtons method the hannpoisson window is defined by\u000a\u000awhere  is a parameter that controls the slope of the exponential\u000a\u000a\u000a other windows \u000a\u000a\u000a lanczos window \u000a\u000aused in lanczos resampling\u000afor the lanczos window sincx is defined as sinxx\u000aalso known as a sinc window because\u000a\u000a is the main lobe of a normalized sinc function\u000a\u000a\u000a comparison of windows \u000a\u000awhen selecting an appropriate window function for an application this comparison graph may be useful the frequency axis has units of fft bins when the window of length n is applied to data and a transform of length n is computed for instance the value at frequency  bin third tick mark is the response that would be measured in bins k and k1 to a sinusoidal signal at frequency k it is relative to the maximum possible response which occurs when the signal frequency is an integer number of bins the value at frequency  is referred to as the maximum scalloping loss of the window which is one metric used to compare windows the rectangular window is noticeably worse than the others in terms of that metric\u000aother metrics that can be seen are the width of the main lobe and the peak level of the sidelobes which respectively determine the ability to resolve comparable strength signals and disparate strength signals the rectangular window for instance is the best choice for the former and the worst choice for the latter what cannot be seen from the graphs is that the rectangular window has the best noise bandwidth which makes it a good candidate for detecting lowlevel sinusoids in an otherwise white noise environment interpolation techniques such as zeropadding and frequencyshifting are available to mitigate its potential scalloping loss\u000a\u000a\u000a overlapping windows \u000awhen the length of a data set to be transformed is larger than necessary to provide the desired frequency resolution a common practice is to subdivide it into smaller sets and window them individually to mitigate the loss at the edges of the window the individual sets may overlap in time see welch method of power spectral analysis and the modified discrete cosine transform\u000a\u000a\u000a twodimensional windows \u000atwodimensional windows are used in eg image processing they can be constructed from onedimensional windows in either of two forms\u000athe separable form  is trivial to compute the radial form  which involves the radius  is isotropic independent on the orientation of the coordinate axes only the gaussian function is both separable and isotropic the separable forms of all other window functions have corners that depend on the choice of the coordinate axes the isotropyanisotropy of a twodimensional window function is shared by its twodimensional fourier transform the difference between the separable and radial forms is akin to the result of diffraction from rectangular vs circular appertures which can be visualized in terms of the product of two sinc functions vs an airy function respectively\u000a\u000a\u000a see also \u000aspectral leakage\u000amultitaper\u000aapodization\u000awelch method\u000ashorttime fourier transform\u000awindow design method\u000akolmogorovzurbenko filter\u000a\u000a\u000a notes \u000a\u000a\u000a
p107
sg32
g35
sg37
NsbsS'parameter_space.txt'
p108
g2
(g3
g4
Ntp109
Rp110
(dp111
g8
g11
sg12
Vin science a parameter space is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model the ranges of values of the parameters may form the axes of a plot and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model\u000aoften the parameters are inputs of a function in which case the technical term for the parameter space is domain of a function\u000aparameter spaces are particularly useful for describing families of probability distributions that depend on parameters more generally in science the term parameter space is used to describe experimental variables for example the concept has been used in the science of soccer in the article parameter space for successful soccer kicks in the study success rates are determined through the use of fourdimensional parameter space volumes\u000ain the context of statistics parameter spaces form the background for parameter estimation as ross describes in his book\u000aparameter space is a subset of pdimensional space consisting of the set of values of  which are allowable in a particular model the values may sometimes be constrained say to the positive quadrant or the unit square or in case of symmetry to the triangular region where say \u000athe idea of intentionally truncating the parameter space has also been advanced elsewhere\u000a\u000a\u000a examplesedit \u000aa simple model of health deterioration after developing lung cancer could include the two parameters gender and smokernonsmoker in which case the parameter space is the following set of four possibilities male smoker male nonsmoker female smoker female nonsmoker \u000athe logistic map  has one parameter r which can take any positive value the parameter space is therefore the set of all positive numbers\u000afor some values of r this function ends up cycling round a few values or fixed on one value these longterm values can be plotted against r in a bifurcation diagram to show the different behaviours of the function for different values of r\u000ain a sine wave model  the parameters are amplitude a  0 angular frequency   0 and phase   s1 thus the parameter space is\u000a\u000ain complex dynamics the parameter space is the complex plane c   z  x  y i  x y  r  where i2  1\u000athe famous mandelbrot set is a subset of this parameter space consisting of the points in the complex plane which give a bounded set of numbers when a particular iterated function is repeatedly applied from that starting point the remaining points which are not in the set give an unbounded set of numbers they tend to infinity when this function is repeatedly applied from that starting point\u000a\u000a\u000a historyedit \u000aparameter space contributed to the liberation of geometry from the confines of threedimensional space for instance the parameter space of spheres in three dimensions has four dimensionsthree for the sphere center and another for the radius according to dirk struik it was the book neue geometrie des raumes 1849 by julius plcker that showed\u000ageometry need not solely be based on points as basic elements lines planes circles spheres can all be used as the elements raumelemente on which a geometry can be based this fertile conception threw new light on both synthetic and algebraic geometry and created new forms of duality the number of dimensions of a particular form of geometry could now be any positive number depending on the number of parameters necessary to define the element\u000athe requirement for higher dimensions is illustrated by plckers line geometry struik writes\u000aplckers geometry of lines in threespace could be considered as a fourdimensional geometry or as klein has stressed as the geometry of a fourdimensional quadric in a fivedimensional space\u000athus the klein quadric describes the parameters of lines in space\u000a\u000a\u000a see alsoedit \u000aconfiguration space\u000adata analysis\u000aparametric equation\u000aparametric surface\u000aphase space\u000a\u000a\u000a notes and referencesedit \u000a\u000abrandon g cook  john eric goff 2006 parameter space for successful soccer kicks european journal of physics 27865\u000aconstance van eeden 2006 restricted parameter space estimation problems admissibility and minimaxity properties lecture notes in statistics 188 springer sciencebusiness media
p112
sg14
g17
sg18
Vin science a parameter space is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model the ranges of values of the parameters may form the axes of a plot and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model\u000aoften the parameters are inputs of a function in which case the technical term for the parameter space is domain of a function\u000aparameter spaces are particularly useful for describing families of probability distributions that depend on parameters more generally in science the term parameter space is used to describe experimental variables for example the concept has been used in the science of soccer in the article parameter space for successful soccer kicks in the study success rates are determined through the use of fourdimensional parameter space volumes\u000ain the context of statistics parameter spaces form the background for parameter estimation as ross describes in his book\u000aparameter space is a subset of pdimensional space consisting of the set of values of  which are allowable in a particular model the values may sometimes be constrained say to the positive quadrant or the unit square or in case of symmetry to the triangular region where say \u000athe idea of intentionally truncating the parameter space has also been advanced elsewhere\u000a\u000a\u000a examplesedit \u000aa simple model of health deterioration after developing lung cancer could include the two parameters gender and smokernonsmoker in which case the parameter space is the following set of four possibilities male smoker male nonsmoker female smoker female nonsmoker \u000athe logistic map  has one parameter r which can take any positive value the parameter space is therefore the set of all positive numbers\u000afor some values of r this function ends up cycling round a few values or fixed on one value these longterm values can be plotted against r in a bifurcation diagram to show the different behaviours of the function for different values of r\u000ain a sine wave model  the parameters are amplitude a  0 angular frequency   0 and phase   s1 thus the parameter space is\u000a\u000ain complex dynamics the parameter space is the complex plane c   z  x  y i  x y  r  where i2  1\u000athe famous mandelbrot set is a subset of this parameter space consisting of the points in the complex plane which give a bounded set of numbers when a particular iterated function is repeatedly applied from that starting point the remaining points which are not in the set give an unbounded set of numbers they tend to infinity when this function is repeatedly applied from that starting point\u000a\u000a\u000a historyedit \u000aparameter space contributed to the liberation of geometry from the confines of threedimensional space for instance the parameter space of spheres in three dimensions has four dimensionsthree for the sphere center and another for the radius according to dirk struik it was the book neue geometrie des raumes 1849 by julius plcker that showed\u000ageometry need not solely be based on points as basic elements lines planes circles spheres can all be used as the elements raumelemente on which a geometry can be based this fertile conception threw new light on both synthetic and algebraic geometry and created new forms of duality the number of dimensions of a particular form of geometry could now be any positive number depending on the number of parameters necessary to define the element\u000athe requirement for higher dimensions is illustrated by plckers line geometry struik writes\u000aplckers geometry of lines in threespace could be considered as a fourdimensional geometry or as klein has stressed as the geometry of a fourdimensional quadric in a fivedimensional space\u000athus the klein quadric describes the parameters of lines in space\u000a\u000a\u000a see alsoedit \u000aconfiguration space\u000adata analysis\u000aparametric equation\u000aparametric surface\u000aphase space\u000a\u000a\u000a notes and referencesedit \u000a\u000abrandon g cook  john eric goff 2006 parameter space for successful soccer kicks european journal of physics 27865\u000aconstance van eeden 2006 restricted parameter space estimation problems admissibility and minimaxity properties lecture notes in statistics 188 springer sciencebusiness media
p113
sg20
g23
sg24
g27
sg30
Vin science a parameter space is the set of all possible combinations of values for all the different parameters contained in a particular mathematical model the ranges of values of the parameters may form the axes of a plot and particular outcomes of the model may be plotted against these axes to illustrate how different regions of the parameter space produce different types of behaviour in the model\u000aoften the parameters are inputs of a function in which case the technical term for the parameter space is domain of a function\u000aparameter spaces are particularly useful for describing families of probability distributions that depend on parameters more generally in science the term parameter space is used to describe experimental variables for example the concept has been used in the science of soccer in the article parameter space for successful soccer kicks in the study success rates are determined through the use of fourdimensional parameter space volumes\u000ain the context of statistics parameter spaces form the background for parameter estimation as ross describes in his book\u000aparameter space is a subset of pdimensional space consisting of the set of values of  which are allowable in a particular model the values may sometimes be constrained say to the positive quadrant or the unit square or in case of symmetry to the triangular region where say \u000athe idea of intentionally truncating the parameter space has also been advanced elsewhere\u000a\u000a\u000a examplesedit \u000aa simple model of health deterioration after developing lung cancer could include the two parameters gender and smokernonsmoker in which case the parameter space is the following set of four possibilities male smoker male nonsmoker female smoker female nonsmoker \u000athe logistic map  has one parameter r which can take any positive value the parameter space is therefore the set of all positive numbers\u000afor some values of r this function ends up cycling round a few values or fixed on one value these longterm values can be plotted against r in a bifurcation diagram to show the different behaviours of the function for different values of r\u000ain a sine wave model  the parameters are amplitude a  0 angular frequency   0 and phase   s1 thus the parameter space is\u000a\u000ain complex dynamics the parameter space is the complex plane c   z  x  y i  x y  r  where i2  1\u000athe famous mandelbrot set is a subset of this parameter space consisting of the points in the complex plane which give a bounded set of numbers when a particular iterated function is repeatedly applied from that starting point the remaining points which are not in the set give an unbounded set of numbers they tend to infinity when this function is repeatedly applied from that starting point\u000a\u000a\u000a historyedit \u000aparameter space contributed to the liberation of geometry from the confines of threedimensional space for instance the parameter space of spheres in three dimensions has four dimensionsthree for the sphere center and another for the radius according to dirk struik it was the book neue geometrie des raumes 1849 by julius plcker that showed\u000ageometry need not solely be based on points as basic elements lines planes circles spheres can all be used as the elements raumelemente on which a geometry can be based this fertile conception threw new light on both synthetic and algebraic geometry and created new forms of duality the number of dimensions of a particular form of geometry could now be any positive number depending on the number of parameters necessary to define the element\u000athe requirement for higher dimensions is illustrated by plckers line geometry struik writes\u000aplckers geometry of lines in threespace could be considered as a fourdimensional geometry or as klein has stressed as the geometry of a fourdimensional quadric in a fivedimensional space\u000athus the klein quadric describes the parameters of lines in space\u000a\u000a\u000a see alsoedit \u000aconfiguration space\u000adata analysis\u000aparametric equation\u000aparametric surface\u000aphase space\u000a\u000a\u000a notes and referencesedit \u000a\u000abrandon g cook  john eric goff 2006 parameter space for successful soccer kicks european journal of physics 27865\u000aconstance van eeden 2006 restricted parameter space estimation problems admissibility and minimaxity properties lecture notes in statistics 188 springer sciencebusiness media
p114
sg32
g35
sg37
NsbsS'entropy_(information_theory).txt'
p115
g2
(g3
g4
Ntp116
Rp117
(dp118
g8
g11
sg12
Vin information theory systems are modeled by a transmitter channel and receiver the transmitter produces messages that are sent through the channel the channel modifies the message in some way the receiver attempts to infer which message was sent in this context entropy more specifically shannon entropy is the expected value average of the information contained in each message messages can be modeled by any flow of information\u000ain a more technical sense there are reasons explained below to define information as the negative of the logarithm of the probability distribution the probability distribution of the events coupled with the information amount of every event forms a random variable whose expected value is the average amount of information or entropy generated by this distribution units of entropy are the shannon nat or hartley depending on the base of the logarithm used to define it though the shannon is commonly referred to as a bit\u000athe logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources for instance the entropy of a coin toss is 1 shannon whereas of m tosses it is m shannons generally you need log2n bits to represent a variable that can take one of n values if n is a power of 2 if these values are equally probable the entropy in shannons is equal to the number of bits equality between number of bits and shannons holds only while all outcomes are equally probable if one of the events is more probable than others observation of that event is less informative conversely rarer events provide more information when observed since observation of less probable events occurs more rarely the net effect is that the entropy thought of as average information received from nonuniformly distributed data is less than log2n entropy is zero when one outcome is certain shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known the meaning of the events observed the meaning of messages does not matter in the definition of entropy entropy only takes into account the probability of observing a specific event so the information it encapsulates is information about the underlying probability distribution not the meaning of the events themselves\u000agenerally entropy refers to disorder or uncertainty shannon entropy was introduced by claude e shannon in his 1948 paper a mathematical theory of communication shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source rnyi entropy generalizes shannon entropy\u000a\u000a\u000a introductionedit \u000aentropy is a measure of unpredictability of information content to get an informal intuitive understanding of the connection between these three english terms consider the example of a poll on some political issue usually such polls happen because the outcome of the poll isnt already known in other words the outcome of the poll is relatively unpredictable and actually performing the poll and learning the results gives some new information these are just different ways of saying that the entropy of the poll results is large now consider the case that the same poll is performed a second time shortly after the first poll since the result of the first poll is already known the outcome of the second poll can be predicted well and the results should not contain much new information in this case the entropy of the second poll result is small relative to the first\u000anow consider the example of a coin toss when the coin is fair that is when the probability of heads is the same as the probability of tails then the entropy of the coin toss is as high as it could be this is because there is no way to predict the outcome of the coin toss ahead of timethe best we can do is predict that the coin will come up heads and our prediction will be correct with probability 12 such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability and learning the actual outcome contains one bit of information contrarily a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads and the outcome can be predicted perfectly\u000aenglish text has fairly low entropy in other words it is fairly predictable even if we dont know exactly what is going to come next we can be fairly certain that for example there will be many more es than zs that the combination qu will be much more common than any other combination with a q in it and that the combination th will be more common than z q or qu after the first few letters one can often guess the rest of the word english text has between 06 and 13 bits of entropy for each character of message\u000athe chinese version of wikipedia points out that chinese characters have a much higher entropy than english each character of chinese has about log212500113 bits almost three times higher than english however the discussion could be much more sophisticated than this simple calculation because in english the usage of words not only characters and redundancy factors could be considered\u000aif a compression scheme is losslessthat is you can always recover the entire original message by decompressingthen a compressed message has the same quantity of information as the original but communicated in fewer characters that is it has more information or a higher entropy per character this means a compressed message has less redundancy roughly speaking shannons source coding theorem says that a lossless compression scheme cannot compress messages on average to have more than one bit of information per bit of message but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme the entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains\u000ashannons theorem also implies that no lossless compression scheme can shorten all messages if some messages come out shorter at least one must come out longer due to the pigeonhole principle in practical use this is generally not a problem because we are usually only interested in compressing certain types of messages for example english documents as opposed to gibberish text or digital photographs rather than noise and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger however the problem can still arise even in everyday use when applying a compression algorithm to already compressed data for example making a zip file of music that is already in the flac audio format is unlikely to achieve much extra saving in space\u000a\u000a\u000a definitionedit \u000anamed after boltzmanns theorem shannon defined the entropy  greek letter eta of a discrete random variable x with possible values x1  xn and probability mass function px as\u000a\u000ahere e is the expected value operator and i is the information content of x ix is itself a random variable\u000athe entropy can explicitly be written as\u000a\u000awhere b is the base of the logarithm used common values of b are 2 eulers number e and 10 and the unit of entropy is shannon for b  2 nat for b  e and hartley for b  10 when b  2 the units of entropy are also commonly referred to as bits\u000ain the case of pxi  0 for some i the value of the corresponding summand 0 logb0 is taken to be 0 which is consistent with the limit\u000a\u000awhen the distribution is continuous rather than discrete the sum is replaced with an integral as\u000a\u000awhere px represents a probability density function\u000aone may also define the conditional entropy of two events x and y taking values xi and yj respectively as\u000a\u000awhere pxi yj is the probability that x  xi and y  yj this quantity should be understood as the amount of randomness in the random variable x given the event y\u000a\u000a\u000a exampleedit \u000a\u000aconsider tossing a coin with known not necessarily fair probabilities of coming up heads or tails this is known as the bernoulli process\u000athe entropy of the unknown result of the next toss of the coin is maximized if the coin is fair that is if heads and tails both have equal probability 12 this is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss the result of each toss of the coin delivers one full bit of information\u000ahowever if we know the coin is not fair but comes up heads or tails with probabilities p and q where p  q then there is less uncertainty every time it is tossed one side is more likely to come up than the other the reduced uncertainty is quantified in a lower entropy on average each toss of the coin delivers less than one full bit of information\u000athe extreme case is that of a doubleheaded coin that never comes up tails or a doubletailed coin that never results in a head then there is no uncertainty the entropy is zero each toss of the coin delivers no new information as the outcome of each coin toss is always certain in this respect entropy can be normalized by dividing it by information length this ratio is called metric entropy and is a measure of the randomness of the information\u000a\u000a\u000a rationaleedit \u000ato understand the meaning of  pi log1pi at first try to define an information function i in terms of an event i with probability pi how much information is acquired due to the observation of event i shannons solution follows from the fundamental properties of information\u000aip  0  information is a nonnegative quantity\u000ai1  0  events that always occur do not communicate information\u000aip1 p2  ip1  ip2  information due to independent events is additive\u000athe last is a crucial property it states that joint probability communicates as much information as two individual events separately particularly if the first event can yield one of n equiprobable outcomes and another has one of m equiprobable outcomes then there are mn possible outcomes of the joint event this means that if log2n bits are needed to encode the first value and log2m to encode the second one needs log2mn  log2m  log2n to encode both shannon discovered that the proper choice of function to quantify information preserving this additivity is logarithmic ie\u000a\u000athe base of the logarithm can be any fixed real number greater than 1 the different units of information bits for log2 trits for log3 nats for the natural logarithm ln and so on are just constant multiples of each other in contrast the entropy would be negative if the base of the logarithm were less than 1 for instance in case of a fair coin toss heads provides log22  1 bit of information which is approximately 0693 nats or 0631 trits because of additivity n tosses provide n bits of information which is approximately 0693n nats or 0631n trits\u000anow suppose we have a distribution where event i can happen with probability pi suppose we have sampled it n times and outcome i was accordingly seen ni  n pi times the total amount of information we have received is\u000a\u000athe average amount of information that we receive with every event is therefore\u000a\u000a\u000a aspectsedit \u000a\u000a\u000a relationship to thermodynamic entropyedit \u000a\u000athe inspiration for adopting the word entropy in information theory came from the close resemblance between shannons formula and very similar known formulae from statistical mechanics\u000ain statistical thermodynamics the most general formula for the thermodynamic entropy s of a thermodynamic system is the gibbs entropy\u000a\u000awhere kb is the boltzmann constant and pi is the probability of a microstate the gibbs entropy was defined by j willard gibbs in 1878 after earlier work by boltzmann 1872\u000athe gibbs entropy translates over almost unchanged into the world of quantum physics to give the von neumann entropy introduced by john von neumann in 1927\u000a\u000awhere  is the density matrix of the quantum mechanical system and tr is the trace\u000aat an everyday practical level the links between information entropy and thermodynamic entropy are not evident physicists and chemists are apt to be more interested in changes in entropy as a system spontaneously evolves away from its initial conditions in accordance with the second law of thermodynamics rather than an unchanging probability distribution and as the minuteness of boltzmanns constant kb indicates the changes in s  kb for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing furthermore in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution which is central to the definition of information entropy\u000aat a multidisciplinary level however connections can be made between thermodynamic and informational entropy although it took many years in the development of the theories of statistical mechanics and information theory to make the relationship fully apparent in fact in the view of jaynes 1957 thermodynamic entropy as explained by statistical mechanics should be seen as an application of shannons information theory the thermodynamic entropy is interpreted as being proportional to the amount of further shannon information needed to define the detailed microscopic state of the system that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics with the constant of proportionality being just the boltzmann constant for example adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables thus making any complete state description longer see article maximum entropy thermodynamics maxwells demon can hypothetically reduce the thermodynamic entropy of a system by using information about the states of individual molecules but as landauer from 1961 and coworkers have shown to function the demon himself must increase thermodynamic entropy in the process by at least the amount of shannon information he proposes to first acquire and store and so the total thermodynamic entropy does not decrease which resolves the paradox landauers principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information though modern computers are far less efficient\u000a\u000a\u000a entropy as information contentedit \u000a\u000aentropy is defined in the context of a probabilistic model independent fair coin flips have an entropy of 1 bit per flip a source that always generates a long string of bs has an entropy of 0 since the next character will always be a b\u000athe entropy rate of a data source means the average number of bits per symbol needed to encode it shannons experiments with human predictors show an information rate between 06 and 13 bits per character in english the ppm compression algorithm can achieve a compression ratio of 15 bits per character in english text\u000afrom the preceding example note the following points\u000athe amount of entropy is not always an integer number of bits\u000amany data bits may not convey information for example data structures often store information redundantly or have identical sections regardless of the information in the data structure\u000ashannons definition of entropy when applied to an information source can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits see caveat below in italics the formula can be derived by calculating the mathematical expectation of the amount of information contained in a digit from the information source see also shannonhartley theorem\u000ashannons entropy measures the information contained in a message as opposed to the portion of the message that is determined or predictable examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs triplets etc see markov chain\u000a\u000a\u000a entropy as a measure of diversityedit \u000a\u000aentropy is one of several ways to measure diversity specifically shannon entropy is the logarithm of 1d the true diversity index with parameter equal to 1\u000a\u000a\u000a data compressionedit \u000a\u000aentropy effectively bounds the performance of the strongest lossless compression possible which can be realized in theory by using the typical set or in practice using huffman lempelziv or arithmetic coding the performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data see also kolmogorov complexity in practice compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors\u000a\u000a\u000a worlds technological capacity to store and communicate informationedit \u000aa 2011 study in science estimates the worlds technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007 therefore estimating the entropy of the technologically available sources\u000athe authors estimate humankind technological capacity to store information fully entropically compressed in 1986 and again in 2007 they break the information into three categoriesto store information on a medium to receive information through a oneway broadcast networks or to exchange information through twoway telecommunication networks\u000a\u000a\u000a limitations of entropy as information contentedit \u000athere are a number of entropyrelated concepts that mathematically quantify information content in some way\u000athe selfinformation of an individual message or symbol taken from a given probability distribution\u000athe entropy of a given probability distribution of messages or symbols and\u000athe entropy rate of a stochastic process\u000athe rate of selfinformation can also be defined for a particular sequence of messages or symbols generated by a given stochastic process this will always be equal to the entropy rate in the case of a stationary process other quantities of information are also used to compare or relate different sources of information\u000ait is important not to confuse the above concepts often it is only clear from context which one is meant for example when someone says that the entropy of the english language is about 1 bit per character they are actually modeling the english language as a stochastic process and talking about its entropy rate shannon himself used the term in this way\u000aalthough entropy is often used as a characterization of the information content of a data source this information content is not absolute it depends crucially on the probabilistic model a source that always generates the same symbol has an entropy rate of 0 but the definition of what a symbol is depends on the alphabet consider a source that produces the string ababababab in which a is always followed by b and vice versa if the probabilistic model considers individual letters as independent the entropy rate of the sequence is 1 bit per character but if the sequence is considered as ab ab ab ab ab  with symbols as twocharacter blocks then the entropy rate is 0 bits per character\u000ahowever if we use very large blocks then the estimate of percharacter entropy rate may become artificially low this is because in reality the probability distribution of the sequence is not knowable exactly it is only an estimate for example suppose one considers the text of every book ever published as a sequence with each symbol being the text of a complete book if there are n published books and each book is only published once the estimate of the probability of each book is 1n and the entropy in bits is log21n  log2n as a practical code this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book this is enormously useful for talking about books but it is not so useful for characterizing the information content of an individual book or of language in general it is not possible to reconstruct the book from its identifier without knowing the probability distribution that is the complete text of all the books the key idea is that the complexity of the probabilistic model must be considered kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model it considers the shortest program for a universal computer that outputs the sequence a code that achieves the entropy rate of a sequence for a given model plus the codebook ie the probabilistic model is one such program but it may not be the shortest\u000afor example the fibonacci sequence is 1 1 2 3 5 8 13  treating the sequence as a message and each number as a symbol there are almost as many symbols as there are characters in the message giving an entropy of approximately log2n so the first 128 symbols of the fibonacci sequence has an entropy of approximately 7 bitssymbol however the sequence can be expressed using a formula fn  fn1  fn2 for n  3 4 5  f1 1 f2  1 and this formula has a much lower entropy and applies to any length of the fibonacci sequence\u000a\u000a\u000a limitations of entropy as a measure of unpredictabilityedit \u000ain cryptanalysis entropy is often roughly used as a measure of the unpredictability of a cryptographic key for example a 128bit key that is randomly generated has 128 bits of entropy it takes on average  guesses to break by brute force if the keys first digit is 0 and the others random then the entropy is 127 bits and it takes on average  guesses\u000ahowever entropy fails to capture the number of guesses required if the possible keys are not of equal probability if the key is half the time password and half the time a true random 128bit key then the entropy is approximately 65 bits yet half the time the key may be guessed on the first try if your first guess is password and on average it takes around  guesses not  to break this password\u000asimilarly consider a 1000000digit binary onetime pad if the pad has 1000000 bits of entropy it is perfect if the pad has 999999 bits of entropy evenly distributed each individual bit of the pad having 0999999 bits of entropy it may still be considered very good but if the pad has 999999 bits of entropy where the first digit is fixed and the remaining 999999 digits are perfectly random then the first digit of the ciphertext will not be encrypted at all\u000a\u000a\u000a data as a markov processedit \u000aa common way to define entropy for text is based on the markov model of text for an order0 source each character is selected independent of the last characters the binary entropy is\u000a\u000awhere pi is the probability of i for a firstorder markov source one in which the probability of selecting a character is dependent only on the immediately preceding character the entropy rate is\u000a\u000awhere i is a state certain preceding characters and  is the probability of j given i as the previous character\u000afor a second order markov source the entropy rate is\u000a\u000a\u000a bary entropyedit \u000ain general the bary entropy of a source   s p with source alphabet s  a1  an and discrete probability distribution p  p1  pn where pi is the probability of ai say pi  pai is defined by\u000a\u000anote the b in bary entropy is the number of different symbols of the ideal alphabet used as a standard yardstick to measure source alphabets in information theory two symbols are necessary and sufficient for an alphabet to encode information therefore the default is to let b  2 binary entropy thus the entropy of the source alphabet with its given empiric probability distribution is a number equal to the number possibly fractional of symbols of the ideal alphabet with an optimal probability distribution necessary to encode for each symbol of the source alphabet also note that optimal probability distribution here means a uniform distribution a source alphabet with n symbols has the highest possible entropy for an alphabet with n symbols when the probability distribution of the alphabet is uniform this optimal entropy turns out to be logbn\u000a\u000a\u000a efficiencyedit \u000aa source alphabet with nonuniform distribution will have less entropy than if those symbols had uniform distribution ie the optimized alphabet this deficiency in entropy can be expressed as a ratio called efficiency\u000a\u000aefficiency has utility in quantifying the effective use of a communications channel this formulation is also referred to as the normalized entropy as the entropy is divided by the maximum entropy \u000a\u000a\u000a characterizationedit \u000ashannon entropy is characterized by a small number of criteria listed below any definition of entropy satisfying these assumptions has the form\u000a\u000awhere k is a constant corresponding to a choice of measurement units\u000ain the following pi  prx  xi and np1  pn  x\u000a\u000a\u000a continuityedit \u000athe measure should be continuous so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount\u000a\u000a\u000a symmetryedit \u000athe measure should be unchanged if the outcomes xi are reordered\u000a etc\u000a\u000a\u000a maximumedit \u000athe measure should be maximal if all the outcomes are equally likely uncertainty is highest when all possible events are equiprobable\u000a\u000afor equiprobable events the entropy should increase with the number of outcomes\u000a\u000a\u000a additivityedit \u000athe amount of entropy should be independent of how the process is regarded as being divided into parts\u000athis last functional relationship characterizes the entropy of a system with subsystems it demands that the entropy of a system can be calculated from the entropies of its subsystems if the interactions between the subsystems are known\u000agiven an ensemble of n uniformly distributed elements that are divided into k boxes subsystems with b1  bk elements each the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes each weighted with the probability of being in that particular box\u000afor positive integers bi where b1    bk  n\u000a\u000achoosing k  n b1    bn  1 this implies that the entropy of a certain outcome is zero 11  0 this implies that the efficiency of a source alphabet with n symbols can be defined simply as being equal to its nary entropy see also redundancy information theory\u000a\u000a\u000a further propertiesedit \u000athe shannon entropy satisfies the following properties for some of which it is useful to interpret entropy as the amount of information learned or uncertainty eliminated by revealing the value of a random variable x\u000aadding or removing an event with probability zero does not contribute to the entropy\u000a\u000a\u000a\u000ait can be confirmed using the jensen inequality that\u000a\u000a\u000a\u000athis maximal entropy of logbn is effectively attained by a source alphabet having a uniform probability distribution uncertainty is maximal when all possible events are equiprobable\u000athe entropy or the amount of information revealed by evaluating xy that is evaluating x and y simultaneously is equal to the information revealed by conducting two consecutive experiments first evaluating the value of y then revealing the value of x given that you know the value of y this may be written as\u000a\u000aif y  fx where f is deterministic then fx applying the previous formula to x fx yields\u000a\u000aso fx  x thus the entropy of a variable can only decrease when the latter is passed through a deterministic function\u000aif x and y are two independent experiments then knowing the value of y doesnt influence our knowledge of the value of x since the two dont influence each other by independence\u000a\u000athe entropy of two simultaneous events is no more than the sum of the entropies of each individual event and are equal if the two events are independent more specifically if x and y are two random variables on the same probability space and x y denotes their cartesian product then\u000a\u000aproving this mathematically follows easily from the previous two properties of entropy\u000a\u000a\u000a extending discrete entropy to the continuous caseedit \u000a\u000a\u000a differential entropyedit \u000a\u000athe shannon entropy is restricted to random variables taking discrete values the corresponding formula for a continuous random variable with probability density function fx with finite or infinite support  on the real line is defined by analogy using the above form of the entropy as an expectation\u000a\u000athis formula is usually referred to as the continuous entropy or differential entropy a precursor of the continuous entropy hf is the expression for the functional  in the theorem of boltzmann\u000aalthough the analogy between both functions is suggestive the following question must be set is the differential entropy a valid extension of the shannon discrete entropy differential entropy lacks a number of properties that the shannon discrete entropy has  it can even be negative  and thus corrections have been suggested notably limiting density of discrete points\u000ato answer this question we must establish a connection between the two functions\u000awe wish to obtain a generally finite measure as the bin size goes to zero in the discrete case the bin size is the implicit width of each of the n finite or infinite bins whose probabilities are denoted by pn as we generalize to the continuous domain we must make this width explicit\u000ato do this start with a continuous function f discretized into bins of size  by the meanvalue theorem there exists a value xi in each bin such that\u000a\u000aand thus the integral of the function f can be approximated in the riemannian sense by\u000a\u000awhere this limit and bin size goes to zero are equivalent\u000awe will denote\u000a\u000aand expanding the logarithm we have\u000a\u000aas   0 we have\u000a\u000abut note that log   as   0 therefore we need a special definition of the differential or continuous entropy\u000a\u000awhich is as said before referred to as the differential entropy this means that the differential entropy is not a limit of the shannon entropy for n   rather it differs from the limit of the shannon entropy by an infinite offset\u000ait turns out as a result that unlike the shannon entropy the differential entropy is not in general a good measure of uncertainty or information for example the differential entropy can be negative also it is not invariant under continuous coordinate transformations\u000a\u000a\u000a relative entropyedit \u000a\u000aanother useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution it is defined as the kullbackleibler divergence from the distribution to a reference measure m as follows assume that a probability distribution p is absolutely continuous with respect to a measure m ie is of the form pdx  fxmdx for some nonnegative mintegrable function f with mintegral 1 then the relative entropy can be defined as\u000a\u000ain this form the relative entropy generalises up to change in sign both the discrete entropy where the measure m is the counting measure and the differential entropy where the measure m is the lebesgue measure if the measure m is itself a probability distribution the relative entropy is nonnegative and zero if p  m as measures it is defined for any measure space hence coordinate independent and invariant under coordinate reparameterizations if one properly takes into account the transformation of the measure m the relative entropy and implicitly entropy and differential entropy do depend on the reference measure m\u000a\u000a\u000a use in combinatoricsedit \u000aentropy has become a useful quantity in combinatorics\u000a\u000a\u000a loomiswhitney inequalityedit \u000aa simple example of this is an alternate proof of the loomiswhitney inequality for every subset a  zd we have\u000a\u000awhere pi is the orthogonal projection in the ith coordinate\u000a\u000athe proof follows as a simple corollary of shearers inequality if x1  xd are random variables and s1  sn are subsets of 1  d such that every integer between 1 and d lies in exactly r of these subsets then\u000a\u000awhere  is the cartesian product of random variables xj with indexes j in si so the dimension of this vector is equal to the size of si\u000awe sketch how loomiswhitney follows from this indeed let x be a uniformly distributed random variable with values in a and so that each point in a occurs with equal probability then by the further properties of entropy mentioned above x  loga where a denotes the cardinality of a let si  1 2  i1 i1  d the range of  is contained in pia and hence  now use this to bound the right side of shearers inequality and exponentiate the opposite sides of the resulting inequality you obtain\u000a\u000a\u000a approximation to binomial coefficientedit \u000afor integers 0  k  n let q  kn then\u000a\u000awhere\u000a\u000ahere is a sketch proof note that  is one term of the expression\u000a\u000arearranging gives the upper bound for the lower bound one first shows using some algebra that it is the largest term in the summation but then\u000a\u000asince there are n  1 terms in the summation rearranging gives the lower bound\u000aa nice interpretation of this is that the number of binary strings of length n with exactly k many 1s is approximately \u000a\u000a\u000a see alsoedit \u000a\u000aconditional entropy\u000across entropy  is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions\u000adiversity index  alternative approaches to quantifying diversity in a probability distribution\u000aentropy arrow of time\u000aentropy encoding  a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols\u000aentropy estimation\u000aentropy power inequality\u000aentropy rate\u000afisher information\u000ahamming distance\u000ahistory of entropy\u000ahistory of information theory\u000ainformation geometry\u000ajoint entropy  is the measure how much entropy is contained in a joint system of two random variables\u000akolmogorovsinai entropy in dynamical systems\u000alevenshtein distance\u000amutual information\u000anegentropy\u000aperplexity\u000aqualitative variation  other measures of statistical dispersion for nominal distributions\u000aquantum relative entropy  a measure of distinguishability between two quantum states\u000arnyi entropy  a generalisation of shannon entropy it is one of a family of functionals for quantifying the diversity uncertainty or randomness of a system\u000arandomness\u000ashannon index\u000atheil index\u000atypoglycemia\u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from shannons entropy on planetmath which is licensed under the creative commons attributionsharealike license\u000a\u000a\u000a further readingedit \u000a\u000a\u000a textbooks on information theoryedit \u000aarndt c 2004 information measures information and its description in science and engineering springer isbn 9783540408550\u000acover t m thomas j a 2006 elements of information theory 2nd edition wileyinterscience isbn 0471241954\u000agray r m 2011 entropy and information theory springer\u000amartin nathaniel fg  england james w 2011 mathematical theory of entropy cambridge university press isbn 9780521177382 \u000ashannon ce weaver w 1949 the mathematical theory of communication univ of illinois press isbn 0252725484\u000astone j v 2014 chapter 1 of information theory a tutorial introduction university of sheffield england isbn 9780956372857\u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 entropy encyclopedia of mathematics springer isbn 9781556080104 \u000aintroduction to entropy and information on principia cybernetica web\u000aentropy an interdisciplinary journal on all aspect of the entropy concept open access\u000adescription of information entropy from tools for thought by howard rheingold\u000aa java applet representing shannons experiment to calculate the entropy of english\u000aslides on information gain and entropy\u000aan intuitive guide to the concept of entropy arising in various sectors of science  a wikibook on the interpretation of the concept of entropy\u000acalculator for shannon entropy estimation and interpretation\u000aa light discussion and derivation of entropy\u000anetwork event detection with entropy measures dr raimund eimann university of auckland pdf 5993 kb  a phd thesis demonstrating how entropy measures may be used in network anomaly detection\u000arosetta code repository of implementations of shannon entropy in different programming languages
p119
sg14
g17
sg18
Vin information theory systems are modeled by a transmitter channel and receiver the transmitter produces messages that are sent through the channel the channel modifies the message in some way the receiver attempts to infer which message was sent in this context entropy more specifically shannon entropy is the expected value average of the information contained in each message messages can be modeled by any flow of information\u000ain a more technical sense there are reasons explained below to define information as the negative of the logarithm of the probability distribution the probability distribution of the events coupled with the information amount of every event forms a random variable whose expected value is the average amount of information or entropy generated by this distribution units of entropy are the shannon nat or hartley depending on the base of the logarithm used to define it though the shannon is commonly referred to as a bit\u000athe logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources for instance the entropy of a coin toss is 1 shannon whereas of m tosses it is m shannons generally you need log2n bits to represent a variable that can take one of n values if n is a power of 2 if these values are equally probable the entropy in shannons is equal to the number of bits equality between number of bits and shannons holds only while all outcomes are equally probable if one of the events is more probable than others observation of that event is less informative conversely rarer events provide more information when observed since observation of less probable events occurs more rarely the net effect is that the entropy thought of as average information received from nonuniformly distributed data is less than log2n entropy is zero when one outcome is certain shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known the meaning of the events observed the meaning of messages does not matter in the definition of entropy entropy only takes into account the probability of observing a specific event so the information it encapsulates is information about the underlying probability distribution not the meaning of the events themselves\u000agenerally entropy refers to disorder or uncertainty shannon entropy was introduced by claude e shannon in his 1948 paper a mathematical theory of communication shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source rnyi entropy generalizes shannon entropy\u000a\u000a\u000a introductionedit \u000aentropy is a measure of unpredictability of information content to get an informal intuitive understanding of the connection between these three english terms consider the example of a poll on some political issue usually such polls happen because the outcome of the poll isnt already known in other words the outcome of the poll is relatively unpredictable and actually performing the poll and learning the results gives some new information these are just different ways of saying that the entropy of the poll results is large now consider the case that the same poll is performed a second time shortly after the first poll since the result of the first poll is already known the outcome of the second poll can be predicted well and the results should not contain much new information in this case the entropy of the second poll result is small relative to the first\u000anow consider the example of a coin toss when the coin is fair that is when the probability of heads is the same as the probability of tails then the entropy of the coin toss is as high as it could be this is because there is no way to predict the outcome of the coin toss ahead of timethe best we can do is predict that the coin will come up heads and our prediction will be correct with probability 12 such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability and learning the actual outcome contains one bit of information contrarily a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads and the outcome can be predicted perfectly\u000aenglish text has fairly low entropy in other words it is fairly predictable even if we dont know exactly what is going to come next we can be fairly certain that for example there will be many more es than zs that the combination qu will be much more common than any other combination with a q in it and that the combination th will be more common than z q or qu after the first few letters one can often guess the rest of the word english text has between 06 and 13 bits of entropy for each character of message\u000athe chinese version of wikipedia points out that chinese characters have a much higher entropy than english each character of chinese has about log212500113 bits almost three times higher than english however the discussion could be much more sophisticated than this simple calculation because in english the usage of words not only characters and redundancy factors could be considered\u000aif a compression scheme is losslessthat is you can always recover the entire original message by decompressingthen a compressed message has the same quantity of information as the original but communicated in fewer characters that is it has more information or a higher entropy per character this means a compressed message has less redundancy roughly speaking shannons source coding theorem says that a lossless compression scheme cannot compress messages on average to have more than one bit of information per bit of message but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme the entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains\u000ashannons theorem also implies that no lossless compression scheme can shorten all messages if some messages come out shorter at least one must come out longer due to the pigeonhole principle in practical use this is generally not a problem because we are usually only interested in compressing certain types of messages for example english documents as opposed to gibberish text or digital photographs rather than noise and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger however the problem can still arise even in everyday use when applying a compression algorithm to already compressed data for example making a zip file of music that is already in the flac audio format is unlikely to achieve much extra saving in space\u000a\u000a\u000a definitionedit \u000anamed after boltzmanns theorem shannon defined the entropy  greek letter eta of a discrete random variable x with possible values x1  xn and probability mass function px as\u000a\u000ahere e is the expected value operator and i is the information content of x ix is itself a random variable\u000athe entropy can explicitly be written as\u000a\u000awhere b is the base of the logarithm used common values of b are 2 eulers number e and 10 and the unit of entropy is shannon for b  2 nat for b  e and hartley for b  10 when b  2 the units of entropy are also commonly referred to as bits\u000ain the case of pxi  0 for some i the value of the corresponding summand 0 logb0 is taken to be 0 which is consistent with the limit\u000a\u000awhen the distribution is continuous rather than discrete the sum is replaced with an integral as\u000a\u000awhere px represents a probability density function\u000aone may also define the conditional entropy of two events x and y taking values xi and yj respectively as\u000a\u000awhere pxi yj is the probability that x  xi and y  yj this quantity should be understood as the amount of randomness in the random variable x given the event y\u000a\u000a\u000a exampleedit \u000a\u000aconsider tossing a coin with known not necessarily fair probabilities of coming up heads or tails this is known as the bernoulli process\u000athe entropy of the unknown result of the next toss of the coin is maximized if the coin is fair that is if heads and tails both have equal probability 12 this is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss the result of each toss of the coin delivers one full bit of information\u000ahowever if we know the coin is not fair but comes up heads or tails with probabilities p and q where p  q then there is less uncertainty every time it is tossed one side is more likely to come up than the other the reduced uncertainty is quantified in a lower entropy on average each toss of the coin delivers less than one full bit of information\u000athe extreme case is that of a doubleheaded coin that never comes up tails or a doubletailed coin that never results in a head then there is no uncertainty the entropy is zero each toss of the coin delivers no new information as the outcome of each coin toss is always certain in this respect entropy can be normalized by dividing it by information length this ratio is called metric entropy and is a measure of the randomness of the information\u000a\u000a\u000a rationaleedit \u000ato understand the meaning of  pi log1pi at first try to define an information function i in terms of an event i with probability pi how much information is acquired due to the observation of event i shannons solution follows from the fundamental properties of information\u000aip  0  information is a nonnegative quantity\u000ai1  0  events that always occur do not communicate information\u000aip1 p2  ip1  ip2  information due to independent events is additive\u000athe last is a crucial property it states that joint probability communicates as much information as two individual events separately particularly if the first event can yield one of n equiprobable outcomes and another has one of m equiprobable outcomes then there are mn possible outcomes of the joint event this means that if log2n bits are needed to encode the first value and log2m to encode the second one needs log2mn  log2m  log2n to encode both shannon discovered that the proper choice of function to quantify information preserving this additivity is logarithmic ie\u000a\u000athe base of the logarithm can be any fixed real number greater than 1 the different units of information bits for log2 trits for log3 nats for the natural logarithm ln and so on are just constant multiples of each other in contrast the entropy would be negative if the base of the logarithm were less than 1 for instance in case of a fair coin toss heads provides log22  1 bit of information which is approximately 0693 nats or 0631 trits because of additivity n tosses provide n bits of information which is approximately 0693n nats or 0631n trits\u000anow suppose we have a distribution where event i can happen with probability pi suppose we have sampled it n times and outcome i was accordingly seen ni  n pi times the total amount of information we have received is\u000a\u000athe average amount of information that we receive with every event is therefore\u000a\u000a\u000a aspectsedit \u000a\u000a\u000a relationship to thermodynamic entropyedit \u000a\u000athe inspiration for adopting the word entropy in information theory came from the close resemblance between shannons formula and very similar known formulae from statistical mechanics\u000ain statistical thermodynamics the most general formula for the thermodynamic entropy s of a thermodynamic system is the gibbs entropy\u000a\u000awhere kb is the boltzmann constant and pi is the probability of a microstate the gibbs entropy was defined by j willard gibbs in 1878 after earlier work by boltzmann 1872\u000athe gibbs entropy translates over almost unchanged into the world of quantum physics to give the von neumann entropy introduced by john von neumann in 1927\u000a\u000awhere  is the density matrix of the quantum mechanical system and tr is the trace\u000aat an everyday practical level the links between information entropy and thermodynamic entropy are not evident physicists and chemists are apt to be more interested in changes in entropy as a system spontaneously evolves away from its initial conditions in accordance with the second law of thermodynamics rather than an unchanging probability distribution and as the minuteness of boltzmanns constant kb indicates the changes in s  kb for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing furthermore in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution which is central to the definition of information entropy\u000aat a multidisciplinary level however connections can be made between thermodynamic and informational entropy although it took many years in the development of the theories of statistical mechanics and information theory to make the relationship fully apparent in fact in the view of jaynes 1957 thermodynamic entropy as explained by statistical mechanics should be seen as an application of shannons information theory the thermodynamic entropy is interpreted as being proportional to the amount of further shannon information needed to define the detailed microscopic state of the system that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics with the constant of proportionality being just the boltzmann constant for example adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables thus making any complete state description longer see article maximum entropy thermodynamics maxwells demon can hypothetically reduce the thermodynamic entropy of a system by using information about the states of individual molecules but as landauer from 1961 and coworkers have shown to function the demon himself must increase thermodynamic entropy in the process by at least the amount of shannon information he proposes to first acquire and store and so the total thermodynamic entropy does not decrease which resolves the paradox landauers principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information though modern computers are far less efficient\u000a\u000a\u000a entropy as information contentedit \u000a\u000aentropy is defined in the context of a probabilistic model independent fair coin flips have an entropy of 1 bit per flip a source that always generates a long string of bs has an entropy of 0 since the next character will always be a b\u000athe entropy rate of a data source means the average number of bits per symbol needed to encode it shannons experiments with human predictors show an information rate between 06 and 13 bits per character in english the ppm compression algorithm can achieve a compression ratio of 15 bits per character in english text\u000afrom the preceding example note the following points\u000athe amount of entropy is not always an integer number of bits\u000amany data bits may not convey information for example data structures often store information redundantly or have identical sections regardless of the information in the data structure\u000ashannons definition of entropy when applied to an information source can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits see caveat below in italics the formula can be derived by calculating the mathematical expectation of the amount of information contained in a digit from the information source see also shannonhartley theorem\u000ashannons entropy measures the information contained in a message as opposed to the portion of the message that is determined or predictable examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs triplets etc see markov chain\u000a\u000a\u000a entropy as a measure of diversityedit \u000a\u000aentropy is one of several ways to measure diversity specifically shannon entropy is the logarithm of 1d the true diversity index with parameter equal to 1\u000a\u000a\u000a data compressionedit \u000a\u000aentropy effectively bounds the performance of the strongest lossless compression possible which can be realized in theory by using the typical set or in practice using huffman lempelziv or arithmetic coding the performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data see also kolmogorov complexity in practice compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors\u000a\u000a\u000a worlds technological capacity to store and communicate informationedit \u000aa 2011 study in science estimates the worlds technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007 therefore estimating the entropy of the technologically available sources\u000athe authors estimate humankind technological capacity to store information fully entropically compressed in 1986 and again in 2007 they break the information into three categoriesto store information on a medium to receive information through a oneway broadcast networks or to exchange information through twoway telecommunication networks\u000a\u000a\u000a limitations of entropy as information contentedit \u000athere are a number of entropyrelated concepts that mathematically quantify information content in some way\u000athe selfinformation of an individual message or symbol taken from a given probability distribution\u000athe entropy of a given probability distribution of messages or symbols and\u000athe entropy rate of a stochastic process\u000athe rate of selfinformation can also be defined for a particular sequence of messages or symbols generated by a given stochastic process this will always be equal to the entropy rate in the case of a stationary process other quantities of information are also used to compare or relate different sources of information\u000ait is important not to confuse the above concepts often it is only clear from context which one is meant for example when someone says that the entropy of the english language is about 1 bit per character they are actually modeling the english language as a stochastic process and talking about its entropy rate shannon himself used the term in this way\u000aalthough entropy is often used as a characterization of the information content of a data source this information content is not absolute it depends crucially on the probabilistic model a source that always generates the same symbol has an entropy rate of 0 but the definition of what a symbol is depends on the alphabet consider a source that produces the string ababababab in which a is always followed by b and vice versa if the probabilistic model considers individual letters as independent the entropy rate of the sequence is 1 bit per character but if the sequence is considered as ab ab ab ab ab  with symbols as twocharacter blocks then the entropy rate is 0 bits per character\u000ahowever if we use very large blocks then the estimate of percharacter entropy rate may become artificially low this is because in reality the probability distribution of the sequence is not knowable exactly it is only an estimate for example suppose one considers the text of every book ever published as a sequence with each symbol being the text of a complete book if there are n published books and each book is only published once the estimate of the probability of each book is 1n and the entropy in bits is log21n  log2n as a practical code this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book this is enormously useful for talking about books but it is not so useful for characterizing the information content of an individual book or of language in general it is not possible to reconstruct the book from its identifier without knowing the probability distribution that is the complete text of all the books the key idea is that the complexity of the probabilistic model must be considered kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model it considers the shortest program for a universal computer that outputs the sequence a code that achieves the entropy rate of a sequence for a given model plus the codebook ie the probabilistic model is one such program but it may not be the shortest\u000afor example the fibonacci sequence is 1 1 2 3 5 8 13  treating the sequence as a message and each number as a symbol there are almost as many symbols as there are characters in the message giving an entropy of approximately log2n so the first 128 symbols of the fibonacci sequence has an entropy of approximately 7 bitssymbol however the sequence can be expressed using a formula fn  fn1  fn2 for n  3 4 5  f1 1 f2  1 and this formula has a much lower entropy and applies to any length of the fibonacci sequence\u000a\u000a\u000a limitations of entropy as a measure of unpredictabilityedit \u000ain cryptanalysis entropy is often roughly used as a measure of the unpredictability of a cryptographic key for example a 128bit key that is randomly generated has 128 bits of entropy it takes on average  guesses to break by brute force if the keys first digit is 0 and the others random then the entropy is 127 bits and it takes on average  guesses\u000ahowever entropy fails to capture the number of guesses required if the possible keys are not of equal probability if the key is half the time password and half the time a true random 128bit key then the entropy is approximately 65 bits yet half the time the key may be guessed on the first try if your first guess is password and on average it takes around  guesses not  to break this password\u000asimilarly consider a 1000000digit binary onetime pad if the pad has 1000000 bits of entropy it is perfect if the pad has 999999 bits of entropy evenly distributed each individual bit of the pad having 0999999 bits of entropy it may still be considered very good but if the pad has 999999 bits of entropy where the first digit is fixed and the remaining 999999 digits are perfectly random then the first digit of the ciphertext will not be encrypted at all\u000a\u000a\u000a data as a markov processedit \u000aa common way to define entropy for text is based on the markov model of text for an order0 source each character is selected independent of the last characters the binary entropy is\u000a\u000awhere pi is the probability of i for a firstorder markov source one in which the probability of selecting a character is dependent only on the immediately preceding character the entropy rate is\u000a\u000awhere i is a state certain preceding characters and  is the probability of j given i as the previous character\u000afor a second order markov source the entropy rate is\u000a\u000a\u000a bary entropyedit \u000ain general the bary entropy of a source   s p with source alphabet s  a1  an and discrete probability distribution p  p1  pn where pi is the probability of ai say pi  pai is defined by\u000a\u000anote the b in bary entropy is the number of different symbols of the ideal alphabet used as a standard yardstick to measure source alphabets in information theory two symbols are necessary and sufficient for an alphabet to encode information therefore the default is to let b  2 binary entropy thus the entropy of the source alphabet with its given empiric probability distribution is a number equal to the number possibly fractional of symbols of the ideal alphabet with an optimal probability distribution necessary to encode for each symbol of the source alphabet also note that optimal probability distribution here means a uniform distribution a source alphabet with n symbols has the highest possible entropy for an alphabet with n symbols when the probability distribution of the alphabet is uniform this optimal entropy turns out to be logbn\u000a\u000a\u000a efficiencyedit \u000aa source alphabet with nonuniform distribution will have less entropy than if those symbols had uniform distribution ie the optimized alphabet this deficiency in entropy can be expressed as a ratio called efficiency\u000a\u000aefficiency has utility in quantifying the effective use of a communications channel this formulation is also referred to as the normalized entropy as the entropy is divided by the maximum entropy \u000a\u000a\u000a characterizationedit \u000ashannon entropy is characterized by a small number of criteria listed below any definition of entropy satisfying these assumptions has the form\u000a\u000awhere k is a constant corresponding to a choice of measurement units\u000ain the following pi  prx  xi and np1  pn  x\u000a\u000a\u000a continuityedit \u000athe measure should be continuous so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount\u000a\u000a\u000a symmetryedit \u000athe measure should be unchanged if the outcomes xi are reordered\u000a etc\u000a\u000a\u000a maximumedit \u000athe measure should be maximal if all the outcomes are equally likely uncertainty is highest when all possible events are equiprobable\u000a\u000afor equiprobable events the entropy should increase with the number of outcomes\u000a\u000a\u000a additivityedit \u000athe amount of entropy should be independent of how the process is regarded as being divided into parts\u000athis last functional relationship characterizes the entropy of a system with subsystems it demands that the entropy of a system can be calculated from the entropies of its subsystems if the interactions between the subsystems are known\u000agiven an ensemble of n uniformly distributed elements that are divided into k boxes subsystems with b1  bk elements each the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes each weighted with the probability of being in that particular box\u000afor positive integers bi where b1    bk  n\u000a\u000achoosing k  n b1    bn  1 this implies that the entropy of a certain outcome is zero 11  0 this implies that the efficiency of a source alphabet with n symbols can be defined simply as being equal to its nary entropy see also redundancy information theory\u000a\u000a\u000a further propertiesedit \u000athe shannon entropy satisfies the following properties for some of which it is useful to interpret entropy as the amount of information learned or uncertainty eliminated by revealing the value of a random variable x\u000aadding or removing an event with probability zero does not contribute to the entropy\u000a\u000a\u000a\u000ait can be confirmed using the jensen inequality that\u000a\u000a\u000a\u000athis maximal entropy of logbn is effectively attained by a source alphabet having a uniform probability distribution uncertainty is maximal when all possible events are equiprobable\u000athe entropy or the amount of information revealed by evaluating xy that is evaluating x and y simultaneously is equal to the information revealed by conducting two consecutive experiments first evaluating the value of y then revealing the value of x given that you know the value of y this may be written as\u000a\u000aif y  fx where f is deterministic then fx applying the previous formula to x fx yields\u000a\u000aso fx  x thus the entropy of a variable can only decrease when the latter is passed through a deterministic function\u000aif x and y are two independent experiments then knowing the value of y doesnt influence our knowledge of the value of x since the two dont influence each other by independence\u000a\u000athe entropy of two simultaneous events is no more than the sum of the entropies of each individual event and are equal if the two events are independent more specifically if x and y are two random variables on the same probability space and x y denotes their cartesian product then\u000a\u000aproving this mathematically follows easily from the previous two properties of entropy\u000a\u000a\u000a extending discrete entropy to the continuous caseedit \u000a\u000a\u000a differential entropyedit \u000a\u000athe shannon entropy is restricted to random variables taking discrete values the corresponding formula for a continuous random variable with probability density function fx with finite or infinite support  on the real line is defined by analogy using the above form of the entropy as an expectation\u000a\u000athis formula is usually referred to as the continuous entropy or differential entropy a precursor of the continuous entropy hf is the expression for the functional  in the theorem of boltzmann\u000aalthough the analogy between both functions is suggestive the following question must be set is the differential entropy a valid extension of the shannon discrete entropy differential entropy lacks a number of properties that the shannon discrete entropy has  it can even be negative  and thus corrections have been suggested notably limiting density of discrete points\u000ato answer this question we must establish a connection between the two functions\u000awe wish to obtain a generally finite measure as the bin size goes to zero in the discrete case the bin size is the implicit width of each of the n finite or infinite bins whose probabilities are denoted by pn as we generalize to the continuous domain we must make this width explicit\u000ato do this start with a continuous function f discretized into bins of size  by the meanvalue theorem there exists a value xi in each bin such that\u000a\u000aand thus the integral of the function f can be approximated in the riemannian sense by\u000a\u000awhere this limit and bin size goes to zero are equivalent\u000awe will denote\u000a\u000aand expanding the logarithm we have\u000a\u000aas   0 we have\u000a\u000abut note that log   as   0 therefore we need a special definition of the differential or continuous entropy\u000a\u000awhich is as said before referred to as the differential entropy this means that the differential entropy is not a limit of the shannon entropy for n   rather it differs from the limit of the shannon entropy by an infinite offset\u000ait turns out as a result that unlike the shannon entropy the differential entropy is not in general a good measure of uncertainty or information for example the differential entropy can be negative also it is not invariant under continuous coordinate transformations\u000a\u000a\u000a relative entropyedit \u000a\u000aanother useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution it is defined as the kullbackleibler divergence from the distribution to a reference measure m as follows assume that a probability distribution p is absolutely continuous with respect to a measure m ie is of the form pdx  fxmdx for some nonnegative mintegrable function f with mintegral 1 then the relative entropy can be defined as\u000a\u000ain this form the relative entropy generalises up to change in sign both the discrete entropy where the measure m is the counting measure and the differential entropy where the measure m is the lebesgue measure if the measure m is itself a probability distribution the relative entropy is nonnegative and zero if p  m as measures it is defined for any measure space hence coordinate independent and invariant under coordinate reparameterizations if one properly takes into account the transformation of the measure m the relative entropy and implicitly entropy and differential entropy do depend on the reference measure m\u000a\u000a\u000a use in combinatoricsedit \u000aentropy has become a useful quantity in combinatorics\u000a\u000a\u000a loomiswhitney inequalityedit \u000aa simple example of this is an alternate proof of the loomiswhitney inequality for every subset a  zd we have\u000a\u000awhere pi is the orthogonal projection in the ith coordinate\u000a\u000athe proof follows as a simple corollary of shearers inequality if x1  xd are random variables and s1  sn are subsets of 1  d such that every integer between 1 and d lies in exactly r of these subsets then\u000a\u000awhere  is the cartesian product of random variables xj with indexes j in si so the dimension of this vector is equal to the size of si\u000awe sketch how loomiswhitney follows from this indeed let x be a uniformly distributed random variable with values in a and so that each point in a occurs with equal probability then by the further properties of entropy mentioned above x  loga where a denotes the cardinality of a let si  1 2  i1 i1  d the range of  is contained in pia and hence  now use this to bound the right side of shearers inequality and exponentiate the opposite sides of the resulting inequality you obtain\u000a\u000a\u000a approximation to binomial coefficientedit \u000afor integers 0  k  n let q  kn then\u000a\u000awhere\u000a\u000ahere is a sketch proof note that  is one term of the expression\u000a\u000arearranging gives the upper bound for the lower bound one first shows using some algebra that it is the largest term in the summation but then\u000a\u000asince there are n  1 terms in the summation rearranging gives the lower bound\u000aa nice interpretation of this is that the number of binary strings of length n with exactly k many 1s is approximately \u000a\u000a\u000a see alsoedit \u000a\u000aconditional entropy\u000across entropy  is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions\u000adiversity index  alternative approaches to quantifying diversity in a probability distribution\u000aentropy arrow of time\u000aentropy encoding  a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols\u000aentropy estimation\u000aentropy power inequality\u000aentropy rate\u000afisher information\u000ahamming distance\u000ahistory of entropy\u000ahistory of information theory\u000ainformation geometry\u000ajoint entropy  is the measure how much entropy is contained in a joint system of two random variables\u000akolmogorovsinai entropy in dynamical systems\u000alevenshtein distance\u000amutual information\u000anegentropy\u000aperplexity\u000aqualitative variation  other measures of statistical dispersion for nominal distributions\u000aquantum relative entropy  a measure of distinguishability between two quantum states\u000arnyi entropy  a generalisation of shannon entropy it is one of a family of functionals for quantifying the diversity uncertainty or randomness of a system\u000arandomness\u000ashannon index\u000atheil index\u000atypoglycemia\u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from shannons entropy on planetmath which is licensed under the creative commons attributionsharealike license\u000a\u000a\u000a further readingedit \u000a\u000a\u000a textbooks on information theoryedit \u000aarndt c 2004 information measures information and its description in science and engineering springer isbn 9783540408550\u000acover t m thomas j a 2006 elements of information theory 2nd edition wileyinterscience isbn 0471241954\u000agray r m 2011 entropy and information theory springer\u000amartin nathaniel fg  england james w 2011 mathematical theory of entropy cambridge university press isbn 9780521177382 \u000ashannon ce weaver w 1949 the mathematical theory of communication univ of illinois press isbn 0252725484\u000astone j v 2014 chapter 1 of information theory a tutorial introduction university of sheffield england isbn 9780956372857\u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 entropy encyclopedia of mathematics springer isbn 9781556080104 \u000aintroduction to entropy and information on principia cybernetica web\u000aentropy an interdisciplinary journal on all aspect of the entropy concept open access\u000adescription of information entropy from tools for thought by howard rheingold\u000aa java applet representing shannons experiment to calculate the entropy of english\u000aslides on information gain and entropy\u000aan intuitive guide to the concept of entropy arising in various sectors of science  a wikibook on the interpretation of the concept of entropy\u000acalculator for shannon entropy estimation and interpretation\u000aa light discussion and derivation of entropy\u000anetwork event detection with entropy measures dr raimund eimann university of auckland pdf 5993 kb  a phd thesis demonstrating how entropy measures may be used in network anomaly detection\u000arosetta code repository of implementations of shannon entropy in different programming languages
p120
sg20
g23
sg24
g27
sg30
Vin information theory systems are modeled by a transmitter channel and receiver the transmitter produces messages that are sent through the channel the channel modifies the message in some way the receiver attempts to infer which message was sent in this context entropy more specifically shannon entropy is the expected value average of the information contained in each message messages can be modeled by any flow of information\u000ain a more technical sense there are reasons explained below to define information as the negative of the logarithm of the probability distribution the probability distribution of the events coupled with the information amount of every event forms a random variable whose expected value is the average amount of information or entropy generated by this distribution units of entropy are the shannon nat or hartley depending on the base of the logarithm used to define it though the shannon is commonly referred to as a bit\u000athe logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources for instance the entropy of a coin toss is 1 shannon whereas of m tosses it is m shannons generally you need log2n bits to represent a variable that can take one of n values if n is a power of 2 if these values are equally probable the entropy in shannons is equal to the number of bits equality between number of bits and shannons holds only while all outcomes are equally probable if one of the events is more probable than others observation of that event is less informative conversely rarer events provide more information when observed since observation of less probable events occurs more rarely the net effect is that the entropy thought of as average information received from nonuniformly distributed data is less than log2n entropy is zero when one outcome is certain shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known the meaning of the events observed the meaning of messages does not matter in the definition of entropy entropy only takes into account the probability of observing a specific event so the information it encapsulates is information about the underlying probability distribution not the meaning of the events themselves\u000agenerally entropy refers to disorder or uncertainty shannon entropy was introduced by claude e shannon in his 1948 paper a mathematical theory of communication shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source rnyi entropy generalizes shannon entropy\u000a\u000a\u000a introductionedit \u000aentropy is a measure of unpredictability of information content to get an informal intuitive understanding of the connection between these three english terms consider the example of a poll on some political issue usually such polls happen because the outcome of the poll isnt already known in other words the outcome of the poll is relatively unpredictable and actually performing the poll and learning the results gives some new information these are just different ways of saying that the entropy of the poll results is large now consider the case that the same poll is performed a second time shortly after the first poll since the result of the first poll is already known the outcome of the second poll can be predicted well and the results should not contain much new information in this case the entropy of the second poll result is small relative to the first\u000anow consider the example of a coin toss when the coin is fair that is when the probability of heads is the same as the probability of tails then the entropy of the coin toss is as high as it could be this is because there is no way to predict the outcome of the coin toss ahead of timethe best we can do is predict that the coin will come up heads and our prediction will be correct with probability 12 such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability and learning the actual outcome contains one bit of information contrarily a coin toss with a coin that has two heads and no tails has zero entropy since the coin will always come up heads and the outcome can be predicted perfectly\u000aenglish text has fairly low entropy in other words it is fairly predictable even if we dont know exactly what is going to come next we can be fairly certain that for example there will be many more es than zs that the combination qu will be much more common than any other combination with a q in it and that the combination th will be more common than z q or qu after the first few letters one can often guess the rest of the word english text has between 06 and 13 bits of entropy for each character of message\u000athe chinese version of wikipedia points out that chinese characters have a much higher entropy than english each character of chinese has about log212500113 bits almost three times higher than english however the discussion could be much more sophisticated than this simple calculation because in english the usage of words not only characters and redundancy factors could be considered\u000aif a compression scheme is losslessthat is you can always recover the entire original message by decompressingthen a compressed message has the same quantity of information as the original but communicated in fewer characters that is it has more information or a higher entropy per character this means a compressed message has less redundancy roughly speaking shannons source coding theorem says that a lossless compression scheme cannot compress messages on average to have more than one bit of information per bit of message but that any value less than one bit of information per bit of message can be attained by employing a suitable coding scheme the entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains\u000ashannons theorem also implies that no lossless compression scheme can shorten all messages if some messages come out shorter at least one must come out longer due to the pigeonhole principle in practical use this is generally not a problem because we are usually only interested in compressing certain types of messages for example english documents as opposed to gibberish text or digital photographs rather than noise and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger however the problem can still arise even in everyday use when applying a compression algorithm to already compressed data for example making a zip file of music that is already in the flac audio format is unlikely to achieve much extra saving in space\u000a\u000a\u000a definitionedit \u000anamed after boltzmanns theorem shannon defined the entropy  greek letter eta of a discrete random variable x with possible values x1  xn and probability mass function px as\u000a\u000ahere e is the expected value operator and i is the information content of x ix is itself a random variable\u000athe entropy can explicitly be written as\u000a\u000awhere b is the base of the logarithm used common values of b are 2 eulers number e and 10 and the unit of entropy is shannon for b  2 nat for b  e and hartley for b  10 when b  2 the units of entropy are also commonly referred to as bits\u000ain the case of pxi  0 for some i the value of the corresponding summand 0 logb0 is taken to be 0 which is consistent with the limit\u000a\u000awhen the distribution is continuous rather than discrete the sum is replaced with an integral as\u000a\u000awhere px represents a probability density function\u000aone may also define the conditional entropy of two events x and y taking values xi and yj respectively as\u000a\u000awhere pxi yj is the probability that x  xi and y  yj this quantity should be understood as the amount of randomness in the random variable x given the event y\u000a\u000a\u000a exampleedit \u000a\u000aconsider tossing a coin with known not necessarily fair probabilities of coming up heads or tails this is known as the bernoulli process\u000athe entropy of the unknown result of the next toss of the coin is maximized if the coin is fair that is if heads and tails both have equal probability 12 this is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss the result of each toss of the coin delivers one full bit of information\u000ahowever if we know the coin is not fair but comes up heads or tails with probabilities p and q where p  q then there is less uncertainty every time it is tossed one side is more likely to come up than the other the reduced uncertainty is quantified in a lower entropy on average each toss of the coin delivers less than one full bit of information\u000athe extreme case is that of a doubleheaded coin that never comes up tails or a doubletailed coin that never results in a head then there is no uncertainty the entropy is zero each toss of the coin delivers no new information as the outcome of each coin toss is always certain in this respect entropy can be normalized by dividing it by information length this ratio is called metric entropy and is a measure of the randomness of the information\u000a\u000a\u000a rationaleedit \u000ato understand the meaning of  pi log1pi at first try to define an information function i in terms of an event i with probability pi how much information is acquired due to the observation of event i shannons solution follows from the fundamental properties of information\u000aip  0  information is a nonnegative quantity\u000ai1  0  events that always occur do not communicate information\u000aip1 p2  ip1  ip2  information due to independent events is additive\u000athe last is a crucial property it states that joint probability communicates as much information as two individual events separately particularly if the first event can yield one of n equiprobable outcomes and another has one of m equiprobable outcomes then there are mn possible outcomes of the joint event this means that if log2n bits are needed to encode the first value and log2m to encode the second one needs log2mn  log2m  log2n to encode both shannon discovered that the proper choice of function to quantify information preserving this additivity is logarithmic ie\u000a\u000athe base of the logarithm can be any fixed real number greater than 1 the different units of information bits for log2 trits for log3 nats for the natural logarithm ln and so on are just constant multiples of each other in contrast the entropy would be negative if the base of the logarithm were less than 1 for instance in case of a fair coin toss heads provides log22  1 bit of information which is approximately 0693 nats or 0631 trits because of additivity n tosses provide n bits of information which is approximately 0693n nats or 0631n trits\u000anow suppose we have a distribution where event i can happen with probability pi suppose we have sampled it n times and outcome i was accordingly seen ni  n pi times the total amount of information we have received is\u000a\u000athe average amount of information that we receive with every event is therefore\u000a\u000a\u000a aspectsedit \u000a\u000a\u000a relationship to thermodynamic entropyedit \u000a\u000athe inspiration for adopting the word entropy in information theory came from the close resemblance between shannons formula and very similar known formulae from statistical mechanics\u000ain statistical thermodynamics the most general formula for the thermodynamic entropy s of a thermodynamic system is the gibbs entropy\u000a\u000awhere kb is the boltzmann constant and pi is the probability of a microstate the gibbs entropy was defined by j willard gibbs in 1878 after earlier work by boltzmann 1872\u000athe gibbs entropy translates over almost unchanged into the world of quantum physics to give the von neumann entropy introduced by john von neumann in 1927\u000a\u000awhere  is the density matrix of the quantum mechanical system and tr is the trace\u000aat an everyday practical level the links between information entropy and thermodynamic entropy are not evident physicists and chemists are apt to be more interested in changes in entropy as a system spontaneously evolves away from its initial conditions in accordance with the second law of thermodynamics rather than an unchanging probability distribution and as the minuteness of boltzmanns constant kb indicates the changes in s  kb for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing furthermore in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution which is central to the definition of information entropy\u000aat a multidisciplinary level however connections can be made between thermodynamic and informational entropy although it took many years in the development of the theories of statistical mechanics and information theory to make the relationship fully apparent in fact in the view of jaynes 1957 thermodynamic entropy as explained by statistical mechanics should be seen as an application of shannons information theory the thermodynamic entropy is interpreted as being proportional to the amount of further shannon information needed to define the detailed microscopic state of the system that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics with the constant of proportionality being just the boltzmann constant for example adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables thus making any complete state description longer see article maximum entropy thermodynamics maxwells demon can hypothetically reduce the thermodynamic entropy of a system by using information about the states of individual molecules but as landauer from 1961 and coworkers have shown to function the demon himself must increase thermodynamic entropy in the process by at least the amount of shannon information he proposes to first acquire and store and so the total thermodynamic entropy does not decrease which resolves the paradox landauers principle imposes a lower bound on the amount of heat a computer must generate to process a given amount of information though modern computers are far less efficient\u000a\u000a\u000a entropy as information contentedit \u000a\u000aentropy is defined in the context of a probabilistic model independent fair coin flips have an entropy of 1 bit per flip a source that always generates a long string of bs has an entropy of 0 since the next character will always be a b\u000athe entropy rate of a data source means the average number of bits per symbol needed to encode it shannons experiments with human predictors show an information rate between 06 and 13 bits per character in english the ppm compression algorithm can achieve a compression ratio of 15 bits per character in english text\u000afrom the preceding example note the following points\u000athe amount of entropy is not always an integer number of bits\u000amany data bits may not convey information for example data structures often store information redundantly or have identical sections regardless of the information in the data structure\u000ashannons definition of entropy when applied to an information source can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits see caveat below in italics the formula can be derived by calculating the mathematical expectation of the amount of information contained in a digit from the information source see also shannonhartley theorem\u000ashannons entropy measures the information contained in a message as opposed to the portion of the message that is determined or predictable examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs triplets etc see markov chain\u000a\u000a\u000a entropy as a measure of diversityedit \u000a\u000aentropy is one of several ways to measure diversity specifically shannon entropy is the logarithm of 1d the true diversity index with parameter equal to 1\u000a\u000a\u000a data compressionedit \u000a\u000aentropy effectively bounds the performance of the strongest lossless compression possible which can be realized in theory by using the typical set or in practice using huffman lempelziv or arithmetic coding the performance of existing data compression algorithms is often used as a rough estimate of the entropy of a block of data see also kolmogorov complexity in practice compression algorithms deliberately include some judicious redundancy in the form of checksums to protect against errors\u000a\u000a\u000a worlds technological capacity to store and communicate informationedit \u000aa 2011 study in science estimates the worlds technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007 therefore estimating the entropy of the technologically available sources\u000athe authors estimate humankind technological capacity to store information fully entropically compressed in 1986 and again in 2007 they break the information into three categoriesto store information on a medium to receive information through a oneway broadcast networks or to exchange information through twoway telecommunication networks\u000a\u000a\u000a limitations of entropy as information contentedit \u000athere are a number of entropyrelated concepts that mathematically quantify information content in some way\u000athe selfinformation of an individual message or symbol taken from a given probability distribution\u000athe entropy of a given probability distribution of messages or symbols and\u000athe entropy rate of a stochastic process\u000athe rate of selfinformation can also be defined for a particular sequence of messages or symbols generated by a given stochastic process this will always be equal to the entropy rate in the case of a stationary process other quantities of information are also used to compare or relate different sources of information\u000ait is important not to confuse the above concepts often it is only clear from context which one is meant for example when someone says that the entropy of the english language is about 1 bit per character they are actually modeling the english language as a stochastic process and talking about its entropy rate shannon himself used the term in this way\u000aalthough entropy is often used as a characterization of the information content of a data source this information content is not absolute it depends crucially on the probabilistic model a source that always generates the same symbol has an entropy rate of 0 but the definition of what a symbol is depends on the alphabet consider a source that produces the string ababababab in which a is always followed by b and vice versa if the probabilistic model considers individual letters as independent the entropy rate of the sequence is 1 bit per character but if the sequence is considered as ab ab ab ab ab  with symbols as twocharacter blocks then the entropy rate is 0 bits per character\u000ahowever if we use very large blocks then the estimate of percharacter entropy rate may become artificially low this is because in reality the probability distribution of the sequence is not knowable exactly it is only an estimate for example suppose one considers the text of every book ever published as a sequence with each symbol being the text of a complete book if there are n published books and each book is only published once the estimate of the probability of each book is 1n and the entropy in bits is log21n  log2n as a practical code this corresponds to assigning each book a unique identifier and using it in place of the text of the book whenever one wants to refer to the book this is enormously useful for talking about books but it is not so useful for characterizing the information content of an individual book or of language in general it is not possible to reconstruct the book from its identifier without knowing the probability distribution that is the complete text of all the books the key idea is that the complexity of the probabilistic model must be considered kolmogorov complexity is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model it considers the shortest program for a universal computer that outputs the sequence a code that achieves the entropy rate of a sequence for a given model plus the codebook ie the probabilistic model is one such program but it may not be the shortest\u000afor example the fibonacci sequence is 1 1 2 3 5 8 13  treating the sequence as a message and each number as a symbol there are almost as many symbols as there are characters in the message giving an entropy of approximately log2n so the first 128 symbols of the fibonacci sequence has an entropy of approximately 7 bitssymbol however the sequence can be expressed using a formula fn  fn1  fn2 for n  3 4 5  f1 1 f2  1 and this formula has a much lower entropy and applies to any length of the fibonacci sequence\u000a\u000a\u000a limitations of entropy as a measure of unpredictabilityedit \u000ain cryptanalysis entropy is often roughly used as a measure of the unpredictability of a cryptographic key for example a 128bit key that is randomly generated has 128 bits of entropy it takes on average  guesses to break by brute force if the keys first digit is 0 and the others random then the entropy is 127 bits and it takes on average  guesses\u000ahowever entropy fails to capture the number of guesses required if the possible keys are not of equal probability if the key is half the time password and half the time a true random 128bit key then the entropy is approximately 65 bits yet half the time the key may be guessed on the first try if your first guess is password and on average it takes around  guesses not  to break this password\u000asimilarly consider a 1000000digit binary onetime pad if the pad has 1000000 bits of entropy it is perfect if the pad has 999999 bits of entropy evenly distributed each individual bit of the pad having 0999999 bits of entropy it may still be considered very good but if the pad has 999999 bits of entropy where the first digit is fixed and the remaining 999999 digits are perfectly random then the first digit of the ciphertext will not be encrypted at all\u000a\u000a\u000a data as a markov processedit \u000aa common way to define entropy for text is based on the markov model of text for an order0 source each character is selected independent of the last characters the binary entropy is\u000a\u000awhere pi is the probability of i for a firstorder markov source one in which the probability of selecting a character is dependent only on the immediately preceding character the entropy rate is\u000a\u000awhere i is a state certain preceding characters and  is the probability of j given i as the previous character\u000afor a second order markov source the entropy rate is\u000a\u000a\u000a bary entropyedit \u000ain general the bary entropy of a source   s p with source alphabet s  a1  an and discrete probability distribution p  p1  pn where pi is the probability of ai say pi  pai is defined by\u000a\u000anote the b in bary entropy is the number of different symbols of the ideal alphabet used as a standard yardstick to measure source alphabets in information theory two symbols are necessary and sufficient for an alphabet to encode information therefore the default is to let b  2 binary entropy thus the entropy of the source alphabet with its given empiric probability distribution is a number equal to the number possibly fractional of symbols of the ideal alphabet with an optimal probability distribution necessary to encode for each symbol of the source alphabet also note that optimal probability distribution here means a uniform distribution a source alphabet with n symbols has the highest possible entropy for an alphabet with n symbols when the probability distribution of the alphabet is uniform this optimal entropy turns out to be logbn\u000a\u000a\u000a efficiencyedit \u000aa source alphabet with nonuniform distribution will have less entropy than if those symbols had uniform distribution ie the optimized alphabet this deficiency in entropy can be expressed as a ratio called efficiency\u000a\u000aefficiency has utility in quantifying the effective use of a communications channel this formulation is also referred to as the normalized entropy as the entropy is divided by the maximum entropy \u000a\u000a\u000a characterizationedit \u000ashannon entropy is characterized by a small number of criteria listed below any definition of entropy satisfying these assumptions has the form\u000a\u000awhere k is a constant corresponding to a choice of measurement units\u000ain the following pi  prx  xi and np1  pn  x\u000a\u000a\u000a continuityedit \u000athe measure should be continuous so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount\u000a\u000a\u000a symmetryedit \u000athe measure should be unchanged if the outcomes xi are reordered\u000a etc\u000a\u000a\u000a maximumedit \u000athe measure should be maximal if all the outcomes are equally likely uncertainty is highest when all possible events are equiprobable\u000a\u000afor equiprobable events the entropy should increase with the number of outcomes\u000a\u000a\u000a additivityedit \u000athe amount of entropy should be independent of how the process is regarded as being divided into parts\u000athis last functional relationship characterizes the entropy of a system with subsystems it demands that the entropy of a system can be calculated from the entropies of its subsystems if the interactions between the subsystems are known\u000agiven an ensemble of n uniformly distributed elements that are divided into k boxes subsystems with b1  bk elements each the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes each weighted with the probability of being in that particular box\u000afor positive integers bi where b1    bk  n\u000a\u000achoosing k  n b1    bn  1 this implies that the entropy of a certain outcome is zero 11  0 this implies that the efficiency of a source alphabet with n symbols can be defined simply as being equal to its nary entropy see also redundancy information theory\u000a\u000a\u000a further propertiesedit \u000athe shannon entropy satisfies the following properties for some of which it is useful to interpret entropy as the amount of information learned or uncertainty eliminated by revealing the value of a random variable x\u000aadding or removing an event with probability zero does not contribute to the entropy\u000a\u000a\u000a\u000ait can be confirmed using the jensen inequality that\u000a\u000a\u000a\u000athis maximal entropy of logbn is effectively attained by a source alphabet having a uniform probability distribution uncertainty is maximal when all possible events are equiprobable\u000athe entropy or the amount of information revealed by evaluating xy that is evaluating x and y simultaneously is equal to the information revealed by conducting two consecutive experiments first evaluating the value of y then revealing the value of x given that you know the value of y this may be written as\u000a\u000aif y  fx where f is deterministic then fx applying the previous formula to x fx yields\u000a\u000aso fx  x thus the entropy of a variable can only decrease when the latter is passed through a deterministic function\u000aif x and y are two independent experiments then knowing the value of y doesnt influence our knowledge of the value of x since the two dont influence each other by independence\u000a\u000athe entropy of two simultaneous events is no more than the sum of the entropies of each individual event and are equal if the two events are independent more specifically if x and y are two random variables on the same probability space and x y denotes their cartesian product then\u000a\u000aproving this mathematically follows easily from the previous two properties of entropy\u000a\u000a\u000a extending discrete entropy to the continuous caseedit \u000a\u000a\u000a differential entropyedit \u000a\u000athe shannon entropy is restricted to random variables taking discrete values the corresponding formula for a continuous random variable with probability density function fx with finite or infinite support  on the real line is defined by analogy using the above form of the entropy as an expectation\u000a\u000athis formula is usually referred to as the continuous entropy or differential entropy a precursor of the continuous entropy hf is the expression for the functional  in the theorem of boltzmann\u000aalthough the analogy between both functions is suggestive the following question must be set is the differential entropy a valid extension of the shannon discrete entropy differential entropy lacks a number of properties that the shannon discrete entropy has  it can even be negative  and thus corrections have been suggested notably limiting density of discrete points\u000ato answer this question we must establish a connection between the two functions\u000awe wish to obtain a generally finite measure as the bin size goes to zero in the discrete case the bin size is the implicit width of each of the n finite or infinite bins whose probabilities are denoted by pn as we generalize to the continuous domain we must make this width explicit\u000ato do this start with a continuous function f discretized into bins of size  by the meanvalue theorem there exists a value xi in each bin such that\u000a\u000aand thus the integral of the function f can be approximated in the riemannian sense by\u000a\u000awhere this limit and bin size goes to zero are equivalent\u000awe will denote\u000a\u000aand expanding the logarithm we have\u000a\u000aas   0 we have\u000a\u000abut note that log   as   0 therefore we need a special definition of the differential or continuous entropy\u000a\u000awhich is as said before referred to as the differential entropy this means that the differential entropy is not a limit of the shannon entropy for n   rather it differs from the limit of the shannon entropy by an infinite offset\u000ait turns out as a result that unlike the shannon entropy the differential entropy is not in general a good measure of uncertainty or information for example the differential entropy can be negative also it is not invariant under continuous coordinate transformations\u000a\u000a\u000a relative entropyedit \u000a\u000aanother useful measure of entropy that works equally well in the discrete and the continuous case is the relative entropy of a distribution it is defined as the kullbackleibler divergence from the distribution to a reference measure m as follows assume that a probability distribution p is absolutely continuous with respect to a measure m ie is of the form pdx  fxmdx for some nonnegative mintegrable function f with mintegral 1 then the relative entropy can be defined as\u000a\u000ain this form the relative entropy generalises up to change in sign both the discrete entropy where the measure m is the counting measure and the differential entropy where the measure m is the lebesgue measure if the measure m is itself a probability distribution the relative entropy is nonnegative and zero if p  m as measures it is defined for any measure space hence coordinate independent and invariant under coordinate reparameterizations if one properly takes into account the transformation of the measure m the relative entropy and implicitly entropy and differential entropy do depend on the reference measure m\u000a\u000a\u000a use in combinatoricsedit \u000aentropy has become a useful quantity in combinatorics\u000a\u000a\u000a loomiswhitney inequalityedit \u000aa simple example of this is an alternate proof of the loomiswhitney inequality for every subset a  zd we have\u000a\u000awhere pi is the orthogonal projection in the ith coordinate\u000a\u000athe proof follows as a simple corollary of shearers inequality if x1  xd are random variables and s1  sn are subsets of 1  d such that every integer between 1 and d lies in exactly r of these subsets then\u000a\u000awhere  is the cartesian product of random variables xj with indexes j in si so the dimension of this vector is equal to the size of si\u000awe sketch how loomiswhitney follows from this indeed let x be a uniformly distributed random variable with values in a and so that each point in a occurs with equal probability then by the further properties of entropy mentioned above x  loga where a denotes the cardinality of a let si  1 2  i1 i1  d the range of  is contained in pia and hence  now use this to bound the right side of shearers inequality and exponentiate the opposite sides of the resulting inequality you obtain\u000a\u000a\u000a approximation to binomial coefficientedit \u000afor integers 0  k  n let q  kn then\u000a\u000awhere\u000a\u000ahere is a sketch proof note that  is one term of the expression\u000a\u000arearranging gives the upper bound for the lower bound one first shows using some algebra that it is the largest term in the summation but then\u000a\u000asince there are n  1 terms in the summation rearranging gives the lower bound\u000aa nice interpretation of this is that the number of binary strings of length n with exactly k many 1s is approximately \u000a\u000a\u000a see alsoedit \u000a\u000aconditional entropy\u000across entropy  is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions\u000adiversity index  alternative approaches to quantifying diversity in a probability distribution\u000aentropy arrow of time\u000aentropy encoding  a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols\u000aentropy estimation\u000aentropy power inequality\u000aentropy rate\u000afisher information\u000ahamming distance\u000ahistory of entropy\u000ahistory of information theory\u000ainformation geometry\u000ajoint entropy  is the measure how much entropy is contained in a joint system of two random variables\u000akolmogorovsinai entropy in dynamical systems\u000alevenshtein distance\u000amutual information\u000anegentropy\u000aperplexity\u000aqualitative variation  other measures of statistical dispersion for nominal distributions\u000aquantum relative entropy  a measure of distinguishability between two quantum states\u000arnyi entropy  a generalisation of shannon entropy it is one of a family of functionals for quantifying the diversity uncertainty or randomness of a system\u000arandomness\u000ashannon index\u000atheil index\u000atypoglycemia\u000a\u000a\u000a referencesedit \u000a\u000athis article incorporates material from shannons entropy on planetmath which is licensed under the creative commons attributionsharealike license\u000a\u000a\u000a further readingedit \u000a\u000a\u000a textbooks on information theoryedit \u000aarndt c 2004 information measures information and its description in science and engineering springer isbn 9783540408550\u000acover t m thomas j a 2006 elements of information theory 2nd edition wileyinterscience isbn 0471241954\u000agray r m 2011 entropy and information theory springer\u000amartin nathaniel fg  england james w 2011 mathematical theory of entropy cambridge university press isbn 9780521177382 \u000ashannon ce weaver w 1949 the mathematical theory of communication univ of illinois press isbn 0252725484\u000astone j v 2014 chapter 1 of information theory a tutorial introduction university of sheffield england isbn 9780956372857\u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 entropy encyclopedia of mathematics springer isbn 9781556080104 \u000aintroduction to entropy and information on principia cybernetica web\u000aentropy an interdisciplinary journal on all aspect of the entropy concept open access\u000adescription of information entropy from tools for thought by howard rheingold\u000aa java applet representing shannons experiment to calculate the entropy of english\u000aslides on information gain and entropy\u000aan intuitive guide to the concept of entropy arising in various sectors of science  a wikibook on the interpretation of the concept of entropy\u000acalculator for shannon entropy estimation and interpretation\u000aa light discussion and derivation of entropy\u000anetwork event detection with entropy measures dr raimund eimann university of auckland pdf 5993 kb  a phd thesis demonstrating how entropy measures may be used in network anomaly detection\u000arosetta code repository of implementations of shannon entropy in different programming languages
p121
sg32
g35
sg37
NsbsS"youden's_j_statistic.txt"
p122
g2
(g3
g4
Ntp123
Rp124
(dp125
g8
g11
sg12
Vyoudens j statistic also called youdens index is a single statistic that captures the performance of a diagnostic test\u000a\u000a\u000a definition \u000aj  sensitivity  specificity  1\u000awith the two righthand quantities being sensitivity and specificity\u000athe index was suggested by wj youden in 1950  as a way of summarising the performance of a diagnostic test its value ranges from 0 to 1 and has a zero value when a diagnostic test gives the same proportion of positive results for groups with and without the disease ie the test is useless a value of 1 indicates that there are no false positives or false negatives ie the test is perfect the index gives equal weight to false positive and false negative values so all tests with the same value of the index give the same proportion of total misclassified results\u000a\u000ayoudens index is often used in conjunction with receiver operating characteristic roc analysis the index is defined for all points of an roc curve and the maximum value of the index may be used as a criterion for selecting the optimum cutoff point when a diagnostic test gives a numeric rather than a dichotomous result the index is represented graphically as the height above the chance line and it is also equivalent to the area under the curve subtended by a single operating point\u000ait is also known as deltap  and generalizes from the dichotomous to the multiclass case as informedness\u000aan unrelated but more commonly used combination of basic statistics is the fscore being the harmonic mean of recall and precision where recall  sensitivity  true positive rate but specificity and precision are separate terms the use of a single index is not generally to be recommended but informedness or youdens index is the probability of an informed decision as opposed to a random guess\u000amatthews correlation coefficient is the geometric mean of the regression coefficient of the problem and its dual where the component regression coefficients of the matthews correlation coefficient are markedness deltap and informedness deltap\u000a\u000a\u000a
p126
sg14
g17
sg18
Vyoudens j statistic also called youdens index is a single statistic that captures the performance of a diagnostic test\u000a\u000a\u000a definition \u000aj  sensitivity  specificity  1\u000awith the two righthand quantities being sensitivity and specificity\u000athe index was suggested by wj youden in 1950  as a way of summarising the performance of a diagnostic test its value ranges from 0 to 1 and has a zero value when a diagnostic test gives the same proportion of positive results for groups with and without the disease ie the test is useless a value of 1 indicates that there are no false positives or false negatives ie the test is perfect the index gives equal weight to false positive and false negative values so all tests with the same value of the index give the same proportion of total misclassified results\u000a\u000ayoudens index is often used in conjunction with receiver operating characteristic roc analysis the index is defined for all points of an roc curve and the maximum value of the index may be used as a criterion for selecting the optimum cutoff point when a diagnostic test gives a numeric rather than a dichotomous result the index is represented graphically as the height above the chance line and it is also equivalent to the area under the curve subtended by a single operating point\u000ait is also known as deltap  and generalizes from the dichotomous to the multiclass case as informedness\u000aan unrelated but more commonly used combination of basic statistics is the fscore being the harmonic mean of recall and precision where recall  sensitivity  true positive rate but specificity and precision are separate terms the use of a single index is not generally to be recommended but informedness or youdens index is the probability of an informed decision as opposed to a random guess\u000amatthews correlation coefficient is the geometric mean of the regression coefficient of the problem and its dual where the component regression coefficients of the matthews correlation coefficient are markedness deltap and informedness deltap
p127
sg20
g23
sg24
g27
sg30
Vyoudens j statistic also called youdens index is a single statistic that captures the performance of a diagnostic test\u000a\u000a\u000a definition \u000aj  sensitivity  specificity  1\u000awith the two righthand quantities being sensitivity and specificity\u000athe index was suggested by wj youden in 1950  as a way of summarising the performance of a diagnostic test its value ranges from 0 to 1 and has a zero value when a diagnostic test gives the same proportion of positive results for groups with and without the disease ie the test is useless a value of 1 indicates that there are no false positives or false negatives ie the test is perfect the index gives equal weight to false positive and false negative values so all tests with the same value of the index give the same proportion of total misclassified results\u000a\u000ayoudens index is often used in conjunction with receiver operating characteristic roc analysis the index is defined for all points of an roc curve and the maximum value of the index may be used as a criterion for selecting the optimum cutoff point when a diagnostic test gives a numeric rather than a dichotomous result the index is represented graphically as the height above the chance line and it is also equivalent to the area under the curve subtended by a single operating point\u000ait is also known as deltap  and generalizes from the dichotomous to the multiclass case as informedness\u000aan unrelated but more commonly used combination of basic statistics is the fscore being the harmonic mean of recall and precision where recall  sensitivity  true positive rate but specificity and precision are separate terms the use of a single index is not generally to be recommended but informedness or youdens index is the probability of an informed decision as opposed to a random guess\u000amatthews correlation coefficient is the geometric mean of the regression coefficient of the problem and its dual where the component regression coefficients of the matthews correlation coefficient are markedness deltap and informedness deltap\u000a\u000a\u000a
p128
sg32
g35
sg37
NsbsS'studentization.txt'
p129
g2
(g3
g4
Ntp130
Rp131
(dp132
g8
g11
sg12
Vin statistics studentization named after william sealy gosset who wrote under the pseudonym student is the adjustment consisting of division of a firstdegree statistic derived from a sample by a samplebased estimate of a population standard deviation the term is also used for the standardisation of a higherdegree statistic by another statistic of the same degree for example an estimate of the third central moment would be standardised by dividing by the cube of the sample standard deviation\u000aa simple example is the process of dividing a sample mean by the sample standard deviation when data arise from a locationscale family the consequence of studentization is that the complication of treating the probability distribution of the mean which depends on both the location and scale parameters has been reduced to considering a distribution which depends only on the location parameter however the fact that a sample standard deviation is used rather than the unknown population standard deviation complicates the mathematics of finding the probability distribution of a studentized statistic\u000ain computational statistics the idea of using studentized statistics is of some importance in the development of confidence intervals with improved properties in the context of resampling and in particular bootstrapping\u000a\u000a\u000a examples \u000astudentized range\u000astudentized residual\u000a\u000a\u000a see also \u000apivotal quantity\u000a\u000a\u000a
p133
sg14
g17
sg18
Vin statistics studentization named after william sealy gosset who wrote under the pseudonym student is the adjustment consisting of division of a firstdegree statistic derived from a sample by a samplebased estimate of a population standard deviation the term is also used for the standardisation of a higherdegree statistic by another statistic of the same degree for example an estimate of the third central moment would be standardised by dividing by the cube of the sample standard deviation\u000aa simple example is the process of dividing a sample mean by the sample standard deviation when data arise from a locationscale family the consequence of studentization is that the complication of treating the probability distribution of the mean which depends on both the location and scale parameters has been reduced to considering a distribution which depends only on the location parameter however the fact that a sample standard deviation is used rather than the unknown population standard deviation complicates the mathematics of finding the probability distribution of a studentized statistic\u000ain computational statistics the idea of using studentized statistics is of some importance in the development of confidence intervals with improved properties in the context of resampling and in particular bootstrapping\u000a\u000a\u000a examples \u000astudentized range\u000astudentized residual\u000a\u000a\u000a see also \u000apivotal quantity
p134
sg20
g23
sg24
g27
sg30
Vin statistics studentization named after william sealy gosset who wrote under the pseudonym student is the adjustment consisting of division of a firstdegree statistic derived from a sample by a samplebased estimate of a population standard deviation the term is also used for the standardisation of a higherdegree statistic by another statistic of the same degree for example an estimate of the third central moment would be standardised by dividing by the cube of the sample standard deviation\u000aa simple example is the process of dividing a sample mean by the sample standard deviation when data arise from a locationscale family the consequence of studentization is that the complication of treating the probability distribution of the mean which depends on both the location and scale parameters has been reduced to considering a distribution which depends only on the location parameter however the fact that a sample standard deviation is used rather than the unknown population standard deviation complicates the mathematics of finding the probability distribution of a studentized statistic\u000ain computational statistics the idea of using studentized statistics is of some importance in the development of confidence intervals with improved properties in the context of resampling and in particular bootstrapping\u000a\u000a\u000a examples \u000astudentized range\u000astudentized residual\u000a\u000a\u000a see also \u000apivotal quantity\u000a\u000a\u000a
p135
sg32
g35
sg37
NsbsS'fisher_consistency.txt'
p136
g2
(g3
g4
Ntp137
Rp138
(dp139
g8
g11
sg12
Vin statistics fisher consistency named after ronald fisher is a desirable property of an estimator asserting that if the estimator were calculated using the entire population rather than a sample the true value of the estimated parameter would be obtained \u000a\u000a\u000a definition \u000asuppose we have a statistical sample x1  xn where each xi follows a cumulative distribution f which depends on an unknown parameter  if an estimator of  based on the sample can be represented as a functional of the empirical distribution function fn\u000a\u000athe estimator is said to be fisher consistent if\u000a\u000aas long as the xi are exchangeable an estimator t defined in terms of the xi can be converted into an estimator t that can be defined in terms of fn by averaging t over all permutations of the data the resulting estimator will have the same expected value as t and its variance will be no larger than that of t\u000aif the strong law of large numbers can be applied the empirical distribution functions fn converge pointwise to f allowing us to express fisher consistency as a limit  the estimator is fisher consistent if\u000a\u000a\u000a finite population example \u000asuppose our sample is obtained from a finite population z1  zm we can represent our sample of size n in terms of the proportion of the sample ni  n taking on each value in the population writing our estimator of  as tn1  n  nm  n the population analogue of the estimator is tp1  pm where pi  px  zi thus we have fisher consistency if tp1  pm  \u000asuppose the parameter of interest is the expected value  and the estimator is the sample mean which can be written\u000a\u000awhere i is the indicator function the population analogue of this expression is\u000a\u000aso we have fisher consistency\u000a\u000a\u000a role in maximum likelihood estimation \u000amaximising the likelihood function l gives an estimate that is fisher consistent for a parameter b if\u000a\u000awhere b0 represents the true value of b\u000a\u000a\u000a relationship to asymptotic consistency and unbiasedness \u000athe term consistency in statistics usually refers to an estimator that is asymptotically consistent fisher consistency and asymptotic consistency are distinct concepts although both aim to define a desirable property of an estimator while many estimators are consistent in both senses neither definition encompasses the other for example suppose we take an estimator tn that is both fisher consistent and asymptotically consistent and then form tn  en where en is a deterministic sequence of nonzero numbers converging to zero this estimator is asymptotically consistent but not fisher consistent for any n alternatively take a sequence of fisher consistent estimators sn then define tn  sn for n  n0 and tn  sn0 for all n n0 this estimator is fisher consistent for all n but not asymptotically consistent a concrete example of this construction would be estimating the population mean as x1 regardless of the sample size\u000athe sample mean is a fisher consistent and unbiased estimate of the population mean but not all fisher consistent estimates are unbiased suppose we observe a sample from a uniform distribution on 0 and we wish to estimate  the sample maximum is fisher consistent but downwardly biased conversely the sample variance is an unbiased estimate of the population variance but is not fisher consistent\u000a\u000a\u000a role in decision theory \u000aa loss function is fisher consistent if the population minimizer of the risk leads to the bayes optimal decision rule\u000a\u000a\u000a
p140
sg14
g17
sg18
Vin statistics fisher consistency named after ronald fisher is a desirable property of an estimator asserting that if the estimator were calculated using the entire population rather than a sample the true value of the estimated parameter would be obtained \u000a\u000a\u000a definition \u000asuppose we have a statistical sample x1  xn where each xi follows a cumulative distribution f which depends on an unknown parameter  if an estimator of  based on the sample can be represented as a functional of the empirical distribution function fn\u000a\u000athe estimator is said to be fisher consistent if\u000a\u000aas long as the xi are exchangeable an estimator t defined in terms of the xi can be converted into an estimator t that can be defined in terms of fn by averaging t over all permutations of the data the resulting estimator will have the same expected value as t and its variance will be no larger than that of t\u000aif the strong law of large numbers can be applied the empirical distribution functions fn converge pointwise to f allowing us to express fisher consistency as a limit  the estimator is fisher consistent if\u000a\u000a\u000a finite population example \u000asuppose our sample is obtained from a finite population z1  zm we can represent our sample of size n in terms of the proportion of the sample ni  n taking on each value in the population writing our estimator of  as tn1  n  nm  n the population analogue of the estimator is tp1  pm where pi  px  zi thus we have fisher consistency if tp1  pm  \u000asuppose the parameter of interest is the expected value  and the estimator is the sample mean which can be written\u000a\u000awhere i is the indicator function the population analogue of this expression is\u000a\u000aso we have fisher consistency\u000a\u000a\u000a role in maximum likelihood estimation \u000amaximising the likelihood function l gives an estimate that is fisher consistent for a parameter b if\u000a\u000awhere b0 represents the true value of b\u000a\u000a\u000a relationship to asymptotic consistency and unbiasedness \u000athe term consistency in statistics usually refers to an estimator that is asymptotically consistent fisher consistency and asymptotic consistency are distinct concepts although both aim to define a desirable property of an estimator while many estimators are consistent in both senses neither definition encompasses the other for example suppose we take an estimator tn that is both fisher consistent and asymptotically consistent and then form tn  en where en is a deterministic sequence of nonzero numbers converging to zero this estimator is asymptotically consistent but not fisher consistent for any n alternatively take a sequence of fisher consistent estimators sn then define tn  sn for n  n0 and tn  sn0 for all n n0 this estimator is fisher consistent for all n but not asymptotically consistent a concrete example of this construction would be estimating the population mean as x1 regardless of the sample size\u000athe sample mean is a fisher consistent and unbiased estimate of the population mean but not all fisher consistent estimates are unbiased suppose we observe a sample from a uniform distribution on 0 and we wish to estimate  the sample maximum is fisher consistent but downwardly biased conversely the sample variance is an unbiased estimate of the population variance but is not fisher consistent\u000a\u000a\u000a role in decision theory \u000aa loss function is fisher consistent if the population minimizer of the risk leads to the bayes optimal decision rule
p141
sg20
g23
sg24
g27
sg30
Vin statistics fisher consistency named after ronald fisher is a desirable property of an estimator asserting that if the estimator were calculated using the entire population rather than a sample the true value of the estimated parameter would be obtained \u000a\u000a\u000a definition \u000asuppose we have a statistical sample x1  xn where each xi follows a cumulative distribution f which depends on an unknown parameter  if an estimator of  based on the sample can be represented as a functional of the empirical distribution function fn\u000a\u000athe estimator is said to be fisher consistent if\u000a\u000aas long as the xi are exchangeable an estimator t defined in terms of the xi can be converted into an estimator t that can be defined in terms of fn by averaging t over all permutations of the data the resulting estimator will have the same expected value as t and its variance will be no larger than that of t\u000aif the strong law of large numbers can be applied the empirical distribution functions fn converge pointwise to f allowing us to express fisher consistency as a limit  the estimator is fisher consistent if\u000a\u000a\u000a finite population example \u000asuppose our sample is obtained from a finite population z1  zm we can represent our sample of size n in terms of the proportion of the sample ni  n taking on each value in the population writing our estimator of  as tn1  n  nm  n the population analogue of the estimator is tp1  pm where pi  px  zi thus we have fisher consistency if tp1  pm  \u000asuppose the parameter of interest is the expected value  and the estimator is the sample mean which can be written\u000a\u000awhere i is the indicator function the population analogue of this expression is\u000a\u000aso we have fisher consistency\u000a\u000a\u000a role in maximum likelihood estimation \u000amaximising the likelihood function l gives an estimate that is fisher consistent for a parameter b if\u000a\u000awhere b0 represents the true value of b\u000a\u000a\u000a relationship to asymptotic consistency and unbiasedness \u000athe term consistency in statistics usually refers to an estimator that is asymptotically consistent fisher consistency and asymptotic consistency are distinct concepts although both aim to define a desirable property of an estimator while many estimators are consistent in both senses neither definition encompasses the other for example suppose we take an estimator tn that is both fisher consistent and asymptotically consistent and then form tn  en where en is a deterministic sequence of nonzero numbers converging to zero this estimator is asymptotically consistent but not fisher consistent for any n alternatively take a sequence of fisher consistent estimators sn then define tn  sn for n  n0 and tn  sn0 for all n n0 this estimator is fisher consistent for all n but not asymptotically consistent a concrete example of this construction would be estimating the population mean as x1 regardless of the sample size\u000athe sample mean is a fisher consistent and unbiased estimate of the population mean but not all fisher consistent estimates are unbiased suppose we observe a sample from a uniform distribution on 0 and we wish to estimate  the sample maximum is fisher consistent but downwardly biased conversely the sample variance is an unbiased estimate of the population variance but is not fisher consistent\u000a\u000a\u000a role in decision theory \u000aa loss function is fisher consistent if the population minimizer of the risk leads to the bayes optimal decision rule\u000a\u000a\u000a
p142
sg32
g35
sg37
NsbsS'pivotal_quantity.txt'
p143
g2
(g3
g4
Ntp144
Rp145
(dp146
g8
g11
sg12
Vin statistics a pivotal quantity or pivot is a function of observations and unobservable parameters whose probability distribution does not depend on the unknown parameters  also referred to as nuisance parameters note that a pivot quantity need not be a statisticthe function and its value can depend on the parameters of the model but its distribution must not if it is a statistic then it is known as an ancillary statistic\u000amore formally let  be a random sample from a distribution that depends on a parameter or vector of parameters  let  be a random variable whose distribution is the same for all  then  is called a pivotal quantity or simply a pivot\u000apivotal quantities are commonly used for normalization to allow data from different data sets to be compared it is relatively easy to construct pivots for location and scale parameters for the former we form differences so that location cancels for the latter ratios so that scale cancels\u000apivotal quantities are fundamental to the construction of test statistics as they allow the statistic to not depend on parameters  for example students tstatistic is for a normal distribution with unknown variance and mean they also provide one method of constructing confidence intervals and the use of pivotal quantities improves performance of the bootstrap in the form of ancillary statistics they can be used to construct frequentist prediction intervals predictive confidence intervals\u000a\u000a\u000a examples \u000a\u000a\u000a normal distribution \u000a\u000aone of the simplest pivotal quantities is the zscore given a normal distribution with mean  and variance  and an observation x the zscore\u000a\u000ahas distribution   a normal distribution with mean 0 and variance 1 similarly since the nsample sample mean has sampling distribution  the zscore of the mean\u000a\u000aalso has distribution  note that while these functions depend on the parameters  and thus one can only compute them if the parameters are known they are not statistics  the distribution is independent of the parameters\u000agiven  independent identically distributed iid observations  from the normal distribution with unknown mean  and variance  a pivotal quantity can be obtained from the function\u000a\u000awhere\u000a\u000aand\u000a\u000aare unbiased estimates of  and  respectively the function  is the students tstatistic for a new value  to be drawn from the same population as the already observed set of values \u000ausing  the function  becomes a pivotal quantity which is also distributed by the students tdistribution with  degrees of freedom as required even though  appears as an argument to the function  the distribution of  does not depend on the parameters  or  of the normal probability distribution that governs the observations \u000athis can be used to compute a prediction interval for the next observation  see prediction interval normal distribution\u000a\u000a\u000a bivariate normal distribution \u000ain more complicated cases it is impossible to construct exact pivots however having approximate pivots improves convergence to asymptotic normality\u000asuppose a sample of size  of vectors  is taken from a bivariate normal distribution with unknown correlation \u000aan estimator of  is the sample pearson moment correlation\u000a\u000awhere  are sample variances of  and  the sample statistic  has an asymptotically normal distribution\u000a\u000ahowever a variancestabilizing transformation\u000a\u000aknown as fishers z transformation of the correlation coefficient allows to make the distribution of  asymptotically independent of unknown parameters\u000a\u000awhere  is the corresponding distribution parameter for finite samples sizes  the random variable  will have distribution closer to normal than that of  an even closer approximation to the standard normal distribution is obtained by using a better approximation for the exact variance the usual form is\u000a\u000a\u000a robustness \u000a\u000afrom the point of view of robust statistics pivotal quantities are robust to changes in the parameters  indeed independent of the parameters  but not in general robust to changes in the model such as violations of the assumption of normality this is fundamental to the robust critique of nonrobust statistics often derived from pivotal quantities such statistics may be robust within the family but are not robust outside it\u000a\u000a\u000a see also \u000anormalization statistics\u000a\u000a\u000a
p147
sg14
g17
sg18
Vin statistics a pivotal quantity or pivot is a function of observations and unobservable parameters whose probability distribution does not depend on the unknown parameters  also referred to as nuisance parameters note that a pivot quantity need not be a statisticthe function and its value can depend on the parameters of the model but its distribution must not if it is a statistic then it is known as an ancillary statistic\u000amore formally let  be a random sample from a distribution that depends on a parameter or vector of parameters  let  be a random variable whose distribution is the same for all  then  is called a pivotal quantity or simply a pivot\u000apivotal quantities are commonly used for normalization to allow data from different data sets to be compared it is relatively easy to construct pivots for location and scale parameters for the former we form differences so that location cancels for the latter ratios so that scale cancels\u000apivotal quantities are fundamental to the construction of test statistics as they allow the statistic to not depend on parameters  for example students tstatistic is for a normal distribution with unknown variance and mean they also provide one method of constructing confidence intervals and the use of pivotal quantities improves performance of the bootstrap in the form of ancillary statistics they can be used to construct frequentist prediction intervals predictive confidence intervals\u000a\u000a\u000a examples \u000a\u000a\u000a normal distribution \u000a\u000aone of the simplest pivotal quantities is the zscore given a normal distribution with mean  and variance  and an observation x the zscore\u000a\u000ahas distribution   a normal distribution with mean 0 and variance 1 similarly since the nsample sample mean has sampling distribution  the zscore of the mean\u000a\u000aalso has distribution  note that while these functions depend on the parameters  and thus one can only compute them if the parameters are known they are not statistics  the distribution is independent of the parameters\u000agiven  independent identically distributed iid observations  from the normal distribution with unknown mean  and variance  a pivotal quantity can be obtained from the function\u000a\u000awhere\u000a\u000aand\u000a\u000aare unbiased estimates of  and  respectively the function  is the students tstatistic for a new value  to be drawn from the same population as the already observed set of values \u000ausing  the function  becomes a pivotal quantity which is also distributed by the students tdistribution with  degrees of freedom as required even though  appears as an argument to the function  the distribution of  does not depend on the parameters  or  of the normal probability distribution that governs the observations \u000athis can be used to compute a prediction interval for the next observation  see prediction interval normal distribution\u000a\u000a\u000a bivariate normal distribution \u000ain more complicated cases it is impossible to construct exact pivots however having approximate pivots improves convergence to asymptotic normality\u000asuppose a sample of size  of vectors  is taken from a bivariate normal distribution with unknown correlation \u000aan estimator of  is the sample pearson moment correlation\u000a\u000awhere  are sample variances of  and  the sample statistic  has an asymptotically normal distribution\u000a\u000ahowever a variancestabilizing transformation\u000a\u000aknown as fishers z transformation of the correlation coefficient allows to make the distribution of  asymptotically independent of unknown parameters\u000a\u000awhere  is the corresponding distribution parameter for finite samples sizes  the random variable  will have distribution closer to normal than that of  an even closer approximation to the standard normal distribution is obtained by using a better approximation for the exact variance the usual form is\u000a\u000a\u000a robustness \u000a\u000afrom the point of view of robust statistics pivotal quantities are robust to changes in the parameters  indeed independent of the parameters  but not in general robust to changes in the model such as violations of the assumption of normality this is fundamental to the robust critique of nonrobust statistics often derived from pivotal quantities such statistics may be robust within the family but are not robust outside it\u000a\u000a\u000a see also \u000anormalization statistics
p148
sg20
g23
sg24
g27
sg30
Vin statistics a pivotal quantity or pivot is a function of observations and unobservable parameters whose probability distribution does not depend on the unknown parameters  also referred to as nuisance parameters note that a pivot quantity need not be a statisticthe function and its value can depend on the parameters of the model but its distribution must not if it is a statistic then it is known as an ancillary statistic\u000amore formally let  be a random sample from a distribution that depends on a parameter or vector of parameters  let  be a random variable whose distribution is the same for all  then  is called a pivotal quantity or simply a pivot\u000apivotal quantities are commonly used for normalization to allow data from different data sets to be compared it is relatively easy to construct pivots for location and scale parameters for the former we form differences so that location cancels for the latter ratios so that scale cancels\u000apivotal quantities are fundamental to the construction of test statistics as they allow the statistic to not depend on parameters  for example students tstatistic is for a normal distribution with unknown variance and mean they also provide one method of constructing confidence intervals and the use of pivotal quantities improves performance of the bootstrap in the form of ancillary statistics they can be used to construct frequentist prediction intervals predictive confidence intervals\u000a\u000a\u000a examples \u000a\u000a\u000a normal distribution \u000a\u000aone of the simplest pivotal quantities is the zscore given a normal distribution with mean  and variance  and an observation x the zscore\u000a\u000ahas distribution   a normal distribution with mean 0 and variance 1 similarly since the nsample sample mean has sampling distribution  the zscore of the mean\u000a\u000aalso has distribution  note that while these functions depend on the parameters  and thus one can only compute them if the parameters are known they are not statistics  the distribution is independent of the parameters\u000agiven  independent identically distributed iid observations  from the normal distribution with unknown mean  and variance  a pivotal quantity can be obtained from the function\u000a\u000awhere\u000a\u000aand\u000a\u000aare unbiased estimates of  and  respectively the function  is the students tstatistic for a new value  to be drawn from the same population as the already observed set of values \u000ausing  the function  becomes a pivotal quantity which is also distributed by the students tdistribution with  degrees of freedom as required even though  appears as an argument to the function  the distribution of  does not depend on the parameters  or  of the normal probability distribution that governs the observations \u000athis can be used to compute a prediction interval for the next observation  see prediction interval normal distribution\u000a\u000a\u000a bivariate normal distribution \u000ain more complicated cases it is impossible to construct exact pivots however having approximate pivots improves convergence to asymptotic normality\u000asuppose a sample of size  of vectors  is taken from a bivariate normal distribution with unknown correlation \u000aan estimator of  is the sample pearson moment correlation\u000a\u000awhere  are sample variances of  and  the sample statistic  has an asymptotically normal distribution\u000a\u000ahowever a variancestabilizing transformation\u000a\u000aknown as fishers z transformation of the correlation coefficient allows to make the distribution of  asymptotically independent of unknown parameters\u000a\u000awhere  is the corresponding distribution parameter for finite samples sizes  the random variable  will have distribution closer to normal than that of  an even closer approximation to the standard normal distribution is obtained by using a better approximation for the exact variance the usual form is\u000a\u000a\u000a robustness \u000a\u000afrom the point of view of robust statistics pivotal quantities are robust to changes in the parameters  indeed independent of the parameters  but not in general robust to changes in the model such as violations of the assumption of normality this is fundamental to the robust critique of nonrobust statistics often derived from pivotal quantities such statistics may be robust within the family but are not robust outside it\u000a\u000a\u000a see also \u000anormalization statistics\u000a\u000a\u000a
p149
sg32
g35
sg37
NsbsS'recursive_partitioning.txt'
p150
g2
(g3
g4
Ntp151
Rp152
(dp153
g8
g11
sg12
Vrecursive partitioning is a statistical method for multivariable analysis recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into subpopulations based on several dichotomous independent variables the process is termed recursive because each subpopulation may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached\u000arecursive partitioning methods have been developed since the 1980s well known methods of recursive partitioning include ross quinlans id3 algorithm and its successors c45 and c50 and classification and regression trees ensemble learning methods such as random forests help to overcome a common criticism of these methods  their vulnerability to overfitting of the data  by employing different algorithms and combining their output in some way\u000athis article focuses on recursive partitioning for medical diagnostic tests but the technique has far wider applications see decision tree\u000aas compared to regression analysis which creates a formula that health care providers can use to calculate the probability that a patient has a disease recursive partition creates a rule such as if a patient has finding x y or z they probably have disease q\u000aa variation is cox linear recursive partitioning\u000a\u000a\u000a advantages and disadvantages \u000acompared to other multivariable methods recursive partitioning has advantages and disadvantages\u000aadvantages are\u000agenerates clinically more intuitive models that do not require the user to perform calculations\u000aallows varying prioritizing of misclassifications in order to create a decision rule that has more sensitivity or specificity\u000amay be more accurate\u000a\u000adisadvantages are\u000adoes not work well for continuous variables\u000amay overfit data\u000a\u000a\u000a examples \u000aexamples are available of using recursive partitioning in research of diagnostic tests goldman used recursive partitioning to prioritize sensitivity in the diagnosis of myocardial infarction among patients with chest pain in the emergency room\u000a\u000a\u000a
p154
sg14
g17
sg18
Vrecursive partitioning is a statistical method for multivariable analysis recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into subpopulations based on several dichotomous independent variables the process is termed recursive because each subpopulation may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached\u000arecursive partitioning methods have been developed since the 1980s well known methods of recursive partitioning include ross quinlans id3 algorithm and its successors c45 and c50 and classification and regression trees ensemble learning methods such as random forests help to overcome a common criticism of these methods  their vulnerability to overfitting of the data  by employing different algorithms and combining their output in some way\u000athis article focuses on recursive partitioning for medical diagnostic tests but the technique has far wider applications see decision tree\u000aas compared to regression analysis which creates a formula that health care providers can use to calculate the probability that a patient has a disease recursive partition creates a rule such as if a patient has finding x y or z they probably have disease q\u000aa variation is cox linear recursive partitioning\u000a\u000a\u000a advantages and disadvantages \u000acompared to other multivariable methods recursive partitioning has advantages and disadvantages\u000aadvantages are\u000agenerates clinically more intuitive models that do not require the user to perform calculations\u000aallows varying prioritizing of misclassifications in order to create a decision rule that has more sensitivity or specificity\u000amay be more accurate\u000a\u000adisadvantages are\u000adoes not work well for continuous variables\u000amay overfit data\u000a\u000a\u000a examples \u000aexamples are available of using recursive partitioning in research of diagnostic tests goldman used recursive partitioning to prioritize sensitivity in the diagnosis of myocardial infarction among patients with chest pain in the emergency room
p155
sg20
g23
sg24
g27
sg30
Vrecursive partitioning is a statistical method for multivariable analysis recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into subpopulations based on several dichotomous independent variables the process is termed recursive because each subpopulation may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached\u000arecursive partitioning methods have been developed since the 1980s well known methods of recursive partitioning include ross quinlans id3 algorithm and its successors c45 and c50 and classification and regression trees ensemble learning methods such as random forests help to overcome a common criticism of these methods  their vulnerability to overfitting of the data  by employing different algorithms and combining their output in some way\u000athis article focuses on recursive partitioning for medical diagnostic tests but the technique has far wider applications see decision tree\u000aas compared to regression analysis which creates a formula that health care providers can use to calculate the probability that a patient has a disease recursive partition creates a rule such as if a patient has finding x y or z they probably have disease q\u000aa variation is cox linear recursive partitioning\u000a\u000a\u000a advantages and disadvantages \u000acompared to other multivariable methods recursive partitioning has advantages and disadvantages\u000aadvantages are\u000agenerates clinically more intuitive models that do not require the user to perform calculations\u000aallows varying prioritizing of misclassifications in order to create a decision rule that has more sensitivity or specificity\u000amay be more accurate\u000a\u000adisadvantages are\u000adoes not work well for continuous variables\u000amay overfit data\u000a\u000a\u000a examples \u000aexamples are available of using recursive partitioning in research of diagnostic tests goldman used recursive partitioning to prioritize sensitivity in the diagnosis of myocardial infarction among patients with chest pain in the emergency room\u000a\u000a\u000a
p156
sg32
g35
sg37
NsbsS'independent_and_identically_distributed_random_variables.txt'
p157
g2
(g3
g4
Ntp158
Rp159
(dp160
g8
g11
sg12
Vin probability theory and statistics a sequence or other collection of random variables is independent and identically distributed iid if each random variable has the same probability distribution as the others and all are mutually independent\u000athe abbreviation iid is particularly common in statistics often as iid sometimes written iid where observations in a sample are often assumed to be effectively iid for the purposes of statistical inference the assumption or requirement that observations be iid tends to simplify the underlying mathematics of many statistical methods see mathematical statistics and statistical theory however in practical applications of statistical modeling the assumption may or may not be realistic to test how realistic the assumption is on a given data set the autocorrelation can be computed lag plots drawn or turning point test performed the generalization of exchangeable random variables is often sufficient and more easily met\u000athe assumption is important in the classical form of the central limit theorem which states that the probability distribution of the sum or average of iid variables with finite variance approaches a normal distribution\u000anote that iid refers to sequences of random variables independent and identically distributed implies an element in the sequence is independent of the random variables that came before it in this way an iid sequence is different from a markov sequence where the probability distribution for the nth random variable is a function of the previous random variable in the sequence for a first order markov sequence an iid sequence does not imply the probabilities for all elements of the sample space or event space must be the same for example repeated throws of loaded dice will produce a sequence that is iid despite the outcomes being biased\u000a\u000a\u000a examplesedit \u000a\u000a\u000a uses in modelingedit \u000athe following are examples or applications of independent and identically distributed iid random variables\u000aa sequence of outcomes of spins of a fair or unfair roulette wheel is iid one implication of this is that if the roulette ball lands on red for example 20 times in a row the next spin is no more or less likely to be black than on any other spin see the gamblers fallacy\u000aa sequence of fair or loaded dice rolls is iid\u000aa sequence of fair or unfair coin flips is iid\u000ain signal processing and image processing the notion of transformation to iid implies two specifications the id id  identically distributed part and the i i  independent part\u000aid the signal level must be balanced on the time axis\u000ai the signal spectrum must be flattened ie transformed by filtering such as deconvolution to a white signal one where all frequencies are equally present\u000a\u000a\u000a uses in inferenceedit \u000aone of the simplest statistical tests the ztest is used to test hypotheses about means of random variables when using the ztest one assumes requires that all observations are iid in order to satisfy the conditions of the central limit theorem\u000a\u000a\u000a generalizationsedit \u000amany results that were first proven under the assumption that the random variables are iid have been shown to be true even under a weaker distributional assumption\u000a\u000a\u000a exchangeable random variablesedit \u000a\u000athe most general notion which shares the main properties of iid variables are exchangeable random variables introduced by bruno de finetti exchangeability means that while variables may not be independent or identically distributed future ones behave like past ones  formally any value of a finite sequence is as likely as any permutation of those values  the joint probability distribution is invariant under the symmetric group\u000athis provides a useful generalization  for example sampling without replacement is not independent but is exchangeable  and is widely used in bayesian statistics\u000a\u000a\u000a lvy processedit \u000a\u000ain stochastic calculus iid variables are thought of as a discrete time lvy process each variable gives how much one changes from one time to another for example a sequence of bernoulli trials is interpreted as the bernoulli process one may generalize this to include continuous time lvy processes and many lvy processes can be seen as limits of iid variablesfor instance the wiener process is the limit of the bernoulli process\u000a\u000a\u000a white noiseedit \u000awhite noise is a simple example of iid\u000a\u000a\u000a see alsoedit \u000ade finettis theorem\u000a\u000a\u000a referencesedit 
p161
sg14
g17
sg18
Vin probability theory and statistics a sequence or other collection of random variables is independent and identically distributed iid if each random variable has the same probability distribution as the others and all are mutually independent\u000athe abbreviation iid is particularly common in statistics often as iid sometimes written iid where observations in a sample are often assumed to be effectively iid for the purposes of statistical inference the assumption or requirement that observations be iid tends to simplify the underlying mathematics of many statistical methods see mathematical statistics and statistical theory however in practical applications of statistical modeling the assumption may or may not be realistic to test how realistic the assumption is on a given data set the autocorrelation can be computed lag plots drawn or turning point test performed the generalization of exchangeable random variables is often sufficient and more easily met\u000athe assumption is important in the classical form of the central limit theorem which states that the probability distribution of the sum or average of iid variables with finite variance approaches a normal distribution\u000anote that iid refers to sequences of random variables independent and identically distributed implies an element in the sequence is independent of the random variables that came before it in this way an iid sequence is different from a markov sequence where the probability distribution for the nth random variable is a function of the previous random variable in the sequence for a first order markov sequence an iid sequence does not imply the probabilities for all elements of the sample space or event space must be the same for example repeated throws of loaded dice will produce a sequence that is iid despite the outcomes being biased\u000a\u000a\u000a examplesedit \u000a\u000a\u000a uses in modelingedit \u000athe following are examples or applications of independent and identically distributed iid random variables\u000aa sequence of outcomes of spins of a fair or unfair roulette wheel is iid one implication of this is that if the roulette ball lands on red for example 20 times in a row the next spin is no more or less likely to be black than on any other spin see the gamblers fallacy\u000aa sequence of fair or loaded dice rolls is iid\u000aa sequence of fair or unfair coin flips is iid\u000ain signal processing and image processing the notion of transformation to iid implies two specifications the id id  identically distributed part and the i i  independent part\u000aid the signal level must be balanced on the time axis\u000ai the signal spectrum must be flattened ie transformed by filtering such as deconvolution to a white signal one where all frequencies are equally present\u000a\u000a\u000a uses in inferenceedit \u000aone of the simplest statistical tests the ztest is used to test hypotheses about means of random variables when using the ztest one assumes requires that all observations are iid in order to satisfy the conditions of the central limit theorem\u000a\u000a\u000a generalizationsedit \u000amany results that were first proven under the assumption that the random variables are iid have been shown to be true even under a weaker distributional assumption\u000a\u000a\u000a exchangeable random variablesedit \u000a\u000athe most general notion which shares the main properties of iid variables are exchangeable random variables introduced by bruno de finetti exchangeability means that while variables may not be independent or identically distributed future ones behave like past ones  formally any value of a finite sequence is as likely as any permutation of those values  the joint probability distribution is invariant under the symmetric group\u000athis provides a useful generalization  for example sampling without replacement is not independent but is exchangeable  and is widely used in bayesian statistics\u000a\u000a\u000a lvy processedit \u000a\u000ain stochastic calculus iid variables are thought of as a discrete time lvy process each variable gives how much one changes from one time to another for example a sequence of bernoulli trials is interpreted as the bernoulli process one may generalize this to include continuous time lvy processes and many lvy processes can be seen as limits of iid variablesfor instance the wiener process is the limit of the bernoulli process\u000a\u000a\u000a white noiseedit \u000awhite noise is a simple example of iid\u000a\u000a\u000a see alsoedit \u000ade finettis theorem\u000a\u000a\u000a referencesedit
p162
sg20
g23
sg24
g27
sg30
Vin probability theory and statistics a sequence or other collection of random variables is independent and identically distributed iid if each random variable has the same probability distribution as the others and all are mutually independent\u000athe abbreviation iid is particularly common in statistics often as iid sometimes written iid where observations in a sample are often assumed to be effectively iid for the purposes of statistical inference the assumption or requirement that observations be iid tends to simplify the underlying mathematics of many statistical methods see mathematical statistics and statistical theory however in practical applications of statistical modeling the assumption may or may not be realistic to test how realistic the assumption is on a given data set the autocorrelation can be computed lag plots drawn or turning point test performed the generalization of exchangeable random variables is often sufficient and more easily met\u000athe assumption is important in the classical form of the central limit theorem which states that the probability distribution of the sum or average of iid variables with finite variance approaches a normal distribution\u000anote that iid refers to sequences of random variables independent and identically distributed implies an element in the sequence is independent of the random variables that came before it in this way an iid sequence is different from a markov sequence where the probability distribution for the nth random variable is a function of the previous random variable in the sequence for a first order markov sequence an iid sequence does not imply the probabilities for all elements of the sample space or event space must be the same for example repeated throws of loaded dice will produce a sequence that is iid despite the outcomes being biased\u000a\u000a\u000a examplesedit \u000a\u000a\u000a uses in modelingedit \u000athe following are examples or applications of independent and identically distributed iid random variables\u000aa sequence of outcomes of spins of a fair or unfair roulette wheel is iid one implication of this is that if the roulette ball lands on red for example 20 times in a row the next spin is no more or less likely to be black than on any other spin see the gamblers fallacy\u000aa sequence of fair or loaded dice rolls is iid\u000aa sequence of fair or unfair coin flips is iid\u000ain signal processing and image processing the notion of transformation to iid implies two specifications the id id  identically distributed part and the i i  independent part\u000aid the signal level must be balanced on the time axis\u000ai the signal spectrum must be flattened ie transformed by filtering such as deconvolution to a white signal one where all frequencies are equally present\u000a\u000a\u000a uses in inferenceedit \u000aone of the simplest statistical tests the ztest is used to test hypotheses about means of random variables when using the ztest one assumes requires that all observations are iid in order to satisfy the conditions of the central limit theorem\u000a\u000a\u000a generalizationsedit \u000amany results that were first proven under the assumption that the random variables are iid have been shown to be true even under a weaker distributional assumption\u000a\u000a\u000a exchangeable random variablesedit \u000a\u000athe most general notion which shares the main properties of iid variables are exchangeable random variables introduced by bruno de finetti exchangeability means that while variables may not be independent or identically distributed future ones behave like past ones  formally any value of a finite sequence is as likely as any permutation of those values  the joint probability distribution is invariant under the symmetric group\u000athis provides a useful generalization  for example sampling without replacement is not independent but is exchangeable  and is widely used in bayesian statistics\u000a\u000a\u000a lvy processedit \u000a\u000ain stochastic calculus iid variables are thought of as a discrete time lvy process each variable gives how much one changes from one time to another for example a sequence of bernoulli trials is interpreted as the bernoulli process one may generalize this to include continuous time lvy processes and many lvy processes can be seen as limits of iid variablesfor instance the wiener process is the limit of the bernoulli process\u000a\u000a\u000a white noiseedit \u000awhite noise is a simple example of iid\u000a\u000a\u000a see alsoedit \u000ade finettis theorem\u000a\u000a\u000a referencesedit 
p163
sg32
g35
sg37
NsbsS'frequency_(statistics).txt'
p164
g2
(g3
g4
Ntp165
Rp166
(dp167
g8
g11
sg12
Vin statistics the frequency or absolute frequency of an event  is the number  of times the event occurred in an experiment or study these frequencies are often graphically represented in histograms\u000a\u000a\u000a types of frequency \u000acumulative frequency refers to the total of the absolute frequencies of all events at or below a certain point in an ordered list of events\u000athe relative frequency or empirical probability of an event refers to the absolute frequency normalized by the total number of events\u000a\u000athe values of  for all events  can be plotted to produce a frequency distribution\u000a\u000a\u000a depictions of frequency \u000athe following are some commonly used methods of depicting frequency\u000a\u000a\u000a histograms \u000aa histogram is a representation of tabulated frequencies shown as adjacent rectangles or squares in some situations erected over discrete intervals bins with an area proportional to the frequency of the observations in the interval the height of a rectangle is also equal to the frequency density of the interval ie the frequency divided by the width of the interval the total area of the histogram is equal to the number of data a histogram may also be normalized displaying relative frequencies it then shows the proportion of cases that fall into each of several categories with the total area equaling 1 the categories are usually specified as consecutive nonoverlapping intervals of a variable the categories intervals must be adjacent and often are chosen to be of the same size the rectangles of a histogram are drawn so that they touch each other to indicate that the original variable is continuous\u000a\u000a\u000a bar graphs \u000a\u000aa bar chart or bar graph is a chart with rectangular bars with lengths proportional to the values that they represent the bars can be plotted vertically or horizontally a vertical bar chart is sometimes called a column bar chart\u000a\u000a\u000a frequency distribution table \u000aa frequency distribution table is an arrangement of the values that one or more variables take in a sample each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval and in this way the table summarizes the distribution of values in the sample an example is shown below\u000a\u000a\u000a frequentist interpretation of probability \u000aunder the frequency interpretation of probability it is assumed that as the length of a series of trials increases without bound the fraction of experiments in which a given event occurs will approach a fixed value known as the limiting relative frequency\u000athis interpretation is often contrasted with bayesian probability in fact the term frequentist was first used by m g kendall in 1949 to contrast with bayesians whom he called nonfrequentists he observed\u000a3we may broadly distinguish two main attitudes one takes probability as a degree of rational belief or some similar ideathe second defines probability in terms of frequencies of occurrence of events or by relative proportions in populations or collectives p 101\u000a\u000a12 it might be thought that the differences between the frequentists and the nonfrequentists if i may call them such are largely due to the differences of the domains which they purport to cover p 104\u000a\u000ai assert that this is not so  the essential distinction between the frequentists and the nonfrequentists is i think that the former in an effort to avoid anything savouring of matters of opinion seek to define probability in terms of the objective properties of a population real or hypothetical whereas the latter do not emphasis in original\u000a\u000a\u000a see also \u000a\u000aaperiodic frequency\u000aprobability density function\u000alaw of large numbers\u000astatistical regularity\u000acumulative frequency analysis\u000a\u000a\u000a
p168
sg14
g17
sg18
Vin statistics the frequency or absolute frequency of an event  is the number  of times the event occurred in an experiment or study these frequencies are often graphically represented in histograms\u000a\u000a\u000a types of frequency \u000acumulative frequency refers to the total of the absolute frequencies of all events at or below a certain point in an ordered list of events\u000athe relative frequency or empirical probability of an event refers to the absolute frequency normalized by the total number of events\u000a\u000athe values of  for all events  can be plotted to produce a frequency distribution\u000a\u000a\u000a depictions of frequency \u000athe following are some commonly used methods of depicting frequency\u000a\u000a\u000a histograms \u000aa histogram is a representation of tabulated frequencies shown as adjacent rectangles or squares in some situations erected over discrete intervals bins with an area proportional to the frequency of the observations in the interval the height of a rectangle is also equal to the frequency density of the interval ie the frequency divided by the width of the interval the total area of the histogram is equal to the number of data a histogram may also be normalized displaying relative frequencies it then shows the proportion of cases that fall into each of several categories with the total area equaling 1 the categories are usually specified as consecutive nonoverlapping intervals of a variable the categories intervals must be adjacent and often are chosen to be of the same size the rectangles of a histogram are drawn so that they touch each other to indicate that the original variable is continuous\u000a\u000a\u000a bar graphs \u000a\u000aa bar chart or bar graph is a chart with rectangular bars with lengths proportional to the values that they represent the bars can be plotted vertically or horizontally a vertical bar chart is sometimes called a column bar chart\u000a\u000a\u000a frequency distribution table \u000aa frequency distribution table is an arrangement of the values that one or more variables take in a sample each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval and in this way the table summarizes the distribution of values in the sample an example is shown below\u000a\u000a\u000a frequentist interpretation of probability \u000aunder the frequency interpretation of probability it is assumed that as the length of a series of trials increases without bound the fraction of experiments in which a given event occurs will approach a fixed value known as the limiting relative frequency\u000athis interpretation is often contrasted with bayesian probability in fact the term frequentist was first used by m g kendall in 1949 to contrast with bayesians whom he called nonfrequentists he observed\u000a3we may broadly distinguish two main attitudes one takes probability as a degree of rational belief or some similar ideathe second defines probability in terms of frequencies of occurrence of events or by relative proportions in populations or collectives p 101\u000a\u000a12 it might be thought that the differences between the frequentists and the nonfrequentists if i may call them such are largely due to the differences of the domains which they purport to cover p 104\u000a\u000ai assert that this is not so  the essential distinction between the frequentists and the nonfrequentists is i think that the former in an effort to avoid anything savouring of matters of opinion seek to define probability in terms of the objective properties of a population real or hypothetical whereas the latter do not emphasis in original\u000a\u000a\u000a see also \u000a\u000aaperiodic frequency\u000aprobability density function\u000alaw of large numbers\u000astatistical regularity\u000acumulative frequency analysis
p169
sg20
g23
sg24
g27
sg30
Vin statistics the frequency or absolute frequency of an event  is the number  of times the event occurred in an experiment or study these frequencies are often graphically represented in histograms\u000a\u000a\u000a types of frequency \u000acumulative frequency refers to the total of the absolute frequencies of all events at or below a certain point in an ordered list of events\u000athe relative frequency or empirical probability of an event refers to the absolute frequency normalized by the total number of events\u000a\u000athe values of  for all events  can be plotted to produce a frequency distribution\u000a\u000a\u000a depictions of frequency \u000athe following are some commonly used methods of depicting frequency\u000a\u000a\u000a histograms \u000aa histogram is a representation of tabulated frequencies shown as adjacent rectangles or squares in some situations erected over discrete intervals bins with an area proportional to the frequency of the observations in the interval the height of a rectangle is also equal to the frequency density of the interval ie the frequency divided by the width of the interval the total area of the histogram is equal to the number of data a histogram may also be normalized displaying relative frequencies it then shows the proportion of cases that fall into each of several categories with the total area equaling 1 the categories are usually specified as consecutive nonoverlapping intervals of a variable the categories intervals must be adjacent and often are chosen to be of the same size the rectangles of a histogram are drawn so that they touch each other to indicate that the original variable is continuous\u000a\u000a\u000a bar graphs \u000a\u000aa bar chart or bar graph is a chart with rectangular bars with lengths proportional to the values that they represent the bars can be plotted vertically or horizontally a vertical bar chart is sometimes called a column bar chart\u000a\u000a\u000a frequency distribution table \u000aa frequency distribution table is an arrangement of the values that one or more variables take in a sample each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval and in this way the table summarizes the distribution of values in the sample an example is shown below\u000a\u000a\u000a frequentist interpretation of probability \u000aunder the frequency interpretation of probability it is assumed that as the length of a series of trials increases without bound the fraction of experiments in which a given event occurs will approach a fixed value known as the limiting relative frequency\u000athis interpretation is often contrasted with bayesian probability in fact the term frequentist was first used by m g kendall in 1949 to contrast with bayesians whom he called nonfrequentists he observed\u000a3we may broadly distinguish two main attitudes one takes probability as a degree of rational belief or some similar ideathe second defines probability in terms of frequencies of occurrence of events or by relative proportions in populations or collectives p 101\u000a\u000a12 it might be thought that the differences between the frequentists and the nonfrequentists if i may call them such are largely due to the differences of the domains which they purport to cover p 104\u000a\u000ai assert that this is not so  the essential distinction between the frequentists and the nonfrequentists is i think that the former in an effort to avoid anything savouring of matters of opinion seek to define probability in terms of the objective properties of a population real or hypothetical whereas the latter do not emphasis in original\u000a\u000a\u000a see also \u000a\u000aaperiodic frequency\u000aprobability density function\u000alaw of large numbers\u000astatistical regularity\u000acumulative frequency analysis\u000a\u000a\u000a
p170
sg32
g35
sg37
NsbsS'maximum_a_posteriori_estimation.txt'
p171
g2
(g3
g4
Ntp172
Rp173
(dp174
g8
g11
sg12
Vin bayesian statistics a maximum a posteriori probability map estimate is a mode of the posterior distribution the map can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data it is closely related to fishers method of maximum likelihood ml but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate map estimation can therefore be seen as a regularization of ml estimation\u000a\u000a\u000a description \u000aassume that we want to estimate an unobserved population parameter  on the basis of observations  let  be the sampling distribution of  so that  is the probability of  when the underlying population parameter is  then the function\u000a\u000ais known as the likelihood function and the estimate\u000a\u000ais the maximum likelihood estimate of \u000anow assume that a prior distribution  over  exists this allows us to treat  as a random variable as in bayesian statistics then the posterior distribution of  is as follows\u000a\u000awhere  is density function of   is the domain of  this is a straightforward application of bayes theorem\u000athe method of maximum a posteriori estimation then estimates  as the mode of the posterior distribution of this random variable\u000a\u000athe denominator of the posterior distribution socalled partition function does not depend on  and therefore plays no role in the optimization observe that the map estimate of  coincides with the ml estimate when the prior  is uniform that is a constant function and when the loss function is of the form\u000a\u000aas  goes to 0 the sequence of bayes estimators approaches the map estimator provided that the distribution of  is unimodal but generally a map estimator is not a bayes estimator unless  is discrete\u000a\u000a\u000a computation \u000amap estimates can be computed in several ways\u000aanalytically when the modes of the posterior distribution can be given in closed form this is the case when conjugate priors are used\u000avia numerical optimization such as the conjugate gradient method or newtons method this usually requires first or second derivatives which have to be evaluated analytically or numerically\u000avia a modification of an expectationmaximization algorithm this does not require derivatives of the posterior density\u000avia a monte carlo method using simulated annealing\u000a\u000a\u000a criticism \u000awhile map estimation is a limit of bayes estimators under the 01 loss function it is not very representative of bayesian methods in general this is because map estimates are point estimates whereas bayesian methods are characterized by the use of distributions to summarize data and draw inferences thus bayesian methods tend to report the posterior mean or median instead together with credible intervals this is both because these estimators are optimal under squarederror and linearerror loss respectively  which are more representative of typical loss functions  and because the posterior distribution may not have a simple analytic form in this case the distribution can be simulated using markov chain monte carlo techniques while optimization to find its modes may be difficult or impossible\u000a\u000ain many types of models such as mixture models the posterior may be multimodal in such a case the usual recommendation is that one should choose the highest mode this is not always feasible global optimization is a difficult problem nor in some cases even possible such as when identifiability issues arise furthermore the highest mode may be uncharacteristic of the majority of the posterior\u000afinally unlike ml estimators the map estimate is not invariant under reparameterization switching from one parameterization to another involves introducing a jacobian that impacts on the location of the maximum\u000aas an example of the difference between bayes estimators mentioned above mean and median estimators and using an map estimate consider the case where there is a need to classify inputs  as either positive or negative for example loans as risky or safe suppose there are just three possible hypotheses about the correct method of classification   and  with posteriors 04 03 and 03 respectively suppose given a new instance   classifies it as positive whereas the other two classify it as negative using the map estimate for the correct classifier   is classified as positive whereas the bayes estimators would average over all hypotheses and classify  as negative\u000a\u000a\u000a example \u000asuppose that we are given a sequence  of iid  random variables and an a priori distribution of  is given by  we wish to find the map estimate of \u000athe function to be maximized is then given by\u000a\u000awhich is equivalent to minimizing the following function of \u000a\u000athus we see that the map estimator for  is given by\u000a\u000awhich turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances\u000athe case of  is called a noninformative prior and leads to an illdefined a priori probability distribution in this case \u000a\u000a\u000a
p175
sg14
g17
sg18
Vin bayesian statistics a maximum a posteriori probability map estimate is a mode of the posterior distribution the map can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data it is closely related to fishers method of maximum likelihood ml but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate map estimation can therefore be seen as a regularization of ml estimation\u000a\u000a\u000a description \u000aassume that we want to estimate an unobserved population parameter  on the basis of observations  let  be the sampling distribution of  so that  is the probability of  when the underlying population parameter is  then the function\u000a\u000ais known as the likelihood function and the estimate\u000a\u000ais the maximum likelihood estimate of \u000anow assume that a prior distribution  over  exists this allows us to treat  as a random variable as in bayesian statistics then the posterior distribution of  is as follows\u000a\u000awhere  is density function of   is the domain of  this is a straightforward application of bayes theorem\u000athe method of maximum a posteriori estimation then estimates  as the mode of the posterior distribution of this random variable\u000a\u000athe denominator of the posterior distribution socalled partition function does not depend on  and therefore plays no role in the optimization observe that the map estimate of  coincides with the ml estimate when the prior  is uniform that is a constant function and when the loss function is of the form\u000a\u000aas  goes to 0 the sequence of bayes estimators approaches the map estimator provided that the distribution of  is unimodal but generally a map estimator is not a bayes estimator unless  is discrete\u000a\u000a\u000a computation \u000amap estimates can be computed in several ways\u000aanalytically when the modes of the posterior distribution can be given in closed form this is the case when conjugate priors are used\u000avia numerical optimization such as the conjugate gradient method or newtons method this usually requires first or second derivatives which have to be evaluated analytically or numerically\u000avia a modification of an expectationmaximization algorithm this does not require derivatives of the posterior density\u000avia a monte carlo method using simulated annealing\u000a\u000a\u000a criticism \u000awhile map estimation is a limit of bayes estimators under the 01 loss function it is not very representative of bayesian methods in general this is because map estimates are point estimates whereas bayesian methods are characterized by the use of distributions to summarize data and draw inferences thus bayesian methods tend to report the posterior mean or median instead together with credible intervals this is both because these estimators are optimal under squarederror and linearerror loss respectively  which are more representative of typical loss functions  and because the posterior distribution may not have a simple analytic form in this case the distribution can be simulated using markov chain monte carlo techniques while optimization to find its modes may be difficult or impossible\u000a\u000ain many types of models such as mixture models the posterior may be multimodal in such a case the usual recommendation is that one should choose the highest mode this is not always feasible global optimization is a difficult problem nor in some cases even possible such as when identifiability issues arise furthermore the highest mode may be uncharacteristic of the majority of the posterior\u000afinally unlike ml estimators the map estimate is not invariant under reparameterization switching from one parameterization to another involves introducing a jacobian that impacts on the location of the maximum\u000aas an example of the difference between bayes estimators mentioned above mean and median estimators and using an map estimate consider the case where there is a need to classify inputs  as either positive or negative for example loans as risky or safe suppose there are just three possible hypotheses about the correct method of classification   and  with posteriors 04 03 and 03 respectively suppose given a new instance   classifies it as positive whereas the other two classify it as negative using the map estimate for the correct classifier   is classified as positive whereas the bayes estimators would average over all hypotheses and classify  as negative\u000a\u000a\u000a example \u000asuppose that we are given a sequence  of iid  random variables and an a priori distribution of  is given by  we wish to find the map estimate of \u000athe function to be maximized is then given by\u000a\u000awhich is equivalent to minimizing the following function of \u000a\u000athus we see that the map estimator for  is given by\u000a\u000awhich turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances\u000athe case of  is called a noninformative prior and leads to an illdefined a priori probability distribution in this case
p176
sg20
g23
sg24
g27
sg30
Vin bayesian statistics a maximum a posteriori probability map estimate is a mode of the posterior distribution the map can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data it is closely related to fishers method of maximum likelihood ml but employs an augmented optimization objective which incorporates a prior distribution over the quantity one wants to estimate map estimation can therefore be seen as a regularization of ml estimation\u000a\u000a\u000a description \u000aassume that we want to estimate an unobserved population parameter  on the basis of observations  let  be the sampling distribution of  so that  is the probability of  when the underlying population parameter is  then the function\u000a\u000ais known as the likelihood function and the estimate\u000a\u000ais the maximum likelihood estimate of \u000anow assume that a prior distribution  over  exists this allows us to treat  as a random variable as in bayesian statistics then the posterior distribution of  is as follows\u000a\u000awhere  is density function of   is the domain of  this is a straightforward application of bayes theorem\u000athe method of maximum a posteriori estimation then estimates  as the mode of the posterior distribution of this random variable\u000a\u000athe denominator of the posterior distribution socalled partition function does not depend on  and therefore plays no role in the optimization observe that the map estimate of  coincides with the ml estimate when the prior  is uniform that is a constant function and when the loss function is of the form\u000a\u000aas  goes to 0 the sequence of bayes estimators approaches the map estimator provided that the distribution of  is unimodal but generally a map estimator is not a bayes estimator unless  is discrete\u000a\u000a\u000a computation \u000amap estimates can be computed in several ways\u000aanalytically when the modes of the posterior distribution can be given in closed form this is the case when conjugate priors are used\u000avia numerical optimization such as the conjugate gradient method or newtons method this usually requires first or second derivatives which have to be evaluated analytically or numerically\u000avia a modification of an expectationmaximization algorithm this does not require derivatives of the posterior density\u000avia a monte carlo method using simulated annealing\u000a\u000a\u000a criticism \u000awhile map estimation is a limit of bayes estimators under the 01 loss function it is not very representative of bayesian methods in general this is because map estimates are point estimates whereas bayesian methods are characterized by the use of distributions to summarize data and draw inferences thus bayesian methods tend to report the posterior mean or median instead together with credible intervals this is both because these estimators are optimal under squarederror and linearerror loss respectively  which are more representative of typical loss functions  and because the posterior distribution may not have a simple analytic form in this case the distribution can be simulated using markov chain monte carlo techniques while optimization to find its modes may be difficult or impossible\u000a\u000ain many types of models such as mixture models the posterior may be multimodal in such a case the usual recommendation is that one should choose the highest mode this is not always feasible global optimization is a difficult problem nor in some cases even possible such as when identifiability issues arise furthermore the highest mode may be uncharacteristic of the majority of the posterior\u000afinally unlike ml estimators the map estimate is not invariant under reparameterization switching from one parameterization to another involves introducing a jacobian that impacts on the location of the maximum\u000aas an example of the difference between bayes estimators mentioned above mean and median estimators and using an map estimate consider the case where there is a need to classify inputs  as either positive or negative for example loans as risky or safe suppose there are just three possible hypotheses about the correct method of classification   and  with posteriors 04 03 and 03 respectively suppose given a new instance   classifies it as positive whereas the other two classify it as negative using the map estimate for the correct classifier   is classified as positive whereas the bayes estimators would average over all hypotheses and classify  as negative\u000a\u000a\u000a example \u000asuppose that we are given a sequence  of iid  random variables and an a priori distribution of  is given by  we wish to find the map estimate of \u000athe function to be maximized is then given by\u000a\u000awhich is equivalent to minimizing the following function of \u000a\u000athus we see that the map estimator for  is given by\u000a\u000awhich turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances\u000athe case of  is called a noninformative prior and leads to an illdefined a priori probability distribution in this case \u000a\u000a\u000a
p177
sg32
g35
sg37
NsbsS'fiducial_inference.txt'
p178
g2
(g3
g4
Ntp179
Rp180
(dp181
g8
g11
sg12
Vfiducial inference is one of a number of different types of statistical inference these are rules intended for general application by which conclusions can be drawn from samples of data in modern statistical practice attempts to work with fiducial inference have fallen out of fashion in favour of frequentist inference bayesian inference and decision theory however fiducial inference is important in the history of statistics since its development led to the parallel development of concepts and tools in theoretical statistics that are widely used some current research in statistical methodology is either explicitly linked to fiducial inference or is closely connected to it\u000a\u000a\u000a background \u000athe general approach of fiducial inference was proposed by ronald fisher here fiducial comes from the latin for faith fiducial inference can be interpreted as an attempt to perform inverse probability without calling on prior probability distributions fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published these counterexamples cast doubt on the coherence of fiducial inference as a system of statistical inference or inductive logic other studies showed that where the steps of fiducial inference are said to lead to fiducial probabilities or fiducial distributions these probabilities lack the property of additivity and so cannot constitute a probability measure\u000athe concept of fiducial inference can be outlined by comparing its treatment of the problem of interval estimation in relation to other modes of statistical inference\u000aa confidence interval in frequentist inference with coverage probability  has the interpretation that among all confidence intervals computed by the same method a proportion  will contain the true value that needs to be estimated this has either a repeated sampling or frequentist interpretation or is the probability that an interval calculated from yettobesampled data will cover the true value however in either case the probability concerned is not the probability that the true value is in the particular interval that has been calculated since at that stage both the true value and the calculated are fixed and are not random\u000acredible intervals in bayesian inference do allow a probability to be given for the event that an interval once it has been calculated does include the true value since it proceeds on the basis that a probability distribution can be associated with the state of knowledge about the true value both before and after the sample of data has been obtained\u000afisher designed the fiducial method to meet perceived problems with the bayesian approach at a time when the frequentist approach had yet to be fully developed such problems related to the need to assign a prior distribution to the unknown values the aim was to have a procedure like the bayesian method whose results could still be given an inverse probability interpretation based on the actual data observed the method proceeds by attempting to derive a fiducial distribution which is a measure of the degree of faith that can be put on any given value of the unknown parameter and is faithful to the data in the sense that the method uses all available information\u000aunfortunately fisher did not give a general definition of the fiducial method and he denied that the method could always be applied his only examples were for a single parameter different generalisations have been given when there are several parameters a relatively complete presentation of the fiducial approach to inference is given by quenouille 1958 while williams 1959 describes the application of fiducial analysis to the calibration problem also known as inverse regression in regression analysis further discussion of fiducial inference is given by kendall  stuart 1973\u000a\u000a\u000a the fiducial distribution \u000afisher required the existence of a sufficient statistic for the fiducial method to apply suppose there is a single sufficient statistic for a single parameter that is suppose that the conditional distribution of the data given the statistic does not depend on the value of the parameter for example suppose that n independent observations are uniformly distributed on the interval  the maximum x of the n observations is a sufficient statistic for  if only x is recorded and the values of the remaining observations are forgotten these remaining observations are equally likely to have had any values in the interval  this statement does not depend on the value of  then x contains all the available information about  and the other observations could have given no further information\u000athe cumulative distribution function of x is\u000a\u000aprobability statements about x may be made for example given  a value of a can be chosen with 0    a    1 such that\u000a\u000athus\u000a\u000athen fisher might say that this statement may be inverted into the form\u000a\u000ain this latter statement  is now regarded as variable and x is fixed whereas previously it was the other way round this distribution of  is the fiducial distribution which may be used to form fiducial intervals that represent degrees of belief\u000athe calculation is identical to the pivotal method for finding a confidence interval but the interpretation is different in fact older books use the terms confidence interval and fiducial interval interchangeably notice that the fiducial distribution is uniquely defined when a single sufficient statistic exists\u000athe pivotal method is based on a random variable that is a function of both the observations and the parameters but whose distribution does not depend on the parameter such random variables are called pivotal quantities by using these probability statements about the observations and parameters may be made in which the probabilities do not depend on the parameters and these may be inverted by solving for the parameters in much the same way as in the example above however this is only equivalent to the fiducial method if the pivotal quantity is uniquely defined based on a sufficient statistic\u000aa fiducial interval could be taken to be just a different name for a confidence interval and give it the fiducial interpretation but the definition might not then be unique fisher would have denied that this interpretation is correct for him the fiducial distribution had to be defined uniquely and it had to use all the information in the sample\u000a\u000a\u000a status of the approach \u000aafter its formulation by fisher fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published\u000afisher admitted that fiducial inference had problems fisher wrote to george a barnard that he was not clear in the head about one problem on fiducial inference and also writing to barnard fisher complained that his theory seemed to have only an asymptotic approach to intelligibility later fisher confessed that i dont understand yet what fiducial probability does we shall have to live with it a long time before we know what its doing for us but it should not be ignored just because we dont yet have a clear interpretation\u000alindley showed that fiducial probability lacked additivity and so was not a probability measure cox points out that the same argument applies to the socalled confidence distribution associated with confidence intervals so the conclusion to be drawn from this is moot fisher sketched proofs of results using fiducial probability when the conclusions of fishers fiducial arguments are not false many have been shown to also follow from bayesian inference\u000ain 1978 jg pederson wrote that the fiducial argument has had very limited success and is now essentially dead davison wrote a few subsequent attempts have been made to resurrect fiducialism but it now seems largely of historical importance particularly in view of its restricted range of applicability when set alongside models of current interest\u000ahowever fiducial inference is still being studied and other current work is ongoing under the name of confidence distributions\u000a\u000a\u000a notes \u000a\u000a\u000a
p182
sg14
g17
sg18
Vfiducial inference is one of a number of different types of statistical inference these are rules intended for general application by which conclusions can be drawn from samples of data in modern statistical practice attempts to work with fiducial inference have fallen out of fashion in favour of frequentist inference bayesian inference and decision theory however fiducial inference is important in the history of statistics since its development led to the parallel development of concepts and tools in theoretical statistics that are widely used some current research in statistical methodology is either explicitly linked to fiducial inference or is closely connected to it\u000a\u000a\u000a background \u000athe general approach of fiducial inference was proposed by ronald fisher here fiducial comes from the latin for faith fiducial inference can be interpreted as an attempt to perform inverse probability without calling on prior probability distributions fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published these counterexamples cast doubt on the coherence of fiducial inference as a system of statistical inference or inductive logic other studies showed that where the steps of fiducial inference are said to lead to fiducial probabilities or fiducial distributions these probabilities lack the property of additivity and so cannot constitute a probability measure\u000athe concept of fiducial inference can be outlined by comparing its treatment of the problem of interval estimation in relation to other modes of statistical inference\u000aa confidence interval in frequentist inference with coverage probability  has the interpretation that among all confidence intervals computed by the same method a proportion  will contain the true value that needs to be estimated this has either a repeated sampling or frequentist interpretation or is the probability that an interval calculated from yettobesampled data will cover the true value however in either case the probability concerned is not the probability that the true value is in the particular interval that has been calculated since at that stage both the true value and the calculated are fixed and are not random\u000acredible intervals in bayesian inference do allow a probability to be given for the event that an interval once it has been calculated does include the true value since it proceeds on the basis that a probability distribution can be associated with the state of knowledge about the true value both before and after the sample of data has been obtained\u000afisher designed the fiducial method to meet perceived problems with the bayesian approach at a time when the frequentist approach had yet to be fully developed such problems related to the need to assign a prior distribution to the unknown values the aim was to have a procedure like the bayesian method whose results could still be given an inverse probability interpretation based on the actual data observed the method proceeds by attempting to derive a fiducial distribution which is a measure of the degree of faith that can be put on any given value of the unknown parameter and is faithful to the data in the sense that the method uses all available information\u000aunfortunately fisher did not give a general definition of the fiducial method and he denied that the method could always be applied his only examples were for a single parameter different generalisations have been given when there are several parameters a relatively complete presentation of the fiducial approach to inference is given by quenouille 1958 while williams 1959 describes the application of fiducial analysis to the calibration problem also known as inverse regression in regression analysis further discussion of fiducial inference is given by kendall  stuart 1973\u000a\u000a\u000a the fiducial distribution \u000afisher required the existence of a sufficient statistic for the fiducial method to apply suppose there is a single sufficient statistic for a single parameter that is suppose that the conditional distribution of the data given the statistic does not depend on the value of the parameter for example suppose that n independent observations are uniformly distributed on the interval  the maximum x of the n observations is a sufficient statistic for  if only x is recorded and the values of the remaining observations are forgotten these remaining observations are equally likely to have had any values in the interval  this statement does not depend on the value of  then x contains all the available information about  and the other observations could have given no further information\u000athe cumulative distribution function of x is\u000a\u000aprobability statements about x may be made for example given  a value of a can be chosen with 0    a    1 such that\u000a\u000athus\u000a\u000athen fisher might say that this statement may be inverted into the form\u000a\u000ain this latter statement  is now regarded as variable and x is fixed whereas previously it was the other way round this distribution of  is the fiducial distribution which may be used to form fiducial intervals that represent degrees of belief\u000athe calculation is identical to the pivotal method for finding a confidence interval but the interpretation is different in fact older books use the terms confidence interval and fiducial interval interchangeably notice that the fiducial distribution is uniquely defined when a single sufficient statistic exists\u000athe pivotal method is based on a random variable that is a function of both the observations and the parameters but whose distribution does not depend on the parameter such random variables are called pivotal quantities by using these probability statements about the observations and parameters may be made in which the probabilities do not depend on the parameters and these may be inverted by solving for the parameters in much the same way as in the example above however this is only equivalent to the fiducial method if the pivotal quantity is uniquely defined based on a sufficient statistic\u000aa fiducial interval could be taken to be just a different name for a confidence interval and give it the fiducial interpretation but the definition might not then be unique fisher would have denied that this interpretation is correct for him the fiducial distribution had to be defined uniquely and it had to use all the information in the sample\u000a\u000a\u000a status of the approach \u000aafter its formulation by fisher fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published\u000afisher admitted that fiducial inference had problems fisher wrote to george a barnard that he was not clear in the head about one problem on fiducial inference and also writing to barnard fisher complained that his theory seemed to have only an asymptotic approach to intelligibility later fisher confessed that i dont understand yet what fiducial probability does we shall have to live with it a long time before we know what its doing for us but it should not be ignored just because we dont yet have a clear interpretation\u000alindley showed that fiducial probability lacked additivity and so was not a probability measure cox points out that the same argument applies to the socalled confidence distribution associated with confidence intervals so the conclusion to be drawn from this is moot fisher sketched proofs of results using fiducial probability when the conclusions of fishers fiducial arguments are not false many have been shown to also follow from bayesian inference\u000ain 1978 jg pederson wrote that the fiducial argument has had very limited success and is now essentially dead davison wrote a few subsequent attempts have been made to resurrect fiducialism but it now seems largely of historical importance particularly in view of its restricted range of applicability when set alongside models of current interest\u000ahowever fiducial inference is still being studied and other current work is ongoing under the name of confidence distributions\u000a\u000a\u000a notes
p183
sg20
g23
sg24
g27
sg30
Vfiducial inference is one of a number of different types of statistical inference these are rules intended for general application by which conclusions can be drawn from samples of data in modern statistical practice attempts to work with fiducial inference have fallen out of fashion in favour of frequentist inference bayesian inference and decision theory however fiducial inference is important in the history of statistics since its development led to the parallel development of concepts and tools in theoretical statistics that are widely used some current research in statistical methodology is either explicitly linked to fiducial inference or is closely connected to it\u000a\u000a\u000a background \u000athe general approach of fiducial inference was proposed by ronald fisher here fiducial comes from the latin for faith fiducial inference can be interpreted as an attempt to perform inverse probability without calling on prior probability distributions fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published these counterexamples cast doubt on the coherence of fiducial inference as a system of statistical inference or inductive logic other studies showed that where the steps of fiducial inference are said to lead to fiducial probabilities or fiducial distributions these probabilities lack the property of additivity and so cannot constitute a probability measure\u000athe concept of fiducial inference can be outlined by comparing its treatment of the problem of interval estimation in relation to other modes of statistical inference\u000aa confidence interval in frequentist inference with coverage probability  has the interpretation that among all confidence intervals computed by the same method a proportion  will contain the true value that needs to be estimated this has either a repeated sampling or frequentist interpretation or is the probability that an interval calculated from yettobesampled data will cover the true value however in either case the probability concerned is not the probability that the true value is in the particular interval that has been calculated since at that stage both the true value and the calculated are fixed and are not random\u000acredible intervals in bayesian inference do allow a probability to be given for the event that an interval once it has been calculated does include the true value since it proceeds on the basis that a probability distribution can be associated with the state of knowledge about the true value both before and after the sample of data has been obtained\u000afisher designed the fiducial method to meet perceived problems with the bayesian approach at a time when the frequentist approach had yet to be fully developed such problems related to the need to assign a prior distribution to the unknown values the aim was to have a procedure like the bayesian method whose results could still be given an inverse probability interpretation based on the actual data observed the method proceeds by attempting to derive a fiducial distribution which is a measure of the degree of faith that can be put on any given value of the unknown parameter and is faithful to the data in the sense that the method uses all available information\u000aunfortunately fisher did not give a general definition of the fiducial method and he denied that the method could always be applied his only examples were for a single parameter different generalisations have been given when there are several parameters a relatively complete presentation of the fiducial approach to inference is given by quenouille 1958 while williams 1959 describes the application of fiducial analysis to the calibration problem also known as inverse regression in regression analysis further discussion of fiducial inference is given by kendall  stuart 1973\u000a\u000a\u000a the fiducial distribution \u000afisher required the existence of a sufficient statistic for the fiducial method to apply suppose there is a single sufficient statistic for a single parameter that is suppose that the conditional distribution of the data given the statistic does not depend on the value of the parameter for example suppose that n independent observations are uniformly distributed on the interval  the maximum x of the n observations is a sufficient statistic for  if only x is recorded and the values of the remaining observations are forgotten these remaining observations are equally likely to have had any values in the interval  this statement does not depend on the value of  then x contains all the available information about  and the other observations could have given no further information\u000athe cumulative distribution function of x is\u000a\u000aprobability statements about x may be made for example given  a value of a can be chosen with 0    a    1 such that\u000a\u000athus\u000a\u000athen fisher might say that this statement may be inverted into the form\u000a\u000ain this latter statement  is now regarded as variable and x is fixed whereas previously it was the other way round this distribution of  is the fiducial distribution which may be used to form fiducial intervals that represent degrees of belief\u000athe calculation is identical to the pivotal method for finding a confidence interval but the interpretation is different in fact older books use the terms confidence interval and fiducial interval interchangeably notice that the fiducial distribution is uniquely defined when a single sufficient statistic exists\u000athe pivotal method is based on a random variable that is a function of both the observations and the parameters but whose distribution does not depend on the parameter such random variables are called pivotal quantities by using these probability statements about the observations and parameters may be made in which the probabilities do not depend on the parameters and these may be inverted by solving for the parameters in much the same way as in the example above however this is only equivalent to the fiducial method if the pivotal quantity is uniquely defined based on a sufficient statistic\u000aa fiducial interval could be taken to be just a different name for a confidence interval and give it the fiducial interpretation but the definition might not then be unique fisher would have denied that this interpretation is correct for him the fiducial distribution had to be defined uniquely and it had to use all the information in the sample\u000a\u000a\u000a status of the approach \u000aafter its formulation by fisher fiducial inference quickly attracted controversy and was never widely accepted indeed counterexamples to the claims of fisher for fiducial inference were soon published\u000afisher admitted that fiducial inference had problems fisher wrote to george a barnard that he was not clear in the head about one problem on fiducial inference and also writing to barnard fisher complained that his theory seemed to have only an asymptotic approach to intelligibility later fisher confessed that i dont understand yet what fiducial probability does we shall have to live with it a long time before we know what its doing for us but it should not be ignored just because we dont yet have a clear interpretation\u000alindley showed that fiducial probability lacked additivity and so was not a probability measure cox points out that the same argument applies to the socalled confidence distribution associated with confidence intervals so the conclusion to be drawn from this is moot fisher sketched proofs of results using fiducial probability when the conclusions of fishers fiducial arguments are not false many have been shown to also follow from bayesian inference\u000ain 1978 jg pederson wrote that the fiducial argument has had very limited success and is now essentially dead davison wrote a few subsequent attempts have been made to resurrect fiducialism but it now seems largely of historical importance particularly in view of its restricted range of applicability when set alongside models of current interest\u000ahowever fiducial inference is still being studied and other current work is ongoing under the name of confidence distributions\u000a\u000a\u000a notes \u000a\u000a\u000a
p184
sg32
g35
sg37
NsbsS'statistical_manifold.txt'
p185
g2
(g3
g4
Ntp186
Rp187
(dp188
g8
g11
sg12
Vin mathematics a statistical manifold is a riemannian manifold each of whose points is a probability distribution statistical manifolds provide a setting for the field of information geometry the fisher information metric provides a metric on these manifolds\u000a\u000a\u000a examples \u000athe family of all normal distributions parametrized by the expected value  and the variance 2  0 with the riemannian metric given by the fisher information matrix is a statistical manifold its geometry is modeled on hyperbolic space\u000aa simple example of a statistical manifold taken from physics would be the canonical ensemble it is a onedimensional manifold with the temperature t serving as the coordinate on the manifold for any fixed temperature t one has a probability space so for a gas of atoms it would be the probability distribution of the velocities of the atoms as one varies the temperature t the probability distribution varies\u000aanother simple example taken from medicine would be the probability distribution of patient outcomes in response to the quantity of medicine administered that is for a fixed dose some patients improve and some do not this is the base probability space if the dosage is varied then the probability of outcomes changes thus the dosage is the coordinate on the manifold to be a smooth manifold one would have to measure outcomes in response to arbitrarily small changes in dosage this is not a practically realizable example unless one has a preexisting mathematical model of doseresponse where the dose can be arbitrarily varied\u000a\u000a\u000a definition \u000alet x be an orientable manifold and let  be a measure on x equivalently let  be a probability space on  with sigma algebra  and probability \u000athe statistical manifold sx of x is defined as the space of all measures  on x with the sigmaalgebra  held fixed note that this space is infinitedimensional it is commonly taken to be a frchet space the points of sx are measures\u000arather than dealing with an infinitedimensional space sx it is common to work with a finitedimensional submanifold defined by considering a set of probability distributions parameterized by some smooth continuouslyvarying parameter  that is one considers only those measures that are selected by the parameter if the parameter  is ndimensional then in general the submanifold will be as well all finitedimensional statistical manifolds can be understood in this way
p189
sg14
g17
sg18
Vin mathematics a statistical manifold is a riemannian manifold each of whose points is a probability distribution statistical manifolds provide a setting for the field of information geometry the fisher information metric provides a metric on these manifolds\u000a\u000a\u000a examples \u000athe family of all normal distributions parametrized by the expected value  and the variance 2  0 with the riemannian metric given by the fisher information matrix is a statistical manifold its geometry is modeled on hyperbolic space\u000aa simple example of a statistical manifold taken from physics would be the canonical ensemble it is a onedimensional manifold with the temperature t serving as the coordinate on the manifold for any fixed temperature t one has a probability space so for a gas of atoms it would be the probability distribution of the velocities of the atoms as one varies the temperature t the probability distribution varies\u000aanother simple example taken from medicine would be the probability distribution of patient outcomes in response to the quantity of medicine administered that is for a fixed dose some patients improve and some do not this is the base probability space if the dosage is varied then the probability of outcomes changes thus the dosage is the coordinate on the manifold to be a smooth manifold one would have to measure outcomes in response to arbitrarily small changes in dosage this is not a practically realizable example unless one has a preexisting mathematical model of doseresponse where the dose can be arbitrarily varied\u000a\u000a\u000a definition \u000alet x be an orientable manifold and let  be a measure on x equivalently let  be a probability space on  with sigma algebra  and probability \u000athe statistical manifold sx of x is defined as the space of all measures  on x with the sigmaalgebra  held fixed note that this space is infinitedimensional it is commonly taken to be a frchet space the points of sx are measures\u000arather than dealing with an infinitedimensional space sx it is common to work with a finitedimensional submanifold defined by considering a set of probability distributions parameterized by some smooth continuouslyvarying parameter  that is one considers only those measures that are selected by the parameter if the parameter  is ndimensional then in general the submanifold will be as well all finitedimensional statistical manifolds can be understood in this way
p190
sg20
g23
sg24
g27
sg30
Vin mathematics a statistical manifold is a riemannian manifold each of whose points is a probability distribution statistical manifolds provide a setting for the field of information geometry the fisher information metric provides a metric on these manifolds\u000a\u000a\u000a examples \u000athe family of all normal distributions parametrized by the expected value  and the variance 2  0 with the riemannian metric given by the fisher information matrix is a statistical manifold its geometry is modeled on hyperbolic space\u000aa simple example of a statistical manifold taken from physics would be the canonical ensemble it is a onedimensional manifold with the temperature t serving as the coordinate on the manifold for any fixed temperature t one has a probability space so for a gas of atoms it would be the probability distribution of the velocities of the atoms as one varies the temperature t the probability distribution varies\u000aanother simple example taken from medicine would be the probability distribution of patient outcomes in response to the quantity of medicine administered that is for a fixed dose some patients improve and some do not this is the base probability space if the dosage is varied then the probability of outcomes changes thus the dosage is the coordinate on the manifold to be a smooth manifold one would have to measure outcomes in response to arbitrarily small changes in dosage this is not a practically realizable example unless one has a preexisting mathematical model of doseresponse where the dose can be arbitrarily varied\u000a\u000a\u000a definition \u000alet x be an orientable manifold and let  be a measure on x equivalently let  be a probability space on  with sigma algebra  and probability \u000athe statistical manifold sx of x is defined as the space of all measures  on x with the sigmaalgebra  held fixed note that this space is infinitedimensional it is commonly taken to be a frchet space the points of sx are measures\u000arather than dealing with an infinitedimensional space sx it is common to work with a finitedimensional submanifold defined by considering a set of probability distributions parameterized by some smooth continuouslyvarying parameter  that is one considers only those measures that are selected by the parameter if the parameter  is ndimensional then in general the submanifold will be as well all finitedimensional statistical manifolds can be understood in this way
p191
sg32
g35
sg37
NsbsS'winsorising.txt'
p192
g2
(g3
g4
Ntp193
Rp194
(dp195
g8
g11
sg12
Vwinsorising or winsorisation is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers it is named after the engineerturnedbiostatistician charles p winsor 18951951 the effect is the same as clipping in signal processing\u000athe distribution of many statistics can be heavily influenced by outliers a typical strategy is to set all outliers to a specified percentile of the data for example a 90 winsorisation would see all data below the 5th percentile set to the 5th percentile and data above the 95th percentile set to the 95th percentile winsorised estimators are usually more robust to outliers than their more standard forms although there are alternatives such as trimming that will achieve a similar effect\u000a\u000a\u000a example \u000aconsider the data set consisting of\u000a\u000athe 5th percentile lies between 40 and 5 while the 95th percentile lies between 101 and 1053 values shown in bold then a 90 winsorisation would result in the following\u000a\u000apython can winsorise data using numpy and scipy libraries \u000a\u000a\u000a distinction from trimming \u000anote that winsorising is not equivalent to simply excluding data which is a simpler procedure called trimming or truncation but is a method of censoring data\u000ain a trimmed estimator the extreme values are discarded in a winsorised estimator the extreme values are instead replaced by certain percentiles the trimmed minimum and maximum\u000athus a winsorised mean is not the same as a truncated mean for instance the 10 trimmed mean is the average of the 5th to 95th percentile of the data while the 90 winsorised mean sets the bottom 5 to the 5th percentile the top 5 to the 95th percentile and then averages the data in the previous example the trimmed mean would be obtained from the smaller set\u000a\u000amore formally they are distinct because the order statistics are not independent\u000a\u000a\u000a
p196
sg14
g17
sg18
Vwinsorising or winsorisation is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers it is named after the engineerturnedbiostatistician charles p winsor 18951951 the effect is the same as clipping in signal processing\u000athe distribution of many statistics can be heavily influenced by outliers a typical strategy is to set all outliers to a specified percentile of the data for example a 90 winsorisation would see all data below the 5th percentile set to the 5th percentile and data above the 95th percentile set to the 95th percentile winsorised estimators are usually more robust to outliers than their more standard forms although there are alternatives such as trimming that will achieve a similar effect\u000a\u000a\u000a example \u000aconsider the data set consisting of\u000a\u000athe 5th percentile lies between 40 and 5 while the 95th percentile lies between 101 and 1053 values shown in bold then a 90 winsorisation would result in the following\u000a\u000apython can winsorise data using numpy and scipy libraries \u000a\u000a\u000a distinction from trimming \u000anote that winsorising is not equivalent to simply excluding data which is a simpler procedure called trimming or truncation but is a method of censoring data\u000ain a trimmed estimator the extreme values are discarded in a winsorised estimator the extreme values are instead replaced by certain percentiles the trimmed minimum and maximum\u000athus a winsorised mean is not the same as a truncated mean for instance the 10 trimmed mean is the average of the 5th to 95th percentile of the data while the 90 winsorised mean sets the bottom 5 to the 5th percentile the top 5 to the 95th percentile and then averages the data in the previous example the trimmed mean would be obtained from the smaller set\u000a\u000amore formally they are distinct because the order statistics are not independent
p197
sg20
g23
sg24
g27
sg30
Vwinsorising or winsorisation is the transformation of statistics by limiting extreme values in the statistical data to reduce the effect of possibly spurious outliers it is named after the engineerturnedbiostatistician charles p winsor 18951951 the effect is the same as clipping in signal processing\u000athe distribution of many statistics can be heavily influenced by outliers a typical strategy is to set all outliers to a specified percentile of the data for example a 90 winsorisation would see all data below the 5th percentile set to the 5th percentile and data above the 95th percentile set to the 95th percentile winsorised estimators are usually more robust to outliers than their more standard forms although there are alternatives such as trimming that will achieve a similar effect\u000a\u000a\u000a example \u000aconsider the data set consisting of\u000a\u000athe 5th percentile lies between 40 and 5 while the 95th percentile lies between 101 and 1053 values shown in bold then a 90 winsorisation would result in the following\u000a\u000apython can winsorise data using numpy and scipy libraries \u000a\u000a\u000a distinction from trimming \u000anote that winsorising is not equivalent to simply excluding data which is a simpler procedure called trimming or truncation but is a method of censoring data\u000ain a trimmed estimator the extreme values are discarded in a winsorised estimator the extreme values are instead replaced by certain percentiles the trimmed minimum and maximum\u000athus a winsorised mean is not the same as a truncated mean for instance the 10 trimmed mean is the average of the 5th to 95th percentile of the data while the 90 winsorised mean sets the bottom 5 to the 5th percentile the top 5 to the 95th percentile and then averages the data in the previous example the trimmed mean would be obtained from the smaller set\u000a\u000amore formally they are distinct because the order statistics are not independent\u000a\u000a\u000a
p198
sg32
g35
sg37
NsbsS'bayesian_inference.txt'
p199
g2
(g3
g4
Ntp200
Rp201
(dp202
g8
g11
sg12
Vbayesian inference is a method of statistical inference in which bayes theorem is used to update the probability for a hypothesis as evidence bayesian inference is an important technique in statistics and especially in mathematical statistics bayesian updating is particularly important in the dynamic analysis of a sequence of data bayesian inference has found application in a wide range of activities including science engineering philosophy medicine sport and law in the philosophy of decision theory bayesian inference is closely related to subjective probability often called bayesian probability\u000a\u000a\u000a introduction to bayes rule \u000a\u000a\u000a formal \u000abayesian inference derives the posterior probability as a consequence of two antecedents a prior probability and a likelihood function derived from a statistical model for the observed data bayesian inference computes the posterior probability according to bayes theorem\u000a\u000awhere\u000a denotes a conditional probability more specifically it means given\u000a stands for any hypothesis whose probability may be affected by data called evidence below often there are competing hypotheses from which one chooses the most probable\u000athe evidence  corresponds to new data that were not used in computing the prior probability\u000a the prior probability is the probability of  before  is observed this indicates ones previous estimate of the probability that a hypothesis is true before gaining the current evidence\u000a the posterior probability is the probability of  given  ie after  is observed this tells us what we want to know the probability of a hypothesis given the observed evidence\u000a is the probability of observing  given  as a function of  with  fixed this is the likelihood the likelihood function should not be confused with  as a function of  rather than of  it indicates the compatibility of the evidence with the given hypothesis\u000a is sometimes termed the marginal likelihood or model evidence this factor is the same for all possible hypotheses being considered this can be seen by the fact that the hypothesis  does not appear anywhere in the symbol unlike for all the other factors this means that this factor does not enter into determining the relative probabilities of different hypotheses\u000anote that for different values of  only the factors  and  affect the value of  as both of these factors appear in the numerator the posterior probability is proportional to both in words\u000amore precisely the posterior probability of a hypothesis is determined by a combination of the inherent likeliness of a hypothesis the prior and the compatibility of the observed evidence with the hypothesis the likelihood\u000amore concisely posterior is proportional to likelihood times prior\u000anote that bayes rule can also be written as follows\u000a\u000awhere the factor  represents the impact of  on the probability of \u000a\u000a\u000a informal \u000aif the evidence does not match up with a hypothesis one should reject the hypothesis but if a hypothesis is extremely unlikely a priori one should also reject it even if the evidence does appear to match up\u000afor example imagine that i have various hypotheses about the nature of a newborn baby of a friend including\u000a the baby is a brownhaired boy\u000a the baby is a blondhaired girl\u000a the baby is a dog\u000athen consider two scenarios\u000aim presented with evidence in the form of a picture of a blondhaired baby girl i find this evidence supports  and opposes  and \u000aim presented with evidence in the form of a picture of a baby dog although this evidence treated in isolation supports  my prior belief in this hypothesis that a human can give birth to a dog is extremely small so the posterior probability is nevertheless small\u000athe critical point about bayesian inference then is that it provides a principled way of combining new evidence with prior beliefs through the application of bayes rule contrast this with frequentist inference which relies only on the evidence as a whole with no reference to prior beliefs furthermore bayes rule can be applied iteratively after observing some evidence the resulting posterior probability can then be treated as a prior probability and a new posterior probability computed from new evidence this allows for bayesian principles to be applied to various kinds of evidence whether viewed all at once or over time this procedure is termed bayesian updating\u000a\u000a\u000a bayesian updating \u000abayesian updating is widely used and computationally convenient however it is not the only updating rule that might be considered rational\u000aian hacking noted that traditional dutch book arguments did not specify bayesian updating they left open the possibility that nonbayesian updating rules could avoid dutch books hacking wrote and neither the dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption not one entails bayesianism so the personalist requires the dynamic assumption to be bayesian it is true that in consistency a personalist could abandon the bayesian model of learning from experience salt could lose its savour\u000aindeed there are nonbayesian updating rules that also avoid dutch books as discussed in the literature on probability kinematics following the publication of richard c jeffreys rule which applies bayes rule to the case where the evidence itself is assigned a probability the additional hypotheses needed to uniquely require bayesian updating have been deemed to be substantial complicated and unsatisfactory\u000a\u000a\u000a formal description of bayesian inference \u000a\u000a\u000a definitions \u000a a data point in general this may in fact be a vector of values\u000a the parameter of the data points distribution ie   this may in fact be a vector of parameters\u000a the hyperparameter of the parameter ie   this may in fact be a vector of hyperparameters\u000a a set of  observed data points ie \u000a a new data point whose distribution is to be predicted\u000a\u000a\u000a bayesian inference \u000athe prior distribution is the distribution of the parameters before any data is observed ie  \u000athe prior distribution might not be easily determined in this case we can use the jeffreys prior to obtain the posterior distribution before updating them with newer observations\u000athe sampling distribution is the distribution of the observed data conditional on its parameters ie   this is also termed the likelihood especially when viewed as a function of the parameters sometimes written  \u000athe marginal likelihood sometimes also termed the evidence is the distribution of the observed data marginalized over the parameters ie  \u000athe posterior distribution is the distribution of the parameters after taking into account the observed data this is determined by bayes rule which forms the heart of bayesian inference\u000a\u000anote that this is expressed in words as posterior is proportional to likelihood times prior or sometimes as posterior  likelihood times prior over evidence\u000a\u000a\u000a bayesian prediction \u000athe posterior predictive distribution is the distribution of a new data point marginalized over the posterior\u000a\u000athe prior predictive distribution is the distribution of a new data point marginalized over the prior\u000a\u000abayesian theory calls for the use of the posterior predictive distribution to do predictive inference ie to predict the distribution of a new unobserved data point that is instead of a fixed point as a prediction a distribution over possible points is returned only this way is the entire posterior distribution of the parameters used by comparison prediction in frequentist statistics often involves finding an optimum point estimate of the parameterseg by maximum likelihood or maximum a posteriori estimation mapand then plugging this estimate into the formula for the distribution of a data point this has the disadvantage that it does not account for any uncertainty in the value of the parameter and hence will underestimate the variance of the predictive distribution\u000ain some instances frequentist statistics can work around this problem for example confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a students tdistribution this correctly estimates the variance due to the fact that 1 the average of normally distributed random variables is also normally distributed 2 the predictive distribution of a normally distributed data point with unknown mean and variance using conjugate or uninformative priors has a students tdistribution in bayesian statistics however the posterior predictive distribution can always be determined exactlyor at least to an arbitrary level of precision when numerical methods are used\u000anote that both types of predictive distributions have the form of a compound probability distribution as does the marginal likelihood in fact if the prior distribution is a conjugate prior and hence the prior and posterior distributions come from the same family it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions the only difference is that the posterior predictive distribution uses the updated values of the hyperparameters applying the bayesian update rules given in the conjugate prior article while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution\u000a\u000a\u000a inference over exclusive and exhaustive possibilities \u000aif evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions bayesian inference may be thought of as acting on this belief distribution as a whole\u000a\u000a\u000a general formulation \u000a\u000asuppose a process is generating independent and identically distributed events  but the probability distribution is unknown let the event space  represent the current state of belief for this process each model is represented by event  the conditional probabilities  are specified to define the models  is the degree of belief in  before the first inference step  is a set of initial prior probabilities these must sum to 1 but are otherwise arbitrary\u000asuppose that the process is observed to generate  for each  the prior  is updated to the posterior  from bayes theorem\u000a\u000aupon observation of further evidence this procedure may be repeated\u000a\u000a\u000a multiple observations \u000afor a sequence of independent and identically distributed observations  it can be shown by induction that repeated application of the above is equivalent to\u000a\u000awhere\u000a\u000a\u000a parametric formulation \u000aby parameterizing the space of models the belief in all models may be updated in a single step the distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space the distributions in this section are expressed as continuous represented by probability densities as this is the usual situation the technique is however equally applicable to discrete distributions\u000alet the vector  span the parameter space let the initial prior distribution over  be  where  is a set of parameters to the prior itself or hyperparameters let  be a sequence of independent and identically distributed event observations where all  are distributed as  for some  bayes theorem is applied to find the posterior distribution over \u000a\u000awhere\u000a\u000a\u000a mathematical properties \u000a\u000a\u000a interpretation of factor \u000a that is if the model were true the evidence would be more likely than is predicted by the current state of belief the reverse applies for a decrease in belief if the belief does not change  that is the evidence is independent of the model if the model were true the evidence would be exactly as likely as predicted by the current state of belief\u000a\u000a\u000a cromwells rule \u000a\u000aif  then  if  then  this can be interpreted to mean that hard convictions are insensitive to counterevidence\u000athe former follows directly from bayes theorem the latter can be derived by applying the first rule to the event not  in place of  yielding if  then  from which the result immediately follows\u000a\u000a\u000a asymptotic behaviour of posterior \u000aconsider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials for sufficiently nice prior probabilities the bernsteinvon mises theorem gives that in the limit of infinite trials the posterior converges to a gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by joseph l doob in 1948 namely if the random variable in consideration has a finite probability space the more general results were obtained later by the statistician david a freedman who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed his 1963 paper treats like doob 1949 the finite case and comes to a satisfactory conclusion however if the random variable has an infinite but countable probability space ie corresponding to a die with infinite many faces the 1965 paper demonstrates that for a dense subset of priors the bernsteinvon mises theorem is not applicable in this case there is almost surely no asymptotic convergence later in the 1980s and 1990s freedman and persi diaconis continued to work on the case of infinite countable probability spaces to summarise there may be insufficient trials to suppress the effects of the initial choice and especially for large but finite systems the convergence might be very slow\u000a\u000a\u000a conjugate priors \u000a\u000ain parameterized form the prior distribution is often assumed to come from a family of distributions called conjugate priors the usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family and the calculation may be expressed in closed form\u000a\u000a\u000a estimates of parameters and predictions \u000ait is often desired to use a posterior distribution to estimate a parameter or variable several methods of bayesian estimation select measurements of central tendency from the posterior distribution\u000afor onedimensional problems a unique median exists for practical continuous problems the posterior median is attractive as a robust estimator\u000aif there exists a finite mean for the posterior distribution then the posterior mean is a method of estimation\u000a\u000ataking a value with the greatest probability defines maximum a posteriori map estimates\u000a\u000athere are examples where no maximum is attained in which case the set of map estimates is empty\u000athere are other methods of estimation that minimize the posterior risk expectedposterior loss with respect to a loss function and these are of interest to statistical decision theory using the sampling distribution frequentist statistics\u000athe posterior predictive distribution of a new observation  that is independent of previous observations is determined by\u000a\u000a\u000a examples \u000a\u000a\u000a probability of a hypothesis \u000asuppose there are two full bowls of cookies bowl 1 has 10 chocolate chip and 30 plain cookies while bowl 2 has 20 of each our friend fred picks a bowl at random and then picks a cookie at random we may assume there is no reason to believe fred treats one bowl differently from another likewise for the cookies the cookie turns out to be a plain one how probable is it that fred picked it out of bowl 1\u000aintuitively it seems clear that the answer should be more than a half since there are more plain cookies in bowl 1 the precise answer is given by bayes theorem let  correspond to bowl 1 and  to bowl 2 it is given that the bowls are identical from freds point of view thus  and the two must add up to 1 so both are equal to 05 the event  is the observation of a plain cookie from the contents of the bowls we know that  and  bayes formula then yields\u000a\u000abefore we observed the cookie the probability we assigned for fred having chosen bowl 1 was the prior probability  which was 05 after observing the cookie we must revise the probability to  which is 06\u000a\u000a\u000a making a prediction \u000a\u000aan archaeologist is working at a site thought to be from the medieval period between the 11th century to the 16th century however it is uncertain exactly when in this period the site was inhabited fragments of pottery are found some of which are glazed and some of which are decorated it is expected that if the site were inhabited during the early medieval period then 1 of the pottery would be glazed and 50 of its area decorated whereas if it had been inhabited in the late medieval period then 81 would be glazed and 5 of its area decorated how confident can the archaeologist be in the date of inhabitation as fragments are unearthed\u000athe degree of belief in the continuous variable  century is to be calculated with the discrete set of events  as evidence assuming linear variation of glaze and decoration with time and that these variables are independent\u000a\u000aassume a uniform prior of  and that trials are independent and identically distributed when a new fragment of type  is discovered bayes theorem is applied to update the degree of belief for each \u000a\u000aa computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph in the simulation the site was inhabited around 1420 or  by calculating the area under the relevant portion of the graph for 50 trials the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries about 1 chance that it was inhabited during the 13th century 63 chance during the 14th century and 36 during the 15th century note that the bernsteinvon mises theorem asserts here the asymptotic convergence to the true distribution because the probability space corresponding to the discrete set of events  is finite see above section on asymptotic behaviour of the posterior\u000a\u000a\u000a in frequentist statistics and decision theory \u000aa decisiontheoretic justification of the use of bayesian inference was given by abraham wald who proved that every unique bayesian procedure is admissible conversely every admissible statistical procedure is either a bayesian procedure or a limit of bayesian procedures\u000awald characterized admissible procedures as bayesian procedures and limits of bayesian procedures making the bayesian formalism a central technique in such areas of frequentist inference as parameter estimation hypothesis testing and computing confidence intervals for example\u000aunder some conditions all admissible procedures are either bayes procedures or limits of bayes procedures in various senses these remarkable results at least in their original form are due essentially to wald they are useful because the property of being bayes is easier to analyze than admissibility\u000ain decision theory a quite general method for proving admissibility consists in exhibiting a procedure as a unique bayes solution\u000ain the first chapters of this work prior distributions with finite support and the corresponding bayes procedures were used to establish some of the main theorems relating to the comparison of experiments bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics including its asymptotic theory there are many problems where a glance at posterior distributions for suitable priors yields immediately interesting information also this technique can hardly be avoided in sequential analysis\u000aa useful fact is that any bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible\u000aan important area of investigation in the development of admissibility ideas has been that of conventional samplingtheory procedures and many interesting results have been obtained\u000a\u000a\u000a model selection \u000a\u000a\u000a applications \u000a\u000a\u000a computer applications \u000abayesian inference has applications in artificial intelligence and expert systems bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s there is also an ever growing connection between bayesian methods and simulationbased monte carlo techniques since complex models cannot be processed in closed form by a bayesian analysis while a graphical model structure may allow for efficient simulation algorithms like the gibbs sampling and other metropolishastings algorithm schemes recently bayesian inference has gained popularity amongst the phylogenetics community for these reasons a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously\u000aas applied to statistical classification bayesian inference has been used in recent years to develop algorithms for identifying email spam applications which make use of bayesian inference for spam filtering include crm114 dspam bogofilter spamassassin spambayes mozilla xeams and others spam classification is treated in more detail in the article on the naive bayes classifier\u000asolomonoffs inductive inference is the theory of prediction based on observations for example predicting the next symbol based upon a given series of symbols the only assumption is that the environment follows some unknown but computable probability distribution it is a formal inductive framework that combines two wellstudied principles of inductive inference bayesian statistics and occams razor solomonoffs universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs for a universal computer that compute something starting with p given some p and any computable but unknown probability distribution from which x is sampled the universal prior and bayes theorem can be used to predict the yet unseen parts of x in optimal fashion\u000a\u000a\u000a in the courtroom \u000abayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant and to see whether in totality it meets their personal threshold for beyond a reasonable doubt bayes theorem is applied successively to all evidence presented with the posterior from one stage becoming the prior for the next the benefit of a bayesian approach is that it gives the juror an unbiased rational mechanism for combining evidence it may be appropriate to explain bayes theorem to jurors in odds form as betting odds are more widely understood than probabilities alternatively a logarithmic approach replacing multiplication with addition might be easier for a jury to handle\u000a\u000aif the existence of the crime is not in doubt only the identity of the culprit it has been suggested that the prior should be uniform over the qualifying population for example if 1000 people could have committed the crime the prior probability of guilt would be 11000\u000athe use of bayes theorem by jurors is controversial in the united kingdom a defence expert witness explained bayes theorem to the jury in r v adams the jury convicted but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use bayes theorem the court of appeal upheld the conviction but it also gave the opinion that to introduce bayes theorem or any similar method into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity deflecting them from their proper task\u000agardnermedwin argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt but rather the probability of the evidence given that the defendant is innocent akin to a frequentist pvalue he argues that if the posterior probability of guilt is to be computed by bayes theorem the prior probability of guilt must be known this will depend on the incidence of the crime which is an unusual piece of evidence to consider in a criminal trial consider the following three propositions\u000aa the known facts and testimony could have arisen if the defendant is guilty\u000ab the known facts and testimony could have arisen if the defendant is innocent\u000ac the defendant is guilty\u000agardnermedwin argues that the jury should believe both a and notb in order to convict a and notb implies the truth of c but the reverse is not true it is possible that b and c are both true but in this case he argues that a jury should acquit even though they know that they will be letting some guilty people go free see also lindleys paradox\u000a\u000a\u000a bayesian epistemology \u000abayesian epistemology is a movement that advocates for bayesian inference as a means of justifying the rules of inductive logic\u000akarl popper and david miller have rejected the alleged rationality of bayesianism ie using bayes rule to make epistemological inferences it is prone to the same vicious circle as any other justificationist epistemology because it presupposes what it attempts to justify according to this view a rational interpretation of bayesian inference would see it merely as a probabilistic version of falsification rejecting the belief commonly held by bayesians that high likelihood achieved by a series of bayesian updates would prove the hypothesis beyond any reasonable doubt or even with likelihood greater than 0\u000a\u000a\u000a other \u000athe scientific method is sometimes interpreted as an application of bayesian inference in this view bayes rule guides or should guide the updating of probabilities about hypotheses conditional on new observations or experiments\u000abayesian search theory is used to search for lost objects\u000abayesian inference in phylogeny\u000abayesian tool for methylation analysis\u000a\u000a\u000a bayes and bayesian inference \u000athe problem considered by bayes in proposition 9 of his essay an essay towards solving a problem in the doctrine of chances is the posterior distribution for the parameter a the success rate of the binomial distribution\u000a\u000a\u000a history \u000a\u000athe term bayesian refers to thomas bayes 17021761 who proved a special case of what is now called bayes theorem however it was pierresimon laplace 17491827 who introduced a general version of the theorem and used it to approach problems in celestial mechanics medical statistics reliability and jurisprudence early bayesian inference which used uniform priors following laplaces principle of insufficient reason was called inverse probability because it infers backwards from observations to parameters or from effects to causes after the 1920s inverse probability was largely supplanted by a collection of methods that came to be called frequentist statistics\u000ain the 20th century the ideas of laplace were further developed in two different directions giving rise to objective and subjective currents in bayesian practice in the objective or noninformative current the statistical analysis depends on only the model assumed the data analyzed and the method assigning the prior which differs from one objective bayesian to another objective bayesian in the subjective or informative current the specification of the prior depends on the belief that is propositions on which the analysis is prepared to act which can summarize information from experts previous studies etc\u000ain the 1980s there was a dramatic growth in research and applications of bayesian methods mostly attributed to the discovery of markov chain monte carlo methods which removed many of the computational problems and an increasing interest in nonstandard complex applications despite growth of bayesian research most undergraduate teaching is still based on frequentist statistics nonetheless bayesian methods are widely accepted and used such as for example in the field of machine learning\u000a\u000a\u000a see also \u000abayes theorem\u000abayesian hierarchical modeling\u000abayesian analysis the journal of the isba\u000ainductive probability\u000ainternational society for bayesian analysis isba\u000ajeffreys prior\u000a\u000a\u000a notes \u000a\u000a\u000a
p203
sg14
g17
sg18
Vbayesian inference is a method of statistical inference in which bayes theorem is used to update the probability for a hypothesis as evidence bayesian inference is an important technique in statistics and especially in mathematical statistics bayesian updating is particularly important in the dynamic analysis of a sequence of data bayesian inference has found application in a wide range of activities including science engineering philosophy medicine sport and law in the philosophy of decision theory bayesian inference is closely related to subjective probability often called bayesian probability\u000a\u000a\u000a introduction to bayes rule \u000a\u000a\u000a formal \u000abayesian inference derives the posterior probability as a consequence of two antecedents a prior probability and a likelihood function derived from a statistical model for the observed data bayesian inference computes the posterior probability according to bayes theorem\u000a\u000awhere\u000a denotes a conditional probability more specifically it means given\u000a stands for any hypothesis whose probability may be affected by data called evidence below often there are competing hypotheses from which one chooses the most probable\u000athe evidence  corresponds to new data that were not used in computing the prior probability\u000a the prior probability is the probability of  before  is observed this indicates ones previous estimate of the probability that a hypothesis is true before gaining the current evidence\u000a the posterior probability is the probability of  given  ie after  is observed this tells us what we want to know the probability of a hypothesis given the observed evidence\u000a is the probability of observing  given  as a function of  with  fixed this is the likelihood the likelihood function should not be confused with  as a function of  rather than of  it indicates the compatibility of the evidence with the given hypothesis\u000a is sometimes termed the marginal likelihood or model evidence this factor is the same for all possible hypotheses being considered this can be seen by the fact that the hypothesis  does not appear anywhere in the symbol unlike for all the other factors this means that this factor does not enter into determining the relative probabilities of different hypotheses\u000anote that for different values of  only the factors  and  affect the value of  as both of these factors appear in the numerator the posterior probability is proportional to both in words\u000amore precisely the posterior probability of a hypothesis is determined by a combination of the inherent likeliness of a hypothesis the prior and the compatibility of the observed evidence with the hypothesis the likelihood\u000amore concisely posterior is proportional to likelihood times prior\u000anote that bayes rule can also be written as follows\u000a\u000awhere the factor  represents the impact of  on the probability of \u000a\u000a\u000a informal \u000aif the evidence does not match up with a hypothesis one should reject the hypothesis but if a hypothesis is extremely unlikely a priori one should also reject it even if the evidence does appear to match up\u000afor example imagine that i have various hypotheses about the nature of a newborn baby of a friend including\u000a the baby is a brownhaired boy\u000a the baby is a blondhaired girl\u000a the baby is a dog\u000athen consider two scenarios\u000aim presented with evidence in the form of a picture of a blondhaired baby girl i find this evidence supports  and opposes  and \u000aim presented with evidence in the form of a picture of a baby dog although this evidence treated in isolation supports  my prior belief in this hypothesis that a human can give birth to a dog is extremely small so the posterior probability is nevertheless small\u000athe critical point about bayesian inference then is that it provides a principled way of combining new evidence with prior beliefs through the application of bayes rule contrast this with frequentist inference which relies only on the evidence as a whole with no reference to prior beliefs furthermore bayes rule can be applied iteratively after observing some evidence the resulting posterior probability can then be treated as a prior probability and a new posterior probability computed from new evidence this allows for bayesian principles to be applied to various kinds of evidence whether viewed all at once or over time this procedure is termed bayesian updating\u000a\u000a\u000a bayesian updating \u000abayesian updating is widely used and computationally convenient however it is not the only updating rule that might be considered rational\u000aian hacking noted that traditional dutch book arguments did not specify bayesian updating they left open the possibility that nonbayesian updating rules could avoid dutch books hacking wrote and neither the dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption not one entails bayesianism so the personalist requires the dynamic assumption to be bayesian it is true that in consistency a personalist could abandon the bayesian model of learning from experience salt could lose its savour\u000aindeed there are nonbayesian updating rules that also avoid dutch books as discussed in the literature on probability kinematics following the publication of richard c jeffreys rule which applies bayes rule to the case where the evidence itself is assigned a probability the additional hypotheses needed to uniquely require bayesian updating have been deemed to be substantial complicated and unsatisfactory\u000a\u000a\u000a formal description of bayesian inference \u000a\u000a\u000a definitions \u000a a data point in general this may in fact be a vector of values\u000a the parameter of the data points distribution ie   this may in fact be a vector of parameters\u000a the hyperparameter of the parameter ie   this may in fact be a vector of hyperparameters\u000a a set of  observed data points ie \u000a a new data point whose distribution is to be predicted\u000a\u000a\u000a bayesian inference \u000athe prior distribution is the distribution of the parameters before any data is observed ie  \u000athe prior distribution might not be easily determined in this case we can use the jeffreys prior to obtain the posterior distribution before updating them with newer observations\u000athe sampling distribution is the distribution of the observed data conditional on its parameters ie   this is also termed the likelihood especially when viewed as a function of the parameters sometimes written  \u000athe marginal likelihood sometimes also termed the evidence is the distribution of the observed data marginalized over the parameters ie  \u000athe posterior distribution is the distribution of the parameters after taking into account the observed data this is determined by bayes rule which forms the heart of bayesian inference\u000a\u000anote that this is expressed in words as posterior is proportional to likelihood times prior or sometimes as posterior  likelihood times prior over evidence\u000a\u000a\u000a bayesian prediction \u000athe posterior predictive distribution is the distribution of a new data point marginalized over the posterior\u000a\u000athe prior predictive distribution is the distribution of a new data point marginalized over the prior\u000a\u000abayesian theory calls for the use of the posterior predictive distribution to do predictive inference ie to predict the distribution of a new unobserved data point that is instead of a fixed point as a prediction a distribution over possible points is returned only this way is the entire posterior distribution of the parameters used by comparison prediction in frequentist statistics often involves finding an optimum point estimate of the parameterseg by maximum likelihood or maximum a posteriori estimation mapand then plugging this estimate into the formula for the distribution of a data point this has the disadvantage that it does not account for any uncertainty in the value of the parameter and hence will underestimate the variance of the predictive distribution\u000ain some instances frequentist statistics can work around this problem for example confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a students tdistribution this correctly estimates the variance due to the fact that 1 the average of normally distributed random variables is also normally distributed 2 the predictive distribution of a normally distributed data point with unknown mean and variance using conjugate or uninformative priors has a students tdistribution in bayesian statistics however the posterior predictive distribution can always be determined exactlyor at least to an arbitrary level of precision when numerical methods are used\u000anote that both types of predictive distributions have the form of a compound probability distribution as does the marginal likelihood in fact if the prior distribution is a conjugate prior and hence the prior and posterior distributions come from the same family it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions the only difference is that the posterior predictive distribution uses the updated values of the hyperparameters applying the bayesian update rules given in the conjugate prior article while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution\u000a\u000a\u000a inference over exclusive and exhaustive possibilities \u000aif evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions bayesian inference may be thought of as acting on this belief distribution as a whole\u000a\u000a\u000a general formulation \u000a\u000asuppose a process is generating independent and identically distributed events  but the probability distribution is unknown let the event space  represent the current state of belief for this process each model is represented by event  the conditional probabilities  are specified to define the models  is the degree of belief in  before the first inference step  is a set of initial prior probabilities these must sum to 1 but are otherwise arbitrary\u000asuppose that the process is observed to generate  for each  the prior  is updated to the posterior  from bayes theorem\u000a\u000aupon observation of further evidence this procedure may be repeated\u000a\u000a\u000a multiple observations \u000afor a sequence of independent and identically distributed observations  it can be shown by induction that repeated application of the above is equivalent to\u000a\u000awhere\u000a\u000a\u000a parametric formulation \u000aby parameterizing the space of models the belief in all models may be updated in a single step the distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space the distributions in this section are expressed as continuous represented by probability densities as this is the usual situation the technique is however equally applicable to discrete distributions\u000alet the vector  span the parameter space let the initial prior distribution over  be  where  is a set of parameters to the prior itself or hyperparameters let  be a sequence of independent and identically distributed event observations where all  are distributed as  for some  bayes theorem is applied to find the posterior distribution over \u000a\u000awhere\u000a\u000a\u000a mathematical properties \u000a\u000a\u000a interpretation of factor \u000a that is if the model were true the evidence would be more likely than is predicted by the current state of belief the reverse applies for a decrease in belief if the belief does not change  that is the evidence is independent of the model if the model were true the evidence would be exactly as likely as predicted by the current state of belief\u000a\u000a\u000a cromwells rule \u000a\u000aif  then  if  then  this can be interpreted to mean that hard convictions are insensitive to counterevidence\u000athe former follows directly from bayes theorem the latter can be derived by applying the first rule to the event not  in place of  yielding if  then  from which the result immediately follows\u000a\u000a\u000a asymptotic behaviour of posterior \u000aconsider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials for sufficiently nice prior probabilities the bernsteinvon mises theorem gives that in the limit of infinite trials the posterior converges to a gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by joseph l doob in 1948 namely if the random variable in consideration has a finite probability space the more general results were obtained later by the statistician david a freedman who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed his 1963 paper treats like doob 1949 the finite case and comes to a satisfactory conclusion however if the random variable has an infinite but countable probability space ie corresponding to a die with infinite many faces the 1965 paper demonstrates that for a dense subset of priors the bernsteinvon mises theorem is not applicable in this case there is almost surely no asymptotic convergence later in the 1980s and 1990s freedman and persi diaconis continued to work on the case of infinite countable probability spaces to summarise there may be insufficient trials to suppress the effects of the initial choice and especially for large but finite systems the convergence might be very slow\u000a\u000a\u000a conjugate priors \u000a\u000ain parameterized form the prior distribution is often assumed to come from a family of distributions called conjugate priors the usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family and the calculation may be expressed in closed form\u000a\u000a\u000a estimates of parameters and predictions \u000ait is often desired to use a posterior distribution to estimate a parameter or variable several methods of bayesian estimation select measurements of central tendency from the posterior distribution\u000afor onedimensional problems a unique median exists for practical continuous problems the posterior median is attractive as a robust estimator\u000aif there exists a finite mean for the posterior distribution then the posterior mean is a method of estimation\u000a\u000ataking a value with the greatest probability defines maximum a posteriori map estimates\u000a\u000athere are examples where no maximum is attained in which case the set of map estimates is empty\u000athere are other methods of estimation that minimize the posterior risk expectedposterior loss with respect to a loss function and these are of interest to statistical decision theory using the sampling distribution frequentist statistics\u000athe posterior predictive distribution of a new observation  that is independent of previous observations is determined by\u000a\u000a\u000a examples \u000a\u000a\u000a probability of a hypothesis \u000asuppose there are two full bowls of cookies bowl 1 has 10 chocolate chip and 30 plain cookies while bowl 2 has 20 of each our friend fred picks a bowl at random and then picks a cookie at random we may assume there is no reason to believe fred treats one bowl differently from another likewise for the cookies the cookie turns out to be a plain one how probable is it that fred picked it out of bowl 1\u000aintuitively it seems clear that the answer should be more than a half since there are more plain cookies in bowl 1 the precise answer is given by bayes theorem let  correspond to bowl 1 and  to bowl 2 it is given that the bowls are identical from freds point of view thus  and the two must add up to 1 so both are equal to 05 the event  is the observation of a plain cookie from the contents of the bowls we know that  and  bayes formula then yields\u000a\u000abefore we observed the cookie the probability we assigned for fred having chosen bowl 1 was the prior probability  which was 05 after observing the cookie we must revise the probability to  which is 06\u000a\u000a\u000a making a prediction \u000a\u000aan archaeologist is working at a site thought to be from the medieval period between the 11th century to the 16th century however it is uncertain exactly when in this period the site was inhabited fragments of pottery are found some of which are glazed and some of which are decorated it is expected that if the site were inhabited during the early medieval period then 1 of the pottery would be glazed and 50 of its area decorated whereas if it had been inhabited in the late medieval period then 81 would be glazed and 5 of its area decorated how confident can the archaeologist be in the date of inhabitation as fragments are unearthed\u000athe degree of belief in the continuous variable  century is to be calculated with the discrete set of events  as evidence assuming linear variation of glaze and decoration with time and that these variables are independent\u000a\u000aassume a uniform prior of  and that trials are independent and identically distributed when a new fragment of type  is discovered bayes theorem is applied to update the degree of belief for each \u000a\u000aa computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph in the simulation the site was inhabited around 1420 or  by calculating the area under the relevant portion of the graph for 50 trials the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries about 1 chance that it was inhabited during the 13th century 63 chance during the 14th century and 36 during the 15th century note that the bernsteinvon mises theorem asserts here the asymptotic convergence to the true distribution because the probability space corresponding to the discrete set of events  is finite see above section on asymptotic behaviour of the posterior\u000a\u000a\u000a in frequentist statistics and decision theory \u000aa decisiontheoretic justification of the use of bayesian inference was given by abraham wald who proved that every unique bayesian procedure is admissible conversely every admissible statistical procedure is either a bayesian procedure or a limit of bayesian procedures\u000awald characterized admissible procedures as bayesian procedures and limits of bayesian procedures making the bayesian formalism a central technique in such areas of frequentist inference as parameter estimation hypothesis testing and computing confidence intervals for example\u000aunder some conditions all admissible procedures are either bayes procedures or limits of bayes procedures in various senses these remarkable results at least in their original form are due essentially to wald they are useful because the property of being bayes is easier to analyze than admissibility\u000ain decision theory a quite general method for proving admissibility consists in exhibiting a procedure as a unique bayes solution\u000ain the first chapters of this work prior distributions with finite support and the corresponding bayes procedures were used to establish some of the main theorems relating to the comparison of experiments bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics including its asymptotic theory there are many problems where a glance at posterior distributions for suitable priors yields immediately interesting information also this technique can hardly be avoided in sequential analysis\u000aa useful fact is that any bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible\u000aan important area of investigation in the development of admissibility ideas has been that of conventional samplingtheory procedures and many interesting results have been obtained\u000a\u000a\u000a model selection \u000a\u000a\u000a applications \u000a\u000a\u000a computer applications \u000abayesian inference has applications in artificial intelligence and expert systems bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s there is also an ever growing connection between bayesian methods and simulationbased monte carlo techniques since complex models cannot be processed in closed form by a bayesian analysis while a graphical model structure may allow for efficient simulation algorithms like the gibbs sampling and other metropolishastings algorithm schemes recently bayesian inference has gained popularity amongst the phylogenetics community for these reasons a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously\u000aas applied to statistical classification bayesian inference has been used in recent years to develop algorithms for identifying email spam applications which make use of bayesian inference for spam filtering include crm114 dspam bogofilter spamassassin spambayes mozilla xeams and others spam classification is treated in more detail in the article on the naive bayes classifier\u000asolomonoffs inductive inference is the theory of prediction based on observations for example predicting the next symbol based upon a given series of symbols the only assumption is that the environment follows some unknown but computable probability distribution it is a formal inductive framework that combines two wellstudied principles of inductive inference bayesian statistics and occams razor solomonoffs universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs for a universal computer that compute something starting with p given some p and any computable but unknown probability distribution from which x is sampled the universal prior and bayes theorem can be used to predict the yet unseen parts of x in optimal fashion\u000a\u000a\u000a in the courtroom \u000abayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant and to see whether in totality it meets their personal threshold for beyond a reasonable doubt bayes theorem is applied successively to all evidence presented with the posterior from one stage becoming the prior for the next the benefit of a bayesian approach is that it gives the juror an unbiased rational mechanism for combining evidence it may be appropriate to explain bayes theorem to jurors in odds form as betting odds are more widely understood than probabilities alternatively a logarithmic approach replacing multiplication with addition might be easier for a jury to handle\u000a\u000aif the existence of the crime is not in doubt only the identity of the culprit it has been suggested that the prior should be uniform over the qualifying population for example if 1000 people could have committed the crime the prior probability of guilt would be 11000\u000athe use of bayes theorem by jurors is controversial in the united kingdom a defence expert witness explained bayes theorem to the jury in r v adams the jury convicted but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use bayes theorem the court of appeal upheld the conviction but it also gave the opinion that to introduce bayes theorem or any similar method into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity deflecting them from their proper task\u000agardnermedwin argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt but rather the probability of the evidence given that the defendant is innocent akin to a frequentist pvalue he argues that if the posterior probability of guilt is to be computed by bayes theorem the prior probability of guilt must be known this will depend on the incidence of the crime which is an unusual piece of evidence to consider in a criminal trial consider the following three propositions\u000aa the known facts and testimony could have arisen if the defendant is guilty\u000ab the known facts and testimony could have arisen if the defendant is innocent\u000ac the defendant is guilty\u000agardnermedwin argues that the jury should believe both a and notb in order to convict a and notb implies the truth of c but the reverse is not true it is possible that b and c are both true but in this case he argues that a jury should acquit even though they know that they will be letting some guilty people go free see also lindleys paradox\u000a\u000a\u000a bayesian epistemology \u000abayesian epistemology is a movement that advocates for bayesian inference as a means of justifying the rules of inductive logic\u000akarl popper and david miller have rejected the alleged rationality of bayesianism ie using bayes rule to make epistemological inferences it is prone to the same vicious circle as any other justificationist epistemology because it presupposes what it attempts to justify according to this view a rational interpretation of bayesian inference would see it merely as a probabilistic version of falsification rejecting the belief commonly held by bayesians that high likelihood achieved by a series of bayesian updates would prove the hypothesis beyond any reasonable doubt or even with likelihood greater than 0\u000a\u000a\u000a other \u000athe scientific method is sometimes interpreted as an application of bayesian inference in this view bayes rule guides or should guide the updating of probabilities about hypotheses conditional on new observations or experiments\u000abayesian search theory is used to search for lost objects\u000abayesian inference in phylogeny\u000abayesian tool for methylation analysis\u000a\u000a\u000a bayes and bayesian inference \u000athe problem considered by bayes in proposition 9 of his essay an essay towards solving a problem in the doctrine of chances is the posterior distribution for the parameter a the success rate of the binomial distribution\u000a\u000a\u000a history \u000a\u000athe term bayesian refers to thomas bayes 17021761 who proved a special case of what is now called bayes theorem however it was pierresimon laplace 17491827 who introduced a general version of the theorem and used it to approach problems in celestial mechanics medical statistics reliability and jurisprudence early bayesian inference which used uniform priors following laplaces principle of insufficient reason was called inverse probability because it infers backwards from observations to parameters or from effects to causes after the 1920s inverse probability was largely supplanted by a collection of methods that came to be called frequentist statistics\u000ain the 20th century the ideas of laplace were further developed in two different directions giving rise to objective and subjective currents in bayesian practice in the objective or noninformative current the statistical analysis depends on only the model assumed the data analyzed and the method assigning the prior which differs from one objective bayesian to another objective bayesian in the subjective or informative current the specification of the prior depends on the belief that is propositions on which the analysis is prepared to act which can summarize information from experts previous studies etc\u000ain the 1980s there was a dramatic growth in research and applications of bayesian methods mostly attributed to the discovery of markov chain monte carlo methods which removed many of the computational problems and an increasing interest in nonstandard complex applications despite growth of bayesian research most undergraduate teaching is still based on frequentist statistics nonetheless bayesian methods are widely accepted and used such as for example in the field of machine learning\u000a\u000a\u000a see also \u000abayes theorem\u000abayesian hierarchical modeling\u000abayesian analysis the journal of the isba\u000ainductive probability\u000ainternational society for bayesian analysis isba\u000ajeffreys prior\u000a\u000a\u000a notes
p204
sg20
g23
sg24
g27
sg30
Vbayesian inference is a method of statistical inference in which bayes theorem is used to update the probability for a hypothesis as evidence bayesian inference is an important technique in statistics and especially in mathematical statistics bayesian updating is particularly important in the dynamic analysis of a sequence of data bayesian inference has found application in a wide range of activities including science engineering philosophy medicine sport and law in the philosophy of decision theory bayesian inference is closely related to subjective probability often called bayesian probability\u000a\u000a\u000a introduction to bayes rule \u000a\u000a\u000a formal \u000abayesian inference derives the posterior probability as a consequence of two antecedents a prior probability and a likelihood function derived from a statistical model for the observed data bayesian inference computes the posterior probability according to bayes theorem\u000a\u000awhere\u000a denotes a conditional probability more specifically it means given\u000a stands for any hypothesis whose probability may be affected by data called evidence below often there are competing hypotheses from which one chooses the most probable\u000athe evidence  corresponds to new data that were not used in computing the prior probability\u000a the prior probability is the probability of  before  is observed this indicates ones previous estimate of the probability that a hypothesis is true before gaining the current evidence\u000a the posterior probability is the probability of  given  ie after  is observed this tells us what we want to know the probability of a hypothesis given the observed evidence\u000a is the probability of observing  given  as a function of  with  fixed this is the likelihood the likelihood function should not be confused with  as a function of  rather than of  it indicates the compatibility of the evidence with the given hypothesis\u000a is sometimes termed the marginal likelihood or model evidence this factor is the same for all possible hypotheses being considered this can be seen by the fact that the hypothesis  does not appear anywhere in the symbol unlike for all the other factors this means that this factor does not enter into determining the relative probabilities of different hypotheses\u000anote that for different values of  only the factors  and  affect the value of  as both of these factors appear in the numerator the posterior probability is proportional to both in words\u000amore precisely the posterior probability of a hypothesis is determined by a combination of the inherent likeliness of a hypothesis the prior and the compatibility of the observed evidence with the hypothesis the likelihood\u000amore concisely posterior is proportional to likelihood times prior\u000anote that bayes rule can also be written as follows\u000a\u000awhere the factor  represents the impact of  on the probability of \u000a\u000a\u000a informal \u000aif the evidence does not match up with a hypothesis one should reject the hypothesis but if a hypothesis is extremely unlikely a priori one should also reject it even if the evidence does appear to match up\u000afor example imagine that i have various hypotheses about the nature of a newborn baby of a friend including\u000a the baby is a brownhaired boy\u000a the baby is a blondhaired girl\u000a the baby is a dog\u000athen consider two scenarios\u000aim presented with evidence in the form of a picture of a blondhaired baby girl i find this evidence supports  and opposes  and \u000aim presented with evidence in the form of a picture of a baby dog although this evidence treated in isolation supports  my prior belief in this hypothesis that a human can give birth to a dog is extremely small so the posterior probability is nevertheless small\u000athe critical point about bayesian inference then is that it provides a principled way of combining new evidence with prior beliefs through the application of bayes rule contrast this with frequentist inference which relies only on the evidence as a whole with no reference to prior beliefs furthermore bayes rule can be applied iteratively after observing some evidence the resulting posterior probability can then be treated as a prior probability and a new posterior probability computed from new evidence this allows for bayesian principles to be applied to various kinds of evidence whether viewed all at once or over time this procedure is termed bayesian updating\u000a\u000a\u000a bayesian updating \u000abayesian updating is widely used and computationally convenient however it is not the only updating rule that might be considered rational\u000aian hacking noted that traditional dutch book arguments did not specify bayesian updating they left open the possibility that nonbayesian updating rules could avoid dutch books hacking wrote and neither the dutch book argument nor any other in the personalist arsenal of proofs of the probability axioms entails the dynamic assumption not one entails bayesianism so the personalist requires the dynamic assumption to be bayesian it is true that in consistency a personalist could abandon the bayesian model of learning from experience salt could lose its savour\u000aindeed there are nonbayesian updating rules that also avoid dutch books as discussed in the literature on probability kinematics following the publication of richard c jeffreys rule which applies bayes rule to the case where the evidence itself is assigned a probability the additional hypotheses needed to uniquely require bayesian updating have been deemed to be substantial complicated and unsatisfactory\u000a\u000a\u000a formal description of bayesian inference \u000a\u000a\u000a definitions \u000a a data point in general this may in fact be a vector of values\u000a the parameter of the data points distribution ie   this may in fact be a vector of parameters\u000a the hyperparameter of the parameter ie   this may in fact be a vector of hyperparameters\u000a a set of  observed data points ie \u000a a new data point whose distribution is to be predicted\u000a\u000a\u000a bayesian inference \u000athe prior distribution is the distribution of the parameters before any data is observed ie  \u000athe prior distribution might not be easily determined in this case we can use the jeffreys prior to obtain the posterior distribution before updating them with newer observations\u000athe sampling distribution is the distribution of the observed data conditional on its parameters ie   this is also termed the likelihood especially when viewed as a function of the parameters sometimes written  \u000athe marginal likelihood sometimes also termed the evidence is the distribution of the observed data marginalized over the parameters ie  \u000athe posterior distribution is the distribution of the parameters after taking into account the observed data this is determined by bayes rule which forms the heart of bayesian inference\u000a\u000anote that this is expressed in words as posterior is proportional to likelihood times prior or sometimes as posterior  likelihood times prior over evidence\u000a\u000a\u000a bayesian prediction \u000athe posterior predictive distribution is the distribution of a new data point marginalized over the posterior\u000a\u000athe prior predictive distribution is the distribution of a new data point marginalized over the prior\u000a\u000abayesian theory calls for the use of the posterior predictive distribution to do predictive inference ie to predict the distribution of a new unobserved data point that is instead of a fixed point as a prediction a distribution over possible points is returned only this way is the entire posterior distribution of the parameters used by comparison prediction in frequentist statistics often involves finding an optimum point estimate of the parameterseg by maximum likelihood or maximum a posteriori estimation mapand then plugging this estimate into the formula for the distribution of a data point this has the disadvantage that it does not account for any uncertainty in the value of the parameter and hence will underestimate the variance of the predictive distribution\u000ain some instances frequentist statistics can work around this problem for example confidence intervals and prediction intervals in frequentist statistics when constructed from a normal distribution with unknown mean and variance are constructed using a students tdistribution this correctly estimates the variance due to the fact that 1 the average of normally distributed random variables is also normally distributed 2 the predictive distribution of a normally distributed data point with unknown mean and variance using conjugate or uninformative priors has a students tdistribution in bayesian statistics however the posterior predictive distribution can always be determined exactlyor at least to an arbitrary level of precision when numerical methods are used\u000anote that both types of predictive distributions have the form of a compound probability distribution as does the marginal likelihood in fact if the prior distribution is a conjugate prior and hence the prior and posterior distributions come from the same family it can easily be seen that both prior and posterior predictive distributions also come from the same family of compound distributions the only difference is that the posterior predictive distribution uses the updated values of the hyperparameters applying the bayesian update rules given in the conjugate prior article while the prior predictive distribution uses the values of the hyperparameters that appear in the prior distribution\u000a\u000a\u000a inference over exclusive and exhaustive possibilities \u000aif evidence is simultaneously used to update belief over a set of exclusive and exhaustive propositions bayesian inference may be thought of as acting on this belief distribution as a whole\u000a\u000a\u000a general formulation \u000a\u000asuppose a process is generating independent and identically distributed events  but the probability distribution is unknown let the event space  represent the current state of belief for this process each model is represented by event  the conditional probabilities  are specified to define the models  is the degree of belief in  before the first inference step  is a set of initial prior probabilities these must sum to 1 but are otherwise arbitrary\u000asuppose that the process is observed to generate  for each  the prior  is updated to the posterior  from bayes theorem\u000a\u000aupon observation of further evidence this procedure may be repeated\u000a\u000a\u000a multiple observations \u000afor a sequence of independent and identically distributed observations  it can be shown by induction that repeated application of the above is equivalent to\u000a\u000awhere\u000a\u000a\u000a parametric formulation \u000aby parameterizing the space of models the belief in all models may be updated in a single step the distribution of belief over the model space may then be thought of as a distribution of belief over the parameter space the distributions in this section are expressed as continuous represented by probability densities as this is the usual situation the technique is however equally applicable to discrete distributions\u000alet the vector  span the parameter space let the initial prior distribution over  be  where  is a set of parameters to the prior itself or hyperparameters let  be a sequence of independent and identically distributed event observations where all  are distributed as  for some  bayes theorem is applied to find the posterior distribution over \u000a\u000awhere\u000a\u000a\u000a mathematical properties \u000a\u000a\u000a interpretation of factor \u000a that is if the model were true the evidence would be more likely than is predicted by the current state of belief the reverse applies for a decrease in belief if the belief does not change  that is the evidence is independent of the model if the model were true the evidence would be exactly as likely as predicted by the current state of belief\u000a\u000a\u000a cromwells rule \u000a\u000aif  then  if  then  this can be interpreted to mean that hard convictions are insensitive to counterevidence\u000athe former follows directly from bayes theorem the latter can be derived by applying the first rule to the event not  in place of  yielding if  then  from which the result immediately follows\u000a\u000a\u000a asymptotic behaviour of posterior \u000aconsider the behaviour of a belief distribution as it is updated a large number of times with independent and identically distributed trials for sufficiently nice prior probabilities the bernsteinvon mises theorem gives that in the limit of infinite trials the posterior converges to a gaussian distribution independent of the initial prior under some conditions firstly outlined and rigorously proven by joseph l doob in 1948 namely if the random variable in consideration has a finite probability space the more general results were obtained later by the statistician david a freedman who published in two seminal research papers in 1963 and 1965 when and under what circumstances the asymptotic behaviour of posterior is guaranteed his 1963 paper treats like doob 1949 the finite case and comes to a satisfactory conclusion however if the random variable has an infinite but countable probability space ie corresponding to a die with infinite many faces the 1965 paper demonstrates that for a dense subset of priors the bernsteinvon mises theorem is not applicable in this case there is almost surely no asymptotic convergence later in the 1980s and 1990s freedman and persi diaconis continued to work on the case of infinite countable probability spaces to summarise there may be insufficient trials to suppress the effects of the initial choice and especially for large but finite systems the convergence might be very slow\u000a\u000a\u000a conjugate priors \u000a\u000ain parameterized form the prior distribution is often assumed to come from a family of distributions called conjugate priors the usefulness of a conjugate prior is that the corresponding posterior distribution will be in the same family and the calculation may be expressed in closed form\u000a\u000a\u000a estimates of parameters and predictions \u000ait is often desired to use a posterior distribution to estimate a parameter or variable several methods of bayesian estimation select measurements of central tendency from the posterior distribution\u000afor onedimensional problems a unique median exists for practical continuous problems the posterior median is attractive as a robust estimator\u000aif there exists a finite mean for the posterior distribution then the posterior mean is a method of estimation\u000a\u000ataking a value with the greatest probability defines maximum a posteriori map estimates\u000a\u000athere are examples where no maximum is attained in which case the set of map estimates is empty\u000athere are other methods of estimation that minimize the posterior risk expectedposterior loss with respect to a loss function and these are of interest to statistical decision theory using the sampling distribution frequentist statistics\u000athe posterior predictive distribution of a new observation  that is independent of previous observations is determined by\u000a\u000a\u000a examples \u000a\u000a\u000a probability of a hypothesis \u000asuppose there are two full bowls of cookies bowl 1 has 10 chocolate chip and 30 plain cookies while bowl 2 has 20 of each our friend fred picks a bowl at random and then picks a cookie at random we may assume there is no reason to believe fred treats one bowl differently from another likewise for the cookies the cookie turns out to be a plain one how probable is it that fred picked it out of bowl 1\u000aintuitively it seems clear that the answer should be more than a half since there are more plain cookies in bowl 1 the precise answer is given by bayes theorem let  correspond to bowl 1 and  to bowl 2 it is given that the bowls are identical from freds point of view thus  and the two must add up to 1 so both are equal to 05 the event  is the observation of a plain cookie from the contents of the bowls we know that  and  bayes formula then yields\u000a\u000abefore we observed the cookie the probability we assigned for fred having chosen bowl 1 was the prior probability  which was 05 after observing the cookie we must revise the probability to  which is 06\u000a\u000a\u000a making a prediction \u000a\u000aan archaeologist is working at a site thought to be from the medieval period between the 11th century to the 16th century however it is uncertain exactly when in this period the site was inhabited fragments of pottery are found some of which are glazed and some of which are decorated it is expected that if the site were inhabited during the early medieval period then 1 of the pottery would be glazed and 50 of its area decorated whereas if it had been inhabited in the late medieval period then 81 would be glazed and 5 of its area decorated how confident can the archaeologist be in the date of inhabitation as fragments are unearthed\u000athe degree of belief in the continuous variable  century is to be calculated with the discrete set of events  as evidence assuming linear variation of glaze and decoration with time and that these variables are independent\u000a\u000aassume a uniform prior of  and that trials are independent and identically distributed when a new fragment of type  is discovered bayes theorem is applied to update the degree of belief for each \u000a\u000aa computer simulation of the changing belief as 50 fragments are unearthed is shown on the graph in the simulation the site was inhabited around 1420 or  by calculating the area under the relevant portion of the graph for 50 trials the archaeologist can say that there is practically no chance the site was inhabited in the 11th and 12th centuries about 1 chance that it was inhabited during the 13th century 63 chance during the 14th century and 36 during the 15th century note that the bernsteinvon mises theorem asserts here the asymptotic convergence to the true distribution because the probability space corresponding to the discrete set of events  is finite see above section on asymptotic behaviour of the posterior\u000a\u000a\u000a in frequentist statistics and decision theory \u000aa decisiontheoretic justification of the use of bayesian inference was given by abraham wald who proved that every unique bayesian procedure is admissible conversely every admissible statistical procedure is either a bayesian procedure or a limit of bayesian procedures\u000awald characterized admissible procedures as bayesian procedures and limits of bayesian procedures making the bayesian formalism a central technique in such areas of frequentist inference as parameter estimation hypothesis testing and computing confidence intervals for example\u000aunder some conditions all admissible procedures are either bayes procedures or limits of bayes procedures in various senses these remarkable results at least in their original form are due essentially to wald they are useful because the property of being bayes is easier to analyze than admissibility\u000ain decision theory a quite general method for proving admissibility consists in exhibiting a procedure as a unique bayes solution\u000ain the first chapters of this work prior distributions with finite support and the corresponding bayes procedures were used to establish some of the main theorems relating to the comparison of experiments bayes procedures with respect to more general prior distributions have played a very important role in the development of statistics including its asymptotic theory there are many problems where a glance at posterior distributions for suitable priors yields immediately interesting information also this technique can hardly be avoided in sequential analysis\u000aa useful fact is that any bayes decision rule obtained by taking a proper prior over the whole parameter space must be admissible\u000aan important area of investigation in the development of admissibility ideas has been that of conventional samplingtheory procedures and many interesting results have been obtained\u000a\u000a\u000a model selection \u000a\u000a\u000a applications \u000a\u000a\u000a computer applications \u000abayesian inference has applications in artificial intelligence and expert systems bayesian inference techniques have been a fundamental part of computerized pattern recognition techniques since the late 1950s there is also an ever growing connection between bayesian methods and simulationbased monte carlo techniques since complex models cannot be processed in closed form by a bayesian analysis while a graphical model structure may allow for efficient simulation algorithms like the gibbs sampling and other metropolishastings algorithm schemes recently bayesian inference has gained popularity amongst the phylogenetics community for these reasons a number of applications allow many demographic and evolutionary parameters to be estimated simultaneously\u000aas applied to statistical classification bayesian inference has been used in recent years to develop algorithms for identifying email spam applications which make use of bayesian inference for spam filtering include crm114 dspam bogofilter spamassassin spambayes mozilla xeams and others spam classification is treated in more detail in the article on the naive bayes classifier\u000asolomonoffs inductive inference is the theory of prediction based on observations for example predicting the next symbol based upon a given series of symbols the only assumption is that the environment follows some unknown but computable probability distribution it is a formal inductive framework that combines two wellstudied principles of inductive inference bayesian statistics and occams razor solomonoffs universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs for a universal computer that compute something starting with p given some p and any computable but unknown probability distribution from which x is sampled the universal prior and bayes theorem can be used to predict the yet unseen parts of x in optimal fashion\u000a\u000a\u000a in the courtroom \u000abayesian inference can be used by jurors to coherently accumulate the evidence for and against a defendant and to see whether in totality it meets their personal threshold for beyond a reasonable doubt bayes theorem is applied successively to all evidence presented with the posterior from one stage becoming the prior for the next the benefit of a bayesian approach is that it gives the juror an unbiased rational mechanism for combining evidence it may be appropriate to explain bayes theorem to jurors in odds form as betting odds are more widely understood than probabilities alternatively a logarithmic approach replacing multiplication with addition might be easier for a jury to handle\u000a\u000aif the existence of the crime is not in doubt only the identity of the culprit it has been suggested that the prior should be uniform over the qualifying population for example if 1000 people could have committed the crime the prior probability of guilt would be 11000\u000athe use of bayes theorem by jurors is controversial in the united kingdom a defence expert witness explained bayes theorem to the jury in r v adams the jury convicted but the case went to appeal on the basis that no means of accumulating evidence had been provided for jurors who did not wish to use bayes theorem the court of appeal upheld the conviction but it also gave the opinion that to introduce bayes theorem or any similar method into a criminal trial plunges the jury into inappropriate and unnecessary realms of theory and complexity deflecting them from their proper task\u000agardnermedwin argues that the criterion on which a verdict in a criminal trial should be based is not the probability of guilt but rather the probability of the evidence given that the defendant is innocent akin to a frequentist pvalue he argues that if the posterior probability of guilt is to be computed by bayes theorem the prior probability of guilt must be known this will depend on the incidence of the crime which is an unusual piece of evidence to consider in a criminal trial consider the following three propositions\u000aa the known facts and testimony could have arisen if the defendant is guilty\u000ab the known facts and testimony could have arisen if the defendant is innocent\u000ac the defendant is guilty\u000agardnermedwin argues that the jury should believe both a and notb in order to convict a and notb implies the truth of c but the reverse is not true it is possible that b and c are both true but in this case he argues that a jury should acquit even though they know that they will be letting some guilty people go free see also lindleys paradox\u000a\u000a\u000a bayesian epistemology \u000abayesian epistemology is a movement that advocates for bayesian inference as a means of justifying the rules of inductive logic\u000akarl popper and david miller have rejected the alleged rationality of bayesianism ie using bayes rule to make epistemological inferences it is prone to the same vicious circle as any other justificationist epistemology because it presupposes what it attempts to justify according to this view a rational interpretation of bayesian inference would see it merely as a probabilistic version of falsification rejecting the belief commonly held by bayesians that high likelihood achieved by a series of bayesian updates would prove the hypothesis beyond any reasonable doubt or even with likelihood greater than 0\u000a\u000a\u000a other \u000athe scientific method is sometimes interpreted as an application of bayesian inference in this view bayes rule guides or should guide the updating of probabilities about hypotheses conditional on new observations or experiments\u000abayesian search theory is used to search for lost objects\u000abayesian inference in phylogeny\u000abayesian tool for methylation analysis\u000a\u000a\u000a bayes and bayesian inference \u000athe problem considered by bayes in proposition 9 of his essay an essay towards solving a problem in the doctrine of chances is the posterior distribution for the parameter a the success rate of the binomial distribution\u000a\u000a\u000a history \u000a\u000athe term bayesian refers to thomas bayes 17021761 who proved a special case of what is now called bayes theorem however it was pierresimon laplace 17491827 who introduced a general version of the theorem and used it to approach problems in celestial mechanics medical statistics reliability and jurisprudence early bayesian inference which used uniform priors following laplaces principle of insufficient reason was called inverse probability because it infers backwards from observations to parameters or from effects to causes after the 1920s inverse probability was largely supplanted by a collection of methods that came to be called frequentist statistics\u000ain the 20th century the ideas of laplace were further developed in two different directions giving rise to objective and subjective currents in bayesian practice in the objective or noninformative current the statistical analysis depends on only the model assumed the data analyzed and the method assigning the prior which differs from one objective bayesian to another objective bayesian in the subjective or informative current the specification of the prior depends on the belief that is propositions on which the analysis is prepared to act which can summarize information from experts previous studies etc\u000ain the 1980s there was a dramatic growth in research and applications of bayesian methods mostly attributed to the discovery of markov chain monte carlo methods which removed many of the computational problems and an increasing interest in nonstandard complex applications despite growth of bayesian research most undergraduate teaching is still based on frequentist statistics nonetheless bayesian methods are widely accepted and used such as for example in the field of machine learning\u000a\u000a\u000a see also \u000abayes theorem\u000abayesian hierarchical modeling\u000abayesian analysis the journal of the isba\u000ainductive probability\u000ainternational society for bayesian analysis isba\u000ajeffreys prior\u000a\u000a\u000a notes \u000a\u000a\u000a
p205
sg32
g35
sg37
NsbsS'sufficient_statistic.txt'
p206
g2
(g3
g4
Ntp207
Rp208
(dp209
g8
g11
sg12
Vin statistics a statistic is sufficient with respect to a statistical model and its associated unknown parameter if no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter in particular a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic as to which of those probability distributions is that of the population from which the sample was taken\u000aroughly given a set  of independent identically distributed data conditioned on an unknown parameter  a sufficient statistic is a function  whose value contains all the information needed to compute any estimate of the parameter eg a maximum likelihood estimate due to the factorization theorem see below for a sufficient statistic  the joint distribution can be written as  from this factorization it can easily be seen that the maximum likelihood estimate of  will interact with  only through  typically the sufficient statistic is a simple function of the data eg the sum of all the data points\u000amore generally the unknown parameter may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified in such a case the sufficient statistic may be a set of functions called a jointly sufficient statistic typically there are as many functions as there are parameters for example for a gaussian distribution with unknown mean and variance the jointly sufficient statistic from which maximum likelihood estimates of both parameters can be estimated consists of two functions the sum of all data points and the sum of all squared data points or equivalently the sample mean and sample variance\u000athe concept due to ronald fisher is equivalent to the statement that conditional on the value of a sufficient statistic for a parameter the joint probability distribution of the data does not depend on that parameter both the statistic and the underlying parameter can be vectors\u000aa related concept is that of linear sufficiency which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic although it is restricted to linear estimators the kolmogorov structure function deals with individual finite data the related notion there is the algorithmic sufficient statistic\u000athe concept of sufficiency has fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form see pitmankoopmandarmois theorem below but remains very important in theoretical work\u000a\u000a\u000a mathematical definitionedit \u000aa statistic tx is sufficient for underlying parameter  precisely if the conditional probability distribution of the data x given the statistic tx does not depend on the parameter  ie\u000a\u000ainstead of this expression the definition still holds if one uses either of the equivalent expressions\u000a or\u000a\u000awhich indicate respectively that the conditional probability of the parameter  given the sufficient statistic t does not depend on the data x and that the conditional probability of the parameter  given the sufficient statistic t and the conditional probability of the data x given the sufficient statistic t are statistically independent\u000a\u000a\u000a exampleedit \u000aas an example the sample mean is sufficient for the mean  of a normal distribution with known variance once the sample mean is known no further information about  can be obtained from the sample itself on the other hand for an arbitrary distribution the median is not sufficient for the mean even if the median of the sample is known knowing the sample itself would provide further information about the population mean for example if the observations that are less than the median are only slightly less but observations exceeding the median exceed it by a large amount then this would have a bearing on ones inference about the population mean\u000a\u000a\u000a fisherneyman factorization theoremedit \u000afishers factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic if the probability density function is x then t is sufficient for  if and only if nonnegative functions g and h can be found such that\u000a\u000aie the density  can be factored into a product such that one factor h does not depend on  and the other factor which does depend on  depends on x only through tx\u000ait is easy to see that if ft is a one to one function and t is a sufficient statistic then ft is a sufficient statistic in particular we can multiply a sufficient statistic by a nonzero constant and get another sufficient statistic\u000a\u000a\u000a likelihood principle interpretationedit \u000aan implication of the theorem is that when using likelihoodbased inference two sets of data yielding the same value for the sufficient statistic tx will always yield the same inferences about  by the factorization criterion the likelihoods dependence on  is only in conjunction with tx as this is the same in both cases the dependence on  will be the same as well leading to identical inferences\u000a\u000a\u000a proofedit \u000adue to hogg and craig let  denote a random sample from a distribution having the pdf fx  for      let y1  u1x1 x2  xn be a statistic whose pdf is g1y1  then y1  u1x1 x2  xn is a sufficient statistic for  if and only if for some function h\u000a\u000afirst suppose that\u000a\u000awe shall make the transformation yi  uix1 x2  xn for i  1  n having inverse functions xi  wiy1 y2  yn for i  1  n and jacobian  thus\u000a\u000athe lefthand member is the joint pdf gy1 y2  yn  of y1  u1x1  xn  yn  unx1  xn in the righthand member  is the pdf of  so that  is the quotient of  and  that is it is the conditional pdf  of  given \u000abut  and thus  was given not to depend upon  since  was not introduced in the transformation and accordingly not in the jacobian  it follows that  does not depend upon  and that  is a sufficient statistics for \u000athe converse is proven by taking\u000a\u000awhere  does not depend upon  because  depend only upon  which are independent on  when conditioned by  a sufficient statistics by hypothesis now divide both members by the absolute value of the nonvanishing jacobian  and replace  by the functions  in  this yields\u000a\u000awhere  is the jacobian with  replaced by their value in terms  the lefthand member is necessarily the joint pdf  of  since  and thus  does not depend upon  then\u000a\u000ais a function that does not depend upon \u000a\u000a\u000a another proofedit \u000aa simpler more illustrative proof is as follows although it applies only in the discrete case\u000awe use the shorthand notation to denote the joint probability of  by  since  is a function of  we have  only when  and zero otherwise and thus\u000a\u000awith the last equality being true by the definition of conditional probability distributions thus  with  and \u000areciprocally if  we have\u000a\u000awith the first equality by the definition of pdf for multiple variables the second by the remark above the third by hypothesis and the fourth because the summation is not over \u000athus the conditional probability distribution is\u000a\u000awith the first equality by definition of conditional probability density the second by the remark above the third by the equality proven above and the fourth by simplification this expression does not depend on  and thus  is a sufficient statistic\u000a\u000a\u000a minimal sufficiencyedit \u000aa sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic in other words sx is minimal sufficient if and only if\u000asx is sufficient and\u000aif tx is sufficient then there exists a function f such that sx  ftx\u000aintuitively a minimal sufficient statistic most efficiently captures all possible information about the parameter \u000aa useful characterization of minimal sufficiency is that when the density f exists sx is minimal sufficient if and only if\u000a is independent of   sx  sy\u000athis follows as a direct consequence from fishers factorization theorem stated above\u000aa case in which there is no minimal sufficient statistic was shown by bahadur 1954 however under mild conditions a minimal sufficient statistic does always exist in particular in euclidean space these conditions always hold if the random variables associated with   are all discrete or are all continuous\u000aif there exists a minimal sufficient statistic and this is usually the case then every complete sufficient statistic is necessarily minimal sufficientnote that this statement does not exclude the option of a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic while it is hard to find cases in which a minimal sufficient statistic does not exist it is not so hard to find cases in which there is no complete statistic\u000athe collection of likelihood ratios  is a minimal sufficient statistic if  is discrete or has a density function\u000a\u000a\u000a examplesedit \u000a\u000a\u000a bernoulli distributionedit \u000aif x1  xn are independent bernoullidistributed random variables with expected value p then the sum tx  x1    xn is a sufficient statistic for p here success corresponds to xi  1 and failure to xi  0 so t is the total number of successes\u000athis is seen by considering the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000aand collecting powers of p and 1  p gives\u000a\u000awhich satisfies the factorization criterion with hx  1 being just a constant\u000anote the crucial feature the unknown parameter p interacts with the data x only via the statistic tx   xi\u000aas a concrete application this gives a procedure for creating a fair coin from a biased coin\u000a\u000a\u000a uniform distributionedit \u000a\u000aif x1  xn are independent and uniformly distributed on the interval 0 then tx  maxx1  xn is sufficient for   the sample maximum is a sufficient statistic for the population maximum\u000ato see this consider the joint probability density function of xx1xn because the observations are independent the pdf can be written as a product of individual densities\u000a\u000awhere 1 is the indicator function thus the density takes form required by the fisherneyman factorization theorem where hx  1minxi0 and the rest of the expression is a function of only  and tx  maxxi\u000ain fact the minimumvariance unbiased estimator mvue for  is\u000a\u000athis is the sample maximum scaled to correct for the bias and is mvue by the lehmannscheff theorem unscaled sample maximum tx is the maximum likelihood estimator for \u000a\u000a\u000a uniform distribution with two parametersedit \u000aif  are independent and uniformly distributed on the interval  where  and  are unknown parameters then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a poisson distributionedit \u000aif x1  xn are independent and have a poisson distribution with parameter  then the sum tx  x1    xn is a sufficient statistic for \u000ato see this consider the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000awhich may be written as\u000a\u000awhich shows that the factorization criterion is satisfied where hx is the reciprocal of the product of the factorials note the parameter  interacts with the data only through its sum tx\u000a\u000a\u000a normal distributionedit \u000aif  are independent and normally distributed with expected value  a parameter and known finite variance  then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athen since  which can be shown simply by expanding this term\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a exponential distributionedit \u000aif  are independent and exponentially distributed with expected value  an unknown realvalued positive parameter then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a gamma distributionedit \u000aif  are independent and distributed as a  where  and  are unknown parameters of a gamma distribution then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a raoblackwell theoremedit \u000asufficiency finds a useful application in the raoblackwell theorem which states that if gx is any kind of estimator of  then typically the conditional expectation of gx given sufficient statistic tx is a better estimator of  and is never worse sometimes one can very easily construct a very crude estimator gx and then evaluate that conditional expected value to get an estimator that is in various senses optimal\u000a\u000a\u000a exponential familyedit \u000a\u000aaccording to the pitmankoopmandarmois theorem among families of probability distributions whose domain does not vary with the parameter being estimated only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases less tersely suppose  are independent identically distributed random variables whose distribution is known to be in some family of probability distributions only if that family is an exponential family is there a possibly vectorvalued sufficient statistic  whose number of scalar components does not increase as the sample size n increases\u000athis theorem shows that sufficiency or rather the existence of a scalar or vectorvalued of bounded dimension sufficient statistic sharply restricts the possible forms of the distribution\u000a\u000a\u000a other types of sufficiencyedit \u000a\u000a\u000a bayesian sufficiencyedit \u000aan alternative formulation of the condition that a statistic be sufficient set in a bayesian context involves the posterior distributions obtained by using the full dataset and by using only a statistic thus the requirement is that for almost every x\u000a\u000ait turns out that this bayesian sufficiency is a consequence of the formulation above however they are not directly equivalent in the infinitedimensional case a range of theoretical results for sufficiency in a bayesian context is available\u000a\u000a\u000a linear sufficiencyedit \u000aa concept called linear sufficiency can be formulated in a bayesian context and more generally first define the best linear predictor of a vector y based on x as  then a linear statistic tx is linear sufficient if\u000a\u000a\u000a see alsoedit \u000acompleteness of a statistic\u000abasus theorem on independence of complete sufficient and ancillary statistics\u000alehmannscheff theorem a complete sufficient estimator is the best estimator of its expectation\u000araoblackwell theorem\u000asufficient dimension reduction\u000aancillary statistic\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000akholevo as 2001 sufficient statistic in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer chapter 4 isbn 0387985026 \u000adodge y 2003 the oxford dictionary of statistical terms oup isbn 0199206139
p210
sg14
g17
sg18
Vin statistics a statistic is sufficient with respect to a statistical model and its associated unknown parameter if no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter in particular a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic as to which of those probability distributions is that of the population from which the sample was taken\u000aroughly given a set  of independent identically distributed data conditioned on an unknown parameter  a sufficient statistic is a function  whose value contains all the information needed to compute any estimate of the parameter eg a maximum likelihood estimate due to the factorization theorem see below for a sufficient statistic  the joint distribution can be written as  from this factorization it can easily be seen that the maximum likelihood estimate of  will interact with  only through  typically the sufficient statistic is a simple function of the data eg the sum of all the data points\u000amore generally the unknown parameter may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified in such a case the sufficient statistic may be a set of functions called a jointly sufficient statistic typically there are as many functions as there are parameters for example for a gaussian distribution with unknown mean and variance the jointly sufficient statistic from which maximum likelihood estimates of both parameters can be estimated consists of two functions the sum of all data points and the sum of all squared data points or equivalently the sample mean and sample variance\u000athe concept due to ronald fisher is equivalent to the statement that conditional on the value of a sufficient statistic for a parameter the joint probability distribution of the data does not depend on that parameter both the statistic and the underlying parameter can be vectors\u000aa related concept is that of linear sufficiency which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic although it is restricted to linear estimators the kolmogorov structure function deals with individual finite data the related notion there is the algorithmic sufficient statistic\u000athe concept of sufficiency has fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form see pitmankoopmandarmois theorem below but remains very important in theoretical work\u000a\u000a\u000a mathematical definitionedit \u000aa statistic tx is sufficient for underlying parameter  precisely if the conditional probability distribution of the data x given the statistic tx does not depend on the parameter  ie\u000a\u000ainstead of this expression the definition still holds if one uses either of the equivalent expressions\u000a or\u000a\u000awhich indicate respectively that the conditional probability of the parameter  given the sufficient statistic t does not depend on the data x and that the conditional probability of the parameter  given the sufficient statistic t and the conditional probability of the data x given the sufficient statistic t are statistically independent\u000a\u000a\u000a exampleedit \u000aas an example the sample mean is sufficient for the mean  of a normal distribution with known variance once the sample mean is known no further information about  can be obtained from the sample itself on the other hand for an arbitrary distribution the median is not sufficient for the mean even if the median of the sample is known knowing the sample itself would provide further information about the population mean for example if the observations that are less than the median are only slightly less but observations exceeding the median exceed it by a large amount then this would have a bearing on ones inference about the population mean\u000a\u000a\u000a fisherneyman factorization theoremedit \u000afishers factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic if the probability density function is x then t is sufficient for  if and only if nonnegative functions g and h can be found such that\u000a\u000aie the density  can be factored into a product such that one factor h does not depend on  and the other factor which does depend on  depends on x only through tx\u000ait is easy to see that if ft is a one to one function and t is a sufficient statistic then ft is a sufficient statistic in particular we can multiply a sufficient statistic by a nonzero constant and get another sufficient statistic\u000a\u000a\u000a likelihood principle interpretationedit \u000aan implication of the theorem is that when using likelihoodbased inference two sets of data yielding the same value for the sufficient statistic tx will always yield the same inferences about  by the factorization criterion the likelihoods dependence on  is only in conjunction with tx as this is the same in both cases the dependence on  will be the same as well leading to identical inferences\u000a\u000a\u000a proofedit \u000adue to hogg and craig let  denote a random sample from a distribution having the pdf fx  for      let y1  u1x1 x2  xn be a statistic whose pdf is g1y1  then y1  u1x1 x2  xn is a sufficient statistic for  if and only if for some function h\u000a\u000afirst suppose that\u000a\u000awe shall make the transformation yi  uix1 x2  xn for i  1  n having inverse functions xi  wiy1 y2  yn for i  1  n and jacobian  thus\u000a\u000athe lefthand member is the joint pdf gy1 y2  yn  of y1  u1x1  xn  yn  unx1  xn in the righthand member  is the pdf of  so that  is the quotient of  and  that is it is the conditional pdf  of  given \u000abut  and thus  was given not to depend upon  since  was not introduced in the transformation and accordingly not in the jacobian  it follows that  does not depend upon  and that  is a sufficient statistics for \u000athe converse is proven by taking\u000a\u000awhere  does not depend upon  because  depend only upon  which are independent on  when conditioned by  a sufficient statistics by hypothesis now divide both members by the absolute value of the nonvanishing jacobian  and replace  by the functions  in  this yields\u000a\u000awhere  is the jacobian with  replaced by their value in terms  the lefthand member is necessarily the joint pdf  of  since  and thus  does not depend upon  then\u000a\u000ais a function that does not depend upon \u000a\u000a\u000a another proofedit \u000aa simpler more illustrative proof is as follows although it applies only in the discrete case\u000awe use the shorthand notation to denote the joint probability of  by  since  is a function of  we have  only when  and zero otherwise and thus\u000a\u000awith the last equality being true by the definition of conditional probability distributions thus  with  and \u000areciprocally if  we have\u000a\u000awith the first equality by the definition of pdf for multiple variables the second by the remark above the third by hypothesis and the fourth because the summation is not over \u000athus the conditional probability distribution is\u000a\u000awith the first equality by definition of conditional probability density the second by the remark above the third by the equality proven above and the fourth by simplification this expression does not depend on  and thus  is a sufficient statistic\u000a\u000a\u000a minimal sufficiencyedit \u000aa sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic in other words sx is minimal sufficient if and only if\u000asx is sufficient and\u000aif tx is sufficient then there exists a function f such that sx  ftx\u000aintuitively a minimal sufficient statistic most efficiently captures all possible information about the parameter \u000aa useful characterization of minimal sufficiency is that when the density f exists sx is minimal sufficient if and only if\u000a is independent of   sx  sy\u000athis follows as a direct consequence from fishers factorization theorem stated above\u000aa case in which there is no minimal sufficient statistic was shown by bahadur 1954 however under mild conditions a minimal sufficient statistic does always exist in particular in euclidean space these conditions always hold if the random variables associated with   are all discrete or are all continuous\u000aif there exists a minimal sufficient statistic and this is usually the case then every complete sufficient statistic is necessarily minimal sufficientnote that this statement does not exclude the option of a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic while it is hard to find cases in which a minimal sufficient statistic does not exist it is not so hard to find cases in which there is no complete statistic\u000athe collection of likelihood ratios  is a minimal sufficient statistic if  is discrete or has a density function\u000a\u000a\u000a examplesedit \u000a\u000a\u000a bernoulli distributionedit \u000aif x1  xn are independent bernoullidistributed random variables with expected value p then the sum tx  x1    xn is a sufficient statistic for p here success corresponds to xi  1 and failure to xi  0 so t is the total number of successes\u000athis is seen by considering the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000aand collecting powers of p and 1  p gives\u000a\u000awhich satisfies the factorization criterion with hx  1 being just a constant\u000anote the crucial feature the unknown parameter p interacts with the data x only via the statistic tx   xi\u000aas a concrete application this gives a procedure for creating a fair coin from a biased coin\u000a\u000a\u000a uniform distributionedit \u000a\u000aif x1  xn are independent and uniformly distributed on the interval 0 then tx  maxx1  xn is sufficient for   the sample maximum is a sufficient statistic for the population maximum\u000ato see this consider the joint probability density function of xx1xn because the observations are independent the pdf can be written as a product of individual densities\u000a\u000awhere 1 is the indicator function thus the density takes form required by the fisherneyman factorization theorem where hx  1minxi0 and the rest of the expression is a function of only  and tx  maxxi\u000ain fact the minimumvariance unbiased estimator mvue for  is\u000a\u000athis is the sample maximum scaled to correct for the bias and is mvue by the lehmannscheff theorem unscaled sample maximum tx is the maximum likelihood estimator for \u000a\u000a\u000a uniform distribution with two parametersedit \u000aif  are independent and uniformly distributed on the interval  where  and  are unknown parameters then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a poisson distributionedit \u000aif x1  xn are independent and have a poisson distribution with parameter  then the sum tx  x1    xn is a sufficient statistic for \u000ato see this consider the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000awhich may be written as\u000a\u000awhich shows that the factorization criterion is satisfied where hx is the reciprocal of the product of the factorials note the parameter  interacts with the data only through its sum tx\u000a\u000a\u000a normal distributionedit \u000aif  are independent and normally distributed with expected value  a parameter and known finite variance  then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athen since  which can be shown simply by expanding this term\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a exponential distributionedit \u000aif  are independent and exponentially distributed with expected value  an unknown realvalued positive parameter then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a gamma distributionedit \u000aif  are independent and distributed as a  where  and  are unknown parameters of a gamma distribution then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a raoblackwell theoremedit \u000asufficiency finds a useful application in the raoblackwell theorem which states that if gx is any kind of estimator of  then typically the conditional expectation of gx given sufficient statistic tx is a better estimator of  and is never worse sometimes one can very easily construct a very crude estimator gx and then evaluate that conditional expected value to get an estimator that is in various senses optimal\u000a\u000a\u000a exponential familyedit \u000a\u000aaccording to the pitmankoopmandarmois theorem among families of probability distributions whose domain does not vary with the parameter being estimated only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases less tersely suppose  are independent identically distributed random variables whose distribution is known to be in some family of probability distributions only if that family is an exponential family is there a possibly vectorvalued sufficient statistic  whose number of scalar components does not increase as the sample size n increases\u000athis theorem shows that sufficiency or rather the existence of a scalar or vectorvalued of bounded dimension sufficient statistic sharply restricts the possible forms of the distribution\u000a\u000a\u000a other types of sufficiencyedit \u000a\u000a\u000a bayesian sufficiencyedit \u000aan alternative formulation of the condition that a statistic be sufficient set in a bayesian context involves the posterior distributions obtained by using the full dataset and by using only a statistic thus the requirement is that for almost every x\u000a\u000ait turns out that this bayesian sufficiency is a consequence of the formulation above however they are not directly equivalent in the infinitedimensional case a range of theoretical results for sufficiency in a bayesian context is available\u000a\u000a\u000a linear sufficiencyedit \u000aa concept called linear sufficiency can be formulated in a bayesian context and more generally first define the best linear predictor of a vector y based on x as  then a linear statistic tx is linear sufficient if\u000a\u000a\u000a see alsoedit \u000acompleteness of a statistic\u000abasus theorem on independence of complete sufficient and ancillary statistics\u000alehmannscheff theorem a complete sufficient estimator is the best estimator of its expectation\u000araoblackwell theorem\u000asufficient dimension reduction\u000aancillary statistic\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000akholevo as 2001 sufficient statistic in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer chapter 4 isbn 0387985026 \u000adodge y 2003 the oxford dictionary of statistical terms oup isbn 0199206139
p211
sg20
g23
sg24
g27
sg30
Vin statistics a statistic is sufficient with respect to a statistical model and its associated unknown parameter if no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter in particular a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than does the statistic as to which of those probability distributions is that of the population from which the sample was taken\u000aroughly given a set  of independent identically distributed data conditioned on an unknown parameter  a sufficient statistic is a function  whose value contains all the information needed to compute any estimate of the parameter eg a maximum likelihood estimate due to the factorization theorem see below for a sufficient statistic  the joint distribution can be written as  from this factorization it can easily be seen that the maximum likelihood estimate of  will interact with  only through  typically the sufficient statistic is a simple function of the data eg the sum of all the data points\u000amore generally the unknown parameter may represent a vector of unknown quantities or may represent everything about the model that is unknown or not fully specified in such a case the sufficient statistic may be a set of functions called a jointly sufficient statistic typically there are as many functions as there are parameters for example for a gaussian distribution with unknown mean and variance the jointly sufficient statistic from which maximum likelihood estimates of both parameters can be estimated consists of two functions the sum of all data points and the sum of all squared data points or equivalently the sample mean and sample variance\u000athe concept due to ronald fisher is equivalent to the statement that conditional on the value of a sufficient statistic for a parameter the joint probability distribution of the data does not depend on that parameter both the statistic and the underlying parameter can be vectors\u000aa related concept is that of linear sufficiency which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic although it is restricted to linear estimators the kolmogorov structure function deals with individual finite data the related notion there is the algorithmic sufficient statistic\u000athe concept of sufficiency has fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form see pitmankoopmandarmois theorem below but remains very important in theoretical work\u000a\u000a\u000a mathematical definitionedit \u000aa statistic tx is sufficient for underlying parameter  precisely if the conditional probability distribution of the data x given the statistic tx does not depend on the parameter  ie\u000a\u000ainstead of this expression the definition still holds if one uses either of the equivalent expressions\u000a or\u000a\u000awhich indicate respectively that the conditional probability of the parameter  given the sufficient statistic t does not depend on the data x and that the conditional probability of the parameter  given the sufficient statistic t and the conditional probability of the data x given the sufficient statistic t are statistically independent\u000a\u000a\u000a exampleedit \u000aas an example the sample mean is sufficient for the mean  of a normal distribution with known variance once the sample mean is known no further information about  can be obtained from the sample itself on the other hand for an arbitrary distribution the median is not sufficient for the mean even if the median of the sample is known knowing the sample itself would provide further information about the population mean for example if the observations that are less than the median are only slightly less but observations exceeding the median exceed it by a large amount then this would have a bearing on ones inference about the population mean\u000a\u000a\u000a fisherneyman factorization theoremedit \u000afishers factorization theorem or factorization criterion provides a convenient characterization of a sufficient statistic if the probability density function is x then t is sufficient for  if and only if nonnegative functions g and h can be found such that\u000a\u000aie the density  can be factored into a product such that one factor h does not depend on  and the other factor which does depend on  depends on x only through tx\u000ait is easy to see that if ft is a one to one function and t is a sufficient statistic then ft is a sufficient statistic in particular we can multiply a sufficient statistic by a nonzero constant and get another sufficient statistic\u000a\u000a\u000a likelihood principle interpretationedit \u000aan implication of the theorem is that when using likelihoodbased inference two sets of data yielding the same value for the sufficient statistic tx will always yield the same inferences about  by the factorization criterion the likelihoods dependence on  is only in conjunction with tx as this is the same in both cases the dependence on  will be the same as well leading to identical inferences\u000a\u000a\u000a proofedit \u000adue to hogg and craig let  denote a random sample from a distribution having the pdf fx  for      let y1  u1x1 x2  xn be a statistic whose pdf is g1y1  then y1  u1x1 x2  xn is a sufficient statistic for  if and only if for some function h\u000a\u000afirst suppose that\u000a\u000awe shall make the transformation yi  uix1 x2  xn for i  1  n having inverse functions xi  wiy1 y2  yn for i  1  n and jacobian  thus\u000a\u000athe lefthand member is the joint pdf gy1 y2  yn  of y1  u1x1  xn  yn  unx1  xn in the righthand member  is the pdf of  so that  is the quotient of  and  that is it is the conditional pdf  of  given \u000abut  and thus  was given not to depend upon  since  was not introduced in the transformation and accordingly not in the jacobian  it follows that  does not depend upon  and that  is a sufficient statistics for \u000athe converse is proven by taking\u000a\u000awhere  does not depend upon  because  depend only upon  which are independent on  when conditioned by  a sufficient statistics by hypothesis now divide both members by the absolute value of the nonvanishing jacobian  and replace  by the functions  in  this yields\u000a\u000awhere  is the jacobian with  replaced by their value in terms  the lefthand member is necessarily the joint pdf  of  since  and thus  does not depend upon  then\u000a\u000ais a function that does not depend upon \u000a\u000a\u000a another proofedit \u000aa simpler more illustrative proof is as follows although it applies only in the discrete case\u000awe use the shorthand notation to denote the joint probability of  by  since  is a function of  we have  only when  and zero otherwise and thus\u000a\u000awith the last equality being true by the definition of conditional probability distributions thus  with  and \u000areciprocally if  we have\u000a\u000awith the first equality by the definition of pdf for multiple variables the second by the remark above the third by hypothesis and the fourth because the summation is not over \u000athus the conditional probability distribution is\u000a\u000awith the first equality by definition of conditional probability density the second by the remark above the third by the equality proven above and the fourth by simplification this expression does not depend on  and thus  is a sufficient statistic\u000a\u000a\u000a minimal sufficiencyedit \u000aa sufficient statistic is minimal sufficient if it can be represented as a function of any other sufficient statistic in other words sx is minimal sufficient if and only if\u000asx is sufficient and\u000aif tx is sufficient then there exists a function f such that sx  ftx\u000aintuitively a minimal sufficient statistic most efficiently captures all possible information about the parameter \u000aa useful characterization of minimal sufficiency is that when the density f exists sx is minimal sufficient if and only if\u000a is independent of   sx  sy\u000athis follows as a direct consequence from fishers factorization theorem stated above\u000aa case in which there is no minimal sufficient statistic was shown by bahadur 1954 however under mild conditions a minimal sufficient statistic does always exist in particular in euclidean space these conditions always hold if the random variables associated with   are all discrete or are all continuous\u000aif there exists a minimal sufficient statistic and this is usually the case then every complete sufficient statistic is necessarily minimal sufficientnote that this statement does not exclude the option of a pathological case in which a complete sufficient exists while there is no minimal sufficient statistic while it is hard to find cases in which a minimal sufficient statistic does not exist it is not so hard to find cases in which there is no complete statistic\u000athe collection of likelihood ratios  is a minimal sufficient statistic if  is discrete or has a density function\u000a\u000a\u000a examplesedit \u000a\u000a\u000a bernoulli distributionedit \u000aif x1  xn are independent bernoullidistributed random variables with expected value p then the sum tx  x1    xn is a sufficient statistic for p here success corresponds to xi  1 and failure to xi  0 so t is the total number of successes\u000athis is seen by considering the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000aand collecting powers of p and 1  p gives\u000a\u000awhich satisfies the factorization criterion with hx  1 being just a constant\u000anote the crucial feature the unknown parameter p interacts with the data x only via the statistic tx   xi\u000aas a concrete application this gives a procedure for creating a fair coin from a biased coin\u000a\u000a\u000a uniform distributionedit \u000a\u000aif x1  xn are independent and uniformly distributed on the interval 0 then tx  maxx1  xn is sufficient for   the sample maximum is a sufficient statistic for the population maximum\u000ato see this consider the joint probability density function of xx1xn because the observations are independent the pdf can be written as a product of individual densities\u000a\u000awhere 1 is the indicator function thus the density takes form required by the fisherneyman factorization theorem where hx  1minxi0 and the rest of the expression is a function of only  and tx  maxxi\u000ain fact the minimumvariance unbiased estimator mvue for  is\u000a\u000athis is the sample maximum scaled to correct for the bias and is mvue by the lehmannscheff theorem unscaled sample maximum tx is the maximum likelihood estimator for \u000a\u000a\u000a uniform distribution with two parametersedit \u000aif  are independent and uniformly distributed on the interval  where  and  are unknown parameters then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a poisson distributionedit \u000aif x1  xn are independent and have a poisson distribution with parameter  then the sum tx  x1    xn is a sufficient statistic for \u000ato see this consider the joint probability distribution\u000a\u000abecause the observations are independent this can be written as\u000a\u000awhich may be written as\u000a\u000awhich shows that the factorization criterion is satisfied where hx is the reciprocal of the product of the factorials note the parameter  interacts with the data only through its sum tx\u000a\u000a\u000a normal distributionedit \u000aif  are independent and normally distributed with expected value  a parameter and known finite variance  then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athen since  which can be shown simply by expanding this term\u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a exponential distributionedit \u000aif  are independent and exponentially distributed with expected value  an unknown realvalued positive parameter then  is a sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a gamma distributionedit \u000aif  are independent and distributed as a  where  and  are unknown parameters of a gamma distribution then  is a twodimensional sufficient statistic for \u000ato see this consider the joint probability density function of  because the observations are independent the pdf can be written as a product of individual densities ie \u000a\u000athe joint density of the sample takes the form required by the fisherneyman factorization theorem by letting\u000a\u000asince  does not depend on the parameter  and  depends only on  through the function \u000athe fisherneyman factorization theorem implies  is a sufficient statistic for \u000a\u000a\u000a raoblackwell theoremedit \u000asufficiency finds a useful application in the raoblackwell theorem which states that if gx is any kind of estimator of  then typically the conditional expectation of gx given sufficient statistic tx is a better estimator of  and is never worse sometimes one can very easily construct a very crude estimator gx and then evaluate that conditional expected value to get an estimator that is in various senses optimal\u000a\u000a\u000a exponential familyedit \u000a\u000aaccording to the pitmankoopmandarmois theorem among families of probability distributions whose domain does not vary with the parameter being estimated only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases less tersely suppose  are independent identically distributed random variables whose distribution is known to be in some family of probability distributions only if that family is an exponential family is there a possibly vectorvalued sufficient statistic  whose number of scalar components does not increase as the sample size n increases\u000athis theorem shows that sufficiency or rather the existence of a scalar or vectorvalued of bounded dimension sufficient statistic sharply restricts the possible forms of the distribution\u000a\u000a\u000a other types of sufficiencyedit \u000a\u000a\u000a bayesian sufficiencyedit \u000aan alternative formulation of the condition that a statistic be sufficient set in a bayesian context involves the posterior distributions obtained by using the full dataset and by using only a statistic thus the requirement is that for almost every x\u000a\u000ait turns out that this bayesian sufficiency is a consequence of the formulation above however they are not directly equivalent in the infinitedimensional case a range of theoretical results for sufficiency in a bayesian context is available\u000a\u000a\u000a linear sufficiencyedit \u000aa concept called linear sufficiency can be formulated in a bayesian context and more generally first define the best linear predictor of a vector y based on x as  then a linear statistic tx is linear sufficient if\u000a\u000a\u000a see alsoedit \u000acompleteness of a statistic\u000abasus theorem on independence of complete sufficient and ancillary statistics\u000alehmannscheff theorem a complete sufficient estimator is the best estimator of its expectation\u000araoblackwell theorem\u000asufficient dimension reduction\u000aancillary statistic\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000akholevo as 2001 sufficient statistic in hazewinkel michiel encyclopedia of mathematics springer isbn 9781556080104 \u000alehmann e l casella g 1998 theory of point estimation 2nd ed springer chapter 4 isbn 0387985026 \u000adodge y 2003 the oxford dictionary of statistical terms oup isbn 0199206139
p212
sg32
g35
sg37
NsbsS'efficient_estimator.txt'
p213
g2
(g3
g4
Ntp214
Rp215
(dp216
g8
g11
sg12
Vin statistics an efficient estimator is an estimator that estimates the quantity of interest in some best possible manner the notion of best possible relies upon the choice of a particular loss function  the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes the most common choice of the loss function is quadratic resulting in the mean squared error criterion of optimality\u000a\u000a\u000a finitesample efficiency \u000asuppose  p      is a parametric model and x  x1  xn are the data sampled from this model let t  tx be an estimator for the parameter  if this estimator is unbiased that is et   then the cramrrao inequality states the variance of this estimator is bounded from below\u000a\u000awhere  is the fisher information matrix of the model at point  generally the variance measures the degree of dispersion of a random variable around its mean thus estimators with small variances are more concentrated they estimate the parameters more precisely we say that the estimator is finitesample efficient estimator in the class of unbiased estimators if it reaches the lower bound in the cramrrao inequality above for all    efficient estimators are always minimum variance unbiased estimators however the converse is false there exist pointestimation problems for which the minimumvariance meanunbiased estimator is inefficient\u000ahistorically finitesample efficiency was an early optimality criterion however this criterion has some limitations\u000afinitesample efficient estimators are extremely rare in fact it was proved that efficient estimation is possible only in an exponential family and only for the natural parameters of that family\u000athis notion of efficiency is restricted to the class of unbiased estimators since there are no good theoretical reasons to require that estimators are unbiased this restriction is inconvenient in fact if we use mean squared error as a selection criterion many biased estimators will slightly outperform the best unbiased ones for example in multivariate statistics for dimension three or more the meanunbiased estimator sample mean is inadmissible regardless of the outcome its performance is worse than for example the jamesstein estimator\u000afinitesample efficiency is based on the variance as a criterion according to which the estimators are judged a more general approach is to use loss functions other than quadratic ones in which case the finitesample efficiency can no longer be formulated\u000a\u000a\u000a example \u000aamong the models encountered in practice efficient estimators exist for the mean  of the normal distribution but not the variance 2 parameter  of the poisson distribution the probability p in the binomial or multinomial distribution\u000aconsider the model of a normal distribution with unknown mean but known variance  p  n 2    r  the data consists of n iid observations from this model x  x1  xn we estimate the parameter  using the sample mean of all observations\u000a\u000athis estimator has mean  and variance of 2n which is equal to the reciprocal of the fisher information from the sample thus the sample mean is a finitesample efficient estimator for the mean of the normal distribution\u000a\u000a\u000a relative efficiency \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000athe relative efficiency is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a see also \u000abayes estimator\u000aconsistent estimator\u000ahodges estimator\u000a\u000a\u000a notes \u000a\u000a\u000a
p217
sg14
g17
sg18
Vin statistics an efficient estimator is an estimator that estimates the quantity of interest in some best possible manner the notion of best possible relies upon the choice of a particular loss function  the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes the most common choice of the loss function is quadratic resulting in the mean squared error criterion of optimality\u000a\u000a\u000a finitesample efficiency \u000asuppose  p      is a parametric model and x  x1  xn are the data sampled from this model let t  tx be an estimator for the parameter  if this estimator is unbiased that is et   then the cramrrao inequality states the variance of this estimator is bounded from below\u000a\u000awhere  is the fisher information matrix of the model at point  generally the variance measures the degree of dispersion of a random variable around its mean thus estimators with small variances are more concentrated they estimate the parameters more precisely we say that the estimator is finitesample efficient estimator in the class of unbiased estimators if it reaches the lower bound in the cramrrao inequality above for all    efficient estimators are always minimum variance unbiased estimators however the converse is false there exist pointestimation problems for which the minimumvariance meanunbiased estimator is inefficient\u000ahistorically finitesample efficiency was an early optimality criterion however this criterion has some limitations\u000afinitesample efficient estimators are extremely rare in fact it was proved that efficient estimation is possible only in an exponential family and only for the natural parameters of that family\u000athis notion of efficiency is restricted to the class of unbiased estimators since there are no good theoretical reasons to require that estimators are unbiased this restriction is inconvenient in fact if we use mean squared error as a selection criterion many biased estimators will slightly outperform the best unbiased ones for example in multivariate statistics for dimension three or more the meanunbiased estimator sample mean is inadmissible regardless of the outcome its performance is worse than for example the jamesstein estimator\u000afinitesample efficiency is based on the variance as a criterion according to which the estimators are judged a more general approach is to use loss functions other than quadratic ones in which case the finitesample efficiency can no longer be formulated\u000a\u000a\u000a example \u000aamong the models encountered in practice efficient estimators exist for the mean  of the normal distribution but not the variance 2 parameter  of the poisson distribution the probability p in the binomial or multinomial distribution\u000aconsider the model of a normal distribution with unknown mean but known variance  p  n 2    r  the data consists of n iid observations from this model x  x1  xn we estimate the parameter  using the sample mean of all observations\u000a\u000athis estimator has mean  and variance of 2n which is equal to the reciprocal of the fisher information from the sample thus the sample mean is a finitesample efficient estimator for the mean of the normal distribution\u000a\u000a\u000a relative efficiency \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000athe relative efficiency is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a see also \u000abayes estimator\u000aconsistent estimator\u000ahodges estimator\u000a\u000a\u000a notes
p218
sg20
g23
sg24
g27
sg30
Vin statistics an efficient estimator is an estimator that estimates the quantity of interest in some best possible manner the notion of best possible relies upon the choice of a particular loss function  the function which quantifies the relative degree of undesirability of estimation errors of different magnitudes the most common choice of the loss function is quadratic resulting in the mean squared error criterion of optimality\u000a\u000a\u000a finitesample efficiency \u000asuppose  p      is a parametric model and x  x1  xn are the data sampled from this model let t  tx be an estimator for the parameter  if this estimator is unbiased that is et   then the cramrrao inequality states the variance of this estimator is bounded from below\u000a\u000awhere  is the fisher information matrix of the model at point  generally the variance measures the degree of dispersion of a random variable around its mean thus estimators with small variances are more concentrated they estimate the parameters more precisely we say that the estimator is finitesample efficient estimator in the class of unbiased estimators if it reaches the lower bound in the cramrrao inequality above for all    efficient estimators are always minimum variance unbiased estimators however the converse is false there exist pointestimation problems for which the minimumvariance meanunbiased estimator is inefficient\u000ahistorically finitesample efficiency was an early optimality criterion however this criterion has some limitations\u000afinitesample efficient estimators are extremely rare in fact it was proved that efficient estimation is possible only in an exponential family and only for the natural parameters of that family\u000athis notion of efficiency is restricted to the class of unbiased estimators since there are no good theoretical reasons to require that estimators are unbiased this restriction is inconvenient in fact if we use mean squared error as a selection criterion many biased estimators will slightly outperform the best unbiased ones for example in multivariate statistics for dimension three or more the meanunbiased estimator sample mean is inadmissible regardless of the outcome its performance is worse than for example the jamesstein estimator\u000afinitesample efficiency is based on the variance as a criterion according to which the estimators are judged a more general approach is to use loss functions other than quadratic ones in which case the finitesample efficiency can no longer be formulated\u000a\u000a\u000a example \u000aamong the models encountered in practice efficient estimators exist for the mean  of the normal distribution but not the variance 2 parameter  of the poisson distribution the probability p in the binomial or multinomial distribution\u000aconsider the model of a normal distribution with unknown mean but known variance  p  n 2    r  the data consists of n iid observations from this model x  x1  xn we estimate the parameter  using the sample mean of all observations\u000a\u000athis estimator has mean  and variance of 2n which is equal to the reciprocal of the fisher information from the sample thus the sample mean is a finitesample efficient estimator for the mean of the normal distribution\u000a\u000a\u000a relative efficiency \u000aif  and  are estimators for the parameter  then  is said to dominate  if\u000aits mean squared error mse is smaller for at least some value of \u000athe mse does not exceed that of  for any value of \u000aformally  dominates  if\u000a\u000aholds for all  with strict inequality holding somewhere\u000athe relative efficiency is defined as\u000a\u000aalthough  is in general a function of  in many cases the dependence drops out if this is so  being greater than one would indicate that  is preferable whatever the true value of \u000a\u000a\u000a asymptotic efficiency \u000afor some estimators they can attain efficiency asymptotically and are thus called asymptotically efficient estimators this can be the case for some maximum likelihood estimators or for any estimators that attain equality of the cramrrao bound asymptotically\u000a\u000a\u000a see also \u000abayes estimator\u000aconsistent estimator\u000ahodges estimator\u000a\u000a\u000a notes \u000a\u000a\u000a
p219
sg32
g35
sg37
NsbsS'spatial_dependence.txt'
p220
g2
(g3
g4
Ntp221
Rp222
(dp223
g8
g11
sg12
Vspatial dependence is the spatial relationship of variable values for themes defined over space such as rainfall or locations for themes defined as objects such as cities spatial dependence is measured as the existence of statistical dependence in a collection of random variables or a collection of random variables each of which is associated with a different geographical location spatial dependence is of importance in applications where it is reasonable to postulate the existence of corresponding set of random variables at locations that have not been included in a sample thus rainfall may be measured at a set of rain gauge locations and such measurements can be considered as outcomes of random variables but rainfall clearly occurs at other locations and would again be random because rainfall exhibits properties of autocorrelation spatial interpolation techniques can be used to estimate rainfall amounts at locations near measured locations\u000aas with other types of statistical dependence the presence of spatial dependence generally leads to estimates of an average value from a sample being less accurate than had the samples been independent although if negative dependence exists a sample average can be better than in the independent case a different problem than that of estimating an overall average is that of spatial interpolation here the problem is to estimate the unobserved random outcomes of variables at locations intermediate to places where measurements are made on that there is spatial dependence between the observed and unobserved random variables\u000atools for exploring spatial dependence include spatial correlation spatial covariance functions and semivariograms\u000amethods for spatial interpolation include kriging which is a type of best linear unbiased prediction\u000athe topic of spatial dependence is of importance to geostatistics and spatial analysis\u000a\u000a\u000a see also \u000aspatial analysis\u000ageostatistics\u000a\u000a\u000a
p224
sg14
g17
sg18
Vspatial dependence is the spatial relationship of variable values for themes defined over space such as rainfall or locations for themes defined as objects such as cities spatial dependence is measured as the existence of statistical dependence in a collection of random variables or a collection of random variables each of which is associated with a different geographical location spatial dependence is of importance in applications where it is reasonable to postulate the existence of corresponding set of random variables at locations that have not been included in a sample thus rainfall may be measured at a set of rain gauge locations and such measurements can be considered as outcomes of random variables but rainfall clearly occurs at other locations and would again be random because rainfall exhibits properties of autocorrelation spatial interpolation techniques can be used to estimate rainfall amounts at locations near measured locations\u000aas with other types of statistical dependence the presence of spatial dependence generally leads to estimates of an average value from a sample being less accurate than had the samples been independent although if negative dependence exists a sample average can be better than in the independent case a different problem than that of estimating an overall average is that of spatial interpolation here the problem is to estimate the unobserved random outcomes of variables at locations intermediate to places where measurements are made on that there is spatial dependence between the observed and unobserved random variables\u000atools for exploring spatial dependence include spatial correlation spatial covariance functions and semivariograms\u000amethods for spatial interpolation include kriging which is a type of best linear unbiased prediction\u000athe topic of spatial dependence is of importance to geostatistics and spatial analysis\u000a\u000a\u000a see also \u000aspatial analysis\u000ageostatistics
p225
sg20
g23
sg24
g27
sg30
Vspatial dependence is the spatial relationship of variable values for themes defined over space such as rainfall or locations for themes defined as objects such as cities spatial dependence is measured as the existence of statistical dependence in a collection of random variables or a collection of random variables each of which is associated with a different geographical location spatial dependence is of importance in applications where it is reasonable to postulate the existence of corresponding set of random variables at locations that have not been included in a sample thus rainfall may be measured at a set of rain gauge locations and such measurements can be considered as outcomes of random variables but rainfall clearly occurs at other locations and would again be random because rainfall exhibits properties of autocorrelation spatial interpolation techniques can be used to estimate rainfall amounts at locations near measured locations\u000aas with other types of statistical dependence the presence of spatial dependence generally leads to estimates of an average value from a sample being less accurate than had the samples been independent although if negative dependence exists a sample average can be better than in the independent case a different problem than that of estimating an overall average is that of spatial interpolation here the problem is to estimate the unobserved random outcomes of variables at locations intermediate to places where measurements are made on that there is spatial dependence between the observed and unobserved random variables\u000atools for exploring spatial dependence include spatial correlation spatial covariance functions and semivariograms\u000amethods for spatial interpolation include kriging which is a type of best linear unbiased prediction\u000athe topic of spatial dependence is of importance to geostatistics and spatial analysis\u000a\u000a\u000a see also \u000aspatial analysis\u000ageostatistics\u000a\u000a\u000a
p226
sg32
g35
sg37
NsbsS"peirce's_criterion.txt"
p227
g2
(g3
g4
Ntp228
Rp229
(dp230
g8
g11
sg12
Vin robust statistics peirces criterion is a rule for eliminating outliers from data sets which was devised by benjamin peirce\u000a\u000a\u000a outliers removed by peirces criterion \u000a\u000a\u000a the problem of outliers \u000a\u000ain data sets containing realnumbered measurements the suspected outliers are the measured values that appear to lie outside the cluster of most of the other data values the outliers would greatly change the estimate of location if the arithmetic average were to be used as a summary statistic of location the problem is that the arithmetic mean is very sensitive to the inclusion of any outliers in statistical terminology the arithmetic mean is not robust\u000ain the presence of outliers the statistician has two options first the statistician may remove the suspected outliers from the data set and then use the arithmetic mean to estimate the location parameter second the statistician may use a robust statistic such as the median statistic\u000apeirces criterion is a statistical procedure for eliminating outliers\u000a\u000a\u000a uses of peirces criterion \u000athe statistician and historian of statistics stephen m stigler wrote the following about benjamin peirce\u000a\u000ain 1852 he published the first significance test designed to tell an investigator whether an outlier should be rejected peirce 1852 1878 the test based on a likelihood ratio type of argument had the distinction of producing an international debate on the wisdom of such actions anscombe 1960 rider 1933 stigler 1973a\u000a\u000apeirces criterion is derived from a statistical analysis of the gaussian distribution unlike some other criteria for removing outliers peirces method can be applied to identify two or more outliers\u000a\u000ait is proposed to determine in a series of  observations the limit of error beyond which all observations involving so great an error may be rejected provided there are as many as  such observations the principle upon which it is proposed to solve this problem is that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many and no more abnormal observations\u000a\u000ahawkins provides a formula for the criterion\u000apeirces criterion was used for decades at the united states coast survey\u000a\u000afrom 1852 to 1867 he served as the director of the longitude determinations of the u s coast survey and from 1867 to 1874 as superintendent of the survey during these years his test was consistently employed by all the clerks of this the most active and mathematically inclined statistical organization of the era\u000a\u000apeirces criterion was discussed in william chauvenets book\u000a\u000a\u000a applications \u000aan application for peirces criterion is removing poor data points from observation pairs in order to perform a regression between the two observations eg a linear regression peirces criteria does not depend on observation data only characteristics of the observation data therefore making it a highly repeatable process that can be calculated independently of other processes this feature makes peirces criteria for identifying outliers ideal in computer applications because it can be written as a call function\u000a\u000a\u000a previous attempts \u000ain 1855 ba gould attempted to make peirces criterion easier to apply by creating tables of values representing values from peirces equations unfortunately there still exists a disconnect between goulds algorithm and the practical application of peirces criterion\u000ain 2003 sm ross university of new haven represents goulds algorithm now called peirces method with a new example data set and workthrough of the algorithm unfortunately this methodology still relies on using lookup tables which have been updated in this work peirces criterion table\u000ain 2008 an attempt to write a pseudocode was made by a danish geologist k thomsen while this code provided some framework for goulds algorithm users were unsuccessful in calculating values reported by either peirce or gould\u000ain 2012 c dardis releases the r package peirce with various methodologies peirces criterion and the chauvenet method with comparisons of outlier removals dardis and fellow contributor simon muller successfully implemented thomsens pseudocode into a function called findx the code is presented in the r implementation section below references for the r package are available online as well as an unpublished review of the r package results\u000ain 2013 a reexamination of goulds algorithm and the utilisation of advanced python programming modules ie numpy and scipy has made it possible to calculate the squarederror threshold values for identifying outliers\u000a\u000a\u000a python implementation \u000ain order to use peirces criteria one must first understand the input and return values regression analysis or the fitting of curves to data results in residual errors or the difference between the fitted curve and the observation points therefore each observation point has a residual error associated with a fitted curve by taking the square ie residual error raised to the power of two residual errors are expressed as positive values if the squared error is too large ie due to a poor observation it can cause problems with the regression parameters eg slope and intercept for a linear curve retrieved from the curve fitting\u000ait was peirces idea to statistically identify what constituted an error as too large and therefore being identified as an outlier which could be removed from the observations to improve the fit between the observations and a curve k thomsen identified that three parameters were needed to perform the calculation the number of observation pairs n the number of outliers to be removed n and the number of regression parameters eg coefficients used in the curvefitting to get the residuals m the end result of this process is to calculate a threshold value of squared error whereby observations with a squared error smaller than this threshold should be kept and observations with a squared error larger than this value should be removed ie as an outlier\u000abecause peirces criteria does not take observations fitting parameters or residual errors as an input the output must be reassociated with the data by taking the average of all the squared errors ie the meansquared error and multiply it by the threshold squared error ie the output of this function it will result in the dataspecific threshold value used to identify outliers\u000athe following python code returns xsquared values for a given n first column and n top row in table 1 m  1 and table 2 m  2 of gould 1855 due to the newtonmethod of iteration lookup tables such as n versus log q table iii in gould 1855 and x versus log r table iii in peirce 1852 and table iv in gould 1855 are no longer necessary\u000a\u000a\u000a python code \u000a\u000a\u000a r implementation \u000athomsens code has been successfully written into the following function call findx by c dardis and s muller in 2012 which returns the maximum error deviation  to complement the python code presented in the previous section the r equivalent of peircedev is also presented here which returns the squared maximum error deviation  these two functions return equivalent values by either squaring the returned value from the findx function or by taking the squareroot of the value returned by the peircedev function differences occur with error handling for example the findx function returns nans for invalid data while peircedev returns 0 which allows for computations to continue without additional na value handling also the findx function does not support any error handling when the number of potential outliers increases towards the number of observations throws missing value error and nan warning\u000ajust as with the python version the squarederror ie  returned by the peircedev function must be multiplied by the meansquared error of the model fit to get the squareddelta value ie 2 use 2 to compare the squarederror values of the model fit any observation pairs with a squarederror greater than 2 are considered outliers and can be removed from the model an iterator should be written to test increasing values of n until the number of outliers identified comparing 2 to modelfit squarederrors is less than those assumed ie peirces n\u000a\u000a\u000a r code \u000a\u000a\u000a notes \u000a a b sm stigler mathematical statistics in the early states the annals of statistics vol 6 no 2 p 246 1978 available online httpwwwjstororgstable2958876\u000a a b quoted in the editorial note on page 516 of the collected writings of peirce 1982 edition the quotation cites a manual of astronomy 2558 by chauvenet\u000a dm hawkins 1980 brief early history in outlier rejection identification of outliers monographs on applied probability and statistics chapman  hall page 10\u000a peirce 1878\u000a a b gould ba on peirces criterion for the rejection of doubtful observations with tables for facilitating its application astronomical journal iss 83 vol 4 no 11 pp 8187 1855 doi 101086100480 available online at httpadsabsharvardedufull1855aj481g\u000a ross sm peirces criterion for the elimination of suspect experimental data journal of engineering technology vol 2 no 2 pp 112 2003 available online httpwwweolucaredusystemfilespiercescriterionpdf\u000a thomsen k topic computing tables for use with peirces criterion  in 1855 and 2008 the math forum  drexel posted 5 oct 2008 available online at httpmathforumorgkbmessagejspamessageid6449606 accessed 15 jul 2013\u000a c dardis package peirce rforge accessed online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircemanualpdfrootpeirce\u000a c dardis peirces criterion for the rejection of nonnormal outliers defining the range of applicability journal of statistical software unpublished available online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircesubpdfrootpeirce\u000a\u000a\u000a
p231
sg14
g17
sg18
Vin robust statistics peirces criterion is a rule for eliminating outliers from data sets which was devised by benjamin peirce\u000a\u000a\u000a outliers removed by peirces criterion \u000a\u000a\u000a the problem of outliers \u000a\u000ain data sets containing realnumbered measurements the suspected outliers are the measured values that appear to lie outside the cluster of most of the other data values the outliers would greatly change the estimate of location if the arithmetic average were to be used as a summary statistic of location the problem is that the arithmetic mean is very sensitive to the inclusion of any outliers in statistical terminology the arithmetic mean is not robust\u000ain the presence of outliers the statistician has two options first the statistician may remove the suspected outliers from the data set and then use the arithmetic mean to estimate the location parameter second the statistician may use a robust statistic such as the median statistic\u000apeirces criterion is a statistical procedure for eliminating outliers\u000a\u000a\u000a uses of peirces criterion \u000athe statistician and historian of statistics stephen m stigler wrote the following about benjamin peirce\u000a\u000ain 1852 he published the first significance test designed to tell an investigator whether an outlier should be rejected peirce 1852 1878 the test based on a likelihood ratio type of argument had the distinction of producing an international debate on the wisdom of such actions anscombe 1960 rider 1933 stigler 1973a\u000a\u000apeirces criterion is derived from a statistical analysis of the gaussian distribution unlike some other criteria for removing outliers peirces method can be applied to identify two or more outliers\u000a\u000ait is proposed to determine in a series of  observations the limit of error beyond which all observations involving so great an error may be rejected provided there are as many as  such observations the principle upon which it is proposed to solve this problem is that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many and no more abnormal observations\u000a\u000ahawkins provides a formula for the criterion\u000apeirces criterion was used for decades at the united states coast survey\u000a\u000afrom 1852 to 1867 he served as the director of the longitude determinations of the u s coast survey and from 1867 to 1874 as superintendent of the survey during these years his test was consistently employed by all the clerks of this the most active and mathematically inclined statistical organization of the era\u000a\u000apeirces criterion was discussed in william chauvenets book\u000a\u000a\u000a applications \u000aan application for peirces criterion is removing poor data points from observation pairs in order to perform a regression between the two observations eg a linear regression peirces criteria does not depend on observation data only characteristics of the observation data therefore making it a highly repeatable process that can be calculated independently of other processes this feature makes peirces criteria for identifying outliers ideal in computer applications because it can be written as a call function\u000a\u000a\u000a previous attempts \u000ain 1855 ba gould attempted to make peirces criterion easier to apply by creating tables of values representing values from peirces equations unfortunately there still exists a disconnect between goulds algorithm and the practical application of peirces criterion\u000ain 2003 sm ross university of new haven represents goulds algorithm now called peirces method with a new example data set and workthrough of the algorithm unfortunately this methodology still relies on using lookup tables which have been updated in this work peirces criterion table\u000ain 2008 an attempt to write a pseudocode was made by a danish geologist k thomsen while this code provided some framework for goulds algorithm users were unsuccessful in calculating values reported by either peirce or gould\u000ain 2012 c dardis releases the r package peirce with various methodologies peirces criterion and the chauvenet method with comparisons of outlier removals dardis and fellow contributor simon muller successfully implemented thomsens pseudocode into a function called findx the code is presented in the r implementation section below references for the r package are available online as well as an unpublished review of the r package results\u000ain 2013 a reexamination of goulds algorithm and the utilisation of advanced python programming modules ie numpy and scipy has made it possible to calculate the squarederror threshold values for identifying outliers\u000a\u000a\u000a python implementation \u000ain order to use peirces criteria one must first understand the input and return values regression analysis or the fitting of curves to data results in residual errors or the difference between the fitted curve and the observation points therefore each observation point has a residual error associated with a fitted curve by taking the square ie residual error raised to the power of two residual errors are expressed as positive values if the squared error is too large ie due to a poor observation it can cause problems with the regression parameters eg slope and intercept for a linear curve retrieved from the curve fitting\u000ait was peirces idea to statistically identify what constituted an error as too large and therefore being identified as an outlier which could be removed from the observations to improve the fit between the observations and a curve k thomsen identified that three parameters were needed to perform the calculation the number of observation pairs n the number of outliers to be removed n and the number of regression parameters eg coefficients used in the curvefitting to get the residuals m the end result of this process is to calculate a threshold value of squared error whereby observations with a squared error smaller than this threshold should be kept and observations with a squared error larger than this value should be removed ie as an outlier\u000abecause peirces criteria does not take observations fitting parameters or residual errors as an input the output must be reassociated with the data by taking the average of all the squared errors ie the meansquared error and multiply it by the threshold squared error ie the output of this function it will result in the dataspecific threshold value used to identify outliers\u000athe following python code returns xsquared values for a given n first column and n top row in table 1 m  1 and table 2 m  2 of gould 1855 due to the newtonmethod of iteration lookup tables such as n versus log q table iii in gould 1855 and x versus log r table iii in peirce 1852 and table iv in gould 1855 are no longer necessary\u000a\u000a\u000a python code \u000a\u000a\u000a r implementation \u000athomsens code has been successfully written into the following function call findx by c dardis and s muller in 2012 which returns the maximum error deviation  to complement the python code presented in the previous section the r equivalent of peircedev is also presented here which returns the squared maximum error deviation  these two functions return equivalent values by either squaring the returned value from the findx function or by taking the squareroot of the value returned by the peircedev function differences occur with error handling for example the findx function returns nans for invalid data while peircedev returns 0 which allows for computations to continue without additional na value handling also the findx function does not support any error handling when the number of potential outliers increases towards the number of observations throws missing value error and nan warning\u000ajust as with the python version the squarederror ie  returned by the peircedev function must be multiplied by the meansquared error of the model fit to get the squareddelta value ie 2 use 2 to compare the squarederror values of the model fit any observation pairs with a squarederror greater than 2 are considered outliers and can be removed from the model an iterator should be written to test increasing values of n until the number of outliers identified comparing 2 to modelfit squarederrors is less than those assumed ie peirces n\u000a\u000a\u000a r code \u000a\u000a\u000a notes \u000a a b sm stigler mathematical statistics in the early states the annals of statistics vol 6 no 2 p 246 1978 available online httpwwwjstororgstable2958876\u000a a b quoted in the editorial note on page 516 of the collected writings of peirce 1982 edition the quotation cites a manual of astronomy 2558 by chauvenet\u000a dm hawkins 1980 brief early history in outlier rejection identification of outliers monographs on applied probability and statistics chapman  hall page 10\u000a peirce 1878\u000a a b gould ba on peirces criterion for the rejection of doubtful observations with tables for facilitating its application astronomical journal iss 83 vol 4 no 11 pp 8187 1855 doi 101086100480 available online at httpadsabsharvardedufull1855aj481g\u000a ross sm peirces criterion for the elimination of suspect experimental data journal of engineering technology vol 2 no 2 pp 112 2003 available online httpwwweolucaredusystemfilespiercescriterionpdf\u000a thomsen k topic computing tables for use with peirces criterion  in 1855 and 2008 the math forum  drexel posted 5 oct 2008 available online at httpmathforumorgkbmessagejspamessageid6449606 accessed 15 jul 2013\u000a c dardis package peirce rforge accessed online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircemanualpdfrootpeirce\u000a c dardis peirces criterion for the rejection of nonnormal outliers defining the range of applicability journal of statistical software unpublished available online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircesubpdfrootpeirce
p232
sg20
g23
sg24
g27
sg30
Vin robust statistics peirces criterion is a rule for eliminating outliers from data sets which was devised by benjamin peirce\u000a\u000a\u000a outliers removed by peirces criterion \u000a\u000a\u000a the problem of outliers \u000a\u000ain data sets containing realnumbered measurements the suspected outliers are the measured values that appear to lie outside the cluster of most of the other data values the outliers would greatly change the estimate of location if the arithmetic average were to be used as a summary statistic of location the problem is that the arithmetic mean is very sensitive to the inclusion of any outliers in statistical terminology the arithmetic mean is not robust\u000ain the presence of outliers the statistician has two options first the statistician may remove the suspected outliers from the data set and then use the arithmetic mean to estimate the location parameter second the statistician may use a robust statistic such as the median statistic\u000apeirces criterion is a statistical procedure for eliminating outliers\u000a\u000a\u000a uses of peirces criterion \u000athe statistician and historian of statistics stephen m stigler wrote the following about benjamin peirce\u000a\u000ain 1852 he published the first significance test designed to tell an investigator whether an outlier should be rejected peirce 1852 1878 the test based on a likelihood ratio type of argument had the distinction of producing an international debate on the wisdom of such actions anscombe 1960 rider 1933 stigler 1973a\u000a\u000apeirces criterion is derived from a statistical analysis of the gaussian distribution unlike some other criteria for removing outliers peirces method can be applied to identify two or more outliers\u000a\u000ait is proposed to determine in a series of  observations the limit of error beyond which all observations involving so great an error may be rejected provided there are as many as  such observations the principle upon which it is proposed to solve this problem is that the proposed observations should be rejected when the probability of the system of errors obtained by retaining them is less than that of the system of errors obtained by their rejection multiplied by the probability of making so many and no more abnormal observations\u000a\u000ahawkins provides a formula for the criterion\u000apeirces criterion was used for decades at the united states coast survey\u000a\u000afrom 1852 to 1867 he served as the director of the longitude determinations of the u s coast survey and from 1867 to 1874 as superintendent of the survey during these years his test was consistently employed by all the clerks of this the most active and mathematically inclined statistical organization of the era\u000a\u000apeirces criterion was discussed in william chauvenets book\u000a\u000a\u000a applications \u000aan application for peirces criterion is removing poor data points from observation pairs in order to perform a regression between the two observations eg a linear regression peirces criteria does not depend on observation data only characteristics of the observation data therefore making it a highly repeatable process that can be calculated independently of other processes this feature makes peirces criteria for identifying outliers ideal in computer applications because it can be written as a call function\u000a\u000a\u000a previous attempts \u000ain 1855 ba gould attempted to make peirces criterion easier to apply by creating tables of values representing values from peirces equations unfortunately there still exists a disconnect between goulds algorithm and the practical application of peirces criterion\u000ain 2003 sm ross university of new haven represents goulds algorithm now called peirces method with a new example data set and workthrough of the algorithm unfortunately this methodology still relies on using lookup tables which have been updated in this work peirces criterion table\u000ain 2008 an attempt to write a pseudocode was made by a danish geologist k thomsen while this code provided some framework for goulds algorithm users were unsuccessful in calculating values reported by either peirce or gould\u000ain 2012 c dardis releases the r package peirce with various methodologies peirces criterion and the chauvenet method with comparisons of outlier removals dardis and fellow contributor simon muller successfully implemented thomsens pseudocode into a function called findx the code is presented in the r implementation section below references for the r package are available online as well as an unpublished review of the r package results\u000ain 2013 a reexamination of goulds algorithm and the utilisation of advanced python programming modules ie numpy and scipy has made it possible to calculate the squarederror threshold values for identifying outliers\u000a\u000a\u000a python implementation \u000ain order to use peirces criteria one must first understand the input and return values regression analysis or the fitting of curves to data results in residual errors or the difference between the fitted curve and the observation points therefore each observation point has a residual error associated with a fitted curve by taking the square ie residual error raised to the power of two residual errors are expressed as positive values if the squared error is too large ie due to a poor observation it can cause problems with the regression parameters eg slope and intercept for a linear curve retrieved from the curve fitting\u000ait was peirces idea to statistically identify what constituted an error as too large and therefore being identified as an outlier which could be removed from the observations to improve the fit between the observations and a curve k thomsen identified that three parameters were needed to perform the calculation the number of observation pairs n the number of outliers to be removed n and the number of regression parameters eg coefficients used in the curvefitting to get the residuals m the end result of this process is to calculate a threshold value of squared error whereby observations with a squared error smaller than this threshold should be kept and observations with a squared error larger than this value should be removed ie as an outlier\u000abecause peirces criteria does not take observations fitting parameters or residual errors as an input the output must be reassociated with the data by taking the average of all the squared errors ie the meansquared error and multiply it by the threshold squared error ie the output of this function it will result in the dataspecific threshold value used to identify outliers\u000athe following python code returns xsquared values for a given n first column and n top row in table 1 m  1 and table 2 m  2 of gould 1855 due to the newtonmethod of iteration lookup tables such as n versus log q table iii in gould 1855 and x versus log r table iii in peirce 1852 and table iv in gould 1855 are no longer necessary\u000a\u000a\u000a python code \u000a\u000a\u000a r implementation \u000athomsens code has been successfully written into the following function call findx by c dardis and s muller in 2012 which returns the maximum error deviation  to complement the python code presented in the previous section the r equivalent of peircedev is also presented here which returns the squared maximum error deviation  these two functions return equivalent values by either squaring the returned value from the findx function or by taking the squareroot of the value returned by the peircedev function differences occur with error handling for example the findx function returns nans for invalid data while peircedev returns 0 which allows for computations to continue without additional na value handling also the findx function does not support any error handling when the number of potential outliers increases towards the number of observations throws missing value error and nan warning\u000ajust as with the python version the squarederror ie  returned by the peircedev function must be multiplied by the meansquared error of the model fit to get the squareddelta value ie 2 use 2 to compare the squarederror values of the model fit any observation pairs with a squarederror greater than 2 are considered outliers and can be removed from the model an iterator should be written to test increasing values of n until the number of outliers identified comparing 2 to modelfit squarederrors is less than those assumed ie peirces n\u000a\u000a\u000a r code \u000a\u000a\u000a notes \u000a a b sm stigler mathematical statistics in the early states the annals of statistics vol 6 no 2 p 246 1978 available online httpwwwjstororgstable2958876\u000a a b quoted in the editorial note on page 516 of the collected writings of peirce 1982 edition the quotation cites a manual of astronomy 2558 by chauvenet\u000a dm hawkins 1980 brief early history in outlier rejection identification of outliers monographs on applied probability and statistics chapman  hall page 10\u000a peirce 1878\u000a a b gould ba on peirces criterion for the rejection of doubtful observations with tables for facilitating its application astronomical journal iss 83 vol 4 no 11 pp 8187 1855 doi 101086100480 available online at httpadsabsharvardedufull1855aj481g\u000a ross sm peirces criterion for the elimination of suspect experimental data journal of engineering technology vol 2 no 2 pp 112 2003 available online httpwwweolucaredusystemfilespiercescriterionpdf\u000a thomsen k topic computing tables for use with peirces criterion  in 1855 and 2008 the math forum  drexel posted 5 oct 2008 available online at httpmathforumorgkbmessagejspamessageid6449606 accessed 15 jul 2013\u000a c dardis package peirce rforge accessed online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircemanualpdfrootpeirce\u000a c dardis peirces criterion for the rejection of nonnormal outliers defining the range of applicability journal of statistical software unpublished available online httpsrforgerprojectorgscmviewvcphpcheckoutpkgpeircepeircesubpdfrootpeirce\u000a\u000a\u000a
p233
sg32
g35
sg37
NsbsS'sensitivity_and_specificity.txt'
p234
g2
(g3
g4
Ntp235
Rp236
(dp237
g8
g11
sg12
Vsensitivity and specificity are statistical measures of the performance of a binary classification test also known in statistics as classification function\u000asensitivity also called the true positive rate or the recall in some fields measures the proportion of positives that are correctly identified as such eg the percentage of sick people who are correctly identified as having the condition\u000aspecificity also called the true negative rate measures the proportion of negatives that are correctly identified as such eg the percentage of healthy people who are correctly identified as not having the condition\u000athus sensitivity quantifies the avoiding of false negatives as specificity does for false positives\u000afor any test there is usually a tradeoff between the measures for instance in an airport security setting in which one is testing for potential threats to safety scanners may be set to trigger on lowrisk items like belt buckles and keys low specificity in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard high sensitivity this tradeoff can be represented graphically as a receiver operating characteristic curve\u000aa perfect predictor would be described as 100 sensitive eg all sick are identified as sick and 100 specific eg no healthy are identified as sick however theoretically any predictor will possess a minimum error bound known as the bayes error rate\u000a\u000a\u000a definitionsedit \u000aimagine a study evaluating a new test that screens people for a disease each person taking the test either has or does not have the disease the test outcome can be positive classifying the person as having the disease or negative classifying the person as not having the disease the test results for each subject may or may not match the subjects actual status in that setting\u000atrue positive sick people correctly identified as sick\u000afalse positive healthy people incorrectly identified as sick\u000atrue negative healthy people correctly identified as healthy\u000afalse negative sick people incorrectly identified as healthy\u000ain general positive  identified and negative  rejected therefore\u000atrue positive  correctly identified\u000afalse positive  incorrectly identified\u000atrue negative  correctly rejected\u000afalse negative  incorrectly rejected\u000alet us consider a group with p positive instances and n negative instances of some condition the four outcomes can be formulated in a 22 contingency table or confusion matrix as follows\u000a\u000a\u000a sensitivityedit \u000asensitivity refers to the tests ability to correctly detect patients who do have the condition consider the example of a medical test used to identify a disease the sensitivity of the test is the proportion of people who test positive for the disease among those who have the disease mathematically this can be expressed as\u000a\u000aa negative result in a test with high sensitivity is useful for ruling out disease a high sensitivity test is reliable when its result is negative since it rarely misdiagnoses those who have the disease a test with 100 sensitivity will recognize all patients with the disease by testing positive a negative test result would definitively rule out presence of the disease in a patient\u000aa positive result in a test with high sensitivity is not useful for ruling in disease suppose a bogus test kit is designed to show only one reading positive when used on diseased patients all patients test positive giving the test 100 sensitivity however sensitivity by definition does not take into account false positives the bogus test also returns positive on all healthy patients giving it a false positive rate of 100 rendering it useless for detecting or ruling in the disease\u000asensitivity is not the same as the precision or positive predictive value ratio of true positives to combined true and false positives which is as much a statement about the proportion of actual positives in the population being tested as it is about the test\u000athe calculation of sensitivity does not take into account indeterminate test results if a test cannot be repeated indeterminate samples either should be excluded from the analysis the number of exclusions should be stated when quoting sensitivity or can be treated as false negatives which gives the worstcase value for sensitivity and may therefore underestimate it\u000aa test with high sensitivity has a low type ii error rate in nonmedical contexts sensitivity is sometimes called recall\u000a\u000a\u000a specificityedit \u000aspecificity relates to the tests ability to correctly detect patients without a condition consider the example of a medical test for diagnosing a disease specificity of a test is the proportion of healthy patients known not to have the disease who will test negative for it mathematically this can also be written as\u000a\u000aa positive result in a test with high specificity is useful for ruling in disease the test rarely gives positive results in healthy patients a test with 100 specificity will read negative and accurately exclude disease from all healthy patients a positive result signifies a high probability of the presence of disease\u000aa negative result in a test with high specificity is not useful for ruling out disease assume a bogus test is designed to read only negative this is administered to healthy patients and reads negative on all of them this will give the test a specificity of 100 specificity by definition does not take into account false negatives the same test will also read negative on diseased patients therefore it has a false negative rate of 100 and will be useless for ruling out disease\u000aa test with a high specificity has a low type i error rate\u000a\u000a\u000a graphical illustrationedit \u000a\u000a\u000a medical examplesedit \u000ain medical diagnosis test sensitivity is the ability of a test to correctly identify those with the disease true positive rate whereas test specificity is the ability of the test to correctly identify those without the disease true negative rate if 100 patients known to have a disease were tested and 43 test positive then the test has 43 sensitivity if 100 with no disease are tested and 96 return a negative result then the test has 96 specificity sensitivity and specificity are prevalenceindependent test characteristics as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest positive and negative predictive values but not sensitivity or specificity are values influenced by the prevalence of disease in the population that is being tested\u000a\u000a\u000a misconceptionsedit \u000ait is often claimed that a highly specific test is effective at ruling in a disease when positive while a highly sensitive test is deemed effective at ruling out a disease when negative this has led to the widely used mnemonics spin and snout according to which a highly specific test when positive rules in disease sppin and a highly sensitive test when negative rules out disease snnout both rules of thumb are however inferentially misleading as the diagnostic power of any test is determined by both its sensitivity and its specificity\u000athe tradeoff between specificity and sensitivity is explored in roc analysis as a trade off between tpr and fpr that is recall and fallout giving them equal weight optimizes informedness  specificitysensitivity1  tprfpr the magnitude of which gives the probability of an informed decision between the two classes 0 represents appropriate use of information 0 represents chancelevel performance 0 represents perverse use of information\u000a\u000a\u000a sensitivity indexedit \u000athe sensitivity index or d pronounced deeprime is a statistic used in signal detection theory it provides the separation between the means of the signal and the noise distributions compared against the standard deviation of the noise distribution for normally distributed signal and noise with mean and standard deviations  and  and  and  respectively d is defined as\u000a \u000aan estimate of d can be also found from measurements of the hit rate and falsealarm rate it is calculated as\u000a\u000ad  zhit rate  zfalse alarm rate\u000a\u000awhere function zp p  01 is the inverse of the cumulative gaussian distribution\u000ad is a dimensionless statistic a higher d indicates that the signal can be more readily detected\u000a\u000a\u000a worked exampleedit \u000a\u000aa worked example\u000aa diagnostic test with sensitivity 67 and specificity 91 is applied to 2030 people to look for a disorder with a population prevalence of 148\u000arelated calculations\u000afalse positive rate   type i error  1  specificity  fp  fp  tn  180  180  1820  9\u000afalse negative rate   type ii error  1  sensitivity  fn  tp  fn  10  20  10  33\u000apower  sensitivity  1  \u000alikelihood ratio positive  sensitivity  1  specificity  067  1  091  74\u000alikelihood ratio negative  1  sensitivity  specificity  1  067  091  037\u000ahence with large numbers of false positives and few false negatives a positive screen test is in itself poor at confirming the disorder ppv  10 and further investigations must be undertaken it did however correctly identify 667 of all cases the sensitivity however as a screening test a negative result is very good at reassuring that a patient does not have the disorder npv  995 and at this initial screen correctly identifies 91 of those who do not have cancer the specificity\u000a\u000a\u000a estimation of errors in quoted sensitivity or specificityedit \u000asensitivity and specificity values alone may be highly misleading the worstcase sensitivity or specificity must be calculated in order to avoid reliance on experiments with few results for example a particular test may easily show 100 sensitivity if tested against the gold standard four times but a single additional test against the gold standard that gave a poor result would imply a sensitivity of only 80 a common way to do this is to state the binomial proportion confidence interval often calculated using a wilson score interval\u000aconfidence intervals for sensitivity and specificity can be calculated giving the range of values within which the correct value lies at a given confidence level eg 95\u000a\u000a\u000a terminology in information retrievaledit \u000ain information retrieval the positive predictive value is called precision and sensitivity is called recall unlike the specificity vs sensitivity tradeoff these measures are both independent of the number of true negatives which is generally unknown and much larger than the actual numbers of relevant and retrieved documents this assumption of very large numbers of true negatives versus positives is rare in other applications\u000athe fscore can be used as a single measure of performance of the test for the positive class the fscore is the harmonic mean of precision and recall\u000a\u000ain the traditional language of statistical hypothesis testing the sensitivity of a test is called the statistical power of the test although the word power in that context has a more general usage that is not applicable in the present context a sensitive test will have fewer type ii errors\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaltman dg bland jm 1994 diagnostic tests 1 sensitivity and specificity bmj 308 6943 1552 doi101136bmj30869431552 pmc 2540489 pmid 8019315 \u000aloong t 2003 understanding sensitivity and specificity with the right side of the brain bmj 327 7417 716719 doi101136bmj3277417716 pmc 200804 pmid 14512479 \u000a\u000a\u000a external linksedit \u000avassar colleges sensitivityspecificity calculator
p238
sg14
g17
sg18
Vsensitivity and specificity are statistical measures of the performance of a binary classification test also known in statistics as classification function\u000asensitivity also called the true positive rate or the recall in some fields measures the proportion of positives that are correctly identified as such eg the percentage of sick people who are correctly identified as having the condition\u000aspecificity also called the true negative rate measures the proportion of negatives that are correctly identified as such eg the percentage of healthy people who are correctly identified as not having the condition\u000athus sensitivity quantifies the avoiding of false negatives as specificity does for false positives\u000afor any test there is usually a tradeoff between the measures for instance in an airport security setting in which one is testing for potential threats to safety scanners may be set to trigger on lowrisk items like belt buckles and keys low specificity in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard high sensitivity this tradeoff can be represented graphically as a receiver operating characteristic curve\u000aa perfect predictor would be described as 100 sensitive eg all sick are identified as sick and 100 specific eg no healthy are identified as sick however theoretically any predictor will possess a minimum error bound known as the bayes error rate\u000a\u000a\u000a definitionsedit \u000aimagine a study evaluating a new test that screens people for a disease each person taking the test either has or does not have the disease the test outcome can be positive classifying the person as having the disease or negative classifying the person as not having the disease the test results for each subject may or may not match the subjects actual status in that setting\u000atrue positive sick people correctly identified as sick\u000afalse positive healthy people incorrectly identified as sick\u000atrue negative healthy people correctly identified as healthy\u000afalse negative sick people incorrectly identified as healthy\u000ain general positive  identified and negative  rejected therefore\u000atrue positive  correctly identified\u000afalse positive  incorrectly identified\u000atrue negative  correctly rejected\u000afalse negative  incorrectly rejected\u000alet us consider a group with p positive instances and n negative instances of some condition the four outcomes can be formulated in a 22 contingency table or confusion matrix as follows\u000a\u000a\u000a sensitivityedit \u000asensitivity refers to the tests ability to correctly detect patients who do have the condition consider the example of a medical test used to identify a disease the sensitivity of the test is the proportion of people who test positive for the disease among those who have the disease mathematically this can be expressed as\u000a\u000aa negative result in a test with high sensitivity is useful for ruling out disease a high sensitivity test is reliable when its result is negative since it rarely misdiagnoses those who have the disease a test with 100 sensitivity will recognize all patients with the disease by testing positive a negative test result would definitively rule out presence of the disease in a patient\u000aa positive result in a test with high sensitivity is not useful for ruling in disease suppose a bogus test kit is designed to show only one reading positive when used on diseased patients all patients test positive giving the test 100 sensitivity however sensitivity by definition does not take into account false positives the bogus test also returns positive on all healthy patients giving it a false positive rate of 100 rendering it useless for detecting or ruling in the disease\u000asensitivity is not the same as the precision or positive predictive value ratio of true positives to combined true and false positives which is as much a statement about the proportion of actual positives in the population being tested as it is about the test\u000athe calculation of sensitivity does not take into account indeterminate test results if a test cannot be repeated indeterminate samples either should be excluded from the analysis the number of exclusions should be stated when quoting sensitivity or can be treated as false negatives which gives the worstcase value for sensitivity and may therefore underestimate it\u000aa test with high sensitivity has a low type ii error rate in nonmedical contexts sensitivity is sometimes called recall\u000a\u000a\u000a specificityedit \u000aspecificity relates to the tests ability to correctly detect patients without a condition consider the example of a medical test for diagnosing a disease specificity of a test is the proportion of healthy patients known not to have the disease who will test negative for it mathematically this can also be written as\u000a\u000aa positive result in a test with high specificity is useful for ruling in disease the test rarely gives positive results in healthy patients a test with 100 specificity will read negative and accurately exclude disease from all healthy patients a positive result signifies a high probability of the presence of disease\u000aa negative result in a test with high specificity is not useful for ruling out disease assume a bogus test is designed to read only negative this is administered to healthy patients and reads negative on all of them this will give the test a specificity of 100 specificity by definition does not take into account false negatives the same test will also read negative on diseased patients therefore it has a false negative rate of 100 and will be useless for ruling out disease\u000aa test with a high specificity has a low type i error rate\u000a\u000a\u000a graphical illustrationedit \u000a\u000a\u000a medical examplesedit \u000ain medical diagnosis test sensitivity is the ability of a test to correctly identify those with the disease true positive rate whereas test specificity is the ability of the test to correctly identify those without the disease true negative rate if 100 patients known to have a disease were tested and 43 test positive then the test has 43 sensitivity if 100 with no disease are tested and 96 return a negative result then the test has 96 specificity sensitivity and specificity are prevalenceindependent test characteristics as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest positive and negative predictive values but not sensitivity or specificity are values influenced by the prevalence of disease in the population that is being tested\u000a\u000a\u000a misconceptionsedit \u000ait is often claimed that a highly specific test is effective at ruling in a disease when positive while a highly sensitive test is deemed effective at ruling out a disease when negative this has led to the widely used mnemonics spin and snout according to which a highly specific test when positive rules in disease sppin and a highly sensitive test when negative rules out disease snnout both rules of thumb are however inferentially misleading as the diagnostic power of any test is determined by both its sensitivity and its specificity\u000athe tradeoff between specificity and sensitivity is explored in roc analysis as a trade off between tpr and fpr that is recall and fallout giving them equal weight optimizes informedness  specificitysensitivity1  tprfpr the magnitude of which gives the probability of an informed decision between the two classes 0 represents appropriate use of information 0 represents chancelevel performance 0 represents perverse use of information\u000a\u000a\u000a sensitivity indexedit \u000athe sensitivity index or d pronounced deeprime is a statistic used in signal detection theory it provides the separation between the means of the signal and the noise distributions compared against the standard deviation of the noise distribution for normally distributed signal and noise with mean and standard deviations  and  and  and  respectively d is defined as\u000a \u000aan estimate of d can be also found from measurements of the hit rate and falsealarm rate it is calculated as\u000a\u000ad  zhit rate  zfalse alarm rate\u000a\u000awhere function zp p  01 is the inverse of the cumulative gaussian distribution\u000ad is a dimensionless statistic a higher d indicates that the signal can be more readily detected\u000a\u000a\u000a worked exampleedit \u000a\u000aa worked example\u000aa diagnostic test with sensitivity 67 and specificity 91 is applied to 2030 people to look for a disorder with a population prevalence of 148\u000arelated calculations\u000afalse positive rate   type i error  1  specificity  fp  fp  tn  180  180  1820  9\u000afalse negative rate   type ii error  1  sensitivity  fn  tp  fn  10  20  10  33\u000apower  sensitivity  1  \u000alikelihood ratio positive  sensitivity  1  specificity  067  1  091  74\u000alikelihood ratio negative  1  sensitivity  specificity  1  067  091  037\u000ahence with large numbers of false positives and few false negatives a positive screen test is in itself poor at confirming the disorder ppv  10 and further investigations must be undertaken it did however correctly identify 667 of all cases the sensitivity however as a screening test a negative result is very good at reassuring that a patient does not have the disorder npv  995 and at this initial screen correctly identifies 91 of those who do not have cancer the specificity\u000a\u000a\u000a estimation of errors in quoted sensitivity or specificityedit \u000asensitivity and specificity values alone may be highly misleading the worstcase sensitivity or specificity must be calculated in order to avoid reliance on experiments with few results for example a particular test may easily show 100 sensitivity if tested against the gold standard four times but a single additional test against the gold standard that gave a poor result would imply a sensitivity of only 80 a common way to do this is to state the binomial proportion confidence interval often calculated using a wilson score interval\u000aconfidence intervals for sensitivity and specificity can be calculated giving the range of values within which the correct value lies at a given confidence level eg 95\u000a\u000a\u000a terminology in information retrievaledit \u000ain information retrieval the positive predictive value is called precision and sensitivity is called recall unlike the specificity vs sensitivity tradeoff these measures are both independent of the number of true negatives which is generally unknown and much larger than the actual numbers of relevant and retrieved documents this assumption of very large numbers of true negatives versus positives is rare in other applications\u000athe fscore can be used as a single measure of performance of the test for the positive class the fscore is the harmonic mean of precision and recall\u000a\u000ain the traditional language of statistical hypothesis testing the sensitivity of a test is called the statistical power of the test although the word power in that context has a more general usage that is not applicable in the present context a sensitive test will have fewer type ii errors\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaltman dg bland jm 1994 diagnostic tests 1 sensitivity and specificity bmj 308 6943 1552 doi101136bmj30869431552 pmc 2540489 pmid 8019315 \u000aloong t 2003 understanding sensitivity and specificity with the right side of the brain bmj 327 7417 716719 doi101136bmj3277417716 pmc 200804 pmid 14512479 \u000a\u000a\u000a external linksedit \u000avassar colleges sensitivityspecificity calculator
p239
sg20
g23
sg24
g27
sg30
Vsensitivity and specificity are statistical measures of the performance of a binary classification test also known in statistics as classification function\u000asensitivity also called the true positive rate or the recall in some fields measures the proportion of positives that are correctly identified as such eg the percentage of sick people who are correctly identified as having the condition\u000aspecificity also called the true negative rate measures the proportion of negatives that are correctly identified as such eg the percentage of healthy people who are correctly identified as not having the condition\u000athus sensitivity quantifies the avoiding of false negatives as specificity does for false positives\u000afor any test there is usually a tradeoff between the measures for instance in an airport security setting in which one is testing for potential threats to safety scanners may be set to trigger on lowrisk items like belt buckles and keys low specificity in order to reduce the risk of missing objects that do pose a threat to the aircraft and those aboard high sensitivity this tradeoff can be represented graphically as a receiver operating characteristic curve\u000aa perfect predictor would be described as 100 sensitive eg all sick are identified as sick and 100 specific eg no healthy are identified as sick however theoretically any predictor will possess a minimum error bound known as the bayes error rate\u000a\u000a\u000a definitionsedit \u000aimagine a study evaluating a new test that screens people for a disease each person taking the test either has or does not have the disease the test outcome can be positive classifying the person as having the disease or negative classifying the person as not having the disease the test results for each subject may or may not match the subjects actual status in that setting\u000atrue positive sick people correctly identified as sick\u000afalse positive healthy people incorrectly identified as sick\u000atrue negative healthy people correctly identified as healthy\u000afalse negative sick people incorrectly identified as healthy\u000ain general positive  identified and negative  rejected therefore\u000atrue positive  correctly identified\u000afalse positive  incorrectly identified\u000atrue negative  correctly rejected\u000afalse negative  incorrectly rejected\u000alet us consider a group with p positive instances and n negative instances of some condition the four outcomes can be formulated in a 22 contingency table or confusion matrix as follows\u000a\u000a\u000a sensitivityedit \u000asensitivity refers to the tests ability to correctly detect patients who do have the condition consider the example of a medical test used to identify a disease the sensitivity of the test is the proportion of people who test positive for the disease among those who have the disease mathematically this can be expressed as\u000a\u000aa negative result in a test with high sensitivity is useful for ruling out disease a high sensitivity test is reliable when its result is negative since it rarely misdiagnoses those who have the disease a test with 100 sensitivity will recognize all patients with the disease by testing positive a negative test result would definitively rule out presence of the disease in a patient\u000aa positive result in a test with high sensitivity is not useful for ruling in disease suppose a bogus test kit is designed to show only one reading positive when used on diseased patients all patients test positive giving the test 100 sensitivity however sensitivity by definition does not take into account false positives the bogus test also returns positive on all healthy patients giving it a false positive rate of 100 rendering it useless for detecting or ruling in the disease\u000asensitivity is not the same as the precision or positive predictive value ratio of true positives to combined true and false positives which is as much a statement about the proportion of actual positives in the population being tested as it is about the test\u000athe calculation of sensitivity does not take into account indeterminate test results if a test cannot be repeated indeterminate samples either should be excluded from the analysis the number of exclusions should be stated when quoting sensitivity or can be treated as false negatives which gives the worstcase value for sensitivity and may therefore underestimate it\u000aa test with high sensitivity has a low type ii error rate in nonmedical contexts sensitivity is sometimes called recall\u000a\u000a\u000a specificityedit \u000aspecificity relates to the tests ability to correctly detect patients without a condition consider the example of a medical test for diagnosing a disease specificity of a test is the proportion of healthy patients known not to have the disease who will test negative for it mathematically this can also be written as\u000a\u000aa positive result in a test with high specificity is useful for ruling in disease the test rarely gives positive results in healthy patients a test with 100 specificity will read negative and accurately exclude disease from all healthy patients a positive result signifies a high probability of the presence of disease\u000aa negative result in a test with high specificity is not useful for ruling out disease assume a bogus test is designed to read only negative this is administered to healthy patients and reads negative on all of them this will give the test a specificity of 100 specificity by definition does not take into account false negatives the same test will also read negative on diseased patients therefore it has a false negative rate of 100 and will be useless for ruling out disease\u000aa test with a high specificity has a low type i error rate\u000a\u000a\u000a graphical illustrationedit \u000a\u000a\u000a medical examplesedit \u000ain medical diagnosis test sensitivity is the ability of a test to correctly identify those with the disease true positive rate whereas test specificity is the ability of the test to correctly identify those without the disease true negative rate if 100 patients known to have a disease were tested and 43 test positive then the test has 43 sensitivity if 100 with no disease are tested and 96 return a negative result then the test has 96 specificity sensitivity and specificity are prevalenceindependent test characteristics as their values are intrinsic to the test and do not depend on the disease prevalence in the population of interest positive and negative predictive values but not sensitivity or specificity are values influenced by the prevalence of disease in the population that is being tested\u000a\u000a\u000a misconceptionsedit \u000ait is often claimed that a highly specific test is effective at ruling in a disease when positive while a highly sensitive test is deemed effective at ruling out a disease when negative this has led to the widely used mnemonics spin and snout according to which a highly specific test when positive rules in disease sppin and a highly sensitive test when negative rules out disease snnout both rules of thumb are however inferentially misleading as the diagnostic power of any test is determined by both its sensitivity and its specificity\u000athe tradeoff between specificity and sensitivity is explored in roc analysis as a trade off between tpr and fpr that is recall and fallout giving them equal weight optimizes informedness  specificitysensitivity1  tprfpr the magnitude of which gives the probability of an informed decision between the two classes 0 represents appropriate use of information 0 represents chancelevel performance 0 represents perverse use of information\u000a\u000a\u000a sensitivity indexedit \u000athe sensitivity index or d pronounced deeprime is a statistic used in signal detection theory it provides the separation between the means of the signal and the noise distributions compared against the standard deviation of the noise distribution for normally distributed signal and noise with mean and standard deviations  and  and  and  respectively d is defined as\u000a \u000aan estimate of d can be also found from measurements of the hit rate and falsealarm rate it is calculated as\u000a\u000ad  zhit rate  zfalse alarm rate\u000a\u000awhere function zp p  01 is the inverse of the cumulative gaussian distribution\u000ad is a dimensionless statistic a higher d indicates that the signal can be more readily detected\u000a\u000a\u000a worked exampleedit \u000a\u000aa worked example\u000aa diagnostic test with sensitivity 67 and specificity 91 is applied to 2030 people to look for a disorder with a population prevalence of 148\u000arelated calculations\u000afalse positive rate   type i error  1  specificity  fp  fp  tn  180  180  1820  9\u000afalse negative rate   type ii error  1  sensitivity  fn  tp  fn  10  20  10  33\u000apower  sensitivity  1  \u000alikelihood ratio positive  sensitivity  1  specificity  067  1  091  74\u000alikelihood ratio negative  1  sensitivity  specificity  1  067  091  037\u000ahence with large numbers of false positives and few false negatives a positive screen test is in itself poor at confirming the disorder ppv  10 and further investigations must be undertaken it did however correctly identify 667 of all cases the sensitivity however as a screening test a negative result is very good at reassuring that a patient does not have the disorder npv  995 and at this initial screen correctly identifies 91 of those who do not have cancer the specificity\u000a\u000a\u000a estimation of errors in quoted sensitivity or specificityedit \u000asensitivity and specificity values alone may be highly misleading the worstcase sensitivity or specificity must be calculated in order to avoid reliance on experiments with few results for example a particular test may easily show 100 sensitivity if tested against the gold standard four times but a single additional test against the gold standard that gave a poor result would imply a sensitivity of only 80 a common way to do this is to state the binomial proportion confidence interval often calculated using a wilson score interval\u000aconfidence intervals for sensitivity and specificity can be calculated giving the range of values within which the correct value lies at a given confidence level eg 95\u000a\u000a\u000a terminology in information retrievaledit \u000ain information retrieval the positive predictive value is called precision and sensitivity is called recall unlike the specificity vs sensitivity tradeoff these measures are both independent of the number of true negatives which is generally unknown and much larger than the actual numbers of relevant and retrieved documents this assumption of very large numbers of true negatives versus positives is rare in other applications\u000athe fscore can be used as a single measure of performance of the test for the positive class the fscore is the harmonic mean of precision and recall\u000a\u000ain the traditional language of statistical hypothesis testing the sensitivity of a test is called the statistical power of the test although the word power in that context has a more general usage that is not applicable in the present context a sensitive test will have fewer type ii errors\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaltman dg bland jm 1994 diagnostic tests 1 sensitivity and specificity bmj 308 6943 1552 doi101136bmj30869431552 pmc 2540489 pmid 8019315 \u000aloong t 2003 understanding sensitivity and specificity with the right side of the brain bmj 327 7417 716719 doi101136bmj3277417716 pmc 200804 pmid 14512479 \u000a\u000a\u000a external linksedit \u000avassar colleges sensitivityspecificity calculator
p240
sg32
g35
sg37
NsbsS'restricted_maximum_likelihood.txt'
p241
g2
(g3
g4
Ntp242
Rp243
(dp244
g8
g11
sg12
Vin statistics the restricted or residual or reduced maximum likelihood reml approach is a particular form of maximum likelihood estimation which does not base estimates on a maximum likelihood fit of all the information but instead uses a likelihood function calculated from a transformed set of data so that nuisance parameters have no effect\u000ain the case of variance component estimation the original data set is replaced by a set of contrasts calculated from the data and the likelihood function is calculated from the probability distribution of these contrasts according to the model for the complete data set in particular reml is used as a method for fitting linear mixed models in contrast to the earlier maximum likelihood estimation reml can produce unbiased estimates of variance and covariance parameters\u000athe idea underlying reml estimation was put forward by m s bartlett in 1937 the first description of the approach applied to estimating components of variance in unbalanced data was by desmond patterson and robin thompson of the university of edinburgh although they did not use the term reml a review of the early literature was given by harville\u000areml estimation is available in a number of generalpurpose statistical software packages including genstat the reml directive sas the mixed procedure spss the mixed command stata the mixed command jmp statistical software and r the lme4 and older nlme packages as well as in more specialist packages such as mlwin hlm asreml statistical parametric mapping and cropstat\u000a\u000a\u000a
p245
sg14
g17
sg18
Vin statistics the restricted or residual or reduced maximum likelihood reml approach is a particular form of maximum likelihood estimation which does not base estimates on a maximum likelihood fit of all the information but instead uses a likelihood function calculated from a transformed set of data so that nuisance parameters have no effect\u000ain the case of variance component estimation the original data set is replaced by a set of contrasts calculated from the data and the likelihood function is calculated from the probability distribution of these contrasts according to the model for the complete data set in particular reml is used as a method for fitting linear mixed models in contrast to the earlier maximum likelihood estimation reml can produce unbiased estimates of variance and covariance parameters\u000athe idea underlying reml estimation was put forward by m s bartlett in 1937 the first description of the approach applied to estimating components of variance in unbalanced data was by desmond patterson and robin thompson of the university of edinburgh although they did not use the term reml a review of the early literature was given by harville\u000areml estimation is available in a number of generalpurpose statistical software packages including genstat the reml directive sas the mixed procedure spss the mixed command stata the mixed command jmp statistical software and r the lme4 and older nlme packages as well as in more specialist packages such as mlwin hlm asreml statistical parametric mapping and cropstat
p246
sg20
g23
sg24
g27
sg30
Vin statistics the restricted or residual or reduced maximum likelihood reml approach is a particular form of maximum likelihood estimation which does not base estimates on a maximum likelihood fit of all the information but instead uses a likelihood function calculated from a transformed set of data so that nuisance parameters have no effect\u000ain the case of variance component estimation the original data set is replaced by a set of contrasts calculated from the data and the likelihood function is calculated from the probability distribution of these contrasts according to the model for the complete data set in particular reml is used as a method for fitting linear mixed models in contrast to the earlier maximum likelihood estimation reml can produce unbiased estimates of variance and covariance parameters\u000athe idea underlying reml estimation was put forward by m s bartlett in 1937 the first description of the approach applied to estimating components of variance in unbalanced data was by desmond patterson and robin thompson of the university of edinburgh although they did not use the term reml a review of the early literature was given by harville\u000areml estimation is available in a number of generalpurpose statistical software packages including genstat the reml directive sas the mixed procedure spss the mixed command stata the mixed command jmp statistical software and r the lme4 and older nlme packages as well as in more specialist packages such as mlwin hlm asreml statistical parametric mapping and cropstat\u000a\u000a\u000a
p247
sg32
g35
sg37
NsbsS'exponential_dispersion_model.txt'
p248
g2
(g3
g4
Ntp249
Rp250
(dp251
g8
g11
sg12
Vexponential dispersion models are statistical models in which the probability distribution is of a special form this class of models represents a generalisation of the exponential family of models which themselves play an important role in statistical theory because they have a special structure which enables deductions to be made about appropriate statistical inference\u000a\u000a\u000a definition \u000aexponential dispersion models are a generalisation of the natural exponential family these have a probability density function which for a multivariate model can be written as\u000a\u000awhere the parameter  has the same dimension as the observation variable  the generalisation includes an extra scalar index parameter  and has density function of the form\u000a\u000athe terminology dispersion parameter is used for  while  is the natural parameter also known as canonical parameter\u000a\u000a\u000a
p252
sg14
g17
sg18
Vexponential dispersion models are statistical models in which the probability distribution is of a special form this class of models represents a generalisation of the exponential family of models which themselves play an important role in statistical theory because they have a special structure which enables deductions to be made about appropriate statistical inference\u000a\u000a\u000a definition \u000aexponential dispersion models are a generalisation of the natural exponential family these have a probability density function which for a multivariate model can be written as\u000a\u000awhere the parameter  has the same dimension as the observation variable  the generalisation includes an extra scalar index parameter  and has density function of the form\u000a\u000athe terminology dispersion parameter is used for  while  is the natural parameter also known as canonical parameter
p253
sg20
g23
sg24
g27
sg30
Vexponential dispersion models are statistical models in which the probability distribution is of a special form this class of models represents a generalisation of the exponential family of models which themselves play an important role in statistical theory because they have a special structure which enables deductions to be made about appropriate statistical inference\u000a\u000a\u000a definition \u000aexponential dispersion models are a generalisation of the natural exponential family these have a probability density function which for a multivariate model can be written as\u000a\u000awhere the parameter  has the same dimension as the observation variable  the generalisation includes an extra scalar index parameter  and has density function of the form\u000a\u000athe terminology dispersion parameter is used for  while  is the natural parameter also known as canonical parameter\u000a\u000a\u000a
p254
sg32
g35
sg37
NsbsS'nuisance_parameter.txt'
p255
g2
(g3
g4
Ntp256
Rp257
(dp258
g8
g11
sg12
Vin statistics a nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest the classic example of a nuisance parameter is the variance 2 of a normal distribution when the mean  is of primary interest\u000anuisance parameters are often variances but not always for example in an errorsinvariables model the unknown true location of each observation is a nuisance parameter in general any parameter which intrudes on the analysis of another may be considered a nuisance parameter a parameter may also cease to be a nuisance if it becomes the object of study as the variance of a distribution may be\u000a\u000a\u000a theoretical statistics \u000athe general treatment of nuisance parameters can be broadly similar between frequentist and bayesian approaches to theoretical statistics it relies on an attempt to partition the likelihood function into components representing information about the parameters of interest and information about the other nuisance parameters this can involve ideas about sufficient statistics and ancillary statistics when this partition can be achieved it may be possible to complete a bayesian analysis for the parameters of interest by determining their joint posterior distribution algebraically the partition allows frequentist theory to develop general estimation approaches in the presence of nuisance parameters if the partition cannot be achieved it may still be possible to make use of an approximate partition\u000ain some special cases it is possible to formulate methods that circumvent the presences of nuisance parameters the ttest provides a practically useful test because the test statistic does not depend on the unknown variance it is a case where use can be made of a pivotal quantity however in other cases no such circumvention is known\u000a\u000a\u000a practical statistics \u000apractical approaches to statistical analysis treat nuisance parameters somewhat differently in frequentist and bayesian methodologies\u000aa general approach in a frequentist analysis can be based on maximum likelihoodratio tests these provide both significance tests and confidence intervals for the parameters of interest which are approximately valid for moderate to large sample sizes and which take account of the presence of nuisance parameters see basu 1977 for some general discussion and spall and garner 1990 for some discussion relative to the identification of parameters in linear dynamic ie state space representation models\u000ain bayesian analysis a generally applicable approach creates random samples from the joint posterior distribution of all the parameters see markov chain monte carlo given these the joint distribution of only the parameters of interest can be readily found by marginalizing over the nuisance parameters however this approach may not always be computationally efficient if some or all of the nuisance parameters can be eliminated on a theoretical basis\u000a\u000a\u000a see also \u000aprofile likelihood\u000a\u000a\u000a
p259
sg14
g17
sg18
Vin statistics a nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest the classic example of a nuisance parameter is the variance 2 of a normal distribution when the mean  is of primary interest\u000anuisance parameters are often variances but not always for example in an errorsinvariables model the unknown true location of each observation is a nuisance parameter in general any parameter which intrudes on the analysis of another may be considered a nuisance parameter a parameter may also cease to be a nuisance if it becomes the object of study as the variance of a distribution may be\u000a\u000a\u000a theoretical statistics \u000athe general treatment of nuisance parameters can be broadly similar between frequentist and bayesian approaches to theoretical statistics it relies on an attempt to partition the likelihood function into components representing information about the parameters of interest and information about the other nuisance parameters this can involve ideas about sufficient statistics and ancillary statistics when this partition can be achieved it may be possible to complete a bayesian analysis for the parameters of interest by determining their joint posterior distribution algebraically the partition allows frequentist theory to develop general estimation approaches in the presence of nuisance parameters if the partition cannot be achieved it may still be possible to make use of an approximate partition\u000ain some special cases it is possible to formulate methods that circumvent the presences of nuisance parameters the ttest provides a practically useful test because the test statistic does not depend on the unknown variance it is a case where use can be made of a pivotal quantity however in other cases no such circumvention is known\u000a\u000a\u000a practical statistics \u000apractical approaches to statistical analysis treat nuisance parameters somewhat differently in frequentist and bayesian methodologies\u000aa general approach in a frequentist analysis can be based on maximum likelihoodratio tests these provide both significance tests and confidence intervals for the parameters of interest which are approximately valid for moderate to large sample sizes and which take account of the presence of nuisance parameters see basu 1977 for some general discussion and spall and garner 1990 for some discussion relative to the identification of parameters in linear dynamic ie state space representation models\u000ain bayesian analysis a generally applicable approach creates random samples from the joint posterior distribution of all the parameters see markov chain monte carlo given these the joint distribution of only the parameters of interest can be readily found by marginalizing over the nuisance parameters however this approach may not always be computationally efficient if some or all of the nuisance parameters can be eliminated on a theoretical basis\u000a\u000a\u000a see also \u000aprofile likelihood
p260
sg20
g23
sg24
g27
sg30
Vin statistics a nuisance parameter is any parameter which is not of immediate interest but which must be accounted for in the analysis of those parameters which are of interest the classic example of a nuisance parameter is the variance 2 of a normal distribution when the mean  is of primary interest\u000anuisance parameters are often variances but not always for example in an errorsinvariables model the unknown true location of each observation is a nuisance parameter in general any parameter which intrudes on the analysis of another may be considered a nuisance parameter a parameter may also cease to be a nuisance if it becomes the object of study as the variance of a distribution may be\u000a\u000a\u000a theoretical statistics \u000athe general treatment of nuisance parameters can be broadly similar between frequentist and bayesian approaches to theoretical statistics it relies on an attempt to partition the likelihood function into components representing information about the parameters of interest and information about the other nuisance parameters this can involve ideas about sufficient statistics and ancillary statistics when this partition can be achieved it may be possible to complete a bayesian analysis for the parameters of interest by determining their joint posterior distribution algebraically the partition allows frequentist theory to develop general estimation approaches in the presence of nuisance parameters if the partition cannot be achieved it may still be possible to make use of an approximate partition\u000ain some special cases it is possible to formulate methods that circumvent the presences of nuisance parameters the ttest provides a practically useful test because the test statistic does not depend on the unknown variance it is a case where use can be made of a pivotal quantity however in other cases no such circumvention is known\u000a\u000a\u000a practical statistics \u000apractical approaches to statistical analysis treat nuisance parameters somewhat differently in frequentist and bayesian methodologies\u000aa general approach in a frequentist analysis can be based on maximum likelihoodratio tests these provide both significance tests and confidence intervals for the parameters of interest which are approximately valid for moderate to large sample sizes and which take account of the presence of nuisance parameters see basu 1977 for some general discussion and spall and garner 1990 for some discussion relative to the identification of parameters in linear dynamic ie state space representation models\u000ain bayesian analysis a generally applicable approach creates random samples from the joint posterior distribution of all the parameters see markov chain monte carlo given these the joint distribution of only the parameters of interest can be readily found by marginalizing over the nuisance parameters however this approach may not always be computationally efficient if some or all of the nuisance parameters can be eliminated on a theoretical basis\u000a\u000a\u000a see also \u000aprofile likelihood\u000a\u000a\u000a
p261
sg32
g35
sg37
NsbsS'loss_function.txt'
p262
g2
(g3
g4
Ntp263
Rp264
(dp265
g8
g11
sg12
Vin mathematical optimization statistics decision theory and machine learning a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some cost associated with the event an optimization problem seeks to minimize a loss function an objective function is either a loss function or its negative sometimes called a reward function a profit function a utility function a fitness function etc in which case it is to be maximized\u000ain statistics typically a loss function is used for parameter estimation and the event in question is some function of the difference between estimated and true values for an instance of data the concept as old as laplace was reintroduced in statistics by abraham wald in the middle of the 20th century in the context of economics for example this is usually economic cost or regret in classification it is the penalty for an incorrect classification of an example in actuarial science it is used in an insurance context to model benefits paid over premiums particularly since the works of harald cramr in the 1920s in optimal control the loss is the penalty for failing to achieve a desired value in financial risk management the function is precisely mapped to a monetary loss\u000a\u000a\u000a use in statisticsedit \u000aparameter estimation for supervised learning tasks such as regression or classification can be formulated as the minimization of a loss function over a training set the goal of estimation is to find a function that models its input well if it were applied to the training set it should predict the values or class labels associated with the samples in that set the loss function quantifies the amount by which the prediction deviates from the actual values\u000a\u000a\u000a definitionedit \u000aformally we begin by considering some family of distributions for a random variable x that is indexed by some \u000amore intuitively we can think of x as our data perhaps  where  iid the x is the set of things the decision rule will be making decisions on there exists some number of possible ways  to model our data x which our decision function can use to make decisions for a finite number of models we can thus think of  as the index to this family of probability models for an infinite family of models it is a set of parameters to the family of distributions\u000aon a more practical note it is important to understand that while it is tempting to think of loss functions as necessarily parametric since they seem to take  as a parameter the fact that  is infinitedimensional is completely incompatible with this notion for example if the family of probability functions is uncountably infinite  indexes an uncountably infinite space\u000afrom here given a set a of possible actions a decision rule is a function    a\u000aa loss function is a real lowerbounded function l on   a for some    the value l x is the cost of action x under parameter \u000a\u000a\u000a expected lossedit \u000athe value of the loss function itself is a random quantity because it depends on the outcome of a random variable x both frequentist and bayesian statistical theory involve making a decision based on the expected value of the loss function however this quantity is defined differently under the two paradigms\u000a\u000a\u000a frequentist expected lossedit \u000awe first define the expected loss in the frequentist context it is obtained by taking the expected value with respect to the probability distribution p of the observed data x this is also referred to as the risk function  of the decision rule  and the parameter  here the decision rule depends on the outcome of x the risk function is given by\u000a\u000a\u000a bayesian expected lossedit \u000ain a bayesian approach the expectation is calculated using the posterior distribution  of the parameter \u000a\u000aone then should choose the action a which minimises the expected loss although this will result in choosing the same action as would be chosen using the frequentist risk the emphasis of the bayesian approach is that one is only interested in choosing the optimal action under the actual observed data whereas choosing the actual bayes optimal decision rule which is a function of all possible observations is a much more difficult problem\u000a\u000a\u000a economic choice under uncertaintyedit \u000ain economics decisionmaking under uncertainty is often modelled using the von neumannmorgenstern utility function of the uncertain variable of interest such as endofperiod wealth since the value of this variable is uncertain so is the value of the utility function it is the expected value of utility that is maximized\u000a\u000a\u000a examplesedit \u000afor a scalar parameter  a decision function whose output  is an estimate of  and a quadratic loss function\u000a\u000athe risk function becomes the mean squared error of the estimate\u000a\u000ain density estimation the unknown parameter is probability density itself the loss function is typically chosen to be a norm in an appropriate function space for example for l2 norm\u000a\u000athe risk function becomes the mean integrated squared error\u000a\u000a\u000a decision rulesedit \u000aa decision rule makes a choice using an optimality criterion some commonly used criteria are\u000aminimax choose the decision rule with the lowest worst loss  that is minimize the worstcase maximum possible loss\u000a\u000ainvariance choose the optimal decision rule which satisfies an invariance requirement\u000achoose the decision rule with the lowest average loss ie minimize the expected value of the loss function\u000a\u000a\u000a selecting a loss functionedit \u000asound statistical practice requires selecting an estimator consistent with the actual acceptable variation experienced in the context of a particular applied problem thus in the applied use of loss functions selecting which statistical method to use to model an applied problem depends on knowing the losses that will be experienced from being wrong under the problems particular circumstances\u000aa common example involves estimating location under typical statistical assumptions the mean or average is the statistic for estimating location that minimizes the expected loss experienced under the squarederror loss function while the median is the estimator that minimizes expected loss experienced under the absolutedifference loss function still different estimators would be optimal under other less common circumstances\u000ain economics when an agent is risk neutral the objective function is simply expressed in monetary terms such as profit income or endofperiod wealth\u000abut for riskaverse or riskloving agents loss is measured as the negative of a utility function which represents satisfaction and is usually interpreted in ordinal terms rather than in cardinal absolute terms\u000aother measures of cost are possible for example mortality or morbidity in the field of public health or safety engineering\u000afor most optimization algorithms it is desirable to have a loss function that is globally continuous and differentiable\u000atwo very commonly used loss functions are the squared loss  and the absolute loss  however the absolute loss has the disadvantage that it is not differentiable at  the squared loss has the disadvantage that it has the tendency to be dominated by outlierswhen summing over a set of s as in   the final sum tends to be the result of a few particularly large avalues rather than an expression of the average avalue\u000athe choice of a loss function is not arbitrary it is very restrictive and sometimes the loss function may be characterized by its desirable properties among the choice principles are for example the requirement of completeness of the class of symmetric statistics in the case of iid observations the principle of complete information and some others\u000a\u000a\u000a loss functions in bayesian statisticsedit \u000aone of the consequences of bayesian inference is that in addition to experimental data the loss function does not in itself wholly determine a decision what is important is the relationship between the loss function and the posterior probability so it is possible to have two different loss functions which lead to the same decision when the prior probability distributions associated with each compensate for the details of each loss function\u000acombining the three elements of the prior probability the data and the loss function then allows decisions to be based on maximizing the subjective expected utility a concept introduced by leonard j savage\u000a\u000a\u000a regretedit \u000a\u000asavage also argued that using nonbayesian methods such as minimax the loss function should be based on the idea of regret ie the loss associated with a decision should be the difference between the consequences of the best decision that could have been taken had the underlying circumstances been known and the decision that was in fact taken before they were known\u000a\u000a\u000a quadratic loss functionedit \u000athe use of a quadratic loss function is common for example when using least squares techniques it is often more mathematically tractable than other loss functions because of the properties of variances as well as being symmetric an error above the target causes the same loss as the same magnitude of error below the target if the target is t then a quadratic loss function is\u000a\u000afor some constant c the value of the constant makes no difference to a decision and can be ignored by setting it equal to 1\u000amany common statistics including ttests regression models design of experiments and much else use least squares methods applied using linear regression theory which is based on the quadratric loss function\u000athe quadratic loss function is also used in linearquadratic optimal control problems in these problems even in the absence of uncertainty it may not be possible to achieve the desired values of all target variables often loss is expressed as a quadratic form in the deviations of the variables of interest from their desired values this approach is tractable because it results in linear firstorder conditions in the context of stochastic control the expected value of the quadratic form is used\u000a\u000a\u000a 01 loss functionedit \u000ain statistics and decision theory a frequently used loss function is the 01 loss function\u000a\u000awhere  is the indicator notation\u000a\u000a\u000a see alsoedit \u000aloss functions for classification\u000adiscounted maximum loss\u000ahinge loss\u000ascoring rule\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaretz kevin bartram shnke m pope peter f apriljune 2011 asymmetric loss functions and the rationality of expected stock returns international journal of forecasting 27 2 413437 doi101016jijforecast200910008 \u000aberger james o 1985 statistical decision theory and bayesian analysis 2nd ed new york springerverlag isbn 0387960988 mr 0804611
p266
sg14
g17
sg18
Vin mathematical optimization statistics decision theory and machine learning a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some cost associated with the event an optimization problem seeks to minimize a loss function an objective function is either a loss function or its negative sometimes called a reward function a profit function a utility function a fitness function etc in which case it is to be maximized\u000ain statistics typically a loss function is used for parameter estimation and the event in question is some function of the difference between estimated and true values for an instance of data the concept as old as laplace was reintroduced in statistics by abraham wald in the middle of the 20th century in the context of economics for example this is usually economic cost or regret in classification it is the penalty for an incorrect classification of an example in actuarial science it is used in an insurance context to model benefits paid over premiums particularly since the works of harald cramr in the 1920s in optimal control the loss is the penalty for failing to achieve a desired value in financial risk management the function is precisely mapped to a monetary loss\u000a\u000a\u000a use in statisticsedit \u000aparameter estimation for supervised learning tasks such as regression or classification can be formulated as the minimization of a loss function over a training set the goal of estimation is to find a function that models its input well if it were applied to the training set it should predict the values or class labels associated with the samples in that set the loss function quantifies the amount by which the prediction deviates from the actual values\u000a\u000a\u000a definitionedit \u000aformally we begin by considering some family of distributions for a random variable x that is indexed by some \u000amore intuitively we can think of x as our data perhaps  where  iid the x is the set of things the decision rule will be making decisions on there exists some number of possible ways  to model our data x which our decision function can use to make decisions for a finite number of models we can thus think of  as the index to this family of probability models for an infinite family of models it is a set of parameters to the family of distributions\u000aon a more practical note it is important to understand that while it is tempting to think of loss functions as necessarily parametric since they seem to take  as a parameter the fact that  is infinitedimensional is completely incompatible with this notion for example if the family of probability functions is uncountably infinite  indexes an uncountably infinite space\u000afrom here given a set a of possible actions a decision rule is a function    a\u000aa loss function is a real lowerbounded function l on   a for some    the value l x is the cost of action x under parameter \u000a\u000a\u000a expected lossedit \u000athe value of the loss function itself is a random quantity because it depends on the outcome of a random variable x both frequentist and bayesian statistical theory involve making a decision based on the expected value of the loss function however this quantity is defined differently under the two paradigms\u000a\u000a\u000a frequentist expected lossedit \u000awe first define the expected loss in the frequentist context it is obtained by taking the expected value with respect to the probability distribution p of the observed data x this is also referred to as the risk function  of the decision rule  and the parameter  here the decision rule depends on the outcome of x the risk function is given by\u000a\u000a\u000a bayesian expected lossedit \u000ain a bayesian approach the expectation is calculated using the posterior distribution  of the parameter \u000a\u000aone then should choose the action a which minimises the expected loss although this will result in choosing the same action as would be chosen using the frequentist risk the emphasis of the bayesian approach is that one is only interested in choosing the optimal action under the actual observed data whereas choosing the actual bayes optimal decision rule which is a function of all possible observations is a much more difficult problem\u000a\u000a\u000a economic choice under uncertaintyedit \u000ain economics decisionmaking under uncertainty is often modelled using the von neumannmorgenstern utility function of the uncertain variable of interest such as endofperiod wealth since the value of this variable is uncertain so is the value of the utility function it is the expected value of utility that is maximized\u000a\u000a\u000a examplesedit \u000afor a scalar parameter  a decision function whose output  is an estimate of  and a quadratic loss function\u000a\u000athe risk function becomes the mean squared error of the estimate\u000a\u000ain density estimation the unknown parameter is probability density itself the loss function is typically chosen to be a norm in an appropriate function space for example for l2 norm\u000a\u000athe risk function becomes the mean integrated squared error\u000a\u000a\u000a decision rulesedit \u000aa decision rule makes a choice using an optimality criterion some commonly used criteria are\u000aminimax choose the decision rule with the lowest worst loss  that is minimize the worstcase maximum possible loss\u000a\u000ainvariance choose the optimal decision rule which satisfies an invariance requirement\u000achoose the decision rule with the lowest average loss ie minimize the expected value of the loss function\u000a\u000a\u000a selecting a loss functionedit \u000asound statistical practice requires selecting an estimator consistent with the actual acceptable variation experienced in the context of a particular applied problem thus in the applied use of loss functions selecting which statistical method to use to model an applied problem depends on knowing the losses that will be experienced from being wrong under the problems particular circumstances\u000aa common example involves estimating location under typical statistical assumptions the mean or average is the statistic for estimating location that minimizes the expected loss experienced under the squarederror loss function while the median is the estimator that minimizes expected loss experienced under the absolutedifference loss function still different estimators would be optimal under other less common circumstances\u000ain economics when an agent is risk neutral the objective function is simply expressed in monetary terms such as profit income or endofperiod wealth\u000abut for riskaverse or riskloving agents loss is measured as the negative of a utility function which represents satisfaction and is usually interpreted in ordinal terms rather than in cardinal absolute terms\u000aother measures of cost are possible for example mortality or morbidity in the field of public health or safety engineering\u000afor most optimization algorithms it is desirable to have a loss function that is globally continuous and differentiable\u000atwo very commonly used loss functions are the squared loss  and the absolute loss  however the absolute loss has the disadvantage that it is not differentiable at  the squared loss has the disadvantage that it has the tendency to be dominated by outlierswhen summing over a set of s as in   the final sum tends to be the result of a few particularly large avalues rather than an expression of the average avalue\u000athe choice of a loss function is not arbitrary it is very restrictive and sometimes the loss function may be characterized by its desirable properties among the choice principles are for example the requirement of completeness of the class of symmetric statistics in the case of iid observations the principle of complete information and some others\u000a\u000a\u000a loss functions in bayesian statisticsedit \u000aone of the consequences of bayesian inference is that in addition to experimental data the loss function does not in itself wholly determine a decision what is important is the relationship between the loss function and the posterior probability so it is possible to have two different loss functions which lead to the same decision when the prior probability distributions associated with each compensate for the details of each loss function\u000acombining the three elements of the prior probability the data and the loss function then allows decisions to be based on maximizing the subjective expected utility a concept introduced by leonard j savage\u000a\u000a\u000a regretedit \u000a\u000asavage also argued that using nonbayesian methods such as minimax the loss function should be based on the idea of regret ie the loss associated with a decision should be the difference between the consequences of the best decision that could have been taken had the underlying circumstances been known and the decision that was in fact taken before they were known\u000a\u000a\u000a quadratic loss functionedit \u000athe use of a quadratic loss function is common for example when using least squares techniques it is often more mathematically tractable than other loss functions because of the properties of variances as well as being symmetric an error above the target causes the same loss as the same magnitude of error below the target if the target is t then a quadratic loss function is\u000a\u000afor some constant c the value of the constant makes no difference to a decision and can be ignored by setting it equal to 1\u000amany common statistics including ttests regression models design of experiments and much else use least squares methods applied using linear regression theory which is based on the quadratric loss function\u000athe quadratic loss function is also used in linearquadratic optimal control problems in these problems even in the absence of uncertainty it may not be possible to achieve the desired values of all target variables often loss is expressed as a quadratic form in the deviations of the variables of interest from their desired values this approach is tractable because it results in linear firstorder conditions in the context of stochastic control the expected value of the quadratic form is used\u000a\u000a\u000a 01 loss functionedit \u000ain statistics and decision theory a frequently used loss function is the 01 loss function\u000a\u000awhere  is the indicator notation\u000a\u000a\u000a see alsoedit \u000aloss functions for classification\u000adiscounted maximum loss\u000ahinge loss\u000ascoring rule\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaretz kevin bartram shnke m pope peter f apriljune 2011 asymmetric loss functions and the rationality of expected stock returns international journal of forecasting 27 2 413437 doi101016jijforecast200910008 \u000aberger james o 1985 statistical decision theory and bayesian analysis 2nd ed new york springerverlag isbn 0387960988 mr 0804611
p267
sg20
g23
sg24
g27
sg30
Vin mathematical optimization statistics decision theory and machine learning a loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some cost associated with the event an optimization problem seeks to minimize a loss function an objective function is either a loss function or its negative sometimes called a reward function a profit function a utility function a fitness function etc in which case it is to be maximized\u000ain statistics typically a loss function is used for parameter estimation and the event in question is some function of the difference between estimated and true values for an instance of data the concept as old as laplace was reintroduced in statistics by abraham wald in the middle of the 20th century in the context of economics for example this is usually economic cost or regret in classification it is the penalty for an incorrect classification of an example in actuarial science it is used in an insurance context to model benefits paid over premiums particularly since the works of harald cramr in the 1920s in optimal control the loss is the penalty for failing to achieve a desired value in financial risk management the function is precisely mapped to a monetary loss\u000a\u000a\u000a use in statisticsedit \u000aparameter estimation for supervised learning tasks such as regression or classification can be formulated as the minimization of a loss function over a training set the goal of estimation is to find a function that models its input well if it were applied to the training set it should predict the values or class labels associated with the samples in that set the loss function quantifies the amount by which the prediction deviates from the actual values\u000a\u000a\u000a definitionedit \u000aformally we begin by considering some family of distributions for a random variable x that is indexed by some \u000amore intuitively we can think of x as our data perhaps  where  iid the x is the set of things the decision rule will be making decisions on there exists some number of possible ways  to model our data x which our decision function can use to make decisions for a finite number of models we can thus think of  as the index to this family of probability models for an infinite family of models it is a set of parameters to the family of distributions\u000aon a more practical note it is important to understand that while it is tempting to think of loss functions as necessarily parametric since they seem to take  as a parameter the fact that  is infinitedimensional is completely incompatible with this notion for example if the family of probability functions is uncountably infinite  indexes an uncountably infinite space\u000afrom here given a set a of possible actions a decision rule is a function    a\u000aa loss function is a real lowerbounded function l on   a for some    the value l x is the cost of action x under parameter \u000a\u000a\u000a expected lossedit \u000athe value of the loss function itself is a random quantity because it depends on the outcome of a random variable x both frequentist and bayesian statistical theory involve making a decision based on the expected value of the loss function however this quantity is defined differently under the two paradigms\u000a\u000a\u000a frequentist expected lossedit \u000awe first define the expected loss in the frequentist context it is obtained by taking the expected value with respect to the probability distribution p of the observed data x this is also referred to as the risk function  of the decision rule  and the parameter  here the decision rule depends on the outcome of x the risk function is given by\u000a\u000a\u000a bayesian expected lossedit \u000ain a bayesian approach the expectation is calculated using the posterior distribution  of the parameter \u000a\u000aone then should choose the action a which minimises the expected loss although this will result in choosing the same action as would be chosen using the frequentist risk the emphasis of the bayesian approach is that one is only interested in choosing the optimal action under the actual observed data whereas choosing the actual bayes optimal decision rule which is a function of all possible observations is a much more difficult problem\u000a\u000a\u000a economic choice under uncertaintyedit \u000ain economics decisionmaking under uncertainty is often modelled using the von neumannmorgenstern utility function of the uncertain variable of interest such as endofperiod wealth since the value of this variable is uncertain so is the value of the utility function it is the expected value of utility that is maximized\u000a\u000a\u000a examplesedit \u000afor a scalar parameter  a decision function whose output  is an estimate of  and a quadratic loss function\u000a\u000athe risk function becomes the mean squared error of the estimate\u000a\u000ain density estimation the unknown parameter is probability density itself the loss function is typically chosen to be a norm in an appropriate function space for example for l2 norm\u000a\u000athe risk function becomes the mean integrated squared error\u000a\u000a\u000a decision rulesedit \u000aa decision rule makes a choice using an optimality criterion some commonly used criteria are\u000aminimax choose the decision rule with the lowest worst loss  that is minimize the worstcase maximum possible loss\u000a\u000ainvariance choose the optimal decision rule which satisfies an invariance requirement\u000achoose the decision rule with the lowest average loss ie minimize the expected value of the loss function\u000a\u000a\u000a selecting a loss functionedit \u000asound statistical practice requires selecting an estimator consistent with the actual acceptable variation experienced in the context of a particular applied problem thus in the applied use of loss functions selecting which statistical method to use to model an applied problem depends on knowing the losses that will be experienced from being wrong under the problems particular circumstances\u000aa common example involves estimating location under typical statistical assumptions the mean or average is the statistic for estimating location that minimizes the expected loss experienced under the squarederror loss function while the median is the estimator that minimizes expected loss experienced under the absolutedifference loss function still different estimators would be optimal under other less common circumstances\u000ain economics when an agent is risk neutral the objective function is simply expressed in monetary terms such as profit income or endofperiod wealth\u000abut for riskaverse or riskloving agents loss is measured as the negative of a utility function which represents satisfaction and is usually interpreted in ordinal terms rather than in cardinal absolute terms\u000aother measures of cost are possible for example mortality or morbidity in the field of public health or safety engineering\u000afor most optimization algorithms it is desirable to have a loss function that is globally continuous and differentiable\u000atwo very commonly used loss functions are the squared loss  and the absolute loss  however the absolute loss has the disadvantage that it is not differentiable at  the squared loss has the disadvantage that it has the tendency to be dominated by outlierswhen summing over a set of s as in   the final sum tends to be the result of a few particularly large avalues rather than an expression of the average avalue\u000athe choice of a loss function is not arbitrary it is very restrictive and sometimes the loss function may be characterized by its desirable properties among the choice principles are for example the requirement of completeness of the class of symmetric statistics in the case of iid observations the principle of complete information and some others\u000a\u000a\u000a loss functions in bayesian statisticsedit \u000aone of the consequences of bayesian inference is that in addition to experimental data the loss function does not in itself wholly determine a decision what is important is the relationship between the loss function and the posterior probability so it is possible to have two different loss functions which lead to the same decision when the prior probability distributions associated with each compensate for the details of each loss function\u000acombining the three elements of the prior probability the data and the loss function then allows decisions to be based on maximizing the subjective expected utility a concept introduced by leonard j savage\u000a\u000a\u000a regretedit \u000a\u000asavage also argued that using nonbayesian methods such as minimax the loss function should be based on the idea of regret ie the loss associated with a decision should be the difference between the consequences of the best decision that could have been taken had the underlying circumstances been known and the decision that was in fact taken before they were known\u000a\u000a\u000a quadratic loss functionedit \u000athe use of a quadratic loss function is common for example when using least squares techniques it is often more mathematically tractable than other loss functions because of the properties of variances as well as being symmetric an error above the target causes the same loss as the same magnitude of error below the target if the target is t then a quadratic loss function is\u000a\u000afor some constant c the value of the constant makes no difference to a decision and can be ignored by setting it equal to 1\u000amany common statistics including ttests regression models design of experiments and much else use least squares methods applied using linear regression theory which is based on the quadratric loss function\u000athe quadratic loss function is also used in linearquadratic optimal control problems in these problems even in the absence of uncertainty it may not be possible to achieve the desired values of all target variables often loss is expressed as a quadratic form in the deviations of the variables of interest from their desired values this approach is tractable because it results in linear firstorder conditions in the context of stochastic control the expected value of the quadratic form is used\u000a\u000a\u000a 01 loss functionedit \u000ain statistics and decision theory a frequently used loss function is the 01 loss function\u000a\u000awhere  is the indicator notation\u000a\u000a\u000a see alsoedit \u000aloss functions for classification\u000adiscounted maximum loss\u000ahinge loss\u000ascoring rule\u000a\u000a\u000a referencesedit \u000a\u000a\u000a further readingedit \u000aaretz kevin bartram shnke m pope peter f apriljune 2011 asymmetric loss functions and the rationality of expected stock returns international journal of forecasting 27 2 413437 doi101016jijforecast200910008 \u000aberger james o 1985 statistical decision theory and bayesian analysis 2nd ed new york springerverlag isbn 0387960988 mr 0804611
p268
sg32
g35
sg37
NsbsS'asymptotic_theory_(statistics).txt'
p269
g2
(g3
g4
Ntp270
Rp271
(dp272
g8
g11
sg12
Vin statistics asymptotic theory or large sample theory is a generic framework for assessment of properties of estimators and statistical tests within this framework it is typically assumed that the sample size n grows indefinitely and the properties of statistical procedures are evaluated in the limit as n  \u000ain practical applications asymptotic theory is applied by treating the asymptotic results as approximately valid for finite sample sizes as well such approach is often criticized for not having any mathematical grounds behind it yet it is used ubiquitously anyway the importance of the asymptotic theory is that it often makes possible to carry out the analysis and state many results which cannot be obtained within the standard finitesample theory\u000a\u000a\u000a overview \u000amost statistical problems begin with a dataset of size n the asymptotic theory proceeds by assuming that it is possible to keep collecting additional data so that the sample size would grow infinitely\u000a\u000aunder this assumption many results can be obtained that are unavailable for samples of finite sizes as an example consider the law of large numbers this law states that for a sequence of iid random variables x1 x2  the sample averages  converge in probability to the population mean exi as n   at the same time for finite n it is impossible to claim anything about the distribution of  if the distributions of individual xis is unknown\u000afor various models slightly different modes of asymptotics may be used\u000afor crosssectional data iid the new observations are sampled independently from the same fixed distribution this is the standard case of n   asymptotics\u000afor longitudinal data time series the sampling method may differ from model to model sometimes the data is assumed to be ergodic in other applications it can be integrated or cointegrated in this case the asymptotic is again taken as the number of observations usually denoted t in this case goes to infinity t  \u000afor panel data it is commonly assumed that one dimension in the data t remains fixed whereas the other dimension grows t  const n  \u000abesides these standard approaches various other alternative asymptotic approaches exist\u000awithin the local asymptotic normality framework it is assumed that the value of the true parameter in the model varies slightly with n such that the nth model corresponds to  this approach lets us study the regularity of estimators\u000awhen statistical tests are studied for their power to distinguish against the alternatives that are close to the null hypothesis it is done within the socalled local alternatives framework the null hypothesis is h0   0 and the alternative is h1  this approach is especially popular for the unit root tests\u000athere are models where the dimension of the parameter space n slowly expands with n reflecting the fact that the more observations a statistician has the more he is tempted to introduce additional parameters in the model an example of this is the weak instruments asymptotic\u000ain kernel density estimation and kernel regression additional parameter  the bandwidth h  is assumed in these models it is typically taken that h  0 as n   however the rate of convergence must be chosen carefully usually h  n15\u000a\u000a\u000a modes of convergence of random variables \u000a\u000a\u000a asymptotic properties \u000a\u000a\u000a estimators \u000aconsistency a sequence of estimators is said to be consistent if it converges in probability to the true value of the parameter being estimated\u000a\u000agenerally an estimator is just some more or less arbitrary function of the data the property of consistency requires that the estimator was estimating the quantity we intended it to as such it is the most important property in the estimation theory estimators that are known to be inconsistent are never used in practice\u000aasymptotic distribution if it is possible to find sequences of nonrandom constants an bn possibly depending on the value of 0 and a nondegenerate distribution g such that\u000a\u000athen the sequence of estimators  is said to have the asymptotic distribution g\u000amost often the estimators encountered in practice have the asymptotically normal distribution with an  0 bn  n and g  n0 v\u000a\u000aasymptotic confidence regions\u000aregularity\u000a\u000a\u000a asymptotic theorems \u000alaw of large numbers\u000acentral limit theorem\u000aslutskys theorem\u000acontinuous mapping theorem\u000a\u000a\u000a notes \u000a\u000a\u000a
p273
sg14
g17
sg18
Vin statistics asymptotic theory or large sample theory is a generic framework for assessment of properties of estimators and statistical tests within this framework it is typically assumed that the sample size n grows indefinitely and the properties of statistical procedures are evaluated in the limit as n  \u000ain practical applications asymptotic theory is applied by treating the asymptotic results as approximately valid for finite sample sizes as well such approach is often criticized for not having any mathematical grounds behind it yet it is used ubiquitously anyway the importance of the asymptotic theory is that it often makes possible to carry out the analysis and state many results which cannot be obtained within the standard finitesample theory\u000a\u000a\u000a overview \u000amost statistical problems begin with a dataset of size n the asymptotic theory proceeds by assuming that it is possible to keep collecting additional data so that the sample size would grow infinitely\u000a\u000aunder this assumption many results can be obtained that are unavailable for samples of finite sizes as an example consider the law of large numbers this law states that for a sequence of iid random variables x1 x2  the sample averages  converge in probability to the population mean exi as n   at the same time for finite n it is impossible to claim anything about the distribution of  if the distributions of individual xis is unknown\u000afor various models slightly different modes of asymptotics may be used\u000afor crosssectional data iid the new observations are sampled independently from the same fixed distribution this is the standard case of n   asymptotics\u000afor longitudinal data time series the sampling method may differ from model to model sometimes the data is assumed to be ergodic in other applications it can be integrated or cointegrated in this case the asymptotic is again taken as the number of observations usually denoted t in this case goes to infinity t  \u000afor panel data it is commonly assumed that one dimension in the data t remains fixed whereas the other dimension grows t  const n  \u000abesides these standard approaches various other alternative asymptotic approaches exist\u000awithin the local asymptotic normality framework it is assumed that the value of the true parameter in the model varies slightly with n such that the nth model corresponds to  this approach lets us study the regularity of estimators\u000awhen statistical tests are studied for their power to distinguish against the alternatives that are close to the null hypothesis it is done within the socalled local alternatives framework the null hypothesis is h0   0 and the alternative is h1  this approach is especially popular for the unit root tests\u000athere are models where the dimension of the parameter space n slowly expands with n reflecting the fact that the more observations a statistician has the more he is tempted to introduce additional parameters in the model an example of this is the weak instruments asymptotic\u000ain kernel density estimation and kernel regression additional parameter  the bandwidth h  is assumed in these models it is typically taken that h  0 as n   however the rate of convergence must be chosen carefully usually h  n15\u000a\u000a\u000a modes of convergence of random variables \u000a\u000a\u000a asymptotic properties \u000a\u000a\u000a estimators \u000aconsistency a sequence of estimators is said to be consistent if it converges in probability to the true value of the parameter being estimated\u000a\u000agenerally an estimator is just some more or less arbitrary function of the data the property of consistency requires that the estimator was estimating the quantity we intended it to as such it is the most important property in the estimation theory estimators that are known to be inconsistent are never used in practice\u000aasymptotic distribution if it is possible to find sequences of nonrandom constants an bn possibly depending on the value of 0 and a nondegenerate distribution g such that\u000a\u000athen the sequence of estimators  is said to have the asymptotic distribution g\u000amost often the estimators encountered in practice have the asymptotically normal distribution with an  0 bn  n and g  n0 v\u000a\u000aasymptotic confidence regions\u000aregularity\u000a\u000a\u000a asymptotic theorems \u000alaw of large numbers\u000acentral limit theorem\u000aslutskys theorem\u000acontinuous mapping theorem\u000a\u000a\u000a notes
p274
sg20
g23
sg24
g27
sg30
Vin statistics asymptotic theory or large sample theory is a generic framework for assessment of properties of estimators and statistical tests within this framework it is typically assumed that the sample size n grows indefinitely and the properties of statistical procedures are evaluated in the limit as n  \u000ain practical applications asymptotic theory is applied by treating the asymptotic results as approximately valid for finite sample sizes as well such approach is often criticized for not having any mathematical grounds behind it yet it is used ubiquitously anyway the importance of the asymptotic theory is that it often makes possible to carry out the analysis and state many results which cannot be obtained within the standard finitesample theory\u000a\u000a\u000a overview \u000amost statistical problems begin with a dataset of size n the asymptotic theory proceeds by assuming that it is possible to keep collecting additional data so that the sample size would grow infinitely\u000a\u000aunder this assumption many results can be obtained that are unavailable for samples of finite sizes as an example consider the law of large numbers this law states that for a sequence of iid random variables x1 x2  the sample averages  converge in probability to the population mean exi as n   at the same time for finite n it is impossible to claim anything about the distribution of  if the distributions of individual xis is unknown\u000afor various models slightly different modes of asymptotics may be used\u000afor crosssectional data iid the new observations are sampled independently from the same fixed distribution this is the standard case of n   asymptotics\u000afor longitudinal data time series the sampling method may differ from model to model sometimes the data is assumed to be ergodic in other applications it can be integrated or cointegrated in this case the asymptotic is again taken as the number of observations usually denoted t in this case goes to infinity t  \u000afor panel data it is commonly assumed that one dimension in the data t remains fixed whereas the other dimension grows t  const n  \u000abesides these standard approaches various other alternative asymptotic approaches exist\u000awithin the local asymptotic normality framework it is assumed that the value of the true parameter in the model varies slightly with n such that the nth model corresponds to  this approach lets us study the regularity of estimators\u000awhen statistical tests are studied for their power to distinguish against the alternatives that are close to the null hypothesis it is done within the socalled local alternatives framework the null hypothesis is h0   0 and the alternative is h1  this approach is especially popular for the unit root tests\u000athere are models where the dimension of the parameter space n slowly expands with n reflecting the fact that the more observations a statistician has the more he is tempted to introduce additional parameters in the model an example of this is the weak instruments asymptotic\u000ain kernel density estimation and kernel regression additional parameter  the bandwidth h  is assumed in these models it is typically taken that h  0 as n   however the rate of convergence must be chosen carefully usually h  n15\u000a\u000a\u000a modes of convergence of random variables \u000a\u000a\u000a asymptotic properties \u000a\u000a\u000a estimators \u000aconsistency a sequence of estimators is said to be consistent if it converges in probability to the true value of the parameter being estimated\u000a\u000agenerally an estimator is just some more or less arbitrary function of the data the property of consistency requires that the estimator was estimating the quantity we intended it to as such it is the most important property in the estimation theory estimators that are known to be inconsistent are never used in practice\u000aasymptotic distribution if it is possible to find sequences of nonrandom constants an bn possibly depending on the value of 0 and a nondegenerate distribution g such that\u000a\u000athen the sequence of estimators  is said to have the asymptotic distribution g\u000amost often the estimators encountered in practice have the asymptotically normal distribution with an  0 bn  n and g  n0 v\u000a\u000aasymptotic confidence regions\u000aregularity\u000a\u000a\u000a asymptotic theorems \u000alaw of large numbers\u000acentral limit theorem\u000aslutskys theorem\u000acontinuous mapping theorem\u000a\u000a\u000a notes \u000a\u000a\u000a
p275
sg32
g35
sg37
NsbsS'statistical_parameter.txt'
p276
g2
(g3
g4
Ntp277
Rp278
(dp279
g8
g11
sg12
Va statistical parameter is a parameter that indexes a family of probability distributions it can be regarded as a numerical characteristic of a population or a statistical model\u000a\u000a\u000a definitionedit \u000aamong parameterized families of distributions are the normal distributions the poisson distributions the binomial distributions and the exponential family of distributions the family of normal distributions has two parameters the mean and the variance if these are specified the distribution is known exactly the family of chisquared distributions on the other hand has only one parameter the number of degrees of freedom\u000ain statistical inference parameters are sometimes taken to be unobservable and in this case the statisticians task is to infer what they can about the parameter based on observations of random variables distributed according to the probability distribution in question or more concretely stated based on a random sample taken from the population of interest in other situations parameters may be fixed by the nature of the sampling procedure used or the kind of statistical procedure being carried out for example the number of degrees of freedom in a pearsons chisquared test\u000aeven if a family of distributions is not specified quantities such as the mean and variance can still be regarded as parameters of the distribution of the population from which a sample is drawn statistical procedures can still attempt to make inferences about such population parameters parameters of this type are given names appropriate to their roles including\u000a\u000alocation parameter\u000adispersion parameter or scale parameter\u000ashape parameter\u000a\u000awhere a probability distribution has a domain over a set of objects that are themselves probability distributions the term concentration parameter is used for quantities that index how variable the outcomes would be\u000aquantities such as regression coefficients are statistical parameters in the above sense since they index the family of conditional probability distributions that describe how the dependent variables are related to the independent variables\u000a\u000a\u000a examplesedit \u000aa parameter is to a population as a statistic is to a sample at a particular time there may be some parameter for the percentage of all voters in a whole country who prefer a particular electoral candidate but it is impractical to ask every voter before an election occurs what their candidate preferences are so a sample of voters will be polled and a statistic the percentage of the polled voters who preferred each candidate will be counted the statistic is then used to make inferences about the parameter the preferences of all voters similarly in some forms of testing of manufactured products rather than destructively testing all products only a sample of products are tested to gather statistics supporting an inference that all the products meet product design parameters\u000a\u000a\u000a see alsoedit \u000aprecision statistics another parameter not specific to any one distribution\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000a\u000a\u000a referencesedit 
p280
sg14
g17
sg18
Va statistical parameter is a parameter that indexes a family of probability distributions it can be regarded as a numerical characteristic of a population or a statistical model\u000a\u000a\u000a definitionedit \u000aamong parameterized families of distributions are the normal distributions the poisson distributions the binomial distributions and the exponential family of distributions the family of normal distributions has two parameters the mean and the variance if these are specified the distribution is known exactly the family of chisquared distributions on the other hand has only one parameter the number of degrees of freedom\u000ain statistical inference parameters are sometimes taken to be unobservable and in this case the statisticians task is to infer what they can about the parameter based on observations of random variables distributed according to the probability distribution in question or more concretely stated based on a random sample taken from the population of interest in other situations parameters may be fixed by the nature of the sampling procedure used or the kind of statistical procedure being carried out for example the number of degrees of freedom in a pearsons chisquared test\u000aeven if a family of distributions is not specified quantities such as the mean and variance can still be regarded as parameters of the distribution of the population from which a sample is drawn statistical procedures can still attempt to make inferences about such population parameters parameters of this type are given names appropriate to their roles including\u000a\u000alocation parameter\u000adispersion parameter or scale parameter\u000ashape parameter\u000a\u000awhere a probability distribution has a domain over a set of objects that are themselves probability distributions the term concentration parameter is used for quantities that index how variable the outcomes would be\u000aquantities such as regression coefficients are statistical parameters in the above sense since they index the family of conditional probability distributions that describe how the dependent variables are related to the independent variables\u000a\u000a\u000a examplesedit \u000aa parameter is to a population as a statistic is to a sample at a particular time there may be some parameter for the percentage of all voters in a whole country who prefer a particular electoral candidate but it is impractical to ask every voter before an election occurs what their candidate preferences are so a sample of voters will be polled and a statistic the percentage of the polled voters who preferred each candidate will be counted the statistic is then used to make inferences about the parameter the preferences of all voters similarly in some forms of testing of manufactured products rather than destructively testing all products only a sample of products are tested to gather statistics supporting an inference that all the products meet product design parameters\u000a\u000a\u000a see alsoedit \u000aprecision statistics another parameter not specific to any one distribution\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000a\u000a\u000a referencesedit
p281
sg20
g23
sg24
g27
sg30
Va statistical parameter is a parameter that indexes a family of probability distributions it can be regarded as a numerical characteristic of a population or a statistical model\u000a\u000a\u000a definitionedit \u000aamong parameterized families of distributions are the normal distributions the poisson distributions the binomial distributions and the exponential family of distributions the family of normal distributions has two parameters the mean and the variance if these are specified the distribution is known exactly the family of chisquared distributions on the other hand has only one parameter the number of degrees of freedom\u000ain statistical inference parameters are sometimes taken to be unobservable and in this case the statisticians task is to infer what they can about the parameter based on observations of random variables distributed according to the probability distribution in question or more concretely stated based on a random sample taken from the population of interest in other situations parameters may be fixed by the nature of the sampling procedure used or the kind of statistical procedure being carried out for example the number of degrees of freedom in a pearsons chisquared test\u000aeven if a family of distributions is not specified quantities such as the mean and variance can still be regarded as parameters of the distribution of the population from which a sample is drawn statistical procedures can still attempt to make inferences about such population parameters parameters of this type are given names appropriate to their roles including\u000a\u000alocation parameter\u000adispersion parameter or scale parameter\u000ashape parameter\u000a\u000awhere a probability distribution has a domain over a set of objects that are themselves probability distributions the term concentration parameter is used for quantities that index how variable the outcomes would be\u000aquantities such as regression coefficients are statistical parameters in the above sense since they index the family of conditional probability distributions that describe how the dependent variables are related to the independent variables\u000a\u000a\u000a examplesedit \u000aa parameter is to a population as a statistic is to a sample at a particular time there may be some parameter for the percentage of all voters in a whole country who prefer a particular electoral candidate but it is impractical to ask every voter before an election occurs what their candidate preferences are so a sample of voters will be polled and a statistic the percentage of the polled voters who preferred each candidate will be counted the statistic is then used to make inferences about the parameter the preferences of all voters similarly in some forms of testing of manufactured products rather than destructively testing all products only a sample of products are tested to gather statistics supporting an inference that all the products meet product design parameters\u000a\u000a\u000a see alsoedit \u000aprecision statistics another parameter not specific to any one distribution\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000a\u000a\u000a referencesedit 
p282
sg32
g35
sg37
NsbsS'conditionality_principle.txt'
p283
g2
(g3
g4
Ntp284
Rp285
(dp286
g8
g11
sg12
Vthe conditionality principle is a fisherian principle of statistical inference that allan birnbaum formally defined and studied in his 1962 jasa article informally the conditionality principle can be taken as the claim that experiments which were not actually performed are statistically irrelevant\u000atogether with the sufficiency principle birnbaums version of the principle implies the famous likelihood principle although the relevance of the proof to data analysis remains controversial among statisticians many bayesians and likelihoodists consider the likelihood principle foundational for statistical inference\u000a\u000a\u000a formulation \u000athe conditionality principle makes an assertion about an experiment e that can be described as a mixture of several component experiments eh where h is an ancillary statistic ie a statistic whose probability distribution does not depend on unknown parameter values this means that observing a specific outcome x of experiment e is equivalent to observing the value of h and taking an observation xh from the component experiment eh\u000athe conditionality principle can be formally stated thus\u000aconditionality principle if e is any experiment having the form of a mixture of component experiments eh then for each outcome  of e  the evidential meaning of any outcome x of any mixture experiment e is the same as that of the corresponding outcome xh of the corresponding component experiment eh ignoring the overall structure of the mixed experiment see birnbaum 1962 an illustration of the conditionality principle in a bioinformatics context is given by barker 2014\u000a\u000a\u000a
p287
sg14
g17
sg18
Vthe conditionality principle is a fisherian principle of statistical inference that allan birnbaum formally defined and studied in his 1962 jasa article informally the conditionality principle can be taken as the claim that experiments which were not actually performed are statistically irrelevant\u000atogether with the sufficiency principle birnbaums version of the principle implies the famous likelihood principle although the relevance of the proof to data analysis remains controversial among statisticians many bayesians and likelihoodists consider the likelihood principle foundational for statistical inference\u000a\u000a\u000a formulation \u000athe conditionality principle makes an assertion about an experiment e that can be described as a mixture of several component experiments eh where h is an ancillary statistic ie a statistic whose probability distribution does not depend on unknown parameter values this means that observing a specific outcome x of experiment e is equivalent to observing the value of h and taking an observation xh from the component experiment eh\u000athe conditionality principle can be formally stated thus\u000aconditionality principle if e is any experiment having the form of a mixture of component experiments eh then for each outcome  of e  the evidential meaning of any outcome x of any mixture experiment e is the same as that of the corresponding outcome xh of the corresponding component experiment eh ignoring the overall structure of the mixed experiment see birnbaum 1962 an illustration of the conditionality principle in a bioinformatics context is given by barker 2014
p288
sg20
g23
sg24
g27
sg30
Vthe conditionality principle is a fisherian principle of statistical inference that allan birnbaum formally defined and studied in his 1962 jasa article informally the conditionality principle can be taken as the claim that experiments which were not actually performed are statistically irrelevant\u000atogether with the sufficiency principle birnbaums version of the principle implies the famous likelihood principle although the relevance of the proof to data analysis remains controversial among statisticians many bayesians and likelihoodists consider the likelihood principle foundational for statistical inference\u000a\u000a\u000a formulation \u000athe conditionality principle makes an assertion about an experiment e that can be described as a mixture of several component experiments eh where h is an ancillary statistic ie a statistic whose probability distribution does not depend on unknown parameter values this means that observing a specific outcome x of experiment e is equivalent to observing the value of h and taking an observation xh from the component experiment eh\u000athe conditionality principle can be formally stated thus\u000aconditionality principle if e is any experiment having the form of a mixture of component experiments eh then for each outcome  of e  the evidential meaning of any outcome x of any mixture experiment e is the same as that of the corresponding outcome xh of the corresponding component experiment eh ignoring the overall structure of the mixed experiment see birnbaum 1962 an illustration of the conditionality principle in a bioinformatics context is given by barker 2014\u000a\u000a\u000a
p289
sg32
g35
sg37
NsbsS'statistical_model.txt'
p290
g2
(g3
g4
Ntp291
Rp292
(dp293
g8
g11
sg12
Va statistical model embodies a set of assumptions concerning the generation of the observed data and similar data from a larger population a model represents often in considerably idealized form the datagenerating process the model assumptions describe a set of probability distributions some of which are assumed to adequately approximate the distribution from which a particular data set is sampled\u000aa model is usually specified by mathematical equations that relate one or more random variables and possibly other nonrandom variables as such a model is a formal representation of a theory herman adr quoting kenneth bollen\u000aall statistical hypothesis tests and all statistical estimators are derived from statistical models more generally statistical models are part of the foundation of statistical inference\u000a\u000a\u000a formal definitionedit \u000ain mathematical terms a statistical model is usually thought of as a pair  where  is the set of possible observations ie the sample space and  is a set of probability distributions on \u000athe intuition behind this definition is as follows it is assumed that there is a true probability distribution that generates the observed data we choose  to represent a set of distributions which contains a distribution that adequately approximates the true distribution note that we do not require that  contains the true distribution and in practice that is rarely the case indeed as burnham  anderson state a model is a simplification or approximation of reality and hence will not reflect all of realitywhence the saying all models are wrong\u000athe set  is almost always parameterized  the set  defines the parameters of the model a parameterization is generally required to have distinct parameter values give rise to distinct distributions ie to meet this condition  a parameterization that meets the condition is said to be identifiable\u000a\u000a\u000a an exampleedit \u000aheight and age are each probabilistically distributed over humans they are stochastically related when we know that a person is of age 10 this influences the chance of the person being 6 feet tall we could formalize that relationship in a linear regression model with the following form heighti  b0  b1agei  i where b0 is the intercept b1 is a parameter that age is multiplied by to get a prediction of height  is the error term and i identifies the person this implies that height is predicted by age with some error\u000aan admissible model must be consistent with all the data points thus the straight line heighti  b0  b1agei is not a model of the data the line cannot be a model unless it exactly fits all the data pointsie all the data points lie perfectly on a straight line the error term i must be included in the model so that the model is consistent with all the data points\u000ato do statistical inference we would first need to assume some probability distributions for the i for instance we might assume that the i distributions are iid gaussian with zero mean in this instance the model would have 3 parameters b0 b1 and the variance of the gaussian distribution\u000awe can formally specify the model in the form  as follows the sample space  of our model comprises the set of all possible pairs age height each possible value of   b0 b1 2 determines a distribution on  denote that distribution by  if  is the set of all possible values of  then  the parameterization is identifiable and this is easy to check\u000ain this example the model is determined by 1 specifying  and 2 making some assumptions relevant to  there are two assumptions that height can be approximated by a linear function of age that errors in the approximation are distributed as iid gaussian the assumptions are sufficient to specify as they are required to do\u000a\u000a\u000a general remarksedit \u000aa statistical model is a special type of mathematical model what distinguishes a statistical model from other mathematical models is that a statistical model is nondeterministic thus in a statistical model specified via mathematical equations some of the variables do not have specific values but instead have probability distributions ie some of the variables are stochastic in the example above  is a stochastic variable without that variable the model would be deterministic\u000astatistical models are often used even when the physical process being modeled is deterministic for instance coin tossing is in principle a deterministic process yet it is commonly modeled as stochastic via a bernoulli process\u000athere are three purposes for a statistical model according to konishi  kitagawa\u000apredictions\u000aextraction of information\u000adescription of stochastic structures\u000a\u000a\u000a dimension of a modeledit \u000asuppose that we have a statistical model  with  the model is said to be parametric if  has a finite dimension in notation we write that  where d is a positive integer  denotes the real numbers other sets can be used in principle here d is called the dimension of the model\u000aas an example if we assume that data arise from a univariate gaussian distribution then we are assuming that\u000a\u000ain this example the dimension d equals 2\u000aas another example suppose that the data consists of points x y that we assume are distributed according to a straight line with iid gaussian residuals with zero mean then the dimension of the statistical model is 3 the intercept of the line the slope of the line and the variance of the distribution of the residuals note that in geometry a straight line has dimension 1\u000aa statistical model is nonparametric if the parameter set  is infinite dimensional a statistical model is semiparametric if it has both finitedimensional and infinitedimensional parameters formally if d is the dimension of  and n is the number of samples both semiparametric and nonparemtric models have  as  if  as  then the model is semiparametric otherwise the model is nonparametric\u000aparametric models are by far the most commonlyused statistical models regarding semiparametric and nonparametric models sir david cox has said these typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\u000a\u000a\u000a nested modelsedit \u000atwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model for example the set of all gaussian distributions has nested within it the set of zeromean gaussian distributions we constrain the mean in the set of all gaussian distributions to get the zeromean distributions\u000ain that example the first model has a higher dimension than the second model the zeromean model has dimension 1 such is usually but not always the case as a different example the set of positivemean gaussian distributions which has dimension 2 is nested within the set of all gaussian distributions\u000a\u000a\u000a comparing modelsedit \u000a\u000ait is assumed that there is a true probability distribution that generates the observed data the main goal of model selection is to make statements about which elements of  are most likely to adequately approximate the true distribution\u000amodels can be compared to each other this can either be done when we have done an exploratory data analysis or a confirmatory data analysis in an exploratory analysis we formulate all models we can think of and see which describes our data best in a confirmatory analysis we check which of the models that we have described before the data was collected best fits the data or test if our only model fits the data\u000acommon criteria for comparing models include r2 bayes factor and the likelihoodratio test together with its generalization relative likelihood\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling they are typically formulated as comparisons of several statistical models relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aadr hj 2008 modelling in adr hj mellenbergh gj advising on research methods a consultants companion huizen the netherlands johannes van kessel publishing pp 271304 \u000aburnham k p anderson d r 2002 model selection and multimodel inference 2nd ed springerverlag isbn 0387953647 \u000acox dr 2006 principles of statistical inference cambridge university press \u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000amccullagh p 2002 what is a statistical model annals of statistics 30 12251310 \u000a\u000a\u000a further readingedit \u000adavison ac 2008 statistical models cambridge university press\u000afreedman da 2009 statistical models cambridge university press\u000ahelland is 2010 steps towards a unified basis for scientific models and methods world scientific\u000akroese dp chan jcc 2014 statistical modeling and computation springer\u000astapleton jh 2007 models for probability and statistical inference wileyinterscience
p294
sg14
g17
sg18
Va statistical model embodies a set of assumptions concerning the generation of the observed data and similar data from a larger population a model represents often in considerably idealized form the datagenerating process the model assumptions describe a set of probability distributions some of which are assumed to adequately approximate the distribution from which a particular data set is sampled\u000aa model is usually specified by mathematical equations that relate one or more random variables and possibly other nonrandom variables as such a model is a formal representation of a theory herman adr quoting kenneth bollen\u000aall statistical hypothesis tests and all statistical estimators are derived from statistical models more generally statistical models are part of the foundation of statistical inference\u000a\u000a\u000a formal definitionedit \u000ain mathematical terms a statistical model is usually thought of as a pair  where  is the set of possible observations ie the sample space and  is a set of probability distributions on \u000athe intuition behind this definition is as follows it is assumed that there is a true probability distribution that generates the observed data we choose  to represent a set of distributions which contains a distribution that adequately approximates the true distribution note that we do not require that  contains the true distribution and in practice that is rarely the case indeed as burnham  anderson state a model is a simplification or approximation of reality and hence will not reflect all of realitywhence the saying all models are wrong\u000athe set  is almost always parameterized  the set  defines the parameters of the model a parameterization is generally required to have distinct parameter values give rise to distinct distributions ie to meet this condition  a parameterization that meets the condition is said to be identifiable\u000a\u000a\u000a an exampleedit \u000aheight and age are each probabilistically distributed over humans they are stochastically related when we know that a person is of age 10 this influences the chance of the person being 6 feet tall we could formalize that relationship in a linear regression model with the following form heighti  b0  b1agei  i where b0 is the intercept b1 is a parameter that age is multiplied by to get a prediction of height  is the error term and i identifies the person this implies that height is predicted by age with some error\u000aan admissible model must be consistent with all the data points thus the straight line heighti  b0  b1agei is not a model of the data the line cannot be a model unless it exactly fits all the data pointsie all the data points lie perfectly on a straight line the error term i must be included in the model so that the model is consistent with all the data points\u000ato do statistical inference we would first need to assume some probability distributions for the i for instance we might assume that the i distributions are iid gaussian with zero mean in this instance the model would have 3 parameters b0 b1 and the variance of the gaussian distribution\u000awe can formally specify the model in the form  as follows the sample space  of our model comprises the set of all possible pairs age height each possible value of   b0 b1 2 determines a distribution on  denote that distribution by  if  is the set of all possible values of  then  the parameterization is identifiable and this is easy to check\u000ain this example the model is determined by 1 specifying  and 2 making some assumptions relevant to  there are two assumptions that height can be approximated by a linear function of age that errors in the approximation are distributed as iid gaussian the assumptions are sufficient to specify as they are required to do\u000a\u000a\u000a general remarksedit \u000aa statistical model is a special type of mathematical model what distinguishes a statistical model from other mathematical models is that a statistical model is nondeterministic thus in a statistical model specified via mathematical equations some of the variables do not have specific values but instead have probability distributions ie some of the variables are stochastic in the example above  is a stochastic variable without that variable the model would be deterministic\u000astatistical models are often used even when the physical process being modeled is deterministic for instance coin tossing is in principle a deterministic process yet it is commonly modeled as stochastic via a bernoulli process\u000athere are three purposes for a statistical model according to konishi  kitagawa\u000apredictions\u000aextraction of information\u000adescription of stochastic structures\u000a\u000a\u000a dimension of a modeledit \u000asuppose that we have a statistical model  with  the model is said to be parametric if  has a finite dimension in notation we write that  where d is a positive integer  denotes the real numbers other sets can be used in principle here d is called the dimension of the model\u000aas an example if we assume that data arise from a univariate gaussian distribution then we are assuming that\u000a\u000ain this example the dimension d equals 2\u000aas another example suppose that the data consists of points x y that we assume are distributed according to a straight line with iid gaussian residuals with zero mean then the dimension of the statistical model is 3 the intercept of the line the slope of the line and the variance of the distribution of the residuals note that in geometry a straight line has dimension 1\u000aa statistical model is nonparametric if the parameter set  is infinite dimensional a statistical model is semiparametric if it has both finitedimensional and infinitedimensional parameters formally if d is the dimension of  and n is the number of samples both semiparametric and nonparemtric models have  as  if  as  then the model is semiparametric otherwise the model is nonparametric\u000aparametric models are by far the most commonlyused statistical models regarding semiparametric and nonparametric models sir david cox has said these typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\u000a\u000a\u000a nested modelsedit \u000atwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model for example the set of all gaussian distributions has nested within it the set of zeromean gaussian distributions we constrain the mean in the set of all gaussian distributions to get the zeromean distributions\u000ain that example the first model has a higher dimension than the second model the zeromean model has dimension 1 such is usually but not always the case as a different example the set of positivemean gaussian distributions which has dimension 2 is nested within the set of all gaussian distributions\u000a\u000a\u000a comparing modelsedit \u000a\u000ait is assumed that there is a true probability distribution that generates the observed data the main goal of model selection is to make statements about which elements of  are most likely to adequately approximate the true distribution\u000amodels can be compared to each other this can either be done when we have done an exploratory data analysis or a confirmatory data analysis in an exploratory analysis we formulate all models we can think of and see which describes our data best in a confirmatory analysis we check which of the models that we have described before the data was collected best fits the data or test if our only model fits the data\u000acommon criteria for comparing models include r2 bayes factor and the likelihoodratio test together with its generalization relative likelihood\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling they are typically formulated as comparisons of several statistical models relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aadr hj 2008 modelling in adr hj mellenbergh gj advising on research methods a consultants companion huizen the netherlands johannes van kessel publishing pp 271304 \u000aburnham k p anderson d r 2002 model selection and multimodel inference 2nd ed springerverlag isbn 0387953647 \u000acox dr 2006 principles of statistical inference cambridge university press \u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000amccullagh p 2002 what is a statistical model annals of statistics 30 12251310 \u000a\u000a\u000a further readingedit \u000adavison ac 2008 statistical models cambridge university press\u000afreedman da 2009 statistical models cambridge university press\u000ahelland is 2010 steps towards a unified basis for scientific models and methods world scientific\u000akroese dp chan jcc 2014 statistical modeling and computation springer\u000astapleton jh 2007 models for probability and statistical inference wileyinterscience
p295
sg20
g23
sg24
g27
sg30
Va statistical model embodies a set of assumptions concerning the generation of the observed data and similar data from a larger population a model represents often in considerably idealized form the datagenerating process the model assumptions describe a set of probability distributions some of which are assumed to adequately approximate the distribution from which a particular data set is sampled\u000aa model is usually specified by mathematical equations that relate one or more random variables and possibly other nonrandom variables as such a model is a formal representation of a theory herman adr quoting kenneth bollen\u000aall statistical hypothesis tests and all statistical estimators are derived from statistical models more generally statistical models are part of the foundation of statistical inference\u000a\u000a\u000a formal definitionedit \u000ain mathematical terms a statistical model is usually thought of as a pair  where  is the set of possible observations ie the sample space and  is a set of probability distributions on \u000athe intuition behind this definition is as follows it is assumed that there is a true probability distribution that generates the observed data we choose  to represent a set of distributions which contains a distribution that adequately approximates the true distribution note that we do not require that  contains the true distribution and in practice that is rarely the case indeed as burnham  anderson state a model is a simplification or approximation of reality and hence will not reflect all of realitywhence the saying all models are wrong\u000athe set  is almost always parameterized  the set  defines the parameters of the model a parameterization is generally required to have distinct parameter values give rise to distinct distributions ie to meet this condition  a parameterization that meets the condition is said to be identifiable\u000a\u000a\u000a an exampleedit \u000aheight and age are each probabilistically distributed over humans they are stochastically related when we know that a person is of age 10 this influences the chance of the person being 6 feet tall we could formalize that relationship in a linear regression model with the following form heighti  b0  b1agei  i where b0 is the intercept b1 is a parameter that age is multiplied by to get a prediction of height  is the error term and i identifies the person this implies that height is predicted by age with some error\u000aan admissible model must be consistent with all the data points thus the straight line heighti  b0  b1agei is not a model of the data the line cannot be a model unless it exactly fits all the data pointsie all the data points lie perfectly on a straight line the error term i must be included in the model so that the model is consistent with all the data points\u000ato do statistical inference we would first need to assume some probability distributions for the i for instance we might assume that the i distributions are iid gaussian with zero mean in this instance the model would have 3 parameters b0 b1 and the variance of the gaussian distribution\u000awe can formally specify the model in the form  as follows the sample space  of our model comprises the set of all possible pairs age height each possible value of   b0 b1 2 determines a distribution on  denote that distribution by  if  is the set of all possible values of  then  the parameterization is identifiable and this is easy to check\u000ain this example the model is determined by 1 specifying  and 2 making some assumptions relevant to  there are two assumptions that height can be approximated by a linear function of age that errors in the approximation are distributed as iid gaussian the assumptions are sufficient to specify as they are required to do\u000a\u000a\u000a general remarksedit \u000aa statistical model is a special type of mathematical model what distinguishes a statistical model from other mathematical models is that a statistical model is nondeterministic thus in a statistical model specified via mathematical equations some of the variables do not have specific values but instead have probability distributions ie some of the variables are stochastic in the example above  is a stochastic variable without that variable the model would be deterministic\u000astatistical models are often used even when the physical process being modeled is deterministic for instance coin tossing is in principle a deterministic process yet it is commonly modeled as stochastic via a bernoulli process\u000athere are three purposes for a statistical model according to konishi  kitagawa\u000apredictions\u000aextraction of information\u000adescription of stochastic structures\u000a\u000a\u000a dimension of a modeledit \u000asuppose that we have a statistical model  with  the model is said to be parametric if  has a finite dimension in notation we write that  where d is a positive integer  denotes the real numbers other sets can be used in principle here d is called the dimension of the model\u000aas an example if we assume that data arise from a univariate gaussian distribution then we are assuming that\u000a\u000ain this example the dimension d equals 2\u000aas another example suppose that the data consists of points x y that we assume are distributed according to a straight line with iid gaussian residuals with zero mean then the dimension of the statistical model is 3 the intercept of the line the slope of the line and the variance of the distribution of the residuals note that in geometry a straight line has dimension 1\u000aa statistical model is nonparametric if the parameter set  is infinite dimensional a statistical model is semiparametric if it has both finitedimensional and infinitedimensional parameters formally if d is the dimension of  and n is the number of samples both semiparametric and nonparemtric models have  as  if  as  then the model is semiparametric otherwise the model is nonparametric\u000aparametric models are by far the most commonlyused statistical models regarding semiparametric and nonparametric models sir david cox has said these typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies\u000a\u000a\u000a nested modelsedit \u000atwo statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model for example the set of all gaussian distributions has nested within it the set of zeromean gaussian distributions we constrain the mean in the set of all gaussian distributions to get the zeromean distributions\u000ain that example the first model has a higher dimension than the second model the zeromean model has dimension 1 such is usually but not always the case as a different example the set of positivemean gaussian distributions which has dimension 2 is nested within the set of all gaussian distributions\u000a\u000a\u000a comparing modelsedit \u000a\u000ait is assumed that there is a true probability distribution that generates the observed data the main goal of model selection is to make statements about which elements of  are most likely to adequately approximate the true distribution\u000amodels can be compared to each other this can either be done when we have done an exploratory data analysis or a confirmatory data analysis in an exploratory analysis we formulate all models we can think of and see which describes our data best in a confirmatory analysis we check which of the models that we have described before the data was collected best fits the data or test if our only model fits the data\u000acommon criteria for comparing models include r2 bayes factor and the likelihoodratio test together with its generalization relative likelihood\u000akonishi  kitagawa state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling they are typically formulated as comparisons of several statistical models relatedly sir david cox has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aadr hj 2008 modelling in adr hj mellenbergh gj advising on research methods a consultants companion huizen the netherlands johannes van kessel publishing pp 271304 \u000aburnham k p anderson d r 2002 model selection and multimodel inference 2nd ed springerverlag isbn 0387953647 \u000acox dr 2006 principles of statistical inference cambridge university press \u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000amccullagh p 2002 what is a statistical model annals of statistics 30 12251310 \u000a\u000a\u000a further readingedit \u000adavison ac 2008 statistical models cambridge university press\u000afreedman da 2009 statistical models cambridge university press\u000ahelland is 2010 steps towards a unified basis for scientific models and methods world scientific\u000akroese dp chan jcc 2014 statistical modeling and computation springer\u000astapleton jh 2007 models for probability and statistical inference wileyinterscience
p296
sg32
g35
sg37
NsbsS'information_geometry.txt'
p297
g2
(g3
g4
Ntp298
Rp299
(dp300
g8
g11
sg12
Vinformation geometry is a branch of mathematics that applies the techniques of differential geometry to the field of probability theory this is done by taking probability distributions for a statistical model as the points of a riemannian manifold forming a statistical manifold the fisher information metric provides the riemannian metric\u000ainformation geometry reached maturity through the work of shunichi amari and other japanese mathematicians in the 1980s amari and nagaokas book methods of information geometry is cited by most works of the relatively young field due to its broad coverage of significant developments attained using the methods of information geometry up to the year 2000 many of these developments were previously only available in japaneselanguage publications\u000a\u000a\u000a introduction \u000athe following introduction is based on methods of information geometry\u000a\u000a\u000a information and probability \u000adefine an nset to be a set v with cardinality  to choose an element v value state point outcome from an nset v one needs to specify  bsets default b2 if one disregards all but the cardinality that is  nats of information are required to specify v equivalently  bits are needed\u000aby considering the occurrences  of values from  one has an alternate way to refer to  through  first one chooses an occurrence  which requires information of  bits to specify v one subtracts the excess information used to choose one  from all those linked to  this is  then  is the number of  portions fitting into  thus one needs  bits to choose one of them so the information variable size code length number of bits needed to refer to  considering its occurrences in a message is\u000a\u000afinally  is the normalized portion of information needed to code all occurrences of one  the averaged code length over all values is   is called the entropy of a random variable \u000a\u000a\u000a statistical model parameters \u000awith a probability distribution  one looks at a variable  through an observation context like a message or an experimental setup\u000athe context can often be identified by a set of parameters through combinatorial reasoning the parameters can have an arbitrary number of dimensions and can be very local or less so as long as the context given by a certain  produces every value of  ie the support  does not change as function of  every  determines one probability distribution for  basically all distributions for which there exists an explicit analytical formula fall into this category binomial normal poisson  the parameters in these cases have a concrete meaning in the underlying setup which is a statistical model for the context of \u000athe parameters are quite different in nature from  itself because they do not describe  but the observation context for \u000aa parameterization of the form\u000a\u000awith\u000a and \u000athat mixes different distributions  is called a mixture distribution mixture or parameterization or mixture for short all such parameterizations are related through an affine transformation  a parameterization with such a transformation rule is called flat\u000aa flat parameterization for  is an exponential or  parameterization because the parameters are in the exponent of  there are several important distributions like normal and poisson that fall into this category these distributions are collectively referred to as exponential family or family the manifold for such distributions is not affine but the  manifold is this is called affine the parameterization  for the exponential family can be mapped to the one above by making  another parameter and extend \u000a\u000a\u000a differential geometry applied to probability \u000ain information geometry the methods of differential geometry are applied to describe the space of probability distributions for one variable  this is done by using a coordinate or atlas  furthermore the probability  must be a differentiable and invertible function of  in this case the  are coordinates of the space and the latter is a differential manifold \u000aderivatives are defined as is usual for a differentiable manifold\u000a\u000awith  for  a realvalued function on \u000agiven a function  on  one may geometrize it by taking it to define a new manifold this is done by defining coordinate functions on this new manifold as\u000a\u000ain this way one geometricizes a function  by encoding it into the coordinates used to describe the system\u000afor  the inverse is  and the resulting manifold of  points is called the representation the  manifold itself is called the representation the  or representations in the sense used here does not refer to the parameterization families of the distribution\u000a\u000a\u000a tangent space \u000a\u000ain standard differential geometry the tangent space on a manifold  at a point  is given by\u000a\u000ain ordinary differential geometry there is no canonical coordinate system on the manifold thus typically all discussion must be with regard to an atlas that is with regard to functions on the manifold as a result tangent spaces and vectors are defined as operators acting on this space of functions so for example in ordinary differential geometry the basis vectors of the tangent space are the operators \u000ahowever with probability distributions  one can calculate valuewise so it is possible to express a tangent space vector directly as   representation  or   representation  and not as operators\u000a\u000a\u000a alpha representation \u000aimportant functions  of  are coded by a parameter  with the important values   and \u000amixed or representation    \u000aexponential or representation     \u000arepresentation       \u000adistributions that allow a flat parameterization  are called collectively family    or family  of distributions and the according manifold is called affine\u000athe  tangent vector is \u000a\u000a\u000a inner product \u000aone may introduce an inner product on the tangent space of manifold  at point  as a linear symmetric and positive definite map\u000a\u000athis allows a riemannian metric to be defined the resulting manifold is a riemannian manifold all of the usual concepts of ordinary differential geometry carry over including the norm\u000a\u000athe line element  the volume element  and the cotangent space\u000a\u000athat is the dual space to the tangent space  from these one may construct tensors as usual\u000a\u000a\u000a fisher metric as inner product \u000afor probability manifolds such an inner product is given by the fisher information metric\u000ahere are equivalent formulas of the fisher information metric\u000a the  base vector in the representation is also called the score\u000a\u000abecause \u000a\u000a this is the same for  and  families\u000a with mimimum  for  entails  and  is applied only to the first parameter and  only to the second is the kullbackleibler divergence or relative entropy applicable to the families\u000afor  one has  is the hellinger distance applicable to the family  also evaluates to the fisher metric\u000athis relation with a divergence  will be revisited further down\u000athe fisher metric is motivated by\u000ait satisfying the requirements for an inner product\u000aits invariance for a sufficient statistic deterministic mapping from one variable to another and more general  for  ie a broadened distribution has smaller \u000ait being the cramrrao bound therefore any  satisfying  belongs to \u000afor any  one has  therefore \u000aso  and therefore  and with inefficient estimator one gets the cramrrao bound \u000a\u000a\u000a affine connection \u000a\u000alike commonly done on riemann manifolds one may define an affine connection or covariant derivative\u000a\u000agiven vector fields  and  lying in the tangent bundle  the affine connection  describes how to differentiate the vector field  along the direction it is itself a vector field it is the sum of the infinitesimal change in the vector field  as one moves along the direction  plus the infinitessimal change of the vector  due to its parallel transport along the direction  that is it takes into account the changing nature of what it means to move a coordinate system in a parallel fashion as one moves about in the manifold in terms of the basis vectors  one has the components\u000a\u000athe  are christoffel symbols the affine connection may be used for defining curvature and torsion like is usual in riemannian geometry\u000a\u000a\u000a alpha connection \u000aa nonmetric connection is not determined by a metric tensor  instead it is and restricted by the requirement that the parallel transport  between points  and  must be a linear combination of the base vectors in  here\u000a\u000aexpresses the parallel transport of  as linear combination of the base vectors in  ie the new  minus the change note that it is not a tensor does not transform as a tensor\u000afor such a metric one can construct a dual connection  to make\u000a\u000afor parallel transport using  and \u000afor the mentioned families the affine connection is called the connection and can also be expressed in more ways\u000a\u000afor \u000a is a metric connection and  with \u000a\u000aie  is dual to  with respect to the fisher metric\u000aif  this is called affine its dual is then affine\u000a\u000aie 0affine and hence  ie 1affine\u000a\u000a\u000a divergence \u000aa function of two distributions points  with minimum  for  entails  and   is applied only to the first parameter and  only to the second  is the direction which brought the two points to be equal when applied to the first parameter and to diverge again when applied to the second parameter ie  the sign cancels in  which we can define to be a metric  if always positive\u000athe absolute derivative of  along  yields candidates for dual connections  this metric and the connections relate to the taylor series expansion  for the first parameter or second parameter here for the first parameter\u000a\u000athe term  is called the divergence or contrast function a good choice is  with  convex for  from jensens inequality it follows that  and for  we have\u000a\u000awhich is the kullbackleibler divergence or relative entropy applicable to the families in the above\u000a\u000ais the fisher metric for  a different  yields\u000a\u000athe hellinger distance applicable to the family is\u000a\u000ain this case  also evaluates to the fisher metric\u000a\u000a\u000a canonical divergence \u000awe now consider two manifolds  and  represented by two sets of coordinate functions  and  the corresponding tangent space basis vectors will be denoted by  and  the bilinear map  associates a quantity  to the dual base vectors this defines an affine connection  for  and affine connection  for  that keep  constant for parallel transport of  and  defined through  and \u000aif  is flat then there exists a coordinate system  that does not change over  in order to keep  constant  must not change either ie  is also flat furthermore in this case we can choose coordinate systems such that\u000a\u000aif  results as a function  on  then making  both coordinate system function sets describe  the connections are such though that  makes  flat and  makes  flat this dual space is denoted as \u000abecause of the linear transform between the flat coordinate systems we have  and \u000abecause  and so for  it is possible to define two potentials  and  through  and   legendre transform these are  and \u000athen\u000a and\u000a\u000a\u000athis naturally leads to the following definition of a canonical divergence\u000a\u000anote the summation that is a representation of the metric due to \u000a\u000a\u000a properties of divergence \u000athe meaning of the canonical divergence depends on the meaning of the metric  and vice versa    for the  metric fisher metric with the dual connections this is the relative entropy for the selfdual euclidean space  leads to \u000asimilar to the euclidean space the following holds\u000atriangular relation  just substitute \u000aif  is not dually flat then this generalizes to\u000athe last part drops in case of dual flatness  is the exponential map\u000apythagorean theorem for  and  meeting on orthogonal lines at    \u000afor  and  with  a autoparallel submanifold  implies that the geodesic connecting  and  is orthogonal to \u000aby projecting  onto  of a curve  one can calculate\u000athe divergence of the curve  where \u000aand  with \u000awith  this becomes \u000afor an autoparallel submanifold parallel transport in it can be expressed with the submanifolds base vectors ie  a onedimensional autoparallel submanifold is a geodesic\u000a\u000a\u000a canonical divergence for the exponential family \u000afor the exponential family  one has  applying  on both sides yields  the other potential    is entropy  and  was used  is the covariance of  the cramrrao bound ie an efficient estimator must be exponential\u000athe canonical divergence is given by the kullbackleibler divergence  and the triangulation is \u000athe minimal divergence to a submanifold given by a restriction like some constant  means maximizing  with  this corresponds to the maximum entropy principle\u000a\u000a\u000a canonical divergence for general alpha families \u000afor general affine manifolds with  one has\u000a\u000athe connection induced by the divergence is not flat unless  then the pythagorean theorem for two curves intersecting orthogonally at  is\u000a\u000a\u000a history \u000athe history of information geometry is associated with the discoveries of at least the following people and many others\u000asir ronald aylmer fisher\u000aharald cramr\u000acalyampudi radhakrishna rao\u000aharold jeffreys\u000asolomon kullback\u000ajeanlouis koszul\u000arichard leibler\u000aclaude shannon\u000aimre csiszr\u000acencov\u000abradley efron\u000apaul vos\u000ashunichi amari\u000ahiroshi nagaoka\u000arobert kass\u000ashinto eguchi\u000aole barndorffnielsen\u000afrank nielsen\u000agiovanni pistone\u000abernard hanzon\u000adamiano brigo\u000a\u000a\u000a applications \u000ainformation geometry can be applied where parametrized distributions play a role\u000ahere an incomplete list\u000astatistical inference\u000atime series and linear systems\u000aquantum systems\u000aneuronal networks\u000amachine learning\u000astatistical mechanics\u000abiology\u000astatistics\u000amathematical finance\u000a\u000a\u000a see also \u000aruppeiner geometry\u000a\u000a\u000a
p301
sg14
g17
sg18
Vinformation geometry is a branch of mathematics that applies the techniques of differential geometry to the field of probability theory this is done by taking probability distributions for a statistical model as the points of a riemannian manifold forming a statistical manifold the fisher information metric provides the riemannian metric\u000ainformation geometry reached maturity through the work of shunichi amari and other japanese mathematicians in the 1980s amari and nagaokas book methods of information geometry is cited by most works of the relatively young field due to its broad coverage of significant developments attained using the methods of information geometry up to the year 2000 many of these developments were previously only available in japaneselanguage publications\u000a\u000a\u000a introduction \u000athe following introduction is based on methods of information geometry\u000a\u000a\u000a information and probability \u000adefine an nset to be a set v with cardinality  to choose an element v value state point outcome from an nset v one needs to specify  bsets default b2 if one disregards all but the cardinality that is  nats of information are required to specify v equivalently  bits are needed\u000aby considering the occurrences  of values from  one has an alternate way to refer to  through  first one chooses an occurrence  which requires information of  bits to specify v one subtracts the excess information used to choose one  from all those linked to  this is  then  is the number of  portions fitting into  thus one needs  bits to choose one of them so the information variable size code length number of bits needed to refer to  considering its occurrences in a message is\u000a\u000afinally  is the normalized portion of information needed to code all occurrences of one  the averaged code length over all values is   is called the entropy of a random variable \u000a\u000a\u000a statistical model parameters \u000awith a probability distribution  one looks at a variable  through an observation context like a message or an experimental setup\u000athe context can often be identified by a set of parameters through combinatorial reasoning the parameters can have an arbitrary number of dimensions and can be very local or less so as long as the context given by a certain  produces every value of  ie the support  does not change as function of  every  determines one probability distribution for  basically all distributions for which there exists an explicit analytical formula fall into this category binomial normal poisson  the parameters in these cases have a concrete meaning in the underlying setup which is a statistical model for the context of \u000athe parameters are quite different in nature from  itself because they do not describe  but the observation context for \u000aa parameterization of the form\u000a\u000awith\u000a and \u000athat mixes different distributions  is called a mixture distribution mixture or parameterization or mixture for short all such parameterizations are related through an affine transformation  a parameterization with such a transformation rule is called flat\u000aa flat parameterization for  is an exponential or  parameterization because the parameters are in the exponent of  there are several important distributions like normal and poisson that fall into this category these distributions are collectively referred to as exponential family or family the manifold for such distributions is not affine but the  manifold is this is called affine the parameterization  for the exponential family can be mapped to the one above by making  another parameter and extend \u000a\u000a\u000a differential geometry applied to probability \u000ain information geometry the methods of differential geometry are applied to describe the space of probability distributions for one variable  this is done by using a coordinate or atlas  furthermore the probability  must be a differentiable and invertible function of  in this case the  are coordinates of the space and the latter is a differential manifold \u000aderivatives are defined as is usual for a differentiable manifold\u000a\u000awith  for  a realvalued function on \u000agiven a function  on  one may geometrize it by taking it to define a new manifold this is done by defining coordinate functions on this new manifold as\u000a\u000ain this way one geometricizes a function  by encoding it into the coordinates used to describe the system\u000afor  the inverse is  and the resulting manifold of  points is called the representation the  manifold itself is called the representation the  or representations in the sense used here does not refer to the parameterization families of the distribution\u000a\u000a\u000a tangent space \u000a\u000ain standard differential geometry the tangent space on a manifold  at a point  is given by\u000a\u000ain ordinary differential geometry there is no canonical coordinate system on the manifold thus typically all discussion must be with regard to an atlas that is with regard to functions on the manifold as a result tangent spaces and vectors are defined as operators acting on this space of functions so for example in ordinary differential geometry the basis vectors of the tangent space are the operators \u000ahowever with probability distributions  one can calculate valuewise so it is possible to express a tangent space vector directly as   representation  or   representation  and not as operators\u000a\u000a\u000a alpha representation \u000aimportant functions  of  are coded by a parameter  with the important values   and \u000amixed or representation    \u000aexponential or representation     \u000arepresentation       \u000adistributions that allow a flat parameterization  are called collectively family    or family  of distributions and the according manifold is called affine\u000athe  tangent vector is \u000a\u000a\u000a inner product \u000aone may introduce an inner product on the tangent space of manifold  at point  as a linear symmetric and positive definite map\u000a\u000athis allows a riemannian metric to be defined the resulting manifold is a riemannian manifold all of the usual concepts of ordinary differential geometry carry over including the norm\u000a\u000athe line element  the volume element  and the cotangent space\u000a\u000athat is the dual space to the tangent space  from these one may construct tensors as usual\u000a\u000a\u000a fisher metric as inner product \u000afor probability manifolds such an inner product is given by the fisher information metric\u000ahere are equivalent formulas of the fisher information metric\u000a the  base vector in the representation is also called the score\u000a\u000abecause \u000a\u000a this is the same for  and  families\u000a with mimimum  for  entails  and  is applied only to the first parameter and  only to the second is the kullbackleibler divergence or relative entropy applicable to the families\u000afor  one has  is the hellinger distance applicable to the family  also evaluates to the fisher metric\u000athis relation with a divergence  will be revisited further down\u000athe fisher metric is motivated by\u000ait satisfying the requirements for an inner product\u000aits invariance for a sufficient statistic deterministic mapping from one variable to another and more general  for  ie a broadened distribution has smaller \u000ait being the cramrrao bound therefore any  satisfying  belongs to \u000afor any  one has  therefore \u000aso  and therefore  and with inefficient estimator one gets the cramrrao bound \u000a\u000a\u000a affine connection \u000a\u000alike commonly done on riemann manifolds one may define an affine connection or covariant derivative\u000a\u000agiven vector fields  and  lying in the tangent bundle  the affine connection  describes how to differentiate the vector field  along the direction it is itself a vector field it is the sum of the infinitesimal change in the vector field  as one moves along the direction  plus the infinitessimal change of the vector  due to its parallel transport along the direction  that is it takes into account the changing nature of what it means to move a coordinate system in a parallel fashion as one moves about in the manifold in terms of the basis vectors  one has the components\u000a\u000athe  are christoffel symbols the affine connection may be used for defining curvature and torsion like is usual in riemannian geometry\u000a\u000a\u000a alpha connection \u000aa nonmetric connection is not determined by a metric tensor  instead it is and restricted by the requirement that the parallel transport  between points  and  must be a linear combination of the base vectors in  here\u000a\u000aexpresses the parallel transport of  as linear combination of the base vectors in  ie the new  minus the change note that it is not a tensor does not transform as a tensor\u000afor such a metric one can construct a dual connection  to make\u000a\u000afor parallel transport using  and \u000afor the mentioned families the affine connection is called the connection and can also be expressed in more ways\u000a\u000afor \u000a is a metric connection and  with \u000a\u000aie  is dual to  with respect to the fisher metric\u000aif  this is called affine its dual is then affine\u000a\u000aie 0affine and hence  ie 1affine\u000a\u000a\u000a divergence \u000aa function of two distributions points  with minimum  for  entails  and   is applied only to the first parameter and  only to the second  is the direction which brought the two points to be equal when applied to the first parameter and to diverge again when applied to the second parameter ie  the sign cancels in  which we can define to be a metric  if always positive\u000athe absolute derivative of  along  yields candidates for dual connections  this metric and the connections relate to the taylor series expansion  for the first parameter or second parameter here for the first parameter\u000a\u000athe term  is called the divergence or contrast function a good choice is  with  convex for  from jensens inequality it follows that  and for  we have\u000a\u000awhich is the kullbackleibler divergence or relative entropy applicable to the families in the above\u000a\u000ais the fisher metric for  a different  yields\u000a\u000athe hellinger distance applicable to the family is\u000a\u000ain this case  also evaluates to the fisher metric\u000a\u000a\u000a canonical divergence \u000awe now consider two manifolds  and  represented by two sets of coordinate functions  and  the corresponding tangent space basis vectors will be denoted by  and  the bilinear map  associates a quantity  to the dual base vectors this defines an affine connection  for  and affine connection  for  that keep  constant for parallel transport of  and  defined through  and \u000aif  is flat then there exists a coordinate system  that does not change over  in order to keep  constant  must not change either ie  is also flat furthermore in this case we can choose coordinate systems such that\u000a\u000aif  results as a function  on  then making  both coordinate system function sets describe  the connections are such though that  makes  flat and  makes  flat this dual space is denoted as \u000abecause of the linear transform between the flat coordinate systems we have  and \u000abecause  and so for  it is possible to define two potentials  and  through  and   legendre transform these are  and \u000athen\u000a and\u000a\u000a\u000athis naturally leads to the following definition of a canonical divergence\u000a\u000anote the summation that is a representation of the metric due to \u000a\u000a\u000a properties of divergence \u000athe meaning of the canonical divergence depends on the meaning of the metric  and vice versa    for the  metric fisher metric with the dual connections this is the relative entropy for the selfdual euclidean space  leads to \u000asimilar to the euclidean space the following holds\u000atriangular relation  just substitute \u000aif  is not dually flat then this generalizes to\u000athe last part drops in case of dual flatness  is the exponential map\u000apythagorean theorem for  and  meeting on orthogonal lines at    \u000afor  and  with  a autoparallel submanifold  implies that the geodesic connecting  and  is orthogonal to \u000aby projecting  onto  of a curve  one can calculate\u000athe divergence of the curve  where \u000aand  with \u000awith  this becomes \u000afor an autoparallel submanifold parallel transport in it can be expressed with the submanifolds base vectors ie  a onedimensional autoparallel submanifold is a geodesic\u000a\u000a\u000a canonical divergence for the exponential family \u000afor the exponential family  one has  applying  on both sides yields  the other potential    is entropy  and  was used  is the covariance of  the cramrrao bound ie an efficient estimator must be exponential\u000athe canonical divergence is given by the kullbackleibler divergence  and the triangulation is \u000athe minimal divergence to a submanifold given by a restriction like some constant  means maximizing  with  this corresponds to the maximum entropy principle\u000a\u000a\u000a canonical divergence for general alpha families \u000afor general affine manifolds with  one has\u000a\u000athe connection induced by the divergence is not flat unless  then the pythagorean theorem for two curves intersecting orthogonally at  is\u000a\u000a\u000a history \u000athe history of information geometry is associated with the discoveries of at least the following people and many others\u000asir ronald aylmer fisher\u000aharald cramr\u000acalyampudi radhakrishna rao\u000aharold jeffreys\u000asolomon kullback\u000ajeanlouis koszul\u000arichard leibler\u000aclaude shannon\u000aimre csiszr\u000acencov\u000abradley efron\u000apaul vos\u000ashunichi amari\u000ahiroshi nagaoka\u000arobert kass\u000ashinto eguchi\u000aole barndorffnielsen\u000afrank nielsen\u000agiovanni pistone\u000abernard hanzon\u000adamiano brigo\u000a\u000a\u000a applications \u000ainformation geometry can be applied where parametrized distributions play a role\u000ahere an incomplete list\u000astatistical inference\u000atime series and linear systems\u000aquantum systems\u000aneuronal networks\u000amachine learning\u000astatistical mechanics\u000abiology\u000astatistics\u000amathematical finance\u000a\u000a\u000a see also \u000aruppeiner geometry
p302
sg20
g23
sg24
g27
sg30
Vinformation geometry is a branch of mathematics that applies the techniques of differential geometry to the field of probability theory this is done by taking probability distributions for a statistical model as the points of a riemannian manifold forming a statistical manifold the fisher information metric provides the riemannian metric\u000ainformation geometry reached maturity through the work of shunichi amari and other japanese mathematicians in the 1980s amari and nagaokas book methods of information geometry is cited by most works of the relatively young field due to its broad coverage of significant developments attained using the methods of information geometry up to the year 2000 many of these developments were previously only available in japaneselanguage publications\u000a\u000a\u000a introduction \u000athe following introduction is based on methods of information geometry\u000a\u000a\u000a information and probability \u000adefine an nset to be a set v with cardinality  to choose an element v value state point outcome from an nset v one needs to specify  bsets default b2 if one disregards all but the cardinality that is  nats of information are required to specify v equivalently  bits are needed\u000aby considering the occurrences  of values from  one has an alternate way to refer to  through  first one chooses an occurrence  which requires information of  bits to specify v one subtracts the excess information used to choose one  from all those linked to  this is  then  is the number of  portions fitting into  thus one needs  bits to choose one of them so the information variable size code length number of bits needed to refer to  considering its occurrences in a message is\u000a\u000afinally  is the normalized portion of information needed to code all occurrences of one  the averaged code length over all values is   is called the entropy of a random variable \u000a\u000a\u000a statistical model parameters \u000awith a probability distribution  one looks at a variable  through an observation context like a message or an experimental setup\u000athe context can often be identified by a set of parameters through combinatorial reasoning the parameters can have an arbitrary number of dimensions and can be very local or less so as long as the context given by a certain  produces every value of  ie the support  does not change as function of  every  determines one probability distribution for  basically all distributions for which there exists an explicit analytical formula fall into this category binomial normal poisson  the parameters in these cases have a concrete meaning in the underlying setup which is a statistical model for the context of \u000athe parameters are quite different in nature from  itself because they do not describe  but the observation context for \u000aa parameterization of the form\u000a\u000awith\u000a and \u000athat mixes different distributions  is called a mixture distribution mixture or parameterization or mixture for short all such parameterizations are related through an affine transformation  a parameterization with such a transformation rule is called flat\u000aa flat parameterization for  is an exponential or  parameterization because the parameters are in the exponent of  there are several important distributions like normal and poisson that fall into this category these distributions are collectively referred to as exponential family or family the manifold for such distributions is not affine but the  manifold is this is called affine the parameterization  for the exponential family can be mapped to the one above by making  another parameter and extend \u000a\u000a\u000a differential geometry applied to probability \u000ain information geometry the methods of differential geometry are applied to describe the space of probability distributions for one variable  this is done by using a coordinate or atlas  furthermore the probability  must be a differentiable and invertible function of  in this case the  are coordinates of the space and the latter is a differential manifold \u000aderivatives are defined as is usual for a differentiable manifold\u000a\u000awith  for  a realvalued function on \u000agiven a function  on  one may geometrize it by taking it to define a new manifold this is done by defining coordinate functions on this new manifold as\u000a\u000ain this way one geometricizes a function  by encoding it into the coordinates used to describe the system\u000afor  the inverse is  and the resulting manifold of  points is called the representation the  manifold itself is called the representation the  or representations in the sense used here does not refer to the parameterization families of the distribution\u000a\u000a\u000a tangent space \u000a\u000ain standard differential geometry the tangent space on a manifold  at a point  is given by\u000a\u000ain ordinary differential geometry there is no canonical coordinate system on the manifold thus typically all discussion must be with regard to an atlas that is with regard to functions on the manifold as a result tangent spaces and vectors are defined as operators acting on this space of functions so for example in ordinary differential geometry the basis vectors of the tangent space are the operators \u000ahowever with probability distributions  one can calculate valuewise so it is possible to express a tangent space vector directly as   representation  or   representation  and not as operators\u000a\u000a\u000a alpha representation \u000aimportant functions  of  are coded by a parameter  with the important values   and \u000amixed or representation    \u000aexponential or representation     \u000arepresentation       \u000adistributions that allow a flat parameterization  are called collectively family    or family  of distributions and the according manifold is called affine\u000athe  tangent vector is \u000a\u000a\u000a inner product \u000aone may introduce an inner product on the tangent space of manifold  at point  as a linear symmetric and positive definite map\u000a\u000athis allows a riemannian metric to be defined the resulting manifold is a riemannian manifold all of the usual concepts of ordinary differential geometry carry over including the norm\u000a\u000athe line element  the volume element  and the cotangent space\u000a\u000athat is the dual space to the tangent space  from these one may construct tensors as usual\u000a\u000a\u000a fisher metric as inner product \u000afor probability manifolds such an inner product is given by the fisher information metric\u000ahere are equivalent formulas of the fisher information metric\u000a the  base vector in the representation is also called the score\u000a\u000abecause \u000a\u000a this is the same for  and  families\u000a with mimimum  for  entails  and  is applied only to the first parameter and  only to the second is the kullbackleibler divergence or relative entropy applicable to the families\u000afor  one has  is the hellinger distance applicable to the family  also evaluates to the fisher metric\u000athis relation with a divergence  will be revisited further down\u000athe fisher metric is motivated by\u000ait satisfying the requirements for an inner product\u000aits invariance for a sufficient statistic deterministic mapping from one variable to another and more general  for  ie a broadened distribution has smaller \u000ait being the cramrrao bound therefore any  satisfying  belongs to \u000afor any  one has  therefore \u000aso  and therefore  and with inefficient estimator one gets the cramrrao bound \u000a\u000a\u000a affine connection \u000a\u000alike commonly done on riemann manifolds one may define an affine connection or covariant derivative\u000a\u000agiven vector fields  and  lying in the tangent bundle  the affine connection  describes how to differentiate the vector field  along the direction it is itself a vector field it is the sum of the infinitesimal change in the vector field  as one moves along the direction  plus the infinitessimal change of the vector  due to its parallel transport along the direction  that is it takes into account the changing nature of what it means to move a coordinate system in a parallel fashion as one moves about in the manifold in terms of the basis vectors  one has the components\u000a\u000athe  are christoffel symbols the affine connection may be used for defining curvature and torsion like is usual in riemannian geometry\u000a\u000a\u000a alpha connection \u000aa nonmetric connection is not determined by a metric tensor  instead it is and restricted by the requirement that the parallel transport  between points  and  must be a linear combination of the base vectors in  here\u000a\u000aexpresses the parallel transport of  as linear combination of the base vectors in  ie the new  minus the change note that it is not a tensor does not transform as a tensor\u000afor such a metric one can construct a dual connection  to make\u000a\u000afor parallel transport using  and \u000afor the mentioned families the affine connection is called the connection and can also be expressed in more ways\u000a\u000afor \u000a is a metric connection and  with \u000a\u000aie  is dual to  with respect to the fisher metric\u000aif  this is called affine its dual is then affine\u000a\u000aie 0affine and hence  ie 1affine\u000a\u000a\u000a divergence \u000aa function of two distributions points  with minimum  for  entails  and   is applied only to the first parameter and  only to the second  is the direction which brought the two points to be equal when applied to the first parameter and to diverge again when applied to the second parameter ie  the sign cancels in  which we can define to be a metric  if always positive\u000athe absolute derivative of  along  yields candidates for dual connections  this metric and the connections relate to the taylor series expansion  for the first parameter or second parameter here for the first parameter\u000a\u000athe term  is called the divergence or contrast function a good choice is  with  convex for  from jensens inequality it follows that  and for  we have\u000a\u000awhich is the kullbackleibler divergence or relative entropy applicable to the families in the above\u000a\u000ais the fisher metric for  a different  yields\u000a\u000athe hellinger distance applicable to the family is\u000a\u000ain this case  also evaluates to the fisher metric\u000a\u000a\u000a canonical divergence \u000awe now consider two manifolds  and  represented by two sets of coordinate functions  and  the corresponding tangent space basis vectors will be denoted by  and  the bilinear map  associates a quantity  to the dual base vectors this defines an affine connection  for  and affine connection  for  that keep  constant for parallel transport of  and  defined through  and \u000aif  is flat then there exists a coordinate system  that does not change over  in order to keep  constant  must not change either ie  is also flat furthermore in this case we can choose coordinate systems such that\u000a\u000aif  results as a function  on  then making  both coordinate system function sets describe  the connections are such though that  makes  flat and  makes  flat this dual space is denoted as \u000abecause of the linear transform between the flat coordinate systems we have  and \u000abecause  and so for  it is possible to define two potentials  and  through  and   legendre transform these are  and \u000athen\u000a and\u000a\u000a\u000athis naturally leads to the following definition of a canonical divergence\u000a\u000anote the summation that is a representation of the metric due to \u000a\u000a\u000a properties of divergence \u000athe meaning of the canonical divergence depends on the meaning of the metric  and vice versa    for the  metric fisher metric with the dual connections this is the relative entropy for the selfdual euclidean space  leads to \u000asimilar to the euclidean space the following holds\u000atriangular relation  just substitute \u000aif  is not dually flat then this generalizes to\u000athe last part drops in case of dual flatness  is the exponential map\u000apythagorean theorem for  and  meeting on orthogonal lines at    \u000afor  and  with  a autoparallel submanifold  implies that the geodesic connecting  and  is orthogonal to \u000aby projecting  onto  of a curve  one can calculate\u000athe divergence of the curve  where \u000aand  with \u000awith  this becomes \u000afor an autoparallel submanifold parallel transport in it can be expressed with the submanifolds base vectors ie  a onedimensional autoparallel submanifold is a geodesic\u000a\u000a\u000a canonical divergence for the exponential family \u000afor the exponential family  one has  applying  on both sides yields  the other potential    is entropy  and  was used  is the covariance of  the cramrrao bound ie an efficient estimator must be exponential\u000athe canonical divergence is given by the kullbackleibler divergence  and the triangulation is \u000athe minimal divergence to a submanifold given by a restriction like some constant  means maximizing  with  this corresponds to the maximum entropy principle\u000a\u000a\u000a canonical divergence for general alpha families \u000afor general affine manifolds with  one has\u000a\u000athe connection induced by the divergence is not flat unless  then the pythagorean theorem for two curves intersecting orthogonally at  is\u000a\u000a\u000a history \u000athe history of information geometry is associated with the discoveries of at least the following people and many others\u000asir ronald aylmer fisher\u000aharald cramr\u000acalyampudi radhakrishna rao\u000aharold jeffreys\u000asolomon kullback\u000ajeanlouis koszul\u000arichard leibler\u000aclaude shannon\u000aimre csiszr\u000acencov\u000abradley efron\u000apaul vos\u000ashunichi amari\u000ahiroshi nagaoka\u000arobert kass\u000ashinto eguchi\u000aole barndorffnielsen\u000afrank nielsen\u000agiovanni pistone\u000abernard hanzon\u000adamiano brigo\u000a\u000a\u000a applications \u000ainformation geometry can be applied where parametrized distributions play a role\u000ahere an incomplete list\u000astatistical inference\u000atime series and linear systems\u000aquantum systems\u000aneuronal networks\u000amachine learning\u000astatistical mechanics\u000abiology\u000astatistics\u000amathematical finance\u000a\u000a\u000a see also \u000aruppeiner geometry\u000a\u000a\u000a
p303
sg32
g35
sg37
NsbsS'extreme_value_theory.txt'
p304
g2
(g3
g4
Ntp305
Rp306
(dp307
g8
g11
sg12
Vextreme value theory or extreme value analysis eva is a branch of statistics dealing with the extreme deviations from the median of probability distributions it seeks to assess from a given ordered sample of a given random variable the probability of events that are more extreme than any previously observed extreme value analysis is widely used in many disciplines such as structural engineering finance earth sciences traffic prediction and geological engineering for example eva might be used in the field of hydrology to estimate the probability of an unusually large flooding event such as the 100year flood similarly for the design of a breakwater a coastal engineer would seek to estimate the 50year wave and design the structure accordingly\u000a\u000a\u000a data analysis \u000atwo approaches exist for practical extreme value analysis the first method relies on deriving block maxima minima series as a preliminary step in many situations it is customary and convenient to extract the annual maxima minima generating an annual maxima series ams the second method relies on extracting from a continuous record the peak values reached for any period during which values exceed a certain threshold falls below a certain threshold this method is generally referred to as the peak over threshold  method pot and can lead to several or no values being extracted in any given year\u000afor ams data the analysis may partly rely on the results of the fishertippettgnedenko theorem leading to the generalized extreme value distribution being selected for fitting however in practice various procedures are applied to select between a wider range of distributions the theorem here relates to the limiting distributions for the minimum or the maximum of a very large collection of independent random variables from the same arbitrary distribution given that the number of relevant random events within a year may be rather limited it is unsurprising that analyses of observed ams data often lead to distributions other than the generalized extreme value distribution being selected\u000afor pot data the analysis involves fitting two distributions one for the number of events in a basic time period and a second for the size of the exceedances a common assumption for the first is the poisson distribution with the generalized pareto distribution being used for the exceedances some further theory needs to be applied in order to derive the distribution of the most extreme value that may be observed in a given period which may be a target of the analysis an alternative target may be to estimate the expected costs associated with events occurring in a given period for pot analyses a tailfitting can be based on the pickandsbalkemade haan theorem\u000a\u000a\u000a applications \u000aapplications of extreme value theory include predicting the probability distribution of\u000aextreme floods\u000athe amounts of large insurance losses\u000aequity risks\u000aday to day market risk\u000athe size of freak waves\u000amutational events during evolution\u000alarge wildfires\u000aenvironmental loads on structures\u000ait can be applied to some characterization of the distribution of the maxima of incomes like in some surveys done in virtually all the national offices of statistics\u000aestimate fastest time humans are capable of running the 100 metres sprint and performances in other athletic disciplines\u000apipeline failures due to pitting corrosion\u000a\u000a\u000a history \u000athe field of extreme value theory was pioneered by leonard tippett 19021985 tippett was employed by the british cotton industry research association where he worked to make cotton thread stronger in his studies he realized that the strength of a thread was controlled by the strength of its weakest fibres with the help of r a fisher tippet obtained three asymptotic limits describing the distributions of extremes emil julius gumbel codified this theory in his 1958 book statistics of extremes including the gumbel distributions that bear his name\u000aa summary of historically important publications relating to extreme value theory can be found on the article list of publications in statistics\u000a\u000a\u000a univariate theory \u000alet  be a sequence of independent and identically distributed variables with distribution function f and let  denote the maximum\u000ain theory the exact distribution of the maximum can be derived\u000a\u000athe associated indicator function  is a bernoulli process with a success probability  that depends on the magnitude  of the extreme event the number of extreme events within  trials thus follows a binomial distribution and the number of trials until an event occurs follows a geometric distribution with expected value and standard deviation of the same order \u000ain practice we might not have the distribution function  but the fishertippettgnedenko theorem provides an asymptotic result if there exist sequences of constants  and  such that\u000a\u000aas  then\u000a\u000awhere  depends on the tail shape of the distribution when normalized g belongs to one of the following nondegenerate distribution families\u000aweibull law  when the distribution of  has a light tail with finite upper bound also known as type 3\u000agumbel law  when the distribution of  has an exponential tail also known as type 1\u000afrchet law  when the distribution of  has a heavy tail including polynomial decay also known as type 2\u000ain all cases \u000a\u000a\u000a see also \u000aextreme risk\u000aextreme weather\u000afishertippettgnedenko theorem\u000ageneralized extreme value distribution\u000alarge deviation theory\u000apareto distribution\u000apickandsbalkemade haan theorem\u000arare events\u000aweibull distribution\u000a\u000a\u000a notes \u000a\u000a\u000a
p308
sg14
g17
sg18
Vextreme value theory or extreme value analysis eva is a branch of statistics dealing with the extreme deviations from the median of probability distributions it seeks to assess from a given ordered sample of a given random variable the probability of events that are more extreme than any previously observed extreme value analysis is widely used in many disciplines such as structural engineering finance earth sciences traffic prediction and geological engineering for example eva might be used in the field of hydrology to estimate the probability of an unusually large flooding event such as the 100year flood similarly for the design of a breakwater a coastal engineer would seek to estimate the 50year wave and design the structure accordingly\u000a\u000a\u000a data analysis \u000atwo approaches exist for practical extreme value analysis the first method relies on deriving block maxima minima series as a preliminary step in many situations it is customary and convenient to extract the annual maxima minima generating an annual maxima series ams the second method relies on extracting from a continuous record the peak values reached for any period during which values exceed a certain threshold falls below a certain threshold this method is generally referred to as the peak over threshold  method pot and can lead to several or no values being extracted in any given year\u000afor ams data the analysis may partly rely on the results of the fishertippettgnedenko theorem leading to the generalized extreme value distribution being selected for fitting however in practice various procedures are applied to select between a wider range of distributions the theorem here relates to the limiting distributions for the minimum or the maximum of a very large collection of independent random variables from the same arbitrary distribution given that the number of relevant random events within a year may be rather limited it is unsurprising that analyses of observed ams data often lead to distributions other than the generalized extreme value distribution being selected\u000afor pot data the analysis involves fitting two distributions one for the number of events in a basic time period and a second for the size of the exceedances a common assumption for the first is the poisson distribution with the generalized pareto distribution being used for the exceedances some further theory needs to be applied in order to derive the distribution of the most extreme value that may be observed in a given period which may be a target of the analysis an alternative target may be to estimate the expected costs associated with events occurring in a given period for pot analyses a tailfitting can be based on the pickandsbalkemade haan theorem\u000a\u000a\u000a applications \u000aapplications of extreme value theory include predicting the probability distribution of\u000aextreme floods\u000athe amounts of large insurance losses\u000aequity risks\u000aday to day market risk\u000athe size of freak waves\u000amutational events during evolution\u000alarge wildfires\u000aenvironmental loads on structures\u000ait can be applied to some characterization of the distribution of the maxima of incomes like in some surveys done in virtually all the national offices of statistics\u000aestimate fastest time humans are capable of running the 100 metres sprint and performances in other athletic disciplines\u000apipeline failures due to pitting corrosion\u000a\u000a\u000a history \u000athe field of extreme value theory was pioneered by leonard tippett 19021985 tippett was employed by the british cotton industry research association where he worked to make cotton thread stronger in his studies he realized that the strength of a thread was controlled by the strength of its weakest fibres with the help of r a fisher tippet obtained three asymptotic limits describing the distributions of extremes emil julius gumbel codified this theory in his 1958 book statistics of extremes including the gumbel distributions that bear his name\u000aa summary of historically important publications relating to extreme value theory can be found on the article list of publications in statistics\u000a\u000a\u000a univariate theory \u000alet  be a sequence of independent and identically distributed variables with distribution function f and let  denote the maximum\u000ain theory the exact distribution of the maximum can be derived\u000a\u000athe associated indicator function  is a bernoulli process with a success probability  that depends on the magnitude  of the extreme event the number of extreme events within  trials thus follows a binomial distribution and the number of trials until an event occurs follows a geometric distribution with expected value and standard deviation of the same order \u000ain practice we might not have the distribution function  but the fishertippettgnedenko theorem provides an asymptotic result if there exist sequences of constants  and  such that\u000a\u000aas  then\u000a\u000awhere  depends on the tail shape of the distribution when normalized g belongs to one of the following nondegenerate distribution families\u000aweibull law  when the distribution of  has a light tail with finite upper bound also known as type 3\u000agumbel law  when the distribution of  has an exponential tail also known as type 1\u000afrchet law  when the distribution of  has a heavy tail including polynomial decay also known as type 2\u000ain all cases \u000a\u000a\u000a see also \u000aextreme risk\u000aextreme weather\u000afishertippettgnedenko theorem\u000ageneralized extreme value distribution\u000alarge deviation theory\u000apareto distribution\u000apickandsbalkemade haan theorem\u000arare events\u000aweibull distribution\u000a\u000a\u000a notes
p309
sg20
g23
sg24
g27
sg30
Vextreme value theory or extreme value analysis eva is a branch of statistics dealing with the extreme deviations from the median of probability distributions it seeks to assess from a given ordered sample of a given random variable the probability of events that are more extreme than any previously observed extreme value analysis is widely used in many disciplines such as structural engineering finance earth sciences traffic prediction and geological engineering for example eva might be used in the field of hydrology to estimate the probability of an unusually large flooding event such as the 100year flood similarly for the design of a breakwater a coastal engineer would seek to estimate the 50year wave and design the structure accordingly\u000a\u000a\u000a data analysis \u000atwo approaches exist for practical extreme value analysis the first method relies on deriving block maxima minima series as a preliminary step in many situations it is customary and convenient to extract the annual maxima minima generating an annual maxima series ams the second method relies on extracting from a continuous record the peak values reached for any period during which values exceed a certain threshold falls below a certain threshold this method is generally referred to as the peak over threshold  method pot and can lead to several or no values being extracted in any given year\u000afor ams data the analysis may partly rely on the results of the fishertippettgnedenko theorem leading to the generalized extreme value distribution being selected for fitting however in practice various procedures are applied to select between a wider range of distributions the theorem here relates to the limiting distributions for the minimum or the maximum of a very large collection of independent random variables from the same arbitrary distribution given that the number of relevant random events within a year may be rather limited it is unsurprising that analyses of observed ams data often lead to distributions other than the generalized extreme value distribution being selected\u000afor pot data the analysis involves fitting two distributions one for the number of events in a basic time period and a second for the size of the exceedances a common assumption for the first is the poisson distribution with the generalized pareto distribution being used for the exceedances some further theory needs to be applied in order to derive the distribution of the most extreme value that may be observed in a given period which may be a target of the analysis an alternative target may be to estimate the expected costs associated with events occurring in a given period for pot analyses a tailfitting can be based on the pickandsbalkemade haan theorem\u000a\u000a\u000a applications \u000aapplications of extreme value theory include predicting the probability distribution of\u000aextreme floods\u000athe amounts of large insurance losses\u000aequity risks\u000aday to day market risk\u000athe size of freak waves\u000amutational events during evolution\u000alarge wildfires\u000aenvironmental loads on structures\u000ait can be applied to some characterization of the distribution of the maxima of incomes like in some surveys done in virtually all the national offices of statistics\u000aestimate fastest time humans are capable of running the 100 metres sprint and performances in other athletic disciplines\u000apipeline failures due to pitting corrosion\u000a\u000a\u000a history \u000athe field of extreme value theory was pioneered by leonard tippett 19021985 tippett was employed by the british cotton industry research association where he worked to make cotton thread stronger in his studies he realized that the strength of a thread was controlled by the strength of its weakest fibres with the help of r a fisher tippet obtained three asymptotic limits describing the distributions of extremes emil julius gumbel codified this theory in his 1958 book statistics of extremes including the gumbel distributions that bear his name\u000aa summary of historically important publications relating to extreme value theory can be found on the article list of publications in statistics\u000a\u000a\u000a univariate theory \u000alet  be a sequence of independent and identically distributed variables with distribution function f and let  denote the maximum\u000ain theory the exact distribution of the maximum can be derived\u000a\u000athe associated indicator function  is a bernoulli process with a success probability  that depends on the magnitude  of the extreme event the number of extreme events within  trials thus follows a binomial distribution and the number of trials until an event occurs follows a geometric distribution with expected value and standard deviation of the same order \u000ain practice we might not have the distribution function  but the fishertippettgnedenko theorem provides an asymptotic result if there exist sequences of constants  and  such that\u000a\u000aas  then\u000a\u000awhere  depends on the tail shape of the distribution when normalized g belongs to one of the following nondegenerate distribution families\u000aweibull law  when the distribution of  has a light tail with finite upper bound also known as type 3\u000agumbel law  when the distribution of  has an exponential tail also known as type 1\u000afrchet law  when the distribution of  has a heavy tail including polynomial decay also known as type 2\u000ain all cases \u000a\u000a\u000a see also \u000aextreme risk\u000aextreme weather\u000afishertippettgnedenko theorem\u000ageneralized extreme value distribution\u000alarge deviation theory\u000apareto distribution\u000apickandsbalkemade haan theorem\u000arare events\u000aweibull distribution\u000a\u000a\u000a notes \u000a\u000a\u000a
p310
sg32
g35
sg37
NsbsS'magnitude_of_completeness.txt'
p311
g2
(g3
g4
Ntp312
Rp313
(dp314
g8
g11
sg12
Vin an earthquake catalog the magnitude of completeness mc is the minimum magnitude above which all earthquakes within a certain region are reliably recorded for example if the mc of a catalog for a specific region is 26 from 1980 to the present this means that all earthquakes above a magnitude 26 have been recorded in the catalog from 1980 to the present time it is important to note that when interpreting this data a mc too high may mean undersampling whereas a value too low could indicate an erroneous seismicity parameter\u000aanother definition includes the lowest magnitude at which 100 of the earthquakes in a spacetime volume are detected\u000a\u000a\u000a
p315
sg14
g17
sg18
Vin an earthquake catalog the magnitude of completeness mc is the minimum magnitude above which all earthquakes within a certain region are reliably recorded for example if the mc of a catalog for a specific region is 26 from 1980 to the present this means that all earthquakes above a magnitude 26 have been recorded in the catalog from 1980 to the present time it is important to note that when interpreting this data a mc too high may mean undersampling whereas a value too low could indicate an erroneous seismicity parameter\u000aanother definition includes the lowest magnitude at which 100 of the earthquakes in a spacetime volume are detected
p316
sg20
g23
sg24
g27
sg30
Vin an earthquake catalog the magnitude of completeness mc is the minimum magnitude above which all earthquakes within a certain region are reliably recorded for example if the mc of a catalog for a specific region is 26 from 1980 to the present this means that all earthquakes above a magnitude 26 have been recorded in the catalog from 1980 to the present time it is important to note that when interpreting this data a mc too high may mean undersampling whereas a value too low could indicate an erroneous seismicity parameter\u000aanother definition includes the lowest magnitude at which 100 of the earthquakes in a spacetime volume are detected\u000a\u000a\u000a
p317
sg32
g35
sg37
NsbsS'response_surface_methodology.txt'
p318
g2
(g3
g4
Ntp319
Rp320
(dp321
g8
g11
sg12
Vin statistics response surface methodology rsm explores the relationships between several explanatory variables and one or more response variables the method was introduced by g e p box and k b wilson in 1951 the main idea of rsm is to use a sequence of designed experiments to obtain an optimal response box and wilson suggest using a seconddegree polynomial model to do this they acknowledge that this model is only an approximation but use it because such a model is easy to estimate and apply even when little is known about the process\u000a\u000a\u000a basic approach of response surface methodology \u000aan easy way to estimate a firstdegree polynomial model is to use a factorial experiment or a fractional factorial design this is sufficient to determine which explanatory variables have an impact on the response variables of interest once it is suspected that only significant explanatory variables are left then a more complicated design such as a central composite design can be implemented to estimate a seconddegree polynomial model which is still only an approximation at best however the seconddegree model can be used to optimize maximize minimize or attain a specific target for\u000a\u000a\u000a important rsm properties and features \u000aresponse surface optimization using jmp software\u000aorthogonality the property that allows individual effects of the kfactors to be estimated independently without or with minimal confounding also orthogonality provides minimum variance estimates of the model coefficient so that they are uncorrelated\u000arotatability the property of rotating points of the design about the center of the factor space the moments of the distribution of the design points are constant\u000auniformity a third property of ccd designs used to control the number of center points is uniform precision or uniformity\u000a\u000a\u000a special geometries \u000a\u000a\u000a cube \u000acubic designs are discussed by kiefer by atkinson donev and tobias and by hardin and sloane\u000a\u000a\u000a sphere \u000aspherical designs are discussed by kiefer and by hardin and sloane\u000a\u000a\u000a simplex geometry and mixture experiments \u000amixture experiments are discussed in many books on the design of experiments and in the responsesurface methodology textbooks of box and draper and of atkinson donev and tobias an extensive discussion and survey appears in the advanced textbook by john cornell\u000a\u000a\u000a extensions \u000a\u000a\u000a multiple objective functions \u000a\u000asome extensions of response surface methodology deal with the multiple response problem multiple response variables create difficulty because what is optimal for one response may not be optimal for other responses other extensions are used to reduce variability in a single response while targeting a specific value or attaining a near maximum or minimum while preventing variability in that response from getting too large\u000a\u000a\u000a practical concerns \u000aresponse surface methodology uses statistical models and therefore practitioners need to be aware that even the best statistical model is an approximation to reality in practice both the models and the parameter values are unknown and subject to uncertainty on top of ignorance of course an estimated optimum point need not be optimum in reality because of the errors of the estimates and of the inadequacies of the model\u000anonetheless response surface methodology has an effective trackrecord of helping researchers improve products and services for example boxs original responsesurface modeling enabled chemical engineers to improve a process that had been stuck at a saddlepoint for years the engineers had not been able to afford to fit a cubic threelevel design to estimate a quadratic model and their biased linearmodels estimated the gradient to be zero boxs design reduced the costs of experimentation so that a quadratic model could be fit which led to a longsought ascent direction\u000a\u000a\u000a see also \u000aplackettburman design\u000aboxbehnken design\u000acentral composite design\u000aioso method based on responsesurface methodology\u000aoptimal designs\u000apolynomial regression\u000apolynomial and rational function modeling\u000asurrogate model\u000aprobabilistic design\u000a\u000a\u000a
p322
sg14
g17
sg18
Vin statistics response surface methodology rsm explores the relationships between several explanatory variables and one or more response variables the method was introduced by g e p box and k b wilson in 1951 the main idea of rsm is to use a sequence of designed experiments to obtain an optimal response box and wilson suggest using a seconddegree polynomial model to do this they acknowledge that this model is only an approximation but use it because such a model is easy to estimate and apply even when little is known about the process\u000a\u000a\u000a basic approach of response surface methodology \u000aan easy way to estimate a firstdegree polynomial model is to use a factorial experiment or a fractional factorial design this is sufficient to determine which explanatory variables have an impact on the response variables of interest once it is suspected that only significant explanatory variables are left then a more complicated design such as a central composite design can be implemented to estimate a seconddegree polynomial model which is still only an approximation at best however the seconddegree model can be used to optimize maximize minimize or attain a specific target for\u000a\u000a\u000a important rsm properties and features \u000aresponse surface optimization using jmp software\u000aorthogonality the property that allows individual effects of the kfactors to be estimated independently without or with minimal confounding also orthogonality provides minimum variance estimates of the model coefficient so that they are uncorrelated\u000arotatability the property of rotating points of the design about the center of the factor space the moments of the distribution of the design points are constant\u000auniformity a third property of ccd designs used to control the number of center points is uniform precision or uniformity\u000a\u000a\u000a special geometries \u000a\u000a\u000a cube \u000acubic designs are discussed by kiefer by atkinson donev and tobias and by hardin and sloane\u000a\u000a\u000a sphere \u000aspherical designs are discussed by kiefer and by hardin and sloane\u000a\u000a\u000a simplex geometry and mixture experiments \u000amixture experiments are discussed in many books on the design of experiments and in the responsesurface methodology textbooks of box and draper and of atkinson donev and tobias an extensive discussion and survey appears in the advanced textbook by john cornell\u000a\u000a\u000a extensions \u000a\u000a\u000a multiple objective functions \u000a\u000asome extensions of response surface methodology deal with the multiple response problem multiple response variables create difficulty because what is optimal for one response may not be optimal for other responses other extensions are used to reduce variability in a single response while targeting a specific value or attaining a near maximum or minimum while preventing variability in that response from getting too large\u000a\u000a\u000a practical concerns \u000aresponse surface methodology uses statistical models and therefore practitioners need to be aware that even the best statistical model is an approximation to reality in practice both the models and the parameter values are unknown and subject to uncertainty on top of ignorance of course an estimated optimum point need not be optimum in reality because of the errors of the estimates and of the inadequacies of the model\u000anonetheless response surface methodology has an effective trackrecord of helping researchers improve products and services for example boxs original responsesurface modeling enabled chemical engineers to improve a process that had been stuck at a saddlepoint for years the engineers had not been able to afford to fit a cubic threelevel design to estimate a quadratic model and their biased linearmodels estimated the gradient to be zero boxs design reduced the costs of experimentation so that a quadratic model could be fit which led to a longsought ascent direction\u000a\u000a\u000a see also \u000aplackettburman design\u000aboxbehnken design\u000acentral composite design\u000aioso method based on responsesurface methodology\u000aoptimal designs\u000apolynomial regression\u000apolynomial and rational function modeling\u000asurrogate model\u000aprobabilistic design
p323
sg20
g23
sg24
g27
sg30
Vin statistics response surface methodology rsm explores the relationships between several explanatory variables and one or more response variables the method was introduced by g e p box and k b wilson in 1951 the main idea of rsm is to use a sequence of designed experiments to obtain an optimal response box and wilson suggest using a seconddegree polynomial model to do this they acknowledge that this model is only an approximation but use it because such a model is easy to estimate and apply even when little is known about the process\u000a\u000a\u000a basic approach of response surface methodology \u000aan easy way to estimate a firstdegree polynomial model is to use a factorial experiment or a fractional factorial design this is sufficient to determine which explanatory variables have an impact on the response variables of interest once it is suspected that only significant explanatory variables are left then a more complicated design such as a central composite design can be implemented to estimate a seconddegree polynomial model which is still only an approximation at best however the seconddegree model can be used to optimize maximize minimize or attain a specific target for\u000a\u000a\u000a important rsm properties and features \u000aresponse surface optimization using jmp software\u000aorthogonality the property that allows individual effects of the kfactors to be estimated independently without or with minimal confounding also orthogonality provides minimum variance estimates of the model coefficient so that they are uncorrelated\u000arotatability the property of rotating points of the design about the center of the factor space the moments of the distribution of the design points are constant\u000auniformity a third property of ccd designs used to control the number of center points is uniform precision or uniformity\u000a\u000a\u000a special geometries \u000a\u000a\u000a cube \u000acubic designs are discussed by kiefer by atkinson donev and tobias and by hardin and sloane\u000a\u000a\u000a sphere \u000aspherical designs are discussed by kiefer and by hardin and sloane\u000a\u000a\u000a simplex geometry and mixture experiments \u000amixture experiments are discussed in many books on the design of experiments and in the responsesurface methodology textbooks of box and draper and of atkinson donev and tobias an extensive discussion and survey appears in the advanced textbook by john cornell\u000a\u000a\u000a extensions \u000a\u000a\u000a multiple objective functions \u000a\u000asome extensions of response surface methodology deal with the multiple response problem multiple response variables create difficulty because what is optimal for one response may not be optimal for other responses other extensions are used to reduce variability in a single response while targeting a specific value or attaining a near maximum or minimum while preventing variability in that response from getting too large\u000a\u000a\u000a practical concerns \u000aresponse surface methodology uses statistical models and therefore practitioners need to be aware that even the best statistical model is an approximation to reality in practice both the models and the parameter values are unknown and subject to uncertainty on top of ignorance of course an estimated optimum point need not be optimum in reality because of the errors of the estimates and of the inadequacies of the model\u000anonetheless response surface methodology has an effective trackrecord of helping researchers improve products and services for example boxs original responsesurface modeling enabled chemical engineers to improve a process that had been stuck at a saddlepoint for years the engineers had not been able to afford to fit a cubic threelevel design to estimate a quadratic model and their biased linearmodels estimated the gradient to be zero boxs design reduced the costs of experimentation so that a quadratic model could be fit which led to a longsought ascent direction\u000a\u000a\u000a see also \u000aplackettburman design\u000aboxbehnken design\u000acentral composite design\u000aioso method based on responsesurface methodology\u000aoptimal designs\u000apolynomial regression\u000apolynomial and rational function modeling\u000asurrogate model\u000aprobabilistic design\u000a\u000a\u000a
p324
sg32
g35
sg37
NsbsS'edgeworth_series.txt'
p325
g2
(g3
g4
Ntp326
Rp327
(dp328
g8
g11
sg12
Vthe gramcharlier a series named in honor of jrgen pedersen gram and carl charlier and the edgeworth series named in honor of francis ysidro edgeworth are series that approximate a probability distribution in terms of its cumulants the series are the same but the arrangement of terms and thus the accuracy of truncating the series differ\u000a\u000a\u000a gramcharlier a series \u000athe key idea of these expansions is to write the characteristic function of the distribution whose probability density function f is to be approximated in terms of the characteristic function of a distribution with known and suitable properties and to recover f through the inverse fourier transform\u000awe examine a continuous random variable let  be the characteristic function of its distribution whose density function is f and  its cumulants we expand in terms of a known distribution with probability density function  characteristic function  and cumulants  the density  is generally chosen to be that of the normal distribution but other choices are possible as well by the definition of the cumulants we have see wallace 1958\u000a and\u000a\u000awhich gives the following formal identity\u000a\u000aby the properties of the fourier transform  is the fourier transform of  where d is the differential operator with respect to x thus after changing  with  on both sides of the equation we find for f the formal expansion\u000a\u000aif  is chosen as the normal density with mean and variance as given by f that is mean  and variance  then the expansion becomes\u000a\u000asince  for all r 2 as higher cumulants of the normal distribution are 0 by expanding the exponential and collecting terms according to the order of the derivatives we arrive at the gramcharlier a series if we include only the first two correction terms to the normal distribution we obtain\u000a\u000awith  and  these are hermite polynomials\u000anote that this expression is not guaranteed to be positive and is therefore not a valid probability distribution the gramcharlier a series diverges in many cases of interestit converges only if  falls off faster than  at infinity cramr 1957 when it does not converge the series is also not a true asymptotic expansion because it is not possible to estimate the error of the expansion for this reason the edgeworth series see next section is generally preferred over the gramcharlier a series\u000a\u000a\u000a the edgeworth series \u000aedgeworth developed a similar expansion as an improvement to the central limit theorem the advantage of the edgeworth series is that the error is controlled so that it is a true asymptotic expansion\u000alet xi be a sequence of independent and identically distributed random variables with mean  and variance 2 and let yn be their standardized sums\u000a\u000alet fn denote the cumulative distribution functions of the variables yn then by the central limit theorem\u000a\u000afor every x as long as the mean and variance are finite\u000anow assume that the random variables xi have mean  variance 2 and higher cumulants rrr if we expand in terms of the standard normal distribution that is if we set\u000a\u000athen the cumulant differences in the formal expression of the characteristic function fnt of fn are\u000a\u000athe edgeworth series is developed similarly to the gramcharlier a series only that now terms are collected according to powers of n thus we have\u000a\u000awhere pjx is a polynomial of degree 3j again after inverse fourier transform the density function fn follows as\u000a\u000athe first five terms of the expansion are\u000a\u000ahere jx is the jth derivative of  at point x remembering that the derivatives of the density of the normal distribution are related to the normal density by nx1nhnxx where hn is the hermite polynomial of order n this explains the alternative representations in terms of the density function blinnikov and moessner 1998 have given a simple algorithm to calculate higherorder terms of the expansion\u000anote that in case of a lattice distributions which have discrete values the edgeworth expansion must be adjusted to account for the discontinuous jumps between lattice points\u000a\u000a\u000a illustration density of the sample mean of 3  \u000a\u000atake  and the sample mean \u000awe can use several distributions for \u000athe exact distribution which follows a gamma distribution   \u000athe asymptotic normal distribution \u000atwo edgeworth expansion of degree 2 and 3\u000a\u000a\u000a disadvantages of the edgeworth expansion \u000aedgeworth expansions can suffer from a few issues\u000athey are not guaranteed to be a proper probability distribution as\u000athe integral of the density needs not integrate to 1\u000aprobabilities can be negative\u000a\u000athey can be inaccurate especially in the tails due to mainly two reasons\u000athey are obtained under a taylor series around the mean\u000athey guarantee asymptotically an absolute error not a relative one this is an issue when one wants to approximate very small quantities for which the absolute error might be small but the relative error important\u000a\u000a\u000a see also \u000acornishfisher expansion\u000aedgeworth binomial tree\u000a\u000a\u000a
p329
sg14
g17
sg18
Vthe gramcharlier a series named in honor of jrgen pedersen gram and carl charlier and the edgeworth series named in honor of francis ysidro edgeworth are series that approximate a probability distribution in terms of its cumulants the series are the same but the arrangement of terms and thus the accuracy of truncating the series differ\u000a\u000a\u000a gramcharlier a series \u000athe key idea of these expansions is to write the characteristic function of the distribution whose probability density function f is to be approximated in terms of the characteristic function of a distribution with known and suitable properties and to recover f through the inverse fourier transform\u000awe examine a continuous random variable let  be the characteristic function of its distribution whose density function is f and  its cumulants we expand in terms of a known distribution with probability density function  characteristic function  and cumulants  the density  is generally chosen to be that of the normal distribution but other choices are possible as well by the definition of the cumulants we have see wallace 1958\u000a and\u000a\u000awhich gives the following formal identity\u000a\u000aby the properties of the fourier transform  is the fourier transform of  where d is the differential operator with respect to x thus after changing  with  on both sides of the equation we find for f the formal expansion\u000a\u000aif  is chosen as the normal density with mean and variance as given by f that is mean  and variance  then the expansion becomes\u000a\u000asince  for all r 2 as higher cumulants of the normal distribution are 0 by expanding the exponential and collecting terms according to the order of the derivatives we arrive at the gramcharlier a series if we include only the first two correction terms to the normal distribution we obtain\u000a\u000awith  and  these are hermite polynomials\u000anote that this expression is not guaranteed to be positive and is therefore not a valid probability distribution the gramcharlier a series diverges in many cases of interestit converges only if  falls off faster than  at infinity cramr 1957 when it does not converge the series is also not a true asymptotic expansion because it is not possible to estimate the error of the expansion for this reason the edgeworth series see next section is generally preferred over the gramcharlier a series\u000a\u000a\u000a the edgeworth series \u000aedgeworth developed a similar expansion as an improvement to the central limit theorem the advantage of the edgeworth series is that the error is controlled so that it is a true asymptotic expansion\u000alet xi be a sequence of independent and identically distributed random variables with mean  and variance 2 and let yn be their standardized sums\u000a\u000alet fn denote the cumulative distribution functions of the variables yn then by the central limit theorem\u000a\u000afor every x as long as the mean and variance are finite\u000anow assume that the random variables xi have mean  variance 2 and higher cumulants rrr if we expand in terms of the standard normal distribution that is if we set\u000a\u000athen the cumulant differences in the formal expression of the characteristic function fnt of fn are\u000a\u000athe edgeworth series is developed similarly to the gramcharlier a series only that now terms are collected according to powers of n thus we have\u000a\u000awhere pjx is a polynomial of degree 3j again after inverse fourier transform the density function fn follows as\u000a\u000athe first five terms of the expansion are\u000a\u000ahere jx is the jth derivative of  at point x remembering that the derivatives of the density of the normal distribution are related to the normal density by nx1nhnxx where hn is the hermite polynomial of order n this explains the alternative representations in terms of the density function blinnikov and moessner 1998 have given a simple algorithm to calculate higherorder terms of the expansion\u000anote that in case of a lattice distributions which have discrete values the edgeworth expansion must be adjusted to account for the discontinuous jumps between lattice points\u000a\u000a\u000a illustration density of the sample mean of 3  \u000a\u000atake  and the sample mean \u000awe can use several distributions for \u000athe exact distribution which follows a gamma distribution   \u000athe asymptotic normal distribution \u000atwo edgeworth expansion of degree 2 and 3\u000a\u000a\u000a disadvantages of the edgeworth expansion \u000aedgeworth expansions can suffer from a few issues\u000athey are not guaranteed to be a proper probability distribution as\u000athe integral of the density needs not integrate to 1\u000aprobabilities can be negative\u000a\u000athey can be inaccurate especially in the tails due to mainly two reasons\u000athey are obtained under a taylor series around the mean\u000athey guarantee asymptotically an absolute error not a relative one this is an issue when one wants to approximate very small quantities for which the absolute error might be small but the relative error important\u000a\u000a\u000a see also \u000acornishfisher expansion\u000aedgeworth binomial tree
p330
sg20
g23
sg24
g27
sg30
Vthe gramcharlier a series named in honor of jrgen pedersen gram and carl charlier and the edgeworth series named in honor of francis ysidro edgeworth are series that approximate a probability distribution in terms of its cumulants the series are the same but the arrangement of terms and thus the accuracy of truncating the series differ\u000a\u000a\u000a gramcharlier a series \u000athe key idea of these expansions is to write the characteristic function of the distribution whose probability density function f is to be approximated in terms of the characteristic function of a distribution with known and suitable properties and to recover f through the inverse fourier transform\u000awe examine a continuous random variable let  be the characteristic function of its distribution whose density function is f and  its cumulants we expand in terms of a known distribution with probability density function  characteristic function  and cumulants  the density  is generally chosen to be that of the normal distribution but other choices are possible as well by the definition of the cumulants we have see wallace 1958\u000a and\u000a\u000awhich gives the following formal identity\u000a\u000aby the properties of the fourier transform  is the fourier transform of  where d is the differential operator with respect to x thus after changing  with  on both sides of the equation we find for f the formal expansion\u000a\u000aif  is chosen as the normal density with mean and variance as given by f that is mean  and variance  then the expansion becomes\u000a\u000asince  for all r 2 as higher cumulants of the normal distribution are 0 by expanding the exponential and collecting terms according to the order of the derivatives we arrive at the gramcharlier a series if we include only the first two correction terms to the normal distribution we obtain\u000a\u000awith  and  these are hermite polynomials\u000anote that this expression is not guaranteed to be positive and is therefore not a valid probability distribution the gramcharlier a series diverges in many cases of interestit converges only if  falls off faster than  at infinity cramr 1957 when it does not converge the series is also not a true asymptotic expansion because it is not possible to estimate the error of the expansion for this reason the edgeworth series see next section is generally preferred over the gramcharlier a series\u000a\u000a\u000a the edgeworth series \u000aedgeworth developed a similar expansion as an improvement to the central limit theorem the advantage of the edgeworth series is that the error is controlled so that it is a true asymptotic expansion\u000alet xi be a sequence of independent and identically distributed random variables with mean  and variance 2 and let yn be their standardized sums\u000a\u000alet fn denote the cumulative distribution functions of the variables yn then by the central limit theorem\u000a\u000afor every x as long as the mean and variance are finite\u000anow assume that the random variables xi have mean  variance 2 and higher cumulants rrr if we expand in terms of the standard normal distribution that is if we set\u000a\u000athen the cumulant differences in the formal expression of the characteristic function fnt of fn are\u000a\u000athe edgeworth series is developed similarly to the gramcharlier a series only that now terms are collected according to powers of n thus we have\u000a\u000awhere pjx is a polynomial of degree 3j again after inverse fourier transform the density function fn follows as\u000a\u000athe first five terms of the expansion are\u000a\u000ahere jx is the jth derivative of  at point x remembering that the derivatives of the density of the normal distribution are related to the normal density by nx1nhnxx where hn is the hermite polynomial of order n this explains the alternative representations in terms of the density function blinnikov and moessner 1998 have given a simple algorithm to calculate higherorder terms of the expansion\u000anote that in case of a lattice distributions which have discrete values the edgeworth expansion must be adjusted to account for the discontinuous jumps between lattice points\u000a\u000a\u000a illustration density of the sample mean of 3  \u000a\u000atake  and the sample mean \u000awe can use several distributions for \u000athe exact distribution which follows a gamma distribution   \u000athe asymptotic normal distribution \u000atwo edgeworth expansion of degree 2 and 3\u000a\u000a\u000a disadvantages of the edgeworth expansion \u000aedgeworth expansions can suffer from a few issues\u000athey are not guaranteed to be a proper probability distribution as\u000athe integral of the density needs not integrate to 1\u000aprobabilities can be negative\u000a\u000athey can be inaccurate especially in the tails due to mainly two reasons\u000athey are obtained under a taylor series around the mean\u000athey guarantee asymptotically an absolute error not a relative one this is an issue when one wants to approximate very small quantities for which the absolute error might be small but the relative error important\u000a\u000a\u000a see also \u000acornishfisher expansion\u000aedgeworth binomial tree\u000a\u000a\u000a
p331
sg32
g35
sg37
NsbsS'statistical_assumption.txt'
p332
g2
(g3
g4
Ntp333
Rp334
(dp335
g8
g11
sg12
Vstatistics like all mathematical disciplines does not infer valid conclusions from nothing inferring interesting conclusions about real statistical populations almost always requires some background assumptions those assumptions must be made carefully because incorrect assumptions can generate wildly inaccurate conclusions\u000ahere are some examples of statistical assumptions\u000aindependence of observations from each other this assumption is an especially common error\u000aindependence of observational error from potential confounding effects\u000aexact or approximate normality of observations\u000alinearity of graded responses to quantitative stimuli eg in linear regression\u000a\u000a\u000a classes of assumptionsedit \u000athere are two approaches to statistical inference modelbased inference and designbased inference both approaches rely on some statistical model to represent the datagenerating process in the modelbased approach the model is taken to be initially unknown and one of the goals is to select an appropriate model for inference in the designbased approach the model is taken to be known and one of the goals is to ensure that the sample data are selected randomly enough for inference\u000astatistical assumptions can be put into two classes depending upon which approach to inference is used\u000amodelbased assumptions these include the following three types\u000adistributional assumptions where a statistical model involves terms relating to random errors assumptions may be made about the probability distribution of these errors in some cases the distributional assumption relates to the observations themselves\u000astructural assumptions statistical relationships between variables are often modelled by equating one variable to a function of another or several others plus a random error models often involve making a structural assumption about the form of the functional relationship eg as in linear regression this can be generalised to models involving relationships between underlying unobserved latent variables\u000acrossvariation assumptions these assumptions involve the joint probability distributions of either the observations themselves or the random errors in a model simple models may include the assumption that observations or errors are statistically independent\u000a\u000adesignbased assumptions these relate to the way observations have been gathered and often involve an assumption of randomization during sampling\u000athe modelbased approach is much the most commonly used in statistical inference the designbased approach is used mainly with survey sampling with the modelbased approach all the assumptions are effectively encoded in the model\u000a\u000a\u000a checking assumptionsedit \u000agiven that the validity of any conclusion drawn from a statistical inference depends on the validity of the assumptions made it is clearly important that those assumptions should be reviewed at some stage some instancesfor example where data are lackingmay require that researchers judge whether an assumption is reasonable researchers can expand this somewhat to consider what effect a departure from the assumptions might produce where more extensive data are available various types of procedures for statistical model validation are availableeg for regression model validation\u000a\u000a\u000a see alsoedit \u000amisuse of statistics\u000arobust statistics\u000astatistical hypothesis testing\u000astatistical theory\u000a\u000a\u000a notesedit \u000a kruskall 1988\u000a koch g g gillings d b 2006 inference designbased vs modelbased encyclopedia of statistical sciences editorkotz s wileyinterscience\u000a cox 2006 ch9\u000a de gruijter et al 2006 22\u000a mcpherson 1990 341\u000a mcpherson 1990 33\u000a de gruijter et al 2006 221\u000a\u000a\u000a referencesedit \u000acox d r 2006 principles of statistical inference cambridge university press\u000ade gruijter j brus d bierkens m knotters m 2006 sampling for natural resource monitoring springerverlag\u000akruskal william december 1988 miracles and statistics the casual assumption of independence asa presidential address journal of the american statistical association 83 404 929940 doi1023072290117 jstor 2290117 \u000amcpherson g 1990 statistics in scientific investigation its basis application and interpretation springerverlag isbn 0387971378
p336
sg14
g17
sg18
Vstatistics like all mathematical disciplines does not infer valid conclusions from nothing inferring interesting conclusions about real statistical populations almost always requires some background assumptions those assumptions must be made carefully because incorrect assumptions can generate wildly inaccurate conclusions\u000ahere are some examples of statistical assumptions\u000aindependence of observations from each other this assumption is an especially common error\u000aindependence of observational error from potential confounding effects\u000aexact or approximate normality of observations\u000alinearity of graded responses to quantitative stimuli eg in linear regression\u000a\u000a\u000a classes of assumptionsedit \u000athere are two approaches to statistical inference modelbased inference and designbased inference both approaches rely on some statistical model to represent the datagenerating process in the modelbased approach the model is taken to be initially unknown and one of the goals is to select an appropriate model for inference in the designbased approach the model is taken to be known and one of the goals is to ensure that the sample data are selected randomly enough for inference\u000astatistical assumptions can be put into two classes depending upon which approach to inference is used\u000amodelbased assumptions these include the following three types\u000adistributional assumptions where a statistical model involves terms relating to random errors assumptions may be made about the probability distribution of these errors in some cases the distributional assumption relates to the observations themselves\u000astructural assumptions statistical relationships between variables are often modelled by equating one variable to a function of another or several others plus a random error models often involve making a structural assumption about the form of the functional relationship eg as in linear regression this can be generalised to models involving relationships between underlying unobserved latent variables\u000acrossvariation assumptions these assumptions involve the joint probability distributions of either the observations themselves or the random errors in a model simple models may include the assumption that observations or errors are statistically independent\u000a\u000adesignbased assumptions these relate to the way observations have been gathered and often involve an assumption of randomization during sampling\u000athe modelbased approach is much the most commonly used in statistical inference the designbased approach is used mainly with survey sampling with the modelbased approach all the assumptions are effectively encoded in the model\u000a\u000a\u000a checking assumptionsedit \u000agiven that the validity of any conclusion drawn from a statistical inference depends on the validity of the assumptions made it is clearly important that those assumptions should be reviewed at some stage some instancesfor example where data are lackingmay require that researchers judge whether an assumption is reasonable researchers can expand this somewhat to consider what effect a departure from the assumptions might produce where more extensive data are available various types of procedures for statistical model validation are availableeg for regression model validation\u000a\u000a\u000a see alsoedit \u000amisuse of statistics\u000arobust statistics\u000astatistical hypothesis testing\u000astatistical theory\u000a\u000a\u000a notesedit \u000a kruskall 1988\u000a koch g g gillings d b 2006 inference designbased vs modelbased encyclopedia of statistical sciences editorkotz s wileyinterscience\u000a cox 2006 ch9\u000a de gruijter et al 2006 22\u000a mcpherson 1990 341\u000a mcpherson 1990 33\u000a de gruijter et al 2006 221\u000a\u000a\u000a referencesedit \u000acox d r 2006 principles of statistical inference cambridge university press\u000ade gruijter j brus d bierkens m knotters m 2006 sampling for natural resource monitoring springerverlag\u000akruskal william december 1988 miracles and statistics the casual assumption of independence asa presidential address journal of the american statistical association 83 404 929940 doi1023072290117 jstor 2290117 \u000amcpherson g 1990 statistics in scientific investigation its basis application and interpretation springerverlag isbn 0387971378
p337
sg20
g23
sg24
g27
sg30
Vstatistics like all mathematical disciplines does not infer valid conclusions from nothing inferring interesting conclusions about real statistical populations almost always requires some background assumptions those assumptions must be made carefully because incorrect assumptions can generate wildly inaccurate conclusions\u000ahere are some examples of statistical assumptions\u000aindependence of observations from each other this assumption is an especially common error\u000aindependence of observational error from potential confounding effects\u000aexact or approximate normality of observations\u000alinearity of graded responses to quantitative stimuli eg in linear regression\u000a\u000a\u000a classes of assumptionsedit \u000athere are two approaches to statistical inference modelbased inference and designbased inference both approaches rely on some statistical model to represent the datagenerating process in the modelbased approach the model is taken to be initially unknown and one of the goals is to select an appropriate model for inference in the designbased approach the model is taken to be known and one of the goals is to ensure that the sample data are selected randomly enough for inference\u000astatistical assumptions can be put into two classes depending upon which approach to inference is used\u000amodelbased assumptions these include the following three types\u000adistributional assumptions where a statistical model involves terms relating to random errors assumptions may be made about the probability distribution of these errors in some cases the distributional assumption relates to the observations themselves\u000astructural assumptions statistical relationships between variables are often modelled by equating one variable to a function of another or several others plus a random error models often involve making a structural assumption about the form of the functional relationship eg as in linear regression this can be generalised to models involving relationships between underlying unobserved latent variables\u000acrossvariation assumptions these assumptions involve the joint probability distributions of either the observations themselves or the random errors in a model simple models may include the assumption that observations or errors are statistically independent\u000a\u000adesignbased assumptions these relate to the way observations have been gathered and often involve an assumption of randomization during sampling\u000athe modelbased approach is much the most commonly used in statistical inference the designbased approach is used mainly with survey sampling with the modelbased approach all the assumptions are effectively encoded in the model\u000a\u000a\u000a checking assumptionsedit \u000agiven that the validity of any conclusion drawn from a statistical inference depends on the validity of the assumptions made it is clearly important that those assumptions should be reviewed at some stage some instancesfor example where data are lackingmay require that researchers judge whether an assumption is reasonable researchers can expand this somewhat to consider what effect a departure from the assumptions might produce where more extensive data are available various types of procedures for statistical model validation are availableeg for regression model validation\u000a\u000a\u000a see alsoedit \u000amisuse of statistics\u000arobust statistics\u000astatistical hypothesis testing\u000astatistical theory\u000a\u000a\u000a notesedit \u000a kruskall 1988\u000a koch g g gillings d b 2006 inference designbased vs modelbased encyclopedia of statistical sciences editorkotz s wileyinterscience\u000a cox 2006 ch9\u000a de gruijter et al 2006 22\u000a mcpherson 1990 341\u000a mcpherson 1990 33\u000a de gruijter et al 2006 221\u000a\u000a\u000a referencesedit \u000acox d r 2006 principles of statistical inference cambridge university press\u000ade gruijter j brus d bierkens m knotters m 2006 sampling for natural resource monitoring springerverlag\u000akruskal william december 1988 miracles and statistics the casual assumption of independence asa presidential address journal of the american statistical association 83 404 929940 doi1023072290117 jstor 2290117 \u000amcpherson g 1990 statistics in scientific investigation its basis application and interpretation springerverlag isbn 0387971378
p338
sg32
g35
sg37
NsbsS'model_selection.txt'
p339
g2
(g3
g4
Ntp340
Rp341
(dp342
g8
g11
sg12
Vmodel selection is the task of selecting a statistical model from a set of candidate models given data in the simplest cases a preexisting set of data is considered however the task can also involve the design of experiments such that the data collected is wellsuited to the problem of model selection given candidate models of similar predictive or explanatory power the simplest model is most likely to be the best choice\u000akonishi  kitagawa 2008 p 75 state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox 2006 p 197 has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a introductionedit \u000a\u000ain its most basic forms model selection is one of the fundamental tasks of scientific inquiry determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations for example when galileo performed his inclined plane experiments he demonstrated that the motion of the balls fitted the parabola predicted by his model\u000aof the countless number of possible mechanisms and processes that could have produced the data how can one even begin to choose the best model the mathematical approach commonly taken decides among a set of candidate models this set must be chosen by the researcher often simple models such as polynomials are used at least initially burnham  anderson 2002 emphasize throughout their book the importance of choosing models based on sound scientific principles such as understanding of the phenomenological processes or mechanisms eg chemical reactions underlying the data\u000aonce the set of candidate models has been chosen the statistical analysis allows us to select the best of these models what is meant by best is controversial a good model selection technique will balance goodness of fit with simplicity more complex models will be better able to adapt their shape to fit the data for example a fifthorder polynomial can exactly fit six points but the additional parameters may not represent anything useful perhaps those six points are really just randomly distributed about a straight line goodness of fit is generally determined using a likelihood ratio approach or an approximation of this leading to a chisquared test the complexity is generally measured by counting the number of parameters in the model\u000amodel selection techniques can be considered as estimators of some physical quantity such as the probability of the model producing the given data the bias and variance are both important measures of the quality of this estimator efficiency is also often considered\u000aa standard example of model selection is that of curve fitting where given a set of points and other background knowledge eg points are a result of iid samples we must select a curve that describes the function that generated the points\u000a\u000a\u000a methods for choosing the set of candidate modelsedit \u000aexploratory data analysis\u000ascientific method\u000a\u000a\u000a criteria for model selectionedit \u000aakaike information criterion\u000abayes factor\u000abayesian information criterion\u000adeviance information criterion\u000afalse discovery rate\u000afocused information criterion\u000amallowss cp\u000aminimum description length algorithmic information theory\u000aminimum message length algorithmic information theory\u000astructural risk minimization\u000astepwise regression\u000acrossvalidation\u000athe most commonlyused criteria are i the akaike information criterion and ii the bayes factor andor the bayesian information criterion which to some extent approximates the bayes factor\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aaho k derryberry d peterson t 2014 model selection for ecologists the worldviews of aic and bic ecology 95 631636 doi1018901314521 \u000aanderson dr 2008 model based inference in the life sciences springer\u000aando t 2010 bayesian model selection and statistical modeling crc press\u000abreiman l 2001 statistical modeling the two cultures statistical science 16 199231 doi101214ss1009213726 \u000aburnham kp anderson dr 2002 model selection and multimodel inference a practical informationtheoretic approach 2nd edition springerverlag isbn 0387953647 this has over 25000 citations on google scholar\u000achamberlin tc 1890 the method of multiple working hypotheses science 15 93 reprinted 1965 science 148 754759 1\u000aclaeskens g hjort nl 2008 model selection and model averaging cambridge university press\u000acox dr 2006 principles of statistical inference cambridge university press\u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000alahiri p 2001 model selection institute of mathematical statistics\u000aleeb h ptscher b m 2009 model selection handbook of financial time series editort g anderson p 889925 springer doi101007978354071297839\u000alukacs p m thompson w l kendall w l gould w r doherty p f jr burnham k p anderson d r 2007 concerns regarding a call for pluralism of information theory and hypothesis testing journal of applied ecology 44 2 456460 doi101111j13652664200601267x \u000amassart p 2007 concentration inequalities and model selection springer\u000awit e van den heuvel e romeijn jw 2012 all models are wrong an introduction to model uncertainty pdf statistica neerlandica 66 217236 doi101111j14679574201200530x 
p343
sg14
g17
sg18
Vmodel selection is the task of selecting a statistical model from a set of candidate models given data in the simplest cases a preexisting set of data is considered however the task can also involve the design of experiments such that the data collected is wellsuited to the problem of model selection given candidate models of similar predictive or explanatory power the simplest model is most likely to be the best choice\u000akonishi  kitagawa 2008 p 75 state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox 2006 p 197 has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a introductionedit \u000a\u000ain its most basic forms model selection is one of the fundamental tasks of scientific inquiry determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations for example when galileo performed his inclined plane experiments he demonstrated that the motion of the balls fitted the parabola predicted by his model\u000aof the countless number of possible mechanisms and processes that could have produced the data how can one even begin to choose the best model the mathematical approach commonly taken decides among a set of candidate models this set must be chosen by the researcher often simple models such as polynomials are used at least initially burnham  anderson 2002 emphasize throughout their book the importance of choosing models based on sound scientific principles such as understanding of the phenomenological processes or mechanisms eg chemical reactions underlying the data\u000aonce the set of candidate models has been chosen the statistical analysis allows us to select the best of these models what is meant by best is controversial a good model selection technique will balance goodness of fit with simplicity more complex models will be better able to adapt their shape to fit the data for example a fifthorder polynomial can exactly fit six points but the additional parameters may not represent anything useful perhaps those six points are really just randomly distributed about a straight line goodness of fit is generally determined using a likelihood ratio approach or an approximation of this leading to a chisquared test the complexity is generally measured by counting the number of parameters in the model\u000amodel selection techniques can be considered as estimators of some physical quantity such as the probability of the model producing the given data the bias and variance are both important measures of the quality of this estimator efficiency is also often considered\u000aa standard example of model selection is that of curve fitting where given a set of points and other background knowledge eg points are a result of iid samples we must select a curve that describes the function that generated the points\u000a\u000a\u000a methods for choosing the set of candidate modelsedit \u000aexploratory data analysis\u000ascientific method\u000a\u000a\u000a criteria for model selectionedit \u000aakaike information criterion\u000abayes factor\u000abayesian information criterion\u000adeviance information criterion\u000afalse discovery rate\u000afocused information criterion\u000amallowss cp\u000aminimum description length algorithmic information theory\u000aminimum message length algorithmic information theory\u000astructural risk minimization\u000astepwise regression\u000acrossvalidation\u000athe most commonlyused criteria are i the akaike information criterion and ii the bayes factor andor the bayesian information criterion which to some extent approximates the bayes factor\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aaho k derryberry d peterson t 2014 model selection for ecologists the worldviews of aic and bic ecology 95 631636 doi1018901314521 \u000aanderson dr 2008 model based inference in the life sciences springer\u000aando t 2010 bayesian model selection and statistical modeling crc press\u000abreiman l 2001 statistical modeling the two cultures statistical science 16 199231 doi101214ss1009213726 \u000aburnham kp anderson dr 2002 model selection and multimodel inference a practical informationtheoretic approach 2nd edition springerverlag isbn 0387953647 this has over 25000 citations on google scholar\u000achamberlin tc 1890 the method of multiple working hypotheses science 15 93 reprinted 1965 science 148 754759 1\u000aclaeskens g hjort nl 2008 model selection and model averaging cambridge university press\u000acox dr 2006 principles of statistical inference cambridge university press\u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000alahiri p 2001 model selection institute of mathematical statistics\u000aleeb h ptscher b m 2009 model selection handbook of financial time series editort g anderson p 889925 springer doi101007978354071297839\u000alukacs p m thompson w l kendall w l gould w r doherty p f jr burnham k p anderson d r 2007 concerns regarding a call for pluralism of information theory and hypothesis testing journal of applied ecology 44 2 456460 doi101111j13652664200601267x \u000amassart p 2007 concentration inequalities and model selection springer\u000awit e van den heuvel e romeijn jw 2012 all models are wrong an introduction to model uncertainty pdf statistica neerlandica 66 217236 doi101111j14679574201200530x
p344
sg20
g23
sg24
g27
sg30
Vmodel selection is the task of selecting a statistical model from a set of candidate models given data in the simplest cases a preexisting set of data is considered however the task can also involve the design of experiments such that the data collected is wellsuited to the problem of model selection given candidate models of similar predictive or explanatory power the simplest model is most likely to be the best choice\u000akonishi  kitagawa 2008 p 75 state the majority of the problems in statistical inference can be considered to be problems related to statistical modeling relatedly sir david cox 2006 p 197 has said how the translation from subjectmatter problem to statistical model is done is often the most critical part of an analysis\u000a\u000a\u000a introductionedit \u000a\u000ain its most basic forms model selection is one of the fundamental tasks of scientific inquiry determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations for example when galileo performed his inclined plane experiments he demonstrated that the motion of the balls fitted the parabola predicted by his model\u000aof the countless number of possible mechanisms and processes that could have produced the data how can one even begin to choose the best model the mathematical approach commonly taken decides among a set of candidate models this set must be chosen by the researcher often simple models such as polynomials are used at least initially burnham  anderson 2002 emphasize throughout their book the importance of choosing models based on sound scientific principles such as understanding of the phenomenological processes or mechanisms eg chemical reactions underlying the data\u000aonce the set of candidate models has been chosen the statistical analysis allows us to select the best of these models what is meant by best is controversial a good model selection technique will balance goodness of fit with simplicity more complex models will be better able to adapt their shape to fit the data for example a fifthorder polynomial can exactly fit six points but the additional parameters may not represent anything useful perhaps those six points are really just randomly distributed about a straight line goodness of fit is generally determined using a likelihood ratio approach or an approximation of this leading to a chisquared test the complexity is generally measured by counting the number of parameters in the model\u000amodel selection techniques can be considered as estimators of some physical quantity such as the probability of the model producing the given data the bias and variance are both important measures of the quality of this estimator efficiency is also often considered\u000aa standard example of model selection is that of curve fitting where given a set of points and other background knowledge eg points are a result of iid samples we must select a curve that describes the function that generated the points\u000a\u000a\u000a methods for choosing the set of candidate modelsedit \u000aexploratory data analysis\u000ascientific method\u000a\u000a\u000a criteria for model selectionedit \u000aakaike information criterion\u000abayes factor\u000abayesian information criterion\u000adeviance information criterion\u000afalse discovery rate\u000afocused information criterion\u000amallowss cp\u000aminimum description length algorithmic information theory\u000aminimum message length algorithmic information theory\u000astructural risk minimization\u000astepwise regression\u000acrossvalidation\u000athe most commonlyused criteria are i the akaike information criterion and ii the bayes factor andor the bayesian information criterion which to some extent approximates the bayes factor\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000aaho k derryberry d peterson t 2014 model selection for ecologists the worldviews of aic and bic ecology 95 631636 doi1018901314521 \u000aanderson dr 2008 model based inference in the life sciences springer\u000aando t 2010 bayesian model selection and statistical modeling crc press\u000abreiman l 2001 statistical modeling the two cultures statistical science 16 199231 doi101214ss1009213726 \u000aburnham kp anderson dr 2002 model selection and multimodel inference a practical informationtheoretic approach 2nd edition springerverlag isbn 0387953647 this has over 25000 citations on google scholar\u000achamberlin tc 1890 the method of multiple working hypotheses science 15 93 reprinted 1965 science 148 754759 1\u000aclaeskens g hjort nl 2008 model selection and model averaging cambridge university press\u000acox dr 2006 principles of statistical inference cambridge university press\u000akonishi s kitagawa g 2008 information criteria and statistical modeling springer \u000alahiri p 2001 model selection institute of mathematical statistics\u000aleeb h ptscher b m 2009 model selection handbook of financial time series editort g anderson p 889925 springer doi101007978354071297839\u000alukacs p m thompson w l kendall w l gould w r doherty p f jr burnham k p anderson d r 2007 concerns regarding a call for pluralism of information theory and hypothesis testing journal of applied ecology 44 2 456460 doi101111j13652664200601267x \u000amassart p 2007 concentration inequalities and model selection springer\u000awit e van den heuvel e romeijn jw 2012 all models are wrong an introduction to model uncertainty pdf statistica neerlandica 66 217236 doi101111j14679574201200530x 
p345
sg32
g35
sg37
NsbsS'decoupling_(probability).txt'
p346
g2
(g3
g4
Ntp347
Rp348
(dp349
g8
g11
sg12
Vin probability and statistics decoupling is a reduction of a sample statistic to an average of the statistic evaluated on several independent sequences of the random variable this sum conditioned on all but one of the independent sequences becomes a sum of independent random variables decoupling is used in the study of u statistics where decoupling should not be confused with hoeffdings decomposition however such decoupling is unrelated to the use of couplings in the study of stochastic processes\u000a\u000a\u000a
p350
sg14
g17
sg18
Vin probability and statistics decoupling is a reduction of a sample statistic to an average of the statistic evaluated on several independent sequences of the random variable this sum conditioned on all but one of the independent sequences becomes a sum of independent random variables decoupling is used in the study of u statistics where decoupling should not be confused with hoeffdings decomposition however such decoupling is unrelated to the use of couplings in the study of stochastic processes
p351
sg20
g23
sg24
g27
sg30
Vin probability and statistics decoupling is a reduction of a sample statistic to an average of the statistic evaluated on several independent sequences of the random variable this sum conditioned on all but one of the independent sequences becomes a sum of independent random variables decoupling is used in the study of u statistics where decoupling should not be confused with hoeffdings decomposition however such decoupling is unrelated to the use of couplings in the study of stochastic processes\u000a\u000a\u000a
p352
sg32
g35
sg37
NsbsS'statistic.txt'
p353
g2
(g3
g4
Ntp354
Rp355
(dp356
g8
g11
sg12
Vnn\u000aa statistic singular is a single measure of some attribute of a sample eg its arithmetic mean value it is calculated by applying a function statistical algorithm to the values of the items of the sample which are known together as a set of data\u000amore formally statistical theory defines a statistic as a function of a sample where the function itself is independent of the samples distribution that is the function can be stated before realization of the data the term statistic is used both for the function and for the value of the function on a given sample\u000aa statistic is distinct from a statistical parameter which is not computable because often the population is much too large to examine and measure all its items however a statistic when used to estimate a population parameter is called an estimator for instance the sample mean is a statistic that estimates the population mean which is a parameter\u000awhen a statistic a function is being used for a specific purpose it may be referred to by a name indicating its purpose in descriptive statistics a descriptive statistic is used to describe the data in estimation theory an estimator is used to estimate a parameter of the distribution population in statistical hypothesis testing a test statistic is used to test a hypothesis however a single statistic can be used for multiple purposes  for example the sample mean can be used to describe a data set to estimate the population mean or to test a hypothesis\u000a\u000a\u000a examples \u000ain calculating the arithmetic mean of a sample for example the algorithm works by summing all the data values observed in the sample and then dividing this sum by the number of data items this single measure the mean of the sample is called a statistic its value is frequently used as an estimate of the mean value of all items comprising the population from which the sample is drawn the population mean is also a single measure however it is not called a statistic because it is not obtained from a sample instead it is called a population parameter because it is obtained from the whole population\u000aother examples of statistics include\u000asample mean discussed in the example above and sample median\u000asample variance and sample standard deviation\u000asample quantiles besides the median eg quartiles and percentiles\u000atest statistics such as t statistics chisquared statistics f statistics\u000aorder statistics including sample maximum and minimum\u000asample moments and functions thereof including kurtosis and skewness\u000avarious functionals of the empirical distribution function\u000a\u000a\u000a properties \u000a\u000a\u000a observability \u000aa statistic is an observable random variable which differentiates it both from a parameter that is a generally unobservable quantity describing a property of a statistical population and from an unobservable random variable such as the difference between an observed measurement and a population average a parameter can only be computed exactly if the entire population can be observed without error for instance in a perfect census or for a population of standardized test takers\u000astatisticians often contemplate a parameterized family of probability distributions any member of which could be the distribution of some measurable aspect of each member of a population from which a sample is drawn randomly for example the parameter may be the average height of 25yearold men in north america the height of the members of a sample of 100 such men are measured the average of those 100 numbers is a statistic the average of the heights of all members of the population is not a statistic unless that has somehow also been ascertained such as by measuring every member of the population the average height of all in the sense of genetically possible 25yearold north american men is a parameter and not a statistic\u000a\u000a\u000a statistical properties \u000aimportant potential properties of statistics include completeness consistency sufficiency unbiasedness minimum mean square error low variance robustness and computational convenience\u000a\u000a\u000a information of a statistic \u000ainformation of a statistic on model parameters can be defined in several ways the most common is the fisher information which is defined on the statistic model induced by the statistic kullback information measure can also be used\u000a\u000a\u000a common misconceptions \u000a\u000a\u000a see also \u000astatistics\u000astatistical theory\u000adescriptive statistics\u000astatistical hypothesis testing\u000awellbehaved statistic\u000a\u000a\u000a
p357
sg14
g17
sg18
Vnn\u000aa statistic singular is a single measure of some attribute of a sample eg its arithmetic mean value it is calculated by applying a function statistical algorithm to the values of the items of the sample which are known together as a set of data\u000amore formally statistical theory defines a statistic as a function of a sample where the function itself is independent of the samples distribution that is the function can be stated before realization of the data the term statistic is used both for the function and for the value of the function on a given sample\u000aa statistic is distinct from a statistical parameter which is not computable because often the population is much too large to examine and measure all its items however a statistic when used to estimate a population parameter is called an estimator for instance the sample mean is a statistic that estimates the population mean which is a parameter\u000awhen a statistic a function is being used for a specific purpose it may be referred to by a name indicating its purpose in descriptive statistics a descriptive statistic is used to describe the data in estimation theory an estimator is used to estimate a parameter of the distribution population in statistical hypothesis testing a test statistic is used to test a hypothesis however a single statistic can be used for multiple purposes  for example the sample mean can be used to describe a data set to estimate the population mean or to test a hypothesis\u000a\u000a\u000a examples \u000ain calculating the arithmetic mean of a sample for example the algorithm works by summing all the data values observed in the sample and then dividing this sum by the number of data items this single measure the mean of the sample is called a statistic its value is frequently used as an estimate of the mean value of all items comprising the population from which the sample is drawn the population mean is also a single measure however it is not called a statistic because it is not obtained from a sample instead it is called a population parameter because it is obtained from the whole population\u000aother examples of statistics include\u000asample mean discussed in the example above and sample median\u000asample variance and sample standard deviation\u000asample quantiles besides the median eg quartiles and percentiles\u000atest statistics such as t statistics chisquared statistics f statistics\u000aorder statistics including sample maximum and minimum\u000asample moments and functions thereof including kurtosis and skewness\u000avarious functionals of the empirical distribution function\u000a\u000a\u000a properties \u000a\u000a\u000a observability \u000aa statistic is an observable random variable which differentiates it both from a parameter that is a generally unobservable quantity describing a property of a statistical population and from an unobservable random variable such as the difference between an observed measurement and a population average a parameter can only be computed exactly if the entire population can be observed without error for instance in a perfect census or for a population of standardized test takers\u000astatisticians often contemplate a parameterized family of probability distributions any member of which could be the distribution of some measurable aspect of each member of a population from which a sample is drawn randomly for example the parameter may be the average height of 25yearold men in north america the height of the members of a sample of 100 such men are measured the average of those 100 numbers is a statistic the average of the heights of all members of the population is not a statistic unless that has somehow also been ascertained such as by measuring every member of the population the average height of all in the sense of genetically possible 25yearold north american men is a parameter and not a statistic\u000a\u000a\u000a statistical properties \u000aimportant potential properties of statistics include completeness consistency sufficiency unbiasedness minimum mean square error low variance robustness and computational convenience\u000a\u000a\u000a information of a statistic \u000ainformation of a statistic on model parameters can be defined in several ways the most common is the fisher information which is defined on the statistic model induced by the statistic kullback information measure can also be used\u000a\u000a\u000a common misconceptions \u000a\u000a\u000a see also \u000astatistics\u000astatistical theory\u000adescriptive statistics\u000astatistical hypothesis testing\u000awellbehaved statistic
p358
sg20
g23
sg24
g27
sg30
Vnn\u000aa statistic singular is a single measure of some attribute of a sample eg its arithmetic mean value it is calculated by applying a function statistical algorithm to the values of the items of the sample which are known together as a set of data\u000amore formally statistical theory defines a statistic as a function of a sample where the function itself is independent of the samples distribution that is the function can be stated before realization of the data the term statistic is used both for the function and for the value of the function on a given sample\u000aa statistic is distinct from a statistical parameter which is not computable because often the population is much too large to examine and measure all its items however a statistic when used to estimate a population parameter is called an estimator for instance the sample mean is a statistic that estimates the population mean which is a parameter\u000awhen a statistic a function is being used for a specific purpose it may be referred to by a name indicating its purpose in descriptive statistics a descriptive statistic is used to describe the data in estimation theory an estimator is used to estimate a parameter of the distribution population in statistical hypothesis testing a test statistic is used to test a hypothesis however a single statistic can be used for multiple purposes  for example the sample mean can be used to describe a data set to estimate the population mean or to test a hypothesis\u000a\u000a\u000a examples \u000ain calculating the arithmetic mean of a sample for example the algorithm works by summing all the data values observed in the sample and then dividing this sum by the number of data items this single measure the mean of the sample is called a statistic its value is frequently used as an estimate of the mean value of all items comprising the population from which the sample is drawn the population mean is also a single measure however it is not called a statistic because it is not obtained from a sample instead it is called a population parameter because it is obtained from the whole population\u000aother examples of statistics include\u000asample mean discussed in the example above and sample median\u000asample variance and sample standard deviation\u000asample quantiles besides the median eg quartiles and percentiles\u000atest statistics such as t statistics chisquared statistics f statistics\u000aorder statistics including sample maximum and minimum\u000asample moments and functions thereof including kurtosis and skewness\u000avarious functionals of the empirical distribution function\u000a\u000a\u000a properties \u000a\u000a\u000a observability \u000aa statistic is an observable random variable which differentiates it both from a parameter that is a generally unobservable quantity describing a property of a statistical population and from an unobservable random variable such as the difference between an observed measurement and a population average a parameter can only be computed exactly if the entire population can be observed without error for instance in a perfect census or for a population of standardized test takers\u000astatisticians often contemplate a parameterized family of probability distributions any member of which could be the distribution of some measurable aspect of each member of a population from which a sample is drawn randomly for example the parameter may be the average height of 25yearold men in north america the height of the members of a sample of 100 such men are measured the average of those 100 numbers is a statistic the average of the heights of all members of the population is not a statistic unless that has somehow also been ascertained such as by measuring every member of the population the average height of all in the sense of genetically possible 25yearold north american men is a parameter and not a statistic\u000a\u000a\u000a statistical properties \u000aimportant potential properties of statistics include completeness consistency sufficiency unbiasedness minimum mean square error low variance robustness and computational convenience\u000a\u000a\u000a information of a statistic \u000ainformation of a statistic on model parameters can be defined in several ways the most common is the fisher information which is defined on the statistic model induced by the statistic kullback information measure can also be used\u000a\u000a\u000a common misconceptions \u000a\u000a\u000a see also \u000astatistics\u000astatistical theory\u000adescriptive statistics\u000astatistical hypothesis testing\u000awellbehaved statistic\u000a\u000a\u000a
p359
sg32
g35
sg37
NsbsS'bias_of_an_estimator.txt'
p360
g2
(g3
g4
Ntp361
Rp362
(dp363
g8
g11
sg12
Vin statistics the bias or bias function of an estimator is the difference between this estimators expected value and the true value of the parameter being estimated an estimator or decision rule with zero bias is called unbiased otherwise the estimator is said to be biased in statistics bias is an objective statement about a function and while not a desired property it is not pejorative unlike the ordinary english use of the term bias\u000abias can also be measured with respect to the median rather than the mean expected value in which case one distinguishes medianunbiased from the usual meanunbiasedness property bias is related to consistency in that consistent estimators are convergent and asymptotically unbiased hence converge to the correct value though individual estimators in a consistent sequence may be biased so long as the bias converges to zero see bias versus consistency\u000aall else equal an unbiased estimator is preferable to a biased estimator but in practice all else is not equal and biased estimators are frequently used generally with small bias when a biased estimator is used the bias is also estimated a biased estimator may be used for various reasons because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute as in unbiased estimation of standard deviation because an estimator is medianunbiased but not meanunbiased or the reverse because a biased estimator reduces some loss function particularly mean squared error compared with unbiased estimators notably in shrinkage estimators or because in some cases being unbiased is too strong a condition and the only unbiased estimators are not useful further meanunbiasedness is not preserved under nonlinear transformations though medianunbiasedness is see effect of transformations for example the sample variance is an unbiased estimator for the population variance but its square root the sample standard deviation is a biased estimator for the population standard deviation these are all illustrated below\u000a\u000a\u000a definitionedit \u000asuppose we have a statistical model parameterized by a real number  giving rise to a probability distribution for observed data  and a statistic  which serves as an estimator of  based on any observed data  that is we assume that our data follows some unknown distribution  where  is a fixed constant that is part of this distribution but is unknown and then we construct some estimator  that maps observed data to values that we hope are close to  then the bias of this estimator relative to the parameter  is defined to be\u000a\u000awhere  denotes expected value over the distribution  ie averaging over all possible observations  the second equation follows since  is measurable with respect to the conditional distribution \u000aan estimator is said to be unbiased if its bias is equal to zero for all values of parameter \u000athere are more general notions of bias and unbiasedness what this article calls bias is called meanbias to distinguish meanbias from the other notions with the notable ones being medianunbiased estimators for more details the general theory of unbiased estimators is briefly discussed near the end of this article\u000ain a simulation experiment concerning the properties of an estimator the bias of the estimator may be assessed using the mean signed difference\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample varianceedit \u000a\u000athe sample variance of a random variable demonstrates two aspects of estimator bias firstly the naive estimator is biased which can be corrected by a scale factor second the unbiased estimator is not optimal in terms of mean squared error mse which can be minimized by using a different scale factor resulting in a biased estimator with lower mse than the unbiased estimator concretely the naive estimator sums the squared deviations and divides by n which is biased dividing instead by n  1 yields an unbiased estimator conversely mse can be minimized by dividing by a different number depending on distribution but this results in a biased estimator this number is always larger than n  1 so this is known as a shrinkage estimator as it shrinks the unbiased estimator towards zero for the normal distribution the optimal value is n  1\u000asuppose x1  xn are independent and identically distributed iid random variables with expectation  and variance 2 if the sample mean and uncorrected sample variance are defined as\u000a\u000athen s2 is a biased estimator of 2 because\u000a\u000ain other words the expected value of the uncorrected sample variance does not equal the population variance 2 unless multiplied by a normalization factor the sample mean on the other hand is an unbiased estimator of the population mean \u000athe reason that s2 is biased stems from the fact that the sample mean is an ordinary least squares ols estimator for   is the number that makes the sum  as small as possible that is when any other number is plugged into this sum the sum can only increase in particular the choice  gives\u000a\u000aand then\u000a\u000anote that the usual definition of sample variance is\u000a\u000aand this is an unbiased estimator of the population variance this can be seen by noting the following formula which follows from the bienaym formula for the term in the inequality for the expectation of the uncorrected sample variance above\u000a\u000athe ratio between the biased uncorrected and unbiased estimates of the variance is known as bessels correction\u000a\u000a\u000a estimating a poisson probabilityedit \u000aa far more extreme case of a biased estimator being better than any unbiased estimator arises from the poisson distribution suppose that x has a poisson distribution with expectation  suppose it is desired to estimate\u000a\u000awith a sample of size 1 for example when incoming calls at a telephone switchboard are modeled as a poisson process and  is the average number of calls per minute then e2 is the probability that no calls arrive in the next two minutes\u000asince the expectation of an unbiased estimator x is equal to the estimand ie\u000a\u000athe only function of the data constituting an unbiased estimator is\u000a\u000ato see this note that when decomposing e from the above expression for expectation the sum that is left is a taylor series expansion of e as well yielding ee  e2 see characterizations of the exponential function\u000aif the observed value of x is 100 then the estimate is 1 although the true value of the quantity being estimated is very likely to be near 0 which is the opposite extreme and if x is observed to be 101 then the estimate is even more absurd it is 1 although the quantity being estimated must be positive\u000athe biased maximum likelihood estimator\u000a\u000ais far better than this unbiased estimator not only is its value always positive but it is also more accurate in the sense that its mean squared error\u000a\u000ais smaller compare the unbiased estimators mse of\u000a\u000athe mses are functions of the true value  the bias of the maximumlikelihood estimator is\u000a\u000a\u000a maximum of a discrete uniform distributionedit \u000a\u000athe bias of maximumlikelihood estimators can be substantial consider a case where n tickets numbered from 1 through to n are placed in a box and one is selected at random giving a value x if n is unknown then the maximumlikelihood estimator of n is x even though the expectation of x is only n  12 we can be certain only that n is at least x and is probably more in this case the natural unbiased estimator is 2x  1\u000a\u000a\u000a medianunbiased estimatorsedit \u000athe theory of medianunbiased estimators was revived by george w brown in 1947\u000a\u000aan estimate of a onedimensional parameter  will be said to be medianunbiased if for fixed  the median of the distribution of the estimate is at the value  ie the estimate underestimates just as often as it overestimates this requirement seems for most purposes to accomplish as much as the meanunbiased requirement and has the additional property that it is invariant under onetoone transformation\u000a\u000afurther properties of medianunbiased estimators have been noted by lehmann birnbaum van der vaart and pfanzagl in particular medianunbiased estimators exist in cases where meanunbiased and maximumlikelihood estimators do not exist besides being invariant under onetoone transformations medianunbiased estimators have surprising robustness unfortunately there is no analogue of raoblackwell theorem for medianunbiased estimation see the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a bias with respect to other loss functionsedit \u000aany minimumvariance meanunbiased estimator minimizes the risk expected loss with respect to the squarederror loss function among meanunbiased estimators as observed by gauss a minimumaverage absolute deviation medianunbiased estimator minimizes the risk with respect to the absolute loss function among medianunbiased estimators as observed by laplace other loss functions are used in statistical theory particularly in robust statistics connections between loss functions and unbiased estimation were studied in many works detailed description of corresponding results is given in chapter 3 of the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a effect of transformationsedit \u000anote that when a transformation is applied to a meanunbiased estimator the result need not be a meanunbiased estimator of its corresponding population statistic by jensens inequality a convex function as transformation will introduce positive bias while a concave function will introduce negative bias and a function of mixed convexity may introduce bias in either direction depending on the specific function and distribution that is for a nonlinear function f and a meanunbiased estimator u of a parameter p the composite estimator fu need not be a meanunbiased estimator of fp for example the square root of the unbiased estimator of the population variance is not a meanunbiased estimator of the population standard deviation the square root of the unbiased sample variance the corrected sample standard deviation is biased the bias depends both on the sampling distribution of the estimator and on the transform and can be quite involved to calculate  see unbiased estimation of standard deviation for a discussion in this case\u000a\u000a\u000a bias variance and mean squared erroredit \u000a\u000awhile bias quantifies the average difference to be expected between an estimator and an underlying parameter an estimator based on a finite sample can additionally be expected to differ from the parameter due to the randomness in the sample\u000aone measure which is used to try to reflect both types of difference is the mean square error\u000a\u000athis can be shown to be equal to the square of the bias plus the variance\u000a\u000awhen the parameter is a vector an analogous decomposition applies\u000a\u000awhere\u000a\u000ais the trace of the covariance matrix of the estimator\u000aan estimator that minimises the bias will not necessarily minimise the mean square error\u000a\u000a\u000a example estimation of population varianceedit \u000afor example suppose an estimator of the form\u000a\u000ais sought for the population variance as above but this time to minimise the mse\u000a\u000aif the variables x1  xn follow a normal distribution then ns22 has a chisquared distribution with n  1 degrees of freedom giving\u000a\u000aand so\u000a\u000awith a little algebra it can be confirmed that it is c  1n  1 which minimises this combined loss function rather than c  1n  1 which minimises just the bias term\u000amore generally it is only in restricted classes of problems that there will be an estimator that minimises the mse independently of the parameter values\u000ahowever it is very common that there may be perceived to be a biasvariance tradeoff such that a small increase in bias can be traded for a larger decrease in variance resulting in a more desirable estimator overall\u000a\u000a\u000a bayesian viewedit \u000amost bayesians are rather unconcerned about unbiasedness at least in the formal samplingtheory sense above of their estimates for example gelman et al 1995 write from a bayesian perspective the principle of unbiasedness is reasonable in the limit of large samples but otherwise it is potentially misleading\u000afundamentally the difference between the bayesian approach and the samplingtheory approach above is that in the samplingtheory approach the parameter is taken as fixed and then probability distributions of a statistic are considered based on the predicted sampling distribution of the data for a bayesian however it is the data which is known and fixed and it is the unknown parameter for which an attempt is made to construct a probability distribution using bayes theorem\u000a\u000ahere the second term the likelihood of the data given the unknown parameter value  depends just on the data obtained and the modelling of the data generation process however a bayesian calculation also includes the first term the prior probability for  which takes account of everything the analyst may know or suspect about  before the data comes in this information plays no part in the samplingtheory approach indeed any attempt to include it would be considered bias away from what was pointed to purely by the data to the extent that bayesian calculations include prior information it is therefore essentially inevitable that their results will not be unbiased in sampling theory terms\u000abut the results of a bayesian approach can differ from the sampling theory approach even if the bayesian tries to adopt an uninformative prior\u000afor example consider again the estimation of an unknown population variance 2 of a normal distribution with unknown mean where it is desired to optimise c in the expected loss function\u000a\u000aa standard choice of uninformative prior for this problem is the jeffreys prior  which is equivalent to adopting a rescalinginvariant flat prior for ln 2\u000aone consequence of adopting this prior is that s22 remains a pivotal quantity ie the probability distribution of s22 depends only on s22 independent of the value of s2 or 2\u000a\u000ahowever whilst\u000a\u000ain contrast\u000a\u000a when the expectation is taken over the probability distribution of 2 given s2 as it is in the bayesian case rather than s2 given 2 one can no longer take 4 as a constant and factor it out the consequence of this is that compared to the samplingtheory calculation the bayesian calculation puts more weight on larger values of 2 properly taking into account as the samplingtheory calculation cannot that under this squaredloss function the consequence of underestimating large values of 2 is more costly in squaredloss terms than that of overestimating small values of 2\u000athe workedout bayesian calculation gives a scaled inverse chisquared distribution with n  1 degrees of freedom for the posterior probability distribution of 2 the expected loss is minimised when cns2  2 this occurs when c  1n  3\u000aeven with an uninformative prior therefore a bayesian calculation may not give the same expectedloss minimising result as the corresponding samplingtheory calculation\u000a\u000a\u000a see alsoedit \u000aomittedvariable bias\u000aconsistent estimator\u000aestimation theory\u000aexpected loss\u000aexpected value\u000aloss function\u000amedian\u000astatistical decision theory\u000aoptimism bias\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000abrown george w on smallsample estimation the annals of mathematical statistics vol 18 no 4 dec 1947 pp 582585 jstor 2236236\u000alehmann e l a general concept of unbiasedness the annals of mathematical statistics vol 22 no 4 dec 1951 pp 587592 jstor 2236928\u000aallan birnbaum 1961 a unified theory of estimation i the annals of mathematical statistics vol 32 no 1 mar 1961 pp 112135\u000avan der vaart h r 1961 some extensions of the idea of bias the annals of mathematical statistics vol 32 no 2 june 1961 pp 436447\u000apfanzagl johann 1994 parametric statistical theory walter de gruyter\u000astuart alan ord keith arnold steven f 2010 classical inference and the linear model kendalls advanced theory of statistics 2a wiley isbn 0470689242 \u000avoinov vassily g nikulin mikhail s 1993 unbiased estimators and their applications 1 univariate case dordrect kluwer academic publishers isbn 0792323823 \u000avoinov vassily g nikulin mikhail s 1996 unbiased estimators and their applications 2 multivariate case dordrect kluwer academic publishers isbn 0792339398 \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 unbiased estimator encyclopedia of mathematics springer isbn 9781556080104
p364
sg14
g17
sg18
Vin statistics the bias or bias function of an estimator is the difference between this estimators expected value and the true value of the parameter being estimated an estimator or decision rule with zero bias is called unbiased otherwise the estimator is said to be biased in statistics bias is an objective statement about a function and while not a desired property it is not pejorative unlike the ordinary english use of the term bias\u000abias can also be measured with respect to the median rather than the mean expected value in which case one distinguishes medianunbiased from the usual meanunbiasedness property bias is related to consistency in that consistent estimators are convergent and asymptotically unbiased hence converge to the correct value though individual estimators in a consistent sequence may be biased so long as the bias converges to zero see bias versus consistency\u000aall else equal an unbiased estimator is preferable to a biased estimator but in practice all else is not equal and biased estimators are frequently used generally with small bias when a biased estimator is used the bias is also estimated a biased estimator may be used for various reasons because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute as in unbiased estimation of standard deviation because an estimator is medianunbiased but not meanunbiased or the reverse because a biased estimator reduces some loss function particularly mean squared error compared with unbiased estimators notably in shrinkage estimators or because in some cases being unbiased is too strong a condition and the only unbiased estimators are not useful further meanunbiasedness is not preserved under nonlinear transformations though medianunbiasedness is see effect of transformations for example the sample variance is an unbiased estimator for the population variance but its square root the sample standard deviation is a biased estimator for the population standard deviation these are all illustrated below\u000a\u000a\u000a definitionedit \u000asuppose we have a statistical model parameterized by a real number  giving rise to a probability distribution for observed data  and a statistic  which serves as an estimator of  based on any observed data  that is we assume that our data follows some unknown distribution  where  is a fixed constant that is part of this distribution but is unknown and then we construct some estimator  that maps observed data to values that we hope are close to  then the bias of this estimator relative to the parameter  is defined to be\u000a\u000awhere  denotes expected value over the distribution  ie averaging over all possible observations  the second equation follows since  is measurable with respect to the conditional distribution \u000aan estimator is said to be unbiased if its bias is equal to zero for all values of parameter \u000athere are more general notions of bias and unbiasedness what this article calls bias is called meanbias to distinguish meanbias from the other notions with the notable ones being medianunbiased estimators for more details the general theory of unbiased estimators is briefly discussed near the end of this article\u000ain a simulation experiment concerning the properties of an estimator the bias of the estimator may be assessed using the mean signed difference\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample varianceedit \u000a\u000athe sample variance of a random variable demonstrates two aspects of estimator bias firstly the naive estimator is biased which can be corrected by a scale factor second the unbiased estimator is not optimal in terms of mean squared error mse which can be minimized by using a different scale factor resulting in a biased estimator with lower mse than the unbiased estimator concretely the naive estimator sums the squared deviations and divides by n which is biased dividing instead by n  1 yields an unbiased estimator conversely mse can be minimized by dividing by a different number depending on distribution but this results in a biased estimator this number is always larger than n  1 so this is known as a shrinkage estimator as it shrinks the unbiased estimator towards zero for the normal distribution the optimal value is n  1\u000asuppose x1  xn are independent and identically distributed iid random variables with expectation  and variance 2 if the sample mean and uncorrected sample variance are defined as\u000a\u000athen s2 is a biased estimator of 2 because\u000a\u000ain other words the expected value of the uncorrected sample variance does not equal the population variance 2 unless multiplied by a normalization factor the sample mean on the other hand is an unbiased estimator of the population mean \u000athe reason that s2 is biased stems from the fact that the sample mean is an ordinary least squares ols estimator for   is the number that makes the sum  as small as possible that is when any other number is plugged into this sum the sum can only increase in particular the choice  gives\u000a\u000aand then\u000a\u000anote that the usual definition of sample variance is\u000a\u000aand this is an unbiased estimator of the population variance this can be seen by noting the following formula which follows from the bienaym formula for the term in the inequality for the expectation of the uncorrected sample variance above\u000a\u000athe ratio between the biased uncorrected and unbiased estimates of the variance is known as bessels correction\u000a\u000a\u000a estimating a poisson probabilityedit \u000aa far more extreme case of a biased estimator being better than any unbiased estimator arises from the poisson distribution suppose that x has a poisson distribution with expectation  suppose it is desired to estimate\u000a\u000awith a sample of size 1 for example when incoming calls at a telephone switchboard are modeled as a poisson process and  is the average number of calls per minute then e2 is the probability that no calls arrive in the next two minutes\u000asince the expectation of an unbiased estimator x is equal to the estimand ie\u000a\u000athe only function of the data constituting an unbiased estimator is\u000a\u000ato see this note that when decomposing e from the above expression for expectation the sum that is left is a taylor series expansion of e as well yielding ee  e2 see characterizations of the exponential function\u000aif the observed value of x is 100 then the estimate is 1 although the true value of the quantity being estimated is very likely to be near 0 which is the opposite extreme and if x is observed to be 101 then the estimate is even more absurd it is 1 although the quantity being estimated must be positive\u000athe biased maximum likelihood estimator\u000a\u000ais far better than this unbiased estimator not only is its value always positive but it is also more accurate in the sense that its mean squared error\u000a\u000ais smaller compare the unbiased estimators mse of\u000a\u000athe mses are functions of the true value  the bias of the maximumlikelihood estimator is\u000a\u000a\u000a maximum of a discrete uniform distributionedit \u000a\u000athe bias of maximumlikelihood estimators can be substantial consider a case where n tickets numbered from 1 through to n are placed in a box and one is selected at random giving a value x if n is unknown then the maximumlikelihood estimator of n is x even though the expectation of x is only n  12 we can be certain only that n is at least x and is probably more in this case the natural unbiased estimator is 2x  1\u000a\u000a\u000a medianunbiased estimatorsedit \u000athe theory of medianunbiased estimators was revived by george w brown in 1947\u000a\u000aan estimate of a onedimensional parameter  will be said to be medianunbiased if for fixed  the median of the distribution of the estimate is at the value  ie the estimate underestimates just as often as it overestimates this requirement seems for most purposes to accomplish as much as the meanunbiased requirement and has the additional property that it is invariant under onetoone transformation\u000a\u000afurther properties of medianunbiased estimators have been noted by lehmann birnbaum van der vaart and pfanzagl in particular medianunbiased estimators exist in cases where meanunbiased and maximumlikelihood estimators do not exist besides being invariant under onetoone transformations medianunbiased estimators have surprising robustness unfortunately there is no analogue of raoblackwell theorem for medianunbiased estimation see the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a bias with respect to other loss functionsedit \u000aany minimumvariance meanunbiased estimator minimizes the risk expected loss with respect to the squarederror loss function among meanunbiased estimators as observed by gauss a minimumaverage absolute deviation medianunbiased estimator minimizes the risk with respect to the absolute loss function among medianunbiased estimators as observed by laplace other loss functions are used in statistical theory particularly in robust statistics connections between loss functions and unbiased estimation were studied in many works detailed description of corresponding results is given in chapter 3 of the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a effect of transformationsedit \u000anote that when a transformation is applied to a meanunbiased estimator the result need not be a meanunbiased estimator of its corresponding population statistic by jensens inequality a convex function as transformation will introduce positive bias while a concave function will introduce negative bias and a function of mixed convexity may introduce bias in either direction depending on the specific function and distribution that is for a nonlinear function f and a meanunbiased estimator u of a parameter p the composite estimator fu need not be a meanunbiased estimator of fp for example the square root of the unbiased estimator of the population variance is not a meanunbiased estimator of the population standard deviation the square root of the unbiased sample variance the corrected sample standard deviation is biased the bias depends both on the sampling distribution of the estimator and on the transform and can be quite involved to calculate  see unbiased estimation of standard deviation for a discussion in this case\u000a\u000a\u000a bias variance and mean squared erroredit \u000a\u000awhile bias quantifies the average difference to be expected between an estimator and an underlying parameter an estimator based on a finite sample can additionally be expected to differ from the parameter due to the randomness in the sample\u000aone measure which is used to try to reflect both types of difference is the mean square error\u000a\u000athis can be shown to be equal to the square of the bias plus the variance\u000a\u000awhen the parameter is a vector an analogous decomposition applies\u000a\u000awhere\u000a\u000ais the trace of the covariance matrix of the estimator\u000aan estimator that minimises the bias will not necessarily minimise the mean square error\u000a\u000a\u000a example estimation of population varianceedit \u000afor example suppose an estimator of the form\u000a\u000ais sought for the population variance as above but this time to minimise the mse\u000a\u000aif the variables x1  xn follow a normal distribution then ns22 has a chisquared distribution with n  1 degrees of freedom giving\u000a\u000aand so\u000a\u000awith a little algebra it can be confirmed that it is c  1n  1 which minimises this combined loss function rather than c  1n  1 which minimises just the bias term\u000amore generally it is only in restricted classes of problems that there will be an estimator that minimises the mse independently of the parameter values\u000ahowever it is very common that there may be perceived to be a biasvariance tradeoff such that a small increase in bias can be traded for a larger decrease in variance resulting in a more desirable estimator overall\u000a\u000a\u000a bayesian viewedit \u000amost bayesians are rather unconcerned about unbiasedness at least in the formal samplingtheory sense above of their estimates for example gelman et al 1995 write from a bayesian perspective the principle of unbiasedness is reasonable in the limit of large samples but otherwise it is potentially misleading\u000afundamentally the difference between the bayesian approach and the samplingtheory approach above is that in the samplingtheory approach the parameter is taken as fixed and then probability distributions of a statistic are considered based on the predicted sampling distribution of the data for a bayesian however it is the data which is known and fixed and it is the unknown parameter for which an attempt is made to construct a probability distribution using bayes theorem\u000a\u000ahere the second term the likelihood of the data given the unknown parameter value  depends just on the data obtained and the modelling of the data generation process however a bayesian calculation also includes the first term the prior probability for  which takes account of everything the analyst may know or suspect about  before the data comes in this information plays no part in the samplingtheory approach indeed any attempt to include it would be considered bias away from what was pointed to purely by the data to the extent that bayesian calculations include prior information it is therefore essentially inevitable that their results will not be unbiased in sampling theory terms\u000abut the results of a bayesian approach can differ from the sampling theory approach even if the bayesian tries to adopt an uninformative prior\u000afor example consider again the estimation of an unknown population variance 2 of a normal distribution with unknown mean where it is desired to optimise c in the expected loss function\u000a\u000aa standard choice of uninformative prior for this problem is the jeffreys prior  which is equivalent to adopting a rescalinginvariant flat prior for ln 2\u000aone consequence of adopting this prior is that s22 remains a pivotal quantity ie the probability distribution of s22 depends only on s22 independent of the value of s2 or 2\u000a\u000ahowever whilst\u000a\u000ain contrast\u000a\u000a when the expectation is taken over the probability distribution of 2 given s2 as it is in the bayesian case rather than s2 given 2 one can no longer take 4 as a constant and factor it out the consequence of this is that compared to the samplingtheory calculation the bayesian calculation puts more weight on larger values of 2 properly taking into account as the samplingtheory calculation cannot that under this squaredloss function the consequence of underestimating large values of 2 is more costly in squaredloss terms than that of overestimating small values of 2\u000athe workedout bayesian calculation gives a scaled inverse chisquared distribution with n  1 degrees of freedom for the posterior probability distribution of 2 the expected loss is minimised when cns2  2 this occurs when c  1n  3\u000aeven with an uninformative prior therefore a bayesian calculation may not give the same expectedloss minimising result as the corresponding samplingtheory calculation\u000a\u000a\u000a see alsoedit \u000aomittedvariable bias\u000aconsistent estimator\u000aestimation theory\u000aexpected loss\u000aexpected value\u000aloss function\u000amedian\u000astatistical decision theory\u000aoptimism bias\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000abrown george w on smallsample estimation the annals of mathematical statistics vol 18 no 4 dec 1947 pp 582585 jstor 2236236\u000alehmann e l a general concept of unbiasedness the annals of mathematical statistics vol 22 no 4 dec 1951 pp 587592 jstor 2236928\u000aallan birnbaum 1961 a unified theory of estimation i the annals of mathematical statistics vol 32 no 1 mar 1961 pp 112135\u000avan der vaart h r 1961 some extensions of the idea of bias the annals of mathematical statistics vol 32 no 2 june 1961 pp 436447\u000apfanzagl johann 1994 parametric statistical theory walter de gruyter\u000astuart alan ord keith arnold steven f 2010 classical inference and the linear model kendalls advanced theory of statistics 2a wiley isbn 0470689242 \u000avoinov vassily g nikulin mikhail s 1993 unbiased estimators and their applications 1 univariate case dordrect kluwer academic publishers isbn 0792323823 \u000avoinov vassily g nikulin mikhail s 1996 unbiased estimators and their applications 2 multivariate case dordrect kluwer academic publishers isbn 0792339398 \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 unbiased estimator encyclopedia of mathematics springer isbn 9781556080104
p365
sg20
g23
sg24
g27
sg30
Vin statistics the bias or bias function of an estimator is the difference between this estimators expected value and the true value of the parameter being estimated an estimator or decision rule with zero bias is called unbiased otherwise the estimator is said to be biased in statistics bias is an objective statement about a function and while not a desired property it is not pejorative unlike the ordinary english use of the term bias\u000abias can also be measured with respect to the median rather than the mean expected value in which case one distinguishes medianunbiased from the usual meanunbiasedness property bias is related to consistency in that consistent estimators are convergent and asymptotically unbiased hence converge to the correct value though individual estimators in a consistent sequence may be biased so long as the bias converges to zero see bias versus consistency\u000aall else equal an unbiased estimator is preferable to a biased estimator but in practice all else is not equal and biased estimators are frequently used generally with small bias when a biased estimator is used the bias is also estimated a biased estimator may be used for various reasons because an unbiased estimator does not exist without further assumptions about a population or is difficult to compute as in unbiased estimation of standard deviation because an estimator is medianunbiased but not meanunbiased or the reverse because a biased estimator reduces some loss function particularly mean squared error compared with unbiased estimators notably in shrinkage estimators or because in some cases being unbiased is too strong a condition and the only unbiased estimators are not useful further meanunbiasedness is not preserved under nonlinear transformations though medianunbiasedness is see effect of transformations for example the sample variance is an unbiased estimator for the population variance but its square root the sample standard deviation is a biased estimator for the population standard deviation these are all illustrated below\u000a\u000a\u000a definitionedit \u000asuppose we have a statistical model parameterized by a real number  giving rise to a probability distribution for observed data  and a statistic  which serves as an estimator of  based on any observed data  that is we assume that our data follows some unknown distribution  where  is a fixed constant that is part of this distribution but is unknown and then we construct some estimator  that maps observed data to values that we hope are close to  then the bias of this estimator relative to the parameter  is defined to be\u000a\u000awhere  denotes expected value over the distribution  ie averaging over all possible observations  the second equation follows since  is measurable with respect to the conditional distribution \u000aan estimator is said to be unbiased if its bias is equal to zero for all values of parameter \u000athere are more general notions of bias and unbiasedness what this article calls bias is called meanbias to distinguish meanbias from the other notions with the notable ones being medianunbiased estimators for more details the general theory of unbiased estimators is briefly discussed near the end of this article\u000ain a simulation experiment concerning the properties of an estimator the bias of the estimator may be assessed using the mean signed difference\u000a\u000a\u000a examplesedit \u000a\u000a\u000a sample varianceedit \u000a\u000athe sample variance of a random variable demonstrates two aspects of estimator bias firstly the naive estimator is biased which can be corrected by a scale factor second the unbiased estimator is not optimal in terms of mean squared error mse which can be minimized by using a different scale factor resulting in a biased estimator with lower mse than the unbiased estimator concretely the naive estimator sums the squared deviations and divides by n which is biased dividing instead by n  1 yields an unbiased estimator conversely mse can be minimized by dividing by a different number depending on distribution but this results in a biased estimator this number is always larger than n  1 so this is known as a shrinkage estimator as it shrinks the unbiased estimator towards zero for the normal distribution the optimal value is n  1\u000asuppose x1  xn are independent and identically distributed iid random variables with expectation  and variance 2 if the sample mean and uncorrected sample variance are defined as\u000a\u000athen s2 is a biased estimator of 2 because\u000a\u000ain other words the expected value of the uncorrected sample variance does not equal the population variance 2 unless multiplied by a normalization factor the sample mean on the other hand is an unbiased estimator of the population mean \u000athe reason that s2 is biased stems from the fact that the sample mean is an ordinary least squares ols estimator for   is the number that makes the sum  as small as possible that is when any other number is plugged into this sum the sum can only increase in particular the choice  gives\u000a\u000aand then\u000a\u000anote that the usual definition of sample variance is\u000a\u000aand this is an unbiased estimator of the population variance this can be seen by noting the following formula which follows from the bienaym formula for the term in the inequality for the expectation of the uncorrected sample variance above\u000a\u000athe ratio between the biased uncorrected and unbiased estimates of the variance is known as bessels correction\u000a\u000a\u000a estimating a poisson probabilityedit \u000aa far more extreme case of a biased estimator being better than any unbiased estimator arises from the poisson distribution suppose that x has a poisson distribution with expectation  suppose it is desired to estimate\u000a\u000awith a sample of size 1 for example when incoming calls at a telephone switchboard are modeled as a poisson process and  is the average number of calls per minute then e2 is the probability that no calls arrive in the next two minutes\u000asince the expectation of an unbiased estimator x is equal to the estimand ie\u000a\u000athe only function of the data constituting an unbiased estimator is\u000a\u000ato see this note that when decomposing e from the above expression for expectation the sum that is left is a taylor series expansion of e as well yielding ee  e2 see characterizations of the exponential function\u000aif the observed value of x is 100 then the estimate is 1 although the true value of the quantity being estimated is very likely to be near 0 which is the opposite extreme and if x is observed to be 101 then the estimate is even more absurd it is 1 although the quantity being estimated must be positive\u000athe biased maximum likelihood estimator\u000a\u000ais far better than this unbiased estimator not only is its value always positive but it is also more accurate in the sense that its mean squared error\u000a\u000ais smaller compare the unbiased estimators mse of\u000a\u000athe mses are functions of the true value  the bias of the maximumlikelihood estimator is\u000a\u000a\u000a maximum of a discrete uniform distributionedit \u000a\u000athe bias of maximumlikelihood estimators can be substantial consider a case where n tickets numbered from 1 through to n are placed in a box and one is selected at random giving a value x if n is unknown then the maximumlikelihood estimator of n is x even though the expectation of x is only n  12 we can be certain only that n is at least x and is probably more in this case the natural unbiased estimator is 2x  1\u000a\u000a\u000a medianunbiased estimatorsedit \u000athe theory of medianunbiased estimators was revived by george w brown in 1947\u000a\u000aan estimate of a onedimensional parameter  will be said to be medianunbiased if for fixed  the median of the distribution of the estimate is at the value  ie the estimate underestimates just as often as it overestimates this requirement seems for most purposes to accomplish as much as the meanunbiased requirement and has the additional property that it is invariant under onetoone transformation\u000a\u000afurther properties of medianunbiased estimators have been noted by lehmann birnbaum van der vaart and pfanzagl in particular medianunbiased estimators exist in cases where meanunbiased and maximumlikelihood estimators do not exist besides being invariant under onetoone transformations medianunbiased estimators have surprising robustness unfortunately there is no analogue of raoblackwell theorem for medianunbiased estimation see the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a bias with respect to other loss functionsedit \u000aany minimumvariance meanunbiased estimator minimizes the risk expected loss with respect to the squarederror loss function among meanunbiased estimators as observed by gauss a minimumaverage absolute deviation medianunbiased estimator minimizes the risk with respect to the absolute loss function among medianunbiased estimators as observed by laplace other loss functions are used in statistical theory particularly in robust statistics connections between loss functions and unbiased estimation were studied in many works detailed description of corresponding results is given in chapter 3 of the book robust and nonrobust models in statistics by lev b klebanov svetlozar t rachev and frank j fabozzi nova scientific publishers inc new york 2009 and references there\u000a\u000a\u000a effect of transformationsedit \u000anote that when a transformation is applied to a meanunbiased estimator the result need not be a meanunbiased estimator of its corresponding population statistic by jensens inequality a convex function as transformation will introduce positive bias while a concave function will introduce negative bias and a function of mixed convexity may introduce bias in either direction depending on the specific function and distribution that is for a nonlinear function f and a meanunbiased estimator u of a parameter p the composite estimator fu need not be a meanunbiased estimator of fp for example the square root of the unbiased estimator of the population variance is not a meanunbiased estimator of the population standard deviation the square root of the unbiased sample variance the corrected sample standard deviation is biased the bias depends both on the sampling distribution of the estimator and on the transform and can be quite involved to calculate  see unbiased estimation of standard deviation for a discussion in this case\u000a\u000a\u000a bias variance and mean squared erroredit \u000a\u000awhile bias quantifies the average difference to be expected between an estimator and an underlying parameter an estimator based on a finite sample can additionally be expected to differ from the parameter due to the randomness in the sample\u000aone measure which is used to try to reflect both types of difference is the mean square error\u000a\u000athis can be shown to be equal to the square of the bias plus the variance\u000a\u000awhen the parameter is a vector an analogous decomposition applies\u000a\u000awhere\u000a\u000ais the trace of the covariance matrix of the estimator\u000aan estimator that minimises the bias will not necessarily minimise the mean square error\u000a\u000a\u000a example estimation of population varianceedit \u000afor example suppose an estimator of the form\u000a\u000ais sought for the population variance as above but this time to minimise the mse\u000a\u000aif the variables x1  xn follow a normal distribution then ns22 has a chisquared distribution with n  1 degrees of freedom giving\u000a\u000aand so\u000a\u000awith a little algebra it can be confirmed that it is c  1n  1 which minimises this combined loss function rather than c  1n  1 which minimises just the bias term\u000amore generally it is only in restricted classes of problems that there will be an estimator that minimises the mse independently of the parameter values\u000ahowever it is very common that there may be perceived to be a biasvariance tradeoff such that a small increase in bias can be traded for a larger decrease in variance resulting in a more desirable estimator overall\u000a\u000a\u000a bayesian viewedit \u000amost bayesians are rather unconcerned about unbiasedness at least in the formal samplingtheory sense above of their estimates for example gelman et al 1995 write from a bayesian perspective the principle of unbiasedness is reasonable in the limit of large samples but otherwise it is potentially misleading\u000afundamentally the difference between the bayesian approach and the samplingtheory approach above is that in the samplingtheory approach the parameter is taken as fixed and then probability distributions of a statistic are considered based on the predicted sampling distribution of the data for a bayesian however it is the data which is known and fixed and it is the unknown parameter for which an attempt is made to construct a probability distribution using bayes theorem\u000a\u000ahere the second term the likelihood of the data given the unknown parameter value  depends just on the data obtained and the modelling of the data generation process however a bayesian calculation also includes the first term the prior probability for  which takes account of everything the analyst may know or suspect about  before the data comes in this information plays no part in the samplingtheory approach indeed any attempt to include it would be considered bias away from what was pointed to purely by the data to the extent that bayesian calculations include prior information it is therefore essentially inevitable that their results will not be unbiased in sampling theory terms\u000abut the results of a bayesian approach can differ from the sampling theory approach even if the bayesian tries to adopt an uninformative prior\u000afor example consider again the estimation of an unknown population variance 2 of a normal distribution with unknown mean where it is desired to optimise c in the expected loss function\u000a\u000aa standard choice of uninformative prior for this problem is the jeffreys prior  which is equivalent to adopting a rescalinginvariant flat prior for ln 2\u000aone consequence of adopting this prior is that s22 remains a pivotal quantity ie the probability distribution of s22 depends only on s22 independent of the value of s2 or 2\u000a\u000ahowever whilst\u000a\u000ain contrast\u000a\u000a when the expectation is taken over the probability distribution of 2 given s2 as it is in the bayesian case rather than s2 given 2 one can no longer take 4 as a constant and factor it out the consequence of this is that compared to the samplingtheory calculation the bayesian calculation puts more weight on larger values of 2 properly taking into account as the samplingtheory calculation cannot that under this squaredloss function the consequence of underestimating large values of 2 is more costly in squaredloss terms than that of overestimating small values of 2\u000athe workedout bayesian calculation gives a scaled inverse chisquared distribution with n  1 degrees of freedom for the posterior probability distribution of 2 the expected loss is minimised when cns2  2 this occurs when c  1n  3\u000aeven with an uninformative prior therefore a bayesian calculation may not give the same expectedloss minimising result as the corresponding samplingtheory calculation\u000a\u000a\u000a see alsoedit \u000aomittedvariable bias\u000aconsistent estimator\u000aestimation theory\u000aexpected loss\u000aexpected value\u000aloss function\u000amedian\u000astatistical decision theory\u000aoptimism bias\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000abrown george w on smallsample estimation the annals of mathematical statistics vol 18 no 4 dec 1947 pp 582585 jstor 2236236\u000alehmann e l a general concept of unbiasedness the annals of mathematical statistics vol 22 no 4 dec 1951 pp 587592 jstor 2236928\u000aallan birnbaum 1961 a unified theory of estimation i the annals of mathematical statistics vol 32 no 1 mar 1961 pp 112135\u000avan der vaart h r 1961 some extensions of the idea of bias the annals of mathematical statistics vol 32 no 2 june 1961 pp 436447\u000apfanzagl johann 1994 parametric statistical theory walter de gruyter\u000astuart alan ord keith arnold steven f 2010 classical inference and the linear model kendalls advanced theory of statistics 2a wiley isbn 0470689242 \u000avoinov vassily g nikulin mikhail s 1993 unbiased estimators and their applications 1 univariate case dordrect kluwer academic publishers isbn 0792323823 \u000avoinov vassily g nikulin mikhail s 1996 unbiased estimators and their applications 2 multivariate case dordrect kluwer academic publishers isbn 0792339398 \u000a\u000a\u000a external linksedit \u000ahazewinkel michiel ed 2001 unbiased estimator encyclopedia of mathematics springer isbn 9781556080104
p366
sg32
g35
sg37
NsbsS'completeness_(statistics).txt'
p367
g2
(g3
g4
Ntp368
Rp369
(dp370
g8
g11
sg12
Vin statistics completeness is a property of a statistic in relation to a model for a set of observed data in essence it is a condition which ensures that the parameters of the probability distribution representing the model can all be estimated on the basis of the statistic it ensures that the distributions corresponding to different values of the parameters are distinct\u000ait is closely related to the idea of identifiability but in statistical theory it is often found as a condition imposed on a sufficient statistic from which certain optimality results are derived\u000a\u000a\u000a definition \u000aconsider a random variable x whose probability distribution belongs to a parametric family of probability distributions p parametrized by \u000aformally a statistic s is a measurable function of x thus a statistic s is evaluated on a random variable x taking the value sx which is itself a random variable a given realization of the random variable x is a datapoint datum on which the statistic s takes the value sx\u000athe statistic s is said to be complete for the distribution of x if for every measurable function g which must be independent of  the following implication holds\u000aegsx  0 for all  implies that pgsx  0  1 for all \u000athe statistic s is said to be boundedly complete if the implication holds for all bounded functions g\u000a\u000a\u000a example 1 bernoulli model \u000athe bernoulli model admits a complete statistic let x be a random sample of size n such that each xi has the same bernoulli distribution with parameter p let t be the number of 1s observed in the sample t is a statistic of x which has a binomial distribution with parameters np if the parameter space for p is 01 then t is a complete statistic to see this note that\u000a\u000aobserve also that neither p nor 1  p can be 0 hence  if and only if\u000a\u000aon denoting p1  p by r one gets\u000a\u000afirst observe that the range of r is the positive reals also egt is a polynomial in r and therefore can only be identical to 0 if all coefficients are 0 that is gt  0 for all t\u000ait is important to notice that the result that all coefficients must be 0 was obtained because of the range of r had the parameter space been finite and with a number of elements smaller than n it might be possible to solve the linear equations in gt obtained by substituting the values of r and get solutions different from 0 for example if n  1 and the parametric space is 05 a single observation t is not complete observe that with the definition\u000a\u000athen egt  0 although gt is not 0 for t  0 nor for t  1\u000a\u000a\u000a example 2 sum of normals \u000athis example will show that in a sample of size 2 from a normal distribution with known variance the statistic x1x2 is complete and sufficient suppose x1 x2 are independent identically distributed random variables normally distributed with expectation  and variance 1 the sum\u000a\u000ais a complete statistic for \u000ato show this it is sufficient to demonstrate that there is no nonzero function  such that the expectation of\u000a\u000aremains zero regardless of the value of \u000athat fact may be seen as follows the probability distribution of x1  x2 is normal with expectation 2 and variance 2 its probability density function in  is therefore proportional to\u000a\u000athe expectation of g above would therefore be a constant times\u000a\u000aa bit of algebra reduces this to\u000a\u000awhere k is nowhere zero and\u000a\u000aas a function of  this is a twosided laplace transform of hx and cannot be identically zero unless hx is zero almost everywhere the exponential is not zero so this can only happen if gx is zero almost everywhere\u000a\u000a\u000a relation to sufficient statistics \u000afor some parametric families a complete sufficient statistic does not exist also a minimal sufficient statistic need not exist a case in which there is no minimal sufficient statistic was shown by bahadur in 1957 under mild conditions a minimal sufficient statistic does always exist in particular these conditions always hold if the random variables associated with p  are all discrete or are all continuous\u000a\u000a\u000a importance of completeness \u000athe notion of completeness has many applications in statistics particularly in the following two theorems of mathematical statistics\u000a\u000a\u000a lehmannscheff theorem \u000acompleteness occurs in the lehmannscheff theorem which states that if a statistic that is unbiased complete and sufficient for some parameter  then it is the best meanunbiased estimator for  in other words this statistic has a smaller expected loss for any convex loss function in many practical applications with the squared lossfunction it has a smaller mean squared error among any estimators with the same expected value\u000asee also minimumvariance unbiased estimator\u000a\u000a\u000a basus theorem \u000abounded completeness occurs in basus theorem which states that a statistic which is both boundedly complete and sufficient is independent of any ancillary statistic\u000a\u000a\u000a bahadurs theorem \u000abounded completeness also occurs in bahadurs theorem if a statistic is sufficient and boundedly complete then it is minimal sufficient\u000a\u000a\u000a notes \u000a\u000a\u000a
p371
sg14
g17
sg18
Vin statistics completeness is a property of a statistic in relation to a model for a set of observed data in essence it is a condition which ensures that the parameters of the probability distribution representing the model can all be estimated on the basis of the statistic it ensures that the distributions corresponding to different values of the parameters are distinct\u000ait is closely related to the idea of identifiability but in statistical theory it is often found as a condition imposed on a sufficient statistic from which certain optimality results are derived\u000a\u000a\u000a definition \u000aconsider a random variable x whose probability distribution belongs to a parametric family of probability distributions p parametrized by \u000aformally a statistic s is a measurable function of x thus a statistic s is evaluated on a random variable x taking the value sx which is itself a random variable a given realization of the random variable x is a datapoint datum on which the statistic s takes the value sx\u000athe statistic s is said to be complete for the distribution of x if for every measurable function g which must be independent of  the following implication holds\u000aegsx  0 for all  implies that pgsx  0  1 for all \u000athe statistic s is said to be boundedly complete if the implication holds for all bounded functions g\u000a\u000a\u000a example 1 bernoulli model \u000athe bernoulli model admits a complete statistic let x be a random sample of size n such that each xi has the same bernoulli distribution with parameter p let t be the number of 1s observed in the sample t is a statistic of x which has a binomial distribution with parameters np if the parameter space for p is 01 then t is a complete statistic to see this note that\u000a\u000aobserve also that neither p nor 1  p can be 0 hence  if and only if\u000a\u000aon denoting p1  p by r one gets\u000a\u000afirst observe that the range of r is the positive reals also egt is a polynomial in r and therefore can only be identical to 0 if all coefficients are 0 that is gt  0 for all t\u000ait is important to notice that the result that all coefficients must be 0 was obtained because of the range of r had the parameter space been finite and with a number of elements smaller than n it might be possible to solve the linear equations in gt obtained by substituting the values of r and get solutions different from 0 for example if n  1 and the parametric space is 05 a single observation t is not complete observe that with the definition\u000a\u000athen egt  0 although gt is not 0 for t  0 nor for t  1\u000a\u000a\u000a example 2 sum of normals \u000athis example will show that in a sample of size 2 from a normal distribution with known variance the statistic x1x2 is complete and sufficient suppose x1 x2 are independent identically distributed random variables normally distributed with expectation  and variance 1 the sum\u000a\u000ais a complete statistic for \u000ato show this it is sufficient to demonstrate that there is no nonzero function  such that the expectation of\u000a\u000aremains zero regardless of the value of \u000athat fact may be seen as follows the probability distribution of x1  x2 is normal with expectation 2 and variance 2 its probability density function in  is therefore proportional to\u000a\u000athe expectation of g above would therefore be a constant times\u000a\u000aa bit of algebra reduces this to\u000a\u000awhere k is nowhere zero and\u000a\u000aas a function of  this is a twosided laplace transform of hx and cannot be identically zero unless hx is zero almost everywhere the exponential is not zero so this can only happen if gx is zero almost everywhere\u000a\u000a\u000a relation to sufficient statistics \u000afor some parametric families a complete sufficient statistic does not exist also a minimal sufficient statistic need not exist a case in which there is no minimal sufficient statistic was shown by bahadur in 1957 under mild conditions a minimal sufficient statistic does always exist in particular these conditions always hold if the random variables associated with p  are all discrete or are all continuous\u000a\u000a\u000a importance of completeness \u000athe notion of completeness has many applications in statistics particularly in the following two theorems of mathematical statistics\u000a\u000a\u000a lehmannscheff theorem \u000acompleteness occurs in the lehmannscheff theorem which states that if a statistic that is unbiased complete and sufficient for some parameter  then it is the best meanunbiased estimator for  in other words this statistic has a smaller expected loss for any convex loss function in many practical applications with the squared lossfunction it has a smaller mean squared error among any estimators with the same expected value\u000asee also minimumvariance unbiased estimator\u000a\u000a\u000a basus theorem \u000abounded completeness occurs in basus theorem which states that a statistic which is both boundedly complete and sufficient is independent of any ancillary statistic\u000a\u000a\u000a bahadurs theorem \u000abounded completeness also occurs in bahadurs theorem if a statistic is sufficient and boundedly complete then it is minimal sufficient\u000a\u000a\u000a notes
p372
sg20
g23
sg24
g27
sg30
Vin statistics completeness is a property of a statistic in relation to a model for a set of observed data in essence it is a condition which ensures that the parameters of the probability distribution representing the model can all be estimated on the basis of the statistic it ensures that the distributions corresponding to different values of the parameters are distinct\u000ait is closely related to the idea of identifiability but in statistical theory it is often found as a condition imposed on a sufficient statistic from which certain optimality results are derived\u000a\u000a\u000a definition \u000aconsider a random variable x whose probability distribution belongs to a parametric family of probability distributions p parametrized by \u000aformally a statistic s is a measurable function of x thus a statistic s is evaluated on a random variable x taking the value sx which is itself a random variable a given realization of the random variable x is a datapoint datum on which the statistic s takes the value sx\u000athe statistic s is said to be complete for the distribution of x if for every measurable function g which must be independent of  the following implication holds\u000aegsx  0 for all  implies that pgsx  0  1 for all \u000athe statistic s is said to be boundedly complete if the implication holds for all bounded functions g\u000a\u000a\u000a example 1 bernoulli model \u000athe bernoulli model admits a complete statistic let x be a random sample of size n such that each xi has the same bernoulli distribution with parameter p let t be the number of 1s observed in the sample t is a statistic of x which has a binomial distribution with parameters np if the parameter space for p is 01 then t is a complete statistic to see this note that\u000a\u000aobserve also that neither p nor 1  p can be 0 hence  if and only if\u000a\u000aon denoting p1  p by r one gets\u000a\u000afirst observe that the range of r is the positive reals also egt is a polynomial in r and therefore can only be identical to 0 if all coefficients are 0 that is gt  0 for all t\u000ait is important to notice that the result that all coefficients must be 0 was obtained because of the range of r had the parameter space been finite and with a number of elements smaller than n it might be possible to solve the linear equations in gt obtained by substituting the values of r and get solutions different from 0 for example if n  1 and the parametric space is 05 a single observation t is not complete observe that with the definition\u000a\u000athen egt  0 although gt is not 0 for t  0 nor for t  1\u000a\u000a\u000a example 2 sum of normals \u000athis example will show that in a sample of size 2 from a normal distribution with known variance the statistic x1x2 is complete and sufficient suppose x1 x2 are independent identically distributed random variables normally distributed with expectation  and variance 1 the sum\u000a\u000ais a complete statistic for \u000ato show this it is sufficient to demonstrate that there is no nonzero function  such that the expectation of\u000a\u000aremains zero regardless of the value of \u000athat fact may be seen as follows the probability distribution of x1  x2 is normal with expectation 2 and variance 2 its probability density function in  is therefore proportional to\u000a\u000athe expectation of g above would therefore be a constant times\u000a\u000aa bit of algebra reduces this to\u000a\u000awhere k is nowhere zero and\u000a\u000aas a function of  this is a twosided laplace transform of hx and cannot be identically zero unless hx is zero almost everywhere the exponential is not zero so this can only happen if gx is zero almost everywhere\u000a\u000a\u000a relation to sufficient statistics \u000afor some parametric families a complete sufficient statistic does not exist also a minimal sufficient statistic need not exist a case in which there is no minimal sufficient statistic was shown by bahadur in 1957 under mild conditions a minimal sufficient statistic does always exist in particular these conditions always hold if the random variables associated with p  are all discrete or are all continuous\u000a\u000a\u000a importance of completeness \u000athe notion of completeness has many applications in statistics particularly in the following two theorems of mathematical statistics\u000a\u000a\u000a lehmannscheff theorem \u000acompleteness occurs in the lehmannscheff theorem which states that if a statistic that is unbiased complete and sufficient for some parameter  then it is the best meanunbiased estimator for  in other words this statistic has a smaller expected loss for any convex loss function in many practical applications with the squared lossfunction it has a smaller mean squared error among any estimators with the same expected value\u000asee also minimumvariance unbiased estimator\u000a\u000a\u000a basus theorem \u000abounded completeness occurs in basus theorem which states that a statistic which is both boundedly complete and sufficient is independent of any ancillary statistic\u000a\u000a\u000a bahadurs theorem \u000abounded completeness also occurs in bahadurs theorem if a statistic is sufficient and boundedly complete then it is minimal sufficient\u000a\u000a\u000a notes \u000a\u000a\u000a
p373
sg32
g35
sg37
NsbsS'a_priori_probability.txt'
p374
g2
(g3
g4
Ntp375
Rp376
(dp377
g8
g11
sg12
Vthe term a priori probability is used in distinguishing the ways in which values for probabilities can be obtained in particular an a priori probability is derived purely by deductive reasoning one way of deriving a priori probabilities is the principle of indifference which has the character of saying that if there are n mutually exclusive and exhaustive events and if they are equally likely then the probability of a given event occurring is 1n similarly the probability of one of a given collection of k events is kn\u000aone disadvantage of defining probabilities in the above way is that it applies only to finite collections of events\u000ain bayesian inference the terms uninformative priors or objective priors refer to particular choices of a priori probabilities note that prior probability is a broader concept\u000asimilar to the distinction in philosophy between a priori and a posteriori in bayesian inference a priori denotes general knowledge about the data distribution before making an inference while a posteriori denotes knowledge that incorporates the results of making an inference\u000a\u000a\u000a
p378
sg14
g17
sg18
Vthe term a priori probability is used in distinguishing the ways in which values for probabilities can be obtained in particular an a priori probability is derived purely by deductive reasoning one way of deriving a priori probabilities is the principle of indifference which has the character of saying that if there are n mutually exclusive and exhaustive events and if they are equally likely then the probability of a given event occurring is 1n similarly the probability of one of a given collection of k events is kn\u000aone disadvantage of defining probabilities in the above way is that it applies only to finite collections of events\u000ain bayesian inference the terms uninformative priors or objective priors refer to particular choices of a priori probabilities note that prior probability is a broader concept\u000asimilar to the distinction in philosophy between a priori and a posteriori in bayesian inference a priori denotes general knowledge about the data distribution before making an inference while a posteriori denotes knowledge that incorporates the results of making an inference
p379
sg20
g23
sg24
g27
sg30
Vthe term a priori probability is used in distinguishing the ways in which values for probabilities can be obtained in particular an a priori probability is derived purely by deductive reasoning one way of deriving a priori probabilities is the principle of indifference which has the character of saying that if there are n mutually exclusive and exhaustive events and if they are equally likely then the probability of a given event occurring is 1n similarly the probability of one of a given collection of k events is kn\u000aone disadvantage of defining probabilities in the above way is that it applies only to finite collections of events\u000ain bayesian inference the terms uninformative priors or objective priors refer to particular choices of a priori probabilities note that prior probability is a broader concept\u000asimilar to the distinction in philosophy between a priori and a posteriori in bayesian inference a priori denotes general knowledge about the data distribution before making an inference while a posteriori denotes knowledge that incorporates the results of making an inference\u000a\u000a\u000a
p380
sg32
g35
sg37
NsbsS'uncertainty.txt'
p381
g2
(g3
g4
Ntp382
Rp383
(dp384
g8
g11
sg12
Vuncertainty is the situation which involves imperfect and  or unknown information in other words it is a term used in subtly different ways in a number of fields including insurance philosophy physics statistics economics finance psychology sociology engineering metrology and information science it applies to predictions of future events to physical measurements that are already made or to the unknown uncertainty arises in partially observable andor stochastic environments as well as due to ignorance andor indolence\u000a\u000a\u000a concepts \u000aalthough the terms are used in various ways among the general public many specialists in decision theory statistics and other quantitative fields have defined uncertainty risk and their measurement as\u000auncertainty the lack of certainty a state of having limited knowledge where it is impossible to exactly describe the existing state a future outcome or more than one possible outcome\u000ameasurement of uncertainty a set of possible states or outcomes where probabilities are assigned to each possible state or outcome  this also includes the application of a probability density function to continuous variable\u000arisk a state of uncertainty where some possible outcomes have an undesired effect or significant loss\u000ameasurement of risk a set of measured uncertainties where some possible outcomes are losses and the magnitudes of those losses  this also includes loss functions over continuous variables\u000aknightian uncertainty in his seminal work risk uncertainty and profit 1921 university of chicago economist frank knight established the important distinction between risk and uncertainty\u000athere are other taxonomies of uncertainties and decisions that include a broader sense of uncertainty and how it should be approached from an ethics perspective\u000a\u000afor example if it is unknown whether or not it will rain tomorrow then there is a state of uncertainty if probabilities are applied to the possible outcomes using weather forecasts or even just a calibrated probability assessment the uncertainty has been quantified suppose it is quantified as a 90 chance of sunshine if there is a major costly outdoor event planned for tomorrow then there is a risk since there is a 10 chance of rain and rain would be undesirable furthermore if this is a business event and 100000 would be lost if it rains then the risk has been quantifieda 10 chance of losing 100000 these situations can be made even more realistic by quantifying light rain vs heavy rain the cost of delays vs outright cancellation etc\u000asome may represent the risk in this example as the expected opportunity loss eol or the chance of the loss multiplied by the amount of the loss 10  100000  10000 that is useful if the organizer of the event is risk neutral which most people are not most would be willing to pay a premium to avoid the loss an insurance company for example would compute an eol as a minimum for any insurance coverage then add onto that other operating costs and profit since many people are willing to buy insurance for many reasons then clearly the eol alone is not the perceived value of avoiding the risk\u000aquantitative uses of the terms uncertainty and risk are fairly consistent from fields such as probability theory actuarial science and information theory some also create new terms without substantially changing the definitions of uncertainty or risk for example surprisal is a variation on uncertainty sometimes used in information theory but outside of the more mathematical uses of the term usage may vary widely in cognitive psychology uncertainty can be real or just a matter of perception such as expectations threats etc\u000avagueness or ambiguity are sometimes described as second order uncertainty where there is uncertainty even about the definitions of uncertain states or outcomes the difference here is that this uncertainty is about the human definitions and concepts not an objective fact of nature it has been argued that ambiguity however is always avoidable while uncertainty of the first order kind is not necessarily avoidable\u000auncertainty may be purely a consequence of a lack of knowledge of obtainable facts that is there may be uncertainty about whether a new rocket design will work but this uncertainty can be removed with further analysis and experimentation at the subatomic level however uncertainty may be a fundamental and unavoidable property of the universe in quantum mechanics the heisenberg uncertainty principle puts limits on how much an observer can ever know about the position and velocity of a particle this may not just be ignorance of potentially obtainable facts but that there is no fact to be found there is some controversy in physics as to whether such uncertainty is an irreducible property of nature or if there are hidden variables that would describe the state of a particle even more exactly than heisenbergs uncertainty principle allows\u000a\u000a\u000a measurements \u000a\u000ain metrology physics and engineering the uncertainty or margin of error of a measurement when explicitly stated is given by a range of values likely to enclose the true value this may be denoted by error bars on a graph or by the following notations\u000ameasured value  uncertainty\u000ameasured value uncertainty\u000auncertainty\u000ameasured valueuncertainty\u000ain the last notation parentheses are the concise notation for the  notation for example applying 10 12 meters in a scientific or engineering application it could be written 7001105000000000000105 m or 70011050000000000001050 m by convention meaning accurate to within one tenth of a meter or one hundredth the precision is symmetric around the last digit in this case its half a tenth up and half a tenth down so 105 means between 1045 and 1055 thus it is understood that 105 means 7001105000000000000105005 and 1050 means 700110500000000000010500005 also written 700110500000000000010505 and 700110500000000000010505 but if the accuracy is within two tenths the uncertainty is  one tenth and it is required to be explicit 700110500000000000010501 and 70011050000000000001050001 or 70011050000000000001051and 700110500000000000010501 the numbers in parenthesis apply to the numeral left of themselves and are not part of that number but part of a notation of uncertainty they apply to the least significant digits for instance 70001007940000000001007947 stands for 7000100794000000000100794000007 while 700010079400000000010079472 stands for 7000100794000000000100794000072 this concise notation is used for example by iupac in stating the atomic mass of elements\u000athe middle notation is used when the error is not symmetrical about the value  for example 70003400000000000003403\u000a02 this can occur when using a logarithmic scale for example\u000aoften the uncertainty of a measurement is found by repeating the measurement enough times to get a good estimate of the standard deviation of the values then any single value has an uncertainty equal to the standard deviation however if the values are averaged then the mean measurement value has a much smaller uncertainty equal to the standard error of the mean which is the standard deviation divided by the square root of the number of measurements this procedure neglects systematic errors however\u000awhen the uncertainty represents the standard error of the measurement then about 683 of the time the true value of the measured quantity falls within the stated uncertainty range for example it is likely that for 317 of the atomic mass values given on the list of elements by atomic mass the true value lies outside of the stated range if the width of the interval is doubled then probably only 46 of the true values lie outside the doubled interval and if the width is tripled probably only 03 lie outside these values follow from the properties of the normal distribution and they apply only if the measurement process produces normally distributed errors in that case the quoted standard errors are easily converted to 683 one sigma 954 two sigma or 997 three sigma confidence intervals\u000ain this context uncertainty depends on both the accuracy and precision of the measurement instrument the lower the accuracy and precision of an instrument the larger the measurement uncertainty is notice that precision is often determined as the standard deviation of the repeated measures of a given value namely using the same method described above to assess measurement uncertainty however this method is correct only when the instrument is accurate when it is inaccurate the uncertainty is larger than the standard deviation of the repeated measures and it appears evident that the uncertainty does not depend only on instrumental precision\u000a\u000a\u000a uncertainty and the media \u000auncertainty in science and science in general is often interpreted much differently in the public sphere than in the scientific community this is due in part to the diversity of the public audience and the tendency for scientists to misunderstand lay audiences and therefore not communicate ideas clearly and effectively one example is explained by the information deficit model also in the public realm there are often many scientific voices giving input on a single topic for example depending on how an issue is reported in the public sphere discrepancies between outcomes of multiple scientific studies due to methodological differences could be interpreted by the public as a lack of consensus in a situation where a consensus does in fact exist this interpretation may have even been intentionally promoted as scientific uncertainty may be managed to reach certain goals for example global warming contrarian activists took the advice of frank luntz to frame global warming as an issue of scientific uncertainty which was a precursor to the conflict frame used by journalists when reporting the issue\u000aindeterminacy can be loosely said to apply to situations in which not all the parameters of the system and their interactions are fully known whereas ignorance refers to situations in which it is not known what is not known these unknowns indeterminacy and ignorance that exist in science are often transformed into uncertainty when reported to the public in order to make issues more manageable since scientific indeterminacy and ignorance are difficult concepts for scientists to convey without losing credibility conversely uncertainty is often interpreted by the public as ignorance the transformation of indeterminacy and ignorance into uncertainty may be related to the publics misinterpretation of uncertainty as ignorance\u000ajournalists often either inflate uncertainty making the science seem more uncertain than it really is or downplay uncertainty making the science seem more certain than it really is one way that journalists inflate uncertainty is by describing new research that contradicts past research without providing context for the change other times journalists give scientists with minority views equal weight as scientists with majority views without adequately describing or explaining the state of scientific consensus on the issue in the same vein journalists often give nonscientists the same amount of attention and importance as scientists\u000ajournalists may downplay uncertainty by eliminating scientists carefully chosen tentative wording and by losing these caveats the information is skewed and presented as more certain and conclusive than it really is also stories with a single source or without any context of previous research mean that the subject at hand is presented as more definitive and certain than it is in reality there is often a product over process approach to science journalism that aids too in the downplaying of uncertainty finally and most notably for this investigation when science is framed by journalists as a triumphant quest uncertainty is erroneously framed as reducible and resolvable\u000asome media routines and organizational factors affect the overstatement of uncertainty other media routines and organizational factors help inflate the certainty of an issue because the general public in the united states generally trusts scientists when science stories are covered without alarmraising cues from special interest organizations religious groups environmental organization political factions etc they are often covered in a business related sense in an economicdevelopment frame or a social progress frame the nature of these frames is to downplay or eliminate uncertainty so when economic and scientific promise are focused on early in the issue cycle as has happened with coverage of plant biotechnology and nanotechnology in the united states the matter in question seems more definitive and certain\u000asometimes too stockholders owners or advertising will pressure a media organization to promote the business aspects of a scientific issue and therefore any uncertainty claims that may compromise the business interests are downplayed or eliminated\u000a\u000a\u000a applications \u000a\u000a\u000a see also \u000a\u000a\u000a
p385
sg14
g17
sg18
Vuncertainty is the situation which involves imperfect and  or unknown information in other words it is a term used in subtly different ways in a number of fields including insurance philosophy physics statistics economics finance psychology sociology engineering metrology and information science it applies to predictions of future events to physical measurements that are already made or to the unknown uncertainty arises in partially observable andor stochastic environments as well as due to ignorance andor indolence\u000a\u000a\u000a concepts \u000aalthough the terms are used in various ways among the general public many specialists in decision theory statistics and other quantitative fields have defined uncertainty risk and their measurement as\u000auncertainty the lack of certainty a state of having limited knowledge where it is impossible to exactly describe the existing state a future outcome or more than one possible outcome\u000ameasurement of uncertainty a set of possible states or outcomes where probabilities are assigned to each possible state or outcome  this also includes the application of a probability density function to continuous variable\u000arisk a state of uncertainty where some possible outcomes have an undesired effect or significant loss\u000ameasurement of risk a set of measured uncertainties where some possible outcomes are losses and the magnitudes of those losses  this also includes loss functions over continuous variables\u000aknightian uncertainty in his seminal work risk uncertainty and profit 1921 university of chicago economist frank knight established the important distinction between risk and uncertainty\u000athere are other taxonomies of uncertainties and decisions that include a broader sense of uncertainty and how it should be approached from an ethics perspective\u000a\u000afor example if it is unknown whether or not it will rain tomorrow then there is a state of uncertainty if probabilities are applied to the possible outcomes using weather forecasts or even just a calibrated probability assessment the uncertainty has been quantified suppose it is quantified as a 90 chance of sunshine if there is a major costly outdoor event planned for tomorrow then there is a risk since there is a 10 chance of rain and rain would be undesirable furthermore if this is a business event and 100000 would be lost if it rains then the risk has been quantifieda 10 chance of losing 100000 these situations can be made even more realistic by quantifying light rain vs heavy rain the cost of delays vs outright cancellation etc\u000asome may represent the risk in this example as the expected opportunity loss eol or the chance of the loss multiplied by the amount of the loss 10  100000  10000 that is useful if the organizer of the event is risk neutral which most people are not most would be willing to pay a premium to avoid the loss an insurance company for example would compute an eol as a minimum for any insurance coverage then add onto that other operating costs and profit since many people are willing to buy insurance for many reasons then clearly the eol alone is not the perceived value of avoiding the risk\u000aquantitative uses of the terms uncertainty and risk are fairly consistent from fields such as probability theory actuarial science and information theory some also create new terms without substantially changing the definitions of uncertainty or risk for example surprisal is a variation on uncertainty sometimes used in information theory but outside of the more mathematical uses of the term usage may vary widely in cognitive psychology uncertainty can be real or just a matter of perception such as expectations threats etc\u000avagueness or ambiguity are sometimes described as second order uncertainty where there is uncertainty even about the definitions of uncertain states or outcomes the difference here is that this uncertainty is about the human definitions and concepts not an objective fact of nature it has been argued that ambiguity however is always avoidable while uncertainty of the first order kind is not necessarily avoidable\u000auncertainty may be purely a consequence of a lack of knowledge of obtainable facts that is there may be uncertainty about whether a new rocket design will work but this uncertainty can be removed with further analysis and experimentation at the subatomic level however uncertainty may be a fundamental and unavoidable property of the universe in quantum mechanics the heisenberg uncertainty principle puts limits on how much an observer can ever know about the position and velocity of a particle this may not just be ignorance of potentially obtainable facts but that there is no fact to be found there is some controversy in physics as to whether such uncertainty is an irreducible property of nature or if there are hidden variables that would describe the state of a particle even more exactly than heisenbergs uncertainty principle allows\u000a\u000a\u000a measurements \u000a\u000ain metrology physics and engineering the uncertainty or margin of error of a measurement when explicitly stated is given by a range of values likely to enclose the true value this may be denoted by error bars on a graph or by the following notations\u000ameasured value  uncertainty\u000ameasured value uncertainty\u000auncertainty\u000ameasured valueuncertainty\u000ain the last notation parentheses are the concise notation for the  notation for example applying 10 12 meters in a scientific or engineering application it could be written 7001105000000000000105 m or 70011050000000000001050 m by convention meaning accurate to within one tenth of a meter or one hundredth the precision is symmetric around the last digit in this case its half a tenth up and half a tenth down so 105 means between 1045 and 1055 thus it is understood that 105 means 7001105000000000000105005 and 1050 means 700110500000000000010500005 also written 700110500000000000010505 and 700110500000000000010505 but if the accuracy is within two tenths the uncertainty is  one tenth and it is required to be explicit 700110500000000000010501 and 70011050000000000001050001 or 70011050000000000001051and 700110500000000000010501 the numbers in parenthesis apply to the numeral left of themselves and are not part of that number but part of a notation of uncertainty they apply to the least significant digits for instance 70001007940000000001007947 stands for 7000100794000000000100794000007 while 700010079400000000010079472 stands for 7000100794000000000100794000072 this concise notation is used for example by iupac in stating the atomic mass of elements\u000athe middle notation is used when the error is not symmetrical about the value  for example 70003400000000000003403\u000a02 this can occur when using a logarithmic scale for example\u000aoften the uncertainty of a measurement is found by repeating the measurement enough times to get a good estimate of the standard deviation of the values then any single value has an uncertainty equal to the standard deviation however if the values are averaged then the mean measurement value has a much smaller uncertainty equal to the standard error of the mean which is the standard deviation divided by the square root of the number of measurements this procedure neglects systematic errors however\u000awhen the uncertainty represents the standard error of the measurement then about 683 of the time the true value of the measured quantity falls within the stated uncertainty range for example it is likely that for 317 of the atomic mass values given on the list of elements by atomic mass the true value lies outside of the stated range if the width of the interval is doubled then probably only 46 of the true values lie outside the doubled interval and if the width is tripled probably only 03 lie outside these values follow from the properties of the normal distribution and they apply only if the measurement process produces normally distributed errors in that case the quoted standard errors are easily converted to 683 one sigma 954 two sigma or 997 three sigma confidence intervals\u000ain this context uncertainty depends on both the accuracy and precision of the measurement instrument the lower the accuracy and precision of an instrument the larger the measurement uncertainty is notice that precision is often determined as the standard deviation of the repeated measures of a given value namely using the same method described above to assess measurement uncertainty however this method is correct only when the instrument is accurate when it is inaccurate the uncertainty is larger than the standard deviation of the repeated measures and it appears evident that the uncertainty does not depend only on instrumental precision\u000a\u000a\u000a uncertainty and the media \u000auncertainty in science and science in general is often interpreted much differently in the public sphere than in the scientific community this is due in part to the diversity of the public audience and the tendency for scientists to misunderstand lay audiences and therefore not communicate ideas clearly and effectively one example is explained by the information deficit model also in the public realm there are often many scientific voices giving input on a single topic for example depending on how an issue is reported in the public sphere discrepancies between outcomes of multiple scientific studies due to methodological differences could be interpreted by the public as a lack of consensus in a situation where a consensus does in fact exist this interpretation may have even been intentionally promoted as scientific uncertainty may be managed to reach certain goals for example global warming contrarian activists took the advice of frank luntz to frame global warming as an issue of scientific uncertainty which was a precursor to the conflict frame used by journalists when reporting the issue\u000aindeterminacy can be loosely said to apply to situations in which not all the parameters of the system and their interactions are fully known whereas ignorance refers to situations in which it is not known what is not known these unknowns indeterminacy and ignorance that exist in science are often transformed into uncertainty when reported to the public in order to make issues more manageable since scientific indeterminacy and ignorance are difficult concepts for scientists to convey without losing credibility conversely uncertainty is often interpreted by the public as ignorance the transformation of indeterminacy and ignorance into uncertainty may be related to the publics misinterpretation of uncertainty as ignorance\u000ajournalists often either inflate uncertainty making the science seem more uncertain than it really is or downplay uncertainty making the science seem more certain than it really is one way that journalists inflate uncertainty is by describing new research that contradicts past research without providing context for the change other times journalists give scientists with minority views equal weight as scientists with majority views without adequately describing or explaining the state of scientific consensus on the issue in the same vein journalists often give nonscientists the same amount of attention and importance as scientists\u000ajournalists may downplay uncertainty by eliminating scientists carefully chosen tentative wording and by losing these caveats the information is skewed and presented as more certain and conclusive than it really is also stories with a single source or without any context of previous research mean that the subject at hand is presented as more definitive and certain than it is in reality there is often a product over process approach to science journalism that aids too in the downplaying of uncertainty finally and most notably for this investigation when science is framed by journalists as a triumphant quest uncertainty is erroneously framed as reducible and resolvable\u000asome media routines and organizational factors affect the overstatement of uncertainty other media routines and organizational factors help inflate the certainty of an issue because the general public in the united states generally trusts scientists when science stories are covered without alarmraising cues from special interest organizations religious groups environmental organization political factions etc they are often covered in a business related sense in an economicdevelopment frame or a social progress frame the nature of these frames is to downplay or eliminate uncertainty so when economic and scientific promise are focused on early in the issue cycle as has happened with coverage of plant biotechnology and nanotechnology in the united states the matter in question seems more definitive and certain\u000asometimes too stockholders owners or advertising will pressure a media organization to promote the business aspects of a scientific issue and therefore any uncertainty claims that may compromise the business interests are downplayed or eliminated\u000a\u000a\u000a applications \u000a\u000a\u000a see also
p386
sg20
g23
sg24
g27
sg30
Vuncertainty is the situation which involves imperfect and  or unknown information in other words it is a term used in subtly different ways in a number of fields including insurance philosophy physics statistics economics finance psychology sociology engineering metrology and information science it applies to predictions of future events to physical measurements that are already made or to the unknown uncertainty arises in partially observable andor stochastic environments as well as due to ignorance andor indolence\u000a\u000a\u000a concepts \u000aalthough the terms are used in various ways among the general public many specialists in decision theory statistics and other quantitative fields have defined uncertainty risk and their measurement as\u000auncertainty the lack of certainty a state of having limited knowledge where it is impossible to exactly describe the existing state a future outcome or more than one possible outcome\u000ameasurement of uncertainty a set of possible states or outcomes where probabilities are assigned to each possible state or outcome  this also includes the application of a probability density function to continuous variable\u000arisk a state of uncertainty where some possible outcomes have an undesired effect or significant loss\u000ameasurement of risk a set of measured uncertainties where some possible outcomes are losses and the magnitudes of those losses  this also includes loss functions over continuous variables\u000aknightian uncertainty in his seminal work risk uncertainty and profit 1921 university of chicago economist frank knight established the important distinction between risk and uncertainty\u000athere are other taxonomies of uncertainties and decisions that include a broader sense of uncertainty and how it should be approached from an ethics perspective\u000a\u000afor example if it is unknown whether or not it will rain tomorrow then there is a state of uncertainty if probabilities are applied to the possible outcomes using weather forecasts or even just a calibrated probability assessment the uncertainty has been quantified suppose it is quantified as a 90 chance of sunshine if there is a major costly outdoor event planned for tomorrow then there is a risk since there is a 10 chance of rain and rain would be undesirable furthermore if this is a business event and 100000 would be lost if it rains then the risk has been quantifieda 10 chance of losing 100000 these situations can be made even more realistic by quantifying light rain vs heavy rain the cost of delays vs outright cancellation etc\u000asome may represent the risk in this example as the expected opportunity loss eol or the chance of the loss multiplied by the amount of the loss 10  100000  10000 that is useful if the organizer of the event is risk neutral which most people are not most would be willing to pay a premium to avoid the loss an insurance company for example would compute an eol as a minimum for any insurance coverage then add onto that other operating costs and profit since many people are willing to buy insurance for many reasons then clearly the eol alone is not the perceived value of avoiding the risk\u000aquantitative uses of the terms uncertainty and risk are fairly consistent from fields such as probability theory actuarial science and information theory some also create new terms without substantially changing the definitions of uncertainty or risk for example surprisal is a variation on uncertainty sometimes used in information theory but outside of the more mathematical uses of the term usage may vary widely in cognitive psychology uncertainty can be real or just a matter of perception such as expectations threats etc\u000avagueness or ambiguity are sometimes described as second order uncertainty where there is uncertainty even about the definitions of uncertain states or outcomes the difference here is that this uncertainty is about the human definitions and concepts not an objective fact of nature it has been argued that ambiguity however is always avoidable while uncertainty of the first order kind is not necessarily avoidable\u000auncertainty may be purely a consequence of a lack of knowledge of obtainable facts that is there may be uncertainty about whether a new rocket design will work but this uncertainty can be removed with further analysis and experimentation at the subatomic level however uncertainty may be a fundamental and unavoidable property of the universe in quantum mechanics the heisenberg uncertainty principle puts limits on how much an observer can ever know about the position and velocity of a particle this may not just be ignorance of potentially obtainable facts but that there is no fact to be found there is some controversy in physics as to whether such uncertainty is an irreducible property of nature or if there are hidden variables that would describe the state of a particle even more exactly than heisenbergs uncertainty principle allows\u000a\u000a\u000a measurements \u000a\u000ain metrology physics and engineering the uncertainty or margin of error of a measurement when explicitly stated is given by a range of values likely to enclose the true value this may be denoted by error bars on a graph or by the following notations\u000ameasured value  uncertainty\u000ameasured value uncertainty\u000auncertainty\u000ameasured valueuncertainty\u000ain the last notation parentheses are the concise notation for the  notation for example applying 10 12 meters in a scientific or engineering application it could be written 7001105000000000000105 m or 70011050000000000001050 m by convention meaning accurate to within one tenth of a meter or one hundredth the precision is symmetric around the last digit in this case its half a tenth up and half a tenth down so 105 means between 1045 and 1055 thus it is understood that 105 means 7001105000000000000105005 and 1050 means 700110500000000000010500005 also written 700110500000000000010505 and 700110500000000000010505 but if the accuracy is within two tenths the uncertainty is  one tenth and it is required to be explicit 700110500000000000010501 and 70011050000000000001050001 or 70011050000000000001051and 700110500000000000010501 the numbers in parenthesis apply to the numeral left of themselves and are not part of that number but part of a notation of uncertainty they apply to the least significant digits for instance 70001007940000000001007947 stands for 7000100794000000000100794000007 while 700010079400000000010079472 stands for 7000100794000000000100794000072 this concise notation is used for example by iupac in stating the atomic mass of elements\u000athe middle notation is used when the error is not symmetrical about the value  for example 70003400000000000003403\u000a02 this can occur when using a logarithmic scale for example\u000aoften the uncertainty of a measurement is found by repeating the measurement enough times to get a good estimate of the standard deviation of the values then any single value has an uncertainty equal to the standard deviation however if the values are averaged then the mean measurement value has a much smaller uncertainty equal to the standard error of the mean which is the standard deviation divided by the square root of the number of measurements this procedure neglects systematic errors however\u000awhen the uncertainty represents the standard error of the measurement then about 683 of the time the true value of the measured quantity falls within the stated uncertainty range for example it is likely that for 317 of the atomic mass values given on the list of elements by atomic mass the true value lies outside of the stated range if the width of the interval is doubled then probably only 46 of the true values lie outside the doubled interval and if the width is tripled probably only 03 lie outside these values follow from the properties of the normal distribution and they apply only if the measurement process produces normally distributed errors in that case the quoted standard errors are easily converted to 683 one sigma 954 two sigma or 997 three sigma confidence intervals\u000ain this context uncertainty depends on both the accuracy and precision of the measurement instrument the lower the accuracy and precision of an instrument the larger the measurement uncertainty is notice that precision is often determined as the standard deviation of the repeated measures of a given value namely using the same method described above to assess measurement uncertainty however this method is correct only when the instrument is accurate when it is inaccurate the uncertainty is larger than the standard deviation of the repeated measures and it appears evident that the uncertainty does not depend only on instrumental precision\u000a\u000a\u000a uncertainty and the media \u000auncertainty in science and science in general is often interpreted much differently in the public sphere than in the scientific community this is due in part to the diversity of the public audience and the tendency for scientists to misunderstand lay audiences and therefore not communicate ideas clearly and effectively one example is explained by the information deficit model also in the public realm there are often many scientific voices giving input on a single topic for example depending on how an issue is reported in the public sphere discrepancies between outcomes of multiple scientific studies due to methodological differences could be interpreted by the public as a lack of consensus in a situation where a consensus does in fact exist this interpretation may have even been intentionally promoted as scientific uncertainty may be managed to reach certain goals for example global warming contrarian activists took the advice of frank luntz to frame global warming as an issue of scientific uncertainty which was a precursor to the conflict frame used by journalists when reporting the issue\u000aindeterminacy can be loosely said to apply to situations in which not all the parameters of the system and their interactions are fully known whereas ignorance refers to situations in which it is not known what is not known these unknowns indeterminacy and ignorance that exist in science are often transformed into uncertainty when reported to the public in order to make issues more manageable since scientific indeterminacy and ignorance are difficult concepts for scientists to convey without losing credibility conversely uncertainty is often interpreted by the public as ignorance the transformation of indeterminacy and ignorance into uncertainty may be related to the publics misinterpretation of uncertainty as ignorance\u000ajournalists often either inflate uncertainty making the science seem more uncertain than it really is or downplay uncertainty making the science seem more certain than it really is one way that journalists inflate uncertainty is by describing new research that contradicts past research without providing context for the change other times journalists give scientists with minority views equal weight as scientists with majority views without adequately describing or explaining the state of scientific consensus on the issue in the same vein journalists often give nonscientists the same amount of attention and importance as scientists\u000ajournalists may downplay uncertainty by eliminating scientists carefully chosen tentative wording and by losing these caveats the information is skewed and presented as more certain and conclusive than it really is also stories with a single source or without any context of previous research mean that the subject at hand is presented as more definitive and certain than it is in reality there is often a product over process approach to science journalism that aids too in the downplaying of uncertainty finally and most notably for this investigation when science is framed by journalists as a triumphant quest uncertainty is erroneously framed as reducible and resolvable\u000asome media routines and organizational factors affect the overstatement of uncertainty other media routines and organizational factors help inflate the certainty of an issue because the general public in the united states generally trusts scientists when science stories are covered without alarmraising cues from special interest organizations religious groups environmental organization political factions etc they are often covered in a business related sense in an economicdevelopment frame or a social progress frame the nature of these frames is to downplay or eliminate uncertainty so when economic and scientific promise are focused on early in the issue cycle as has happened with coverage of plant biotechnology and nanotechnology in the united states the matter in question seems more definitive and certain\u000asometimes too stockholders owners or advertising will pressure a media organization to promote the business aspects of a scientific issue and therefore any uncertainty claims that may compromise the business interests are downplayed or eliminated\u000a\u000a\u000a applications \u000a\u000a\u000a see also \u000a\u000a\u000a
p387
sg32
g35
sg37
NsbsS'ancillary_statistic.txt'
p388
g2
(g3
g4
Ntp389
Rp390
(dp391
g8
g11
sg12
Vin statistics an ancillary statistic is a statistic whose sampling distribution does not depend on the parameters of the model an ancillary statistic is a pivotal quantity that is also a statistic ancillary statistics can be used to construct prediction intervals\u000athis concept was introduced by the statistical geneticist sir ronald fisher\u000a\u000a\u000a example \u000asuppose x1  xn are independent and identically distributed and are normally distributed with unknown expected value  and known variance 1 let\u000a\u000abe the sample mean\u000athe following statistical measures of dispersion of the sample\u000arange maxx1  xn  minx1  xn\u000ainterquartile range q3  q1\u000asample variance\u000a\u000aare all ancillary statistics because their sampling distributions do not change as  changes computationally this is because in the formulas the  terms cancel  adding a constant number to a distribution and all samples changes its sample maximum and minimum by the same amount so it does not change their difference and likewise for others these measures of dispersion do not depend on location\u000aconversely given iid normal variables with known mean 1 and unknown variance 2 the sample mean  is not an ancillary statistic of the variance as the sampling distribution of the sample mean is n1 2n which does depend on  2  this measure of location specifically its standard error depends on dispersion\u000a\u000a\u000a ancillary complement \u000agiven a statistic t that is not sufficient an ancillary complement is a statistic u that is ancillary and such that t u is sufficient intuitively an ancillary complement adds the missing information without duplicating any\u000athe statistic is particularly useful if one takes t to be a maximum likelihood estimator which in general will not be sufficient then one can ask for an ancillary complement in this case fisher argues that one must condition on an ancillary complement to determine information content one should consider the fisher information content of t to not be the marginal of t but the conditional distribution of t given u how much information does t add this is not possible in general as no ancillary complement need exist and if one exists it need not be unique nor does a maximum ancillary complement exist\u000a\u000a\u000a example \u000ain baseball suppose a scout observes a batter in n atbats suppose unrealistically that the number n is chosen by some random process that is independent of the batters ability  say a coin is tossed after each atbat and the result determines whether the scout will stay to watch the batters next atbat the eventual data are the number n of atbats and the number x of hits the data x n are a sufficient statistic the observed batting average xn fails to convey all of the information available in the data because it fails to report the number n of atbats eg a batting average of 040 which is very high based on only five atbats does not inspire anywhere near as much confidence in the players ability than a 040 average based on 100 atbats the number n of atbats is an ancillary statistic because\u000ait is a part of the observable data it is a statistic and\u000aits probability distribution does not depend on the batters ability since it was chosen by a random process independent of the batters ability\u000athis ancillary statistic is an ancillary complement to the observed batting average xn ie the batting average xn is not a sufficient statistic in that it conveys less than all of the relevant information in the data but conjoined with n it becomes sufficient\u000a\u000a\u000a see also \u000abasus theorem\u000aprediction interval\u000agroup family\u000aconditionality principle\u000a\u000a\u000a notes 
p392
sg14
g17
sg18
Vin statistics an ancillary statistic is a statistic whose sampling distribution does not depend on the parameters of the model an ancillary statistic is a pivotal quantity that is also a statistic ancillary statistics can be used to construct prediction intervals\u000athis concept was introduced by the statistical geneticist sir ronald fisher\u000a\u000a\u000a example \u000asuppose x1  xn are independent and identically distributed and are normally distributed with unknown expected value  and known variance 1 let\u000a\u000abe the sample mean\u000athe following statistical measures of dispersion of the sample\u000arange maxx1  xn  minx1  xn\u000ainterquartile range q3  q1\u000asample variance\u000a\u000aare all ancillary statistics because their sampling distributions do not change as  changes computationally this is because in the formulas the  terms cancel  adding a constant number to a distribution and all samples changes its sample maximum and minimum by the same amount so it does not change their difference and likewise for others these measures of dispersion do not depend on location\u000aconversely given iid normal variables with known mean 1 and unknown variance 2 the sample mean  is not an ancillary statistic of the variance as the sampling distribution of the sample mean is n1 2n which does depend on  2  this measure of location specifically its standard error depends on dispersion\u000a\u000a\u000a ancillary complement \u000agiven a statistic t that is not sufficient an ancillary complement is a statistic u that is ancillary and such that t u is sufficient intuitively an ancillary complement adds the missing information without duplicating any\u000athe statistic is particularly useful if one takes t to be a maximum likelihood estimator which in general will not be sufficient then one can ask for an ancillary complement in this case fisher argues that one must condition on an ancillary complement to determine information content one should consider the fisher information content of t to not be the marginal of t but the conditional distribution of t given u how much information does t add this is not possible in general as no ancillary complement need exist and if one exists it need not be unique nor does a maximum ancillary complement exist\u000a\u000a\u000a example \u000ain baseball suppose a scout observes a batter in n atbats suppose unrealistically that the number n is chosen by some random process that is independent of the batters ability  say a coin is tossed after each atbat and the result determines whether the scout will stay to watch the batters next atbat the eventual data are the number n of atbats and the number x of hits the data x n are a sufficient statistic the observed batting average xn fails to convey all of the information available in the data because it fails to report the number n of atbats eg a batting average of 040 which is very high based on only five atbats does not inspire anywhere near as much confidence in the players ability than a 040 average based on 100 atbats the number n of atbats is an ancillary statistic because\u000ait is a part of the observable data it is a statistic and\u000aits probability distribution does not depend on the batters ability since it was chosen by a random process independent of the batters ability\u000athis ancillary statistic is an ancillary complement to the observed batting average xn ie the batting average xn is not a sufficient statistic in that it conveys less than all of the relevant information in the data but conjoined with n it becomes sufficient\u000a\u000a\u000a see also \u000abasus theorem\u000aprediction interval\u000agroup family\u000aconditionality principle\u000a\u000a\u000a notes
p393
sg20
g23
sg24
g27
sg30
Vin statistics an ancillary statistic is a statistic whose sampling distribution does not depend on the parameters of the model an ancillary statistic is a pivotal quantity that is also a statistic ancillary statistics can be used to construct prediction intervals\u000athis concept was introduced by the statistical geneticist sir ronald fisher\u000a\u000a\u000a example \u000asuppose x1  xn are independent and identically distributed and are normally distributed with unknown expected value  and known variance 1 let\u000a\u000abe the sample mean\u000athe following statistical measures of dispersion of the sample\u000arange maxx1  xn  minx1  xn\u000ainterquartile range q3  q1\u000asample variance\u000a\u000aare all ancillary statistics because their sampling distributions do not change as  changes computationally this is because in the formulas the  terms cancel  adding a constant number to a distribution and all samples changes its sample maximum and minimum by the same amount so it does not change their difference and likewise for others these measures of dispersion do not depend on location\u000aconversely given iid normal variables with known mean 1 and unknown variance 2 the sample mean  is not an ancillary statistic of the variance as the sampling distribution of the sample mean is n1 2n which does depend on  2  this measure of location specifically its standard error depends on dispersion\u000a\u000a\u000a ancillary complement \u000agiven a statistic t that is not sufficient an ancillary complement is a statistic u that is ancillary and such that t u is sufficient intuitively an ancillary complement adds the missing information without duplicating any\u000athe statistic is particularly useful if one takes t to be a maximum likelihood estimator which in general will not be sufficient then one can ask for an ancillary complement in this case fisher argues that one must condition on an ancillary complement to determine information content one should consider the fisher information content of t to not be the marginal of t but the conditional distribution of t given u how much information does t add this is not possible in general as no ancillary complement need exist and if one exists it need not be unique nor does a maximum ancillary complement exist\u000a\u000a\u000a example \u000ain baseball suppose a scout observes a batter in n atbats suppose unrealistically that the number n is chosen by some random process that is independent of the batters ability  say a coin is tossed after each atbat and the result determines whether the scout will stay to watch the batters next atbat the eventual data are the number n of atbats and the number x of hits the data x n are a sufficient statistic the observed batting average xn fails to convey all of the information available in the data because it fails to report the number n of atbats eg a batting average of 040 which is very high based on only five atbats does not inspire anywhere near as much confidence in the players ability than a 040 average based on 100 atbats the number n of atbats is an ancillary statistic because\u000ait is a part of the observable data it is a statistic and\u000aits probability distribution does not depend on the batters ability since it was chosen by a random process independent of the batters ability\u000athis ancillary statistic is an ancillary complement to the observed batting average xn ie the batting average xn is not a sufficient statistic in that it conveys less than all of the relevant information in the data but conjoined with n it becomes sufficient\u000a\u000a\u000a see also \u000abasus theorem\u000aprediction interval\u000agroup family\u000aconditionality principle\u000a\u000a\u000a notes 
p394
sg32
g35
sg37
NsbsS'errors_and_residuals.txt'
p395
g2
(g3
g4
Ntp396
Rp397
(dp398
g8
g11
sg12
Vin statistics and optimization errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its theoretical value the error or disturbance of an observed value is the deviation of the observed value from the unobservable true value of a quantity of interest for example a population mean and the residual of an observed value is the difference between the observed value and the estimated value of the quantity of interest for example a sample mean the distinction is most important in regression analysis where it leads to the concept of studentized residuals\u000a\u000a\u000a introductionedit \u000asuppose there is a series of observations from a univariate distribution and we want to estimate the mean of that distribution the socalled location model in this case the errors are the deviations of the observations from the population mean while the residuals are the deviations of the observations from the sample mean\u000aa statistical error or disturbance is the amount by which an observation differs from its expected value the latter being based on the whole population from which the statistical unit was chosen randomly for example if the mean height in a population of 21yearold men is 175 meters and one randomly chosen man is 180 meters tall then the error is 005 meters if the randomly chosen man is 170 meters tall then the error is 005 meters the expected value being the mean of the entire population is typically unobservable and hence the statistical error cannot be observed either\u000aa residual or fitting deviation on the other hand is an observable estimate of the unobservable statistical error consider the previous example with mens heights and suppose we have a random sample of n people the sample mean could serve as a good estimator of the population mean then we have\u000athe difference between the height of each man in the sample and the unobservable population mean is a statistical error whereas\u000athe difference between the height of each man in the sample and the observable sample mean is a residual\u000anote that the sum of the residuals within a random sample is necessarily zero and thus the residuals are necessarily not independent the statistical errors on the other hand are independent and their sum within the random sample is almost surely not zero\u000aone can standardize statistical errors especially of a normal distribution in a zscore or standard score and standardize residuals in a tstatistic or more generally studentized residuals\u000a\u000a\u000a in univariate distributionsedit \u000aif we assume a normally distributed population with mean  and standard deviation  and choose individuals independently then we have\u000a\u000aand the sample mean\u000a\u000ais a random variable distributed thus\u000a\u000athe statistical errors are then\u000a\u000awhereas the residuals are\u000a\u000athe sum of squares of the statistical errors divided by 2 has a chisquared distribution with n degrees of freedom\u000a\u000athis quantity however is not observable the sum of squares of the residuals on the other hand is observable the quotient of that sum by 2 has a chisquared distribution with only n  1 degrees of freedom\u000a\u000athis difference between n and n  1 degrees of freedom results in bessels correction for the estimation of sample variance of a population with unknown mean and unknown variance though if the mean is known no correction is necessary\u000a\u000a\u000a remarkedit \u000ait is remarkable that the sum of squares of the residuals and the sample mean can be shown to be independent of each other using eg basus theorem that fact and the normal and chisquared distributions given above form the basis of calculations involving the quotient\u000a\u000athe probability distributions of the numerator and the denominator separately depend on the value of the unobservable population standard deviation  but  appears in both the numerator and the denominator and cancels that is fortunate because it means that even though we do not know  we know the probability distribution of this quotient it has a students tdistribution with n  1 degrees of freedom we can therefore use this quotient to find a confidence interval for \u000a\u000a\u000a regressionsedit \u000ain regression analysis the distinction between errors and residuals is subtle and important and leads to the concept of studentized residuals given an unobservable function that relates the independent variable to the dependent variable  say a line  the deviations of the dependent variable observations from this function are the unobservable errors if one runs a regression on some data then the deviations of the dependent variable observations from the fitted function are the residuals\u000ahowever a terminological difference arises in the expression mean squared error mse the mean squared error of a regression is a number computed from the sum of squares of the computed residuals and not of the unobservable errors if that sum of squares is divided by n the number of observations the result is the mean of the squared residuals since this is a biased estimate of the variance of the unobserved errors the bias is removed by multiplying the mean of the squared residuals by n  df where df is the number of degrees of freedom n minus the number of parameters being estimated this method gets exactly the same answer as the method using the mean of the squared error this latter formula serves as an unbiased estimate of the variance of the unobserved errors and is called the mean squared error\u000aanother method to calculate the mean square of error when analyzing the variance of linear regression using a technique like that used in anova they are the same because anova is a type of regression the sum of squares of the residuals aka sum of squares of the error is divided by the degrees of freedom where the degrees of freedom equals np1 where p is the number of parameters or predictors used in the model ie the number of variables in the regression equation one can then also calculate the mean square of the model by dividing the sum of squares of the model minus the degrees of freedom which is just the number of parameters then the f value can be calculated by divided msmodel by mserror and we can then determine significance which is why you want the mean squares to begin with\u000ahowever because of the behavior of the process of regression the distributions of residuals at different data points of the input variable may vary even if the errors themselves are identically distributed concretely in a linear regression where the errors are identically distributed the variability of residuals of inputs in the middle of the domain will be higher than the variability of residuals at the ends of the domain linear regressions fit endpoints better than the middle this is also reflected in the influence functions of various data points on the regression coefficients endpoints have more influence\u000athus to compare residuals at different inputs one needs to adjust the residuals by the expected variability of residuals which is called studentizing this is particularly important in the case of detecting outliers a large residual may be expected in the middle of the domain but considered an outlier at the end of the domain\u000a\u000a\u000a other uses of the word error in statisticsedit \u000a\u000athe use of the term error as discussed in the sections above is in the sense of a deviation of a value from a hypothetical unobserved value at least two other uses also occur in statistics both referring to observable prediction errors\u000amean square error or mean squared error abbreviated mse and root mean square error rmse refer to the amount by which the values predicted by an estimator differ from the quantities being estimated typically outside the sample from which the model was estimated\u000asum of squared errors typically abbreviated sse or sse refers to the residual sum of squares the sum of squared residuals of a regression this is the sum of the squares of the deviations of the actual values from the predicted values within the sample used for estimation likewise the sum of absolute errors sae refers to the sum of the absolute values of the residuals which is minimized in the least absolute deviations approach to regression\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000acook r dennis weisberg sanford 1982 residuals and influence in regression repr ed new york chapman and hall isbn 041224280x retrieved 23 february 2013 \u000aweisberg sanford 1985 applied linear regression 2nd ed new york wiley isbn 9780471879572 retrieved 23 february 2013 \u000ahazewinkel michiel ed 2001 errors theory of encyclopedia of mathematics springer isbn 9781556080104
p399
sg14
g17
sg18
Vin statistics and optimization errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its theoretical value the error or disturbance of an observed value is the deviation of the observed value from the unobservable true value of a quantity of interest for example a population mean and the residual of an observed value is the difference between the observed value and the estimated value of the quantity of interest for example a sample mean the distinction is most important in regression analysis where it leads to the concept of studentized residuals\u000a\u000a\u000a introductionedit \u000asuppose there is a series of observations from a univariate distribution and we want to estimate the mean of that distribution the socalled location model in this case the errors are the deviations of the observations from the population mean while the residuals are the deviations of the observations from the sample mean\u000aa statistical error or disturbance is the amount by which an observation differs from its expected value the latter being based on the whole population from which the statistical unit was chosen randomly for example if the mean height in a population of 21yearold men is 175 meters and one randomly chosen man is 180 meters tall then the error is 005 meters if the randomly chosen man is 170 meters tall then the error is 005 meters the expected value being the mean of the entire population is typically unobservable and hence the statistical error cannot be observed either\u000aa residual or fitting deviation on the other hand is an observable estimate of the unobservable statistical error consider the previous example with mens heights and suppose we have a random sample of n people the sample mean could serve as a good estimator of the population mean then we have\u000athe difference between the height of each man in the sample and the unobservable population mean is a statistical error whereas\u000athe difference between the height of each man in the sample and the observable sample mean is a residual\u000anote that the sum of the residuals within a random sample is necessarily zero and thus the residuals are necessarily not independent the statistical errors on the other hand are independent and their sum within the random sample is almost surely not zero\u000aone can standardize statistical errors especially of a normal distribution in a zscore or standard score and standardize residuals in a tstatistic or more generally studentized residuals\u000a\u000a\u000a in univariate distributionsedit \u000aif we assume a normally distributed population with mean  and standard deviation  and choose individuals independently then we have\u000a\u000aand the sample mean\u000a\u000ais a random variable distributed thus\u000a\u000athe statistical errors are then\u000a\u000awhereas the residuals are\u000a\u000athe sum of squares of the statistical errors divided by 2 has a chisquared distribution with n degrees of freedom\u000a\u000athis quantity however is not observable the sum of squares of the residuals on the other hand is observable the quotient of that sum by 2 has a chisquared distribution with only n  1 degrees of freedom\u000a\u000athis difference between n and n  1 degrees of freedom results in bessels correction for the estimation of sample variance of a population with unknown mean and unknown variance though if the mean is known no correction is necessary\u000a\u000a\u000a remarkedit \u000ait is remarkable that the sum of squares of the residuals and the sample mean can be shown to be independent of each other using eg basus theorem that fact and the normal and chisquared distributions given above form the basis of calculations involving the quotient\u000a\u000athe probability distributions of the numerator and the denominator separately depend on the value of the unobservable population standard deviation  but  appears in both the numerator and the denominator and cancels that is fortunate because it means that even though we do not know  we know the probability distribution of this quotient it has a students tdistribution with n  1 degrees of freedom we can therefore use this quotient to find a confidence interval for \u000a\u000a\u000a regressionsedit \u000ain regression analysis the distinction between errors and residuals is subtle and important and leads to the concept of studentized residuals given an unobservable function that relates the independent variable to the dependent variable  say a line  the deviations of the dependent variable observations from this function are the unobservable errors if one runs a regression on some data then the deviations of the dependent variable observations from the fitted function are the residuals\u000ahowever a terminological difference arises in the expression mean squared error mse the mean squared error of a regression is a number computed from the sum of squares of the computed residuals and not of the unobservable errors if that sum of squares is divided by n the number of observations the result is the mean of the squared residuals since this is a biased estimate of the variance of the unobserved errors the bias is removed by multiplying the mean of the squared residuals by n  df where df is the number of degrees of freedom n minus the number of parameters being estimated this method gets exactly the same answer as the method using the mean of the squared error this latter formula serves as an unbiased estimate of the variance of the unobserved errors and is called the mean squared error\u000aanother method to calculate the mean square of error when analyzing the variance of linear regression using a technique like that used in anova they are the same because anova is a type of regression the sum of squares of the residuals aka sum of squares of the error is divided by the degrees of freedom where the degrees of freedom equals np1 where p is the number of parameters or predictors used in the model ie the number of variables in the regression equation one can then also calculate the mean square of the model by dividing the sum of squares of the model minus the degrees of freedom which is just the number of parameters then the f value can be calculated by divided msmodel by mserror and we can then determine significance which is why you want the mean squares to begin with\u000ahowever because of the behavior of the process of regression the distributions of residuals at different data points of the input variable may vary even if the errors themselves are identically distributed concretely in a linear regression where the errors are identically distributed the variability of residuals of inputs in the middle of the domain will be higher than the variability of residuals at the ends of the domain linear regressions fit endpoints better than the middle this is also reflected in the influence functions of various data points on the regression coefficients endpoints have more influence\u000athus to compare residuals at different inputs one needs to adjust the residuals by the expected variability of residuals which is called studentizing this is particularly important in the case of detecting outliers a large residual may be expected in the middle of the domain but considered an outlier at the end of the domain\u000a\u000a\u000a other uses of the word error in statisticsedit \u000a\u000athe use of the term error as discussed in the sections above is in the sense of a deviation of a value from a hypothetical unobserved value at least two other uses also occur in statistics both referring to observable prediction errors\u000amean square error or mean squared error abbreviated mse and root mean square error rmse refer to the amount by which the values predicted by an estimator differ from the quantities being estimated typically outside the sample from which the model was estimated\u000asum of squared errors typically abbreviated sse or sse refers to the residual sum of squares the sum of squared residuals of a regression this is the sum of the squares of the deviations of the actual values from the predicted values within the sample used for estimation likewise the sum of absolute errors sae refers to the sum of the absolute values of the residuals which is minimized in the least absolute deviations approach to regression\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000acook r dennis weisberg sanford 1982 residuals and influence in regression repr ed new york chapman and hall isbn 041224280x retrieved 23 february 2013 \u000aweisberg sanford 1985 applied linear regression 2nd ed new york wiley isbn 9780471879572 retrieved 23 february 2013 \u000ahazewinkel michiel ed 2001 errors theory of encyclopedia of mathematics springer isbn 9781556080104
p400
sg20
g23
sg24
g27
sg30
Vin statistics and optimization errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its theoretical value the error or disturbance of an observed value is the deviation of the observed value from the unobservable true value of a quantity of interest for example a population mean and the residual of an observed value is the difference between the observed value and the estimated value of the quantity of interest for example a sample mean the distinction is most important in regression analysis where it leads to the concept of studentized residuals\u000a\u000a\u000a introductionedit \u000asuppose there is a series of observations from a univariate distribution and we want to estimate the mean of that distribution the socalled location model in this case the errors are the deviations of the observations from the population mean while the residuals are the deviations of the observations from the sample mean\u000aa statistical error or disturbance is the amount by which an observation differs from its expected value the latter being based on the whole population from which the statistical unit was chosen randomly for example if the mean height in a population of 21yearold men is 175 meters and one randomly chosen man is 180 meters tall then the error is 005 meters if the randomly chosen man is 170 meters tall then the error is 005 meters the expected value being the mean of the entire population is typically unobservable and hence the statistical error cannot be observed either\u000aa residual or fitting deviation on the other hand is an observable estimate of the unobservable statistical error consider the previous example with mens heights and suppose we have a random sample of n people the sample mean could serve as a good estimator of the population mean then we have\u000athe difference between the height of each man in the sample and the unobservable population mean is a statistical error whereas\u000athe difference between the height of each man in the sample and the observable sample mean is a residual\u000anote that the sum of the residuals within a random sample is necessarily zero and thus the residuals are necessarily not independent the statistical errors on the other hand are independent and their sum within the random sample is almost surely not zero\u000aone can standardize statistical errors especially of a normal distribution in a zscore or standard score and standardize residuals in a tstatistic or more generally studentized residuals\u000a\u000a\u000a in univariate distributionsedit \u000aif we assume a normally distributed population with mean  and standard deviation  and choose individuals independently then we have\u000a\u000aand the sample mean\u000a\u000ais a random variable distributed thus\u000a\u000athe statistical errors are then\u000a\u000awhereas the residuals are\u000a\u000athe sum of squares of the statistical errors divided by 2 has a chisquared distribution with n degrees of freedom\u000a\u000athis quantity however is not observable the sum of squares of the residuals on the other hand is observable the quotient of that sum by 2 has a chisquared distribution with only n  1 degrees of freedom\u000a\u000athis difference between n and n  1 degrees of freedom results in bessels correction for the estimation of sample variance of a population with unknown mean and unknown variance though if the mean is known no correction is necessary\u000a\u000a\u000a remarkedit \u000ait is remarkable that the sum of squares of the residuals and the sample mean can be shown to be independent of each other using eg basus theorem that fact and the normal and chisquared distributions given above form the basis of calculations involving the quotient\u000a\u000athe probability distributions of the numerator and the denominator separately depend on the value of the unobservable population standard deviation  but  appears in both the numerator and the denominator and cancels that is fortunate because it means that even though we do not know  we know the probability distribution of this quotient it has a students tdistribution with n  1 degrees of freedom we can therefore use this quotient to find a confidence interval for \u000a\u000a\u000a regressionsedit \u000ain regression analysis the distinction between errors and residuals is subtle and important and leads to the concept of studentized residuals given an unobservable function that relates the independent variable to the dependent variable  say a line  the deviations of the dependent variable observations from this function are the unobservable errors if one runs a regression on some data then the deviations of the dependent variable observations from the fitted function are the residuals\u000ahowever a terminological difference arises in the expression mean squared error mse the mean squared error of a regression is a number computed from the sum of squares of the computed residuals and not of the unobservable errors if that sum of squares is divided by n the number of observations the result is the mean of the squared residuals since this is a biased estimate of the variance of the unobserved errors the bias is removed by multiplying the mean of the squared residuals by n  df where df is the number of degrees of freedom n minus the number of parameters being estimated this method gets exactly the same answer as the method using the mean of the squared error this latter formula serves as an unbiased estimate of the variance of the unobserved errors and is called the mean squared error\u000aanother method to calculate the mean square of error when analyzing the variance of linear regression using a technique like that used in anova they are the same because anova is a type of regression the sum of squares of the residuals aka sum of squares of the error is divided by the degrees of freedom where the degrees of freedom equals np1 where p is the number of parameters or predictors used in the model ie the number of variables in the regression equation one can then also calculate the mean square of the model by dividing the sum of squares of the model minus the degrees of freedom which is just the number of parameters then the f value can be calculated by divided msmodel by mserror and we can then determine significance which is why you want the mean squares to begin with\u000ahowever because of the behavior of the process of regression the distributions of residuals at different data points of the input variable may vary even if the errors themselves are identically distributed concretely in a linear regression where the errors are identically distributed the variability of residuals of inputs in the middle of the domain will be higher than the variability of residuals at the ends of the domain linear regressions fit endpoints better than the middle this is also reflected in the influence functions of various data points on the regression coefficients endpoints have more influence\u000athus to compare residuals at different inputs one needs to adjust the residuals by the expected variability of residuals which is called studentizing this is particularly important in the case of detecting outliers a large residual may be expected in the middle of the domain but considered an outlier at the end of the domain\u000a\u000a\u000a other uses of the word error in statisticsedit \u000a\u000athe use of the term error as discussed in the sections above is in the sense of a deviation of a value from a hypothetical unobserved value at least two other uses also occur in statistics both referring to observable prediction errors\u000amean square error or mean squared error abbreviated mse and root mean square error rmse refer to the amount by which the values predicted by an estimator differ from the quantities being estimated typically outside the sample from which the model was estimated\u000asum of squared errors typically abbreviated sse or sse refers to the residual sum of squares the sum of squared residuals of a regression this is the sum of the squares of the deviations of the actual values from the predicted values within the sample used for estimation likewise the sum of absolute errors sae refers to the sum of the absolute values of the residuals which is minimized in the least absolute deviations approach to regression\u000a\u000a\u000a see alsoedit \u000a\u000a\u000a referencesedit \u000a\u000acook r dennis weisberg sanford 1982 residuals and influence in regression repr ed new york chapman and hall isbn 041224280x retrieved 23 february 2013 \u000aweisberg sanford 1985 applied linear regression 2nd ed new york wiley isbn 9780471879572 retrieved 23 february 2013 \u000ahazewinkel michiel ed 2001 errors theory of encyclopedia of mathematics springer isbn 9781556080104
p401
sg32
g35
sg37
NsbsS'parametric_model.txt'
p402
g2
(g3
g4
Ntp403
Rp404
(dp405
g8
g11
sg12
Vin statistics a parametric model or parametric family or finitedimensional model is a family of distributions that can be described using a finite number of parameters these parameters are usually collected together to form a single kdimensional parameter vector   1 2  k\u000aparametric models are contrasted with the semiparametric seminonparametric and nonparametric models all of which consist of an infinite set of parameters for description the distinction between these four classes is as follows\u000ain a parametric model all the parameters are in finitedimensional parameter spaces\u000aa model is nonparametric if all the parameters are in infinitedimensional parameter spaces\u000aa semiparametric model contains finitedimensional parameters of interest and infinitedimensional nuisance parameters\u000aa seminonparametric model has both finitedimensional and infinitedimensional unknown parameters of interest\u000asome statisticians believe that the concepts parametric nonparametric and semiparametric are ambiguous it can also be noted that the set of all probability measures has cardinality of continuum and therefore it is possible to parametrize any model at all by a single number in 01 interval this difficulty can be avoided by considering only smooth parametric models\u000a\u000a\u000a definitionedit \u000aa parametric model is a collection of probability distributions such that each member of this collection p is described by a finitedimensional parameter  the set of all allowable values for the parameter is denoted   rk and the model itself is written as\u000a\u000awhen the model consists of absolutely continuous distributions it is often specified in terms of corresponding probability density functions\u000a\u000athe parametric model is called identifiable if the mapping   p is invertible that is there are no two different parameter values 1 and 2 such that p1  p2\u000a\u000a\u000a examplesedit \u000athe poisson family of distributions is parametrized by a single number   0\u000a\u000awhere p is the probability mass function this family is an exponential family\u000athe normal family is parametrized by    where   r is a location parameter and   0 is a scale parameter this parametrized family is both an exponential family and a locationscale family\u000a\u000athe weibull translation model has three parameters     \u000a\u000athis model is not regular see definition below unless we restrict  to lie in the interval 2 \u000a\u000a\u000a regular parametric modeledit \u000alet  be a fixed finite measure on a measurable space   and  the collection of all probability measures dominated by  then we will call  a regular parametric model if the following requirements are met\u000a is an open subset of rk\u000athe map\u000a\u000afrom  to l2 is frchet differentiable there exists a vector  such that\u000a\u000awhere  denotes matrix transpose\u000athe map  defined above is continuous on \u000athe kk fisher information matrix\u000a\u000ais nonsingular\u000a\u000a\u000a propertiesedit \u000asufficient conditions for regularity of a parametric model in terms of ordinary differentiability of the density function  are followingthe density function x is continuously differentiable in  for almost all x with gradient \u000athe score function\u000a\u000abelongs to the space lp of squareintegrable functions with respect to the measure p\u000athe fisher information matrix i defined as\u000a\u000ais nonsingular and continuous in \u000aif conditions iiii hold then the parametric model is regular\u000a\u000alocal asymptotic normality\u000aif the regular parametric model is identifiable then there exists a uniformly consistent and efficient estimator of its parameter \u000a\u000a\u000a see alsoedit \u000astatistical model\u000aparametric family\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000aparametricism\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit 
p406
sg14
g17
sg18
Vin statistics a parametric model or parametric family or finitedimensional model is a family of distributions that can be described using a finite number of parameters these parameters are usually collected together to form a single kdimensional parameter vector   1 2  k\u000aparametric models are contrasted with the semiparametric seminonparametric and nonparametric models all of which consist of an infinite set of parameters for description the distinction between these four classes is as follows\u000ain a parametric model all the parameters are in finitedimensional parameter spaces\u000aa model is nonparametric if all the parameters are in infinitedimensional parameter spaces\u000aa semiparametric model contains finitedimensional parameters of interest and infinitedimensional nuisance parameters\u000aa seminonparametric model has both finitedimensional and infinitedimensional unknown parameters of interest\u000asome statisticians believe that the concepts parametric nonparametric and semiparametric are ambiguous it can also be noted that the set of all probability measures has cardinality of continuum and therefore it is possible to parametrize any model at all by a single number in 01 interval this difficulty can be avoided by considering only smooth parametric models\u000a\u000a\u000a definitionedit \u000aa parametric model is a collection of probability distributions such that each member of this collection p is described by a finitedimensional parameter  the set of all allowable values for the parameter is denoted   rk and the model itself is written as\u000a\u000awhen the model consists of absolutely continuous distributions it is often specified in terms of corresponding probability density functions\u000a\u000athe parametric model is called identifiable if the mapping   p is invertible that is there are no two different parameter values 1 and 2 such that p1  p2\u000a\u000a\u000a examplesedit \u000athe poisson family of distributions is parametrized by a single number   0\u000a\u000awhere p is the probability mass function this family is an exponential family\u000athe normal family is parametrized by    where   r is a location parameter and   0 is a scale parameter this parametrized family is both an exponential family and a locationscale family\u000a\u000athe weibull translation model has three parameters     \u000a\u000athis model is not regular see definition below unless we restrict  to lie in the interval 2 \u000a\u000a\u000a regular parametric modeledit \u000alet  be a fixed finite measure on a measurable space   and  the collection of all probability measures dominated by  then we will call  a regular parametric model if the following requirements are met\u000a is an open subset of rk\u000athe map\u000a\u000afrom  to l2 is frchet differentiable there exists a vector  such that\u000a\u000awhere  denotes matrix transpose\u000athe map  defined above is continuous on \u000athe kk fisher information matrix\u000a\u000ais nonsingular\u000a\u000a\u000a propertiesedit \u000asufficient conditions for regularity of a parametric model in terms of ordinary differentiability of the density function  are followingthe density function x is continuously differentiable in  for almost all x with gradient \u000athe score function\u000a\u000abelongs to the space lp of squareintegrable functions with respect to the measure p\u000athe fisher information matrix i defined as\u000a\u000ais nonsingular and continuous in \u000aif conditions iiii hold then the parametric model is regular\u000a\u000alocal asymptotic normality\u000aif the regular parametric model is identifiable then there exists a uniformly consistent and efficient estimator of its parameter \u000a\u000a\u000a see alsoedit \u000astatistical model\u000aparametric family\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000aparametricism\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit
p407
sg20
g23
sg24
g27
sg30
Vin statistics a parametric model or parametric family or finitedimensional model is a family of distributions that can be described using a finite number of parameters these parameters are usually collected together to form a single kdimensional parameter vector   1 2  k\u000aparametric models are contrasted with the semiparametric seminonparametric and nonparametric models all of which consist of an infinite set of parameters for description the distinction between these four classes is as follows\u000ain a parametric model all the parameters are in finitedimensional parameter spaces\u000aa model is nonparametric if all the parameters are in infinitedimensional parameter spaces\u000aa semiparametric model contains finitedimensional parameters of interest and infinitedimensional nuisance parameters\u000aa seminonparametric model has both finitedimensional and infinitedimensional unknown parameters of interest\u000asome statisticians believe that the concepts parametric nonparametric and semiparametric are ambiguous it can also be noted that the set of all probability measures has cardinality of continuum and therefore it is possible to parametrize any model at all by a single number in 01 interval this difficulty can be avoided by considering only smooth parametric models\u000a\u000a\u000a definitionedit \u000aa parametric model is a collection of probability distributions such that each member of this collection p is described by a finitedimensional parameter  the set of all allowable values for the parameter is denoted   rk and the model itself is written as\u000a\u000awhen the model consists of absolutely continuous distributions it is often specified in terms of corresponding probability density functions\u000a\u000athe parametric model is called identifiable if the mapping   p is invertible that is there are no two different parameter values 1 and 2 such that p1  p2\u000a\u000a\u000a examplesedit \u000athe poisson family of distributions is parametrized by a single number   0\u000a\u000awhere p is the probability mass function this family is an exponential family\u000athe normal family is parametrized by    where   r is a location parameter and   0 is a scale parameter this parametrized family is both an exponential family and a locationscale family\u000a\u000athe weibull translation model has three parameters     \u000a\u000athis model is not regular see definition below unless we restrict  to lie in the interval 2 \u000a\u000a\u000a regular parametric modeledit \u000alet  be a fixed finite measure on a measurable space   and  the collection of all probability measures dominated by  then we will call  a regular parametric model if the following requirements are met\u000a is an open subset of rk\u000athe map\u000a\u000afrom  to l2 is frchet differentiable there exists a vector  such that\u000a\u000awhere  denotes matrix transpose\u000athe map  defined above is continuous on \u000athe kk fisher information matrix\u000a\u000ais nonsingular\u000a\u000a\u000a propertiesedit \u000asufficient conditions for regularity of a parametric model in terms of ordinary differentiability of the density function  are followingthe density function x is continuously differentiable in  for almost all x with gradient \u000athe score function\u000a\u000abelongs to the space lp of squareintegrable functions with respect to the measure p\u000athe fisher information matrix i defined as\u000a\u000ais nonsingular and continuous in \u000aif conditions iiii hold then the parametric model is regular\u000a\u000alocal asymptotic normality\u000aif the regular parametric model is identifiable then there exists a uniformly consistent and efficient estimator of its parameter \u000a\u000a\u000a see alsoedit \u000astatistical model\u000aparametric family\u000aparametrization ie coordinate system\u000aparsimony with regards to the tradeoff of many or few parameters in data fitting\u000aparametricism\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit 
p408
sg32
g35
sg37
NsbsS'coherence_(statistics).txt'
p409
g2
(g3
g4
Ntp410
Rp411
(dp412
g8
g11
sg12
Vin probability theory and statistics coherence can have several different meanings\u000a\u000a\u000a in personal probability \u000awhen dealing with personal probability assessments or supposed probabilities derived in nonstandard ways it is a property of selfconsistency across a whole set of such assessments\u000a\u000a\u000a in gambling strategy \u000aone way of expressing such selfconsistency is in terms of responses to various betting propositions as described in relation to coherence philosophical gambling strategy\u000a\u000a\u000a in bayesian decision theory \u000athe coherency principle in bayesian decision theory is the assumption that personal probabilities follow the ordinary rules for probability calculations where the validity of these rules corresponds to the selfconsistency just referred to and thus that consistent decisions can be obtained from these probabilities\u000a\u000a\u000a in time series analysis \u000ain time series analysis and particularly in spectral analysis it is used to describe the strength of association between two series where the possible dependence between the two series is not limited to simultaneous values but may include leading lagged and smoothed relationships\u000athe concepts here are sometimes known as coherency and are essentially those set out for coherence signal processing however note that the quantity coefficient of coherence may sometimes be called the squared coherence\u000a\u000a\u000a
p413
sg14
g17
sg18
Vin probability theory and statistics coherence can have several different meanings\u000a\u000a\u000a in personal probability \u000awhen dealing with personal probability assessments or supposed probabilities derived in nonstandard ways it is a property of selfconsistency across a whole set of such assessments\u000a\u000a\u000a in gambling strategy \u000aone way of expressing such selfconsistency is in terms of responses to various betting propositions as described in relation to coherence philosophical gambling strategy\u000a\u000a\u000a in bayesian decision theory \u000athe coherency principle in bayesian decision theory is the assumption that personal probabilities follow the ordinary rules for probability calculations where the validity of these rules corresponds to the selfconsistency just referred to and thus that consistent decisions can be obtained from these probabilities\u000a\u000a\u000a in time series analysis \u000ain time series analysis and particularly in spectral analysis it is used to describe the strength of association between two series where the possible dependence between the two series is not limited to simultaneous values but may include leading lagged and smoothed relationships\u000athe concepts here are sometimes known as coherency and are essentially those set out for coherence signal processing however note that the quantity coefficient of coherence may sometimes be called the squared coherence
p414
sg20
g23
sg24
g27
sg30
Vin probability theory and statistics coherence can have several different meanings\u000a\u000a\u000a in personal probability \u000awhen dealing with personal probability assessments or supposed probabilities derived in nonstandard ways it is a property of selfconsistency across a whole set of such assessments\u000a\u000a\u000a in gambling strategy \u000aone way of expressing such selfconsistency is in terms of responses to various betting propositions as described in relation to coherence philosophical gambling strategy\u000a\u000a\u000a in bayesian decision theory \u000athe coherency principle in bayesian decision theory is the assumption that personal probabilities follow the ordinary rules for probability calculations where the validity of these rules corresponds to the selfconsistency just referred to and thus that consistent decisions can be obtained from these probabilities\u000a\u000a\u000a in time series analysis \u000ain time series analysis and particularly in spectral analysis it is used to describe the strength of association between two series where the possible dependence between the two series is not limited to simultaneous values but may include leading lagged and smoothed relationships\u000athe concepts here are sometimes known as coherency and are essentially those set out for coherence signal processing however note that the quantity coefficient of coherence may sometimes be called the squared coherence\u000a\u000a\u000a
p415
sg32
g35
sg37
NsbsS'likelihood-ratio_test.txt'
p416
g2
(g3
g4
Ntp417
Rp418
(dp419
g8
g11
sg12
Vin statistics a likelihood ratio test is a statistical test used to compare the goodness of fit of two models one of which the null model is a special case of the other the alternative model the test is based on the likelihood ratio which expresses how many times more likely the data are under one model than the other this likelihood ratio or equivalently its logarithm can then be used to compute a pvalue or compared to a critical value to decide whether to reject the null model in favour of the alternative model when the logarithm of the likelihood ratio is used the statistic is known as a loglikelihood ratio statistic and the probability distribution of this test statistic assuming that the null model is true can be approximated using wilkss theorem\u000ain the case of distinguishing between two models each of which has no unknown parameters use of the likelihood ratio test can be justified by the neymanpearson lemma which demonstrates that such a test has the highest power among all competitors\u000a\u000a\u000a useedit \u000aeach of the two competing models the null model and the alternative model is separately fitted to the data and the loglikelihood recorded the test statistic often denoted by d is twice the difference in these loglikelihoods\u000a\u000athe model with more parameters will always fit at least as well have an equal or greater loglikelihood whether it fits significantly better and should thus be preferred is determined by deriving the probability or pvalue of the difference d where the null hypothesis represents a special case of the alternative hypothesis the probability distribution of the test statistic is approximately a chisquared distribution with degrees of freedom equal to df2  df1  symbols df1 and df2 represent the number of free parameters of models 1 and 2 the null model and the alternative model respectively\u000ahere is an example of use if the null model has 1 parameter and a loglikelihood of 8024 and the alternative model has 3 parameters and a loglikelihood of 8012 then the probability of this difference is that of chisquared value of 28024  8012  24 with 3  1  2 degrees of freedom certain assumptions must be met for the statistic to follow a chisquared distribution and often empirical pvalues are computed\u000athe likelihoodratio test requires nested models ie models in which the more complex one can be transformed into the simpler model by imposing a set of constraints on the parameters if the models are not nested then a generalization of the likelihoodratio test can usually be used instead the relative likelihood\u000a\u000a\u000a simplevssimple hypothesesedit \u000a\u000aa statistical model is often a parametrized family of probability density functions or probability mass functions  a simplevssimple hypothesis test has completely specified models under both the null and alternative hypotheses which for convenience are written in terms of fixed values of a notional parameter \u000a\u000anote that under either hypothesis the distribution of the data is fully specified there are no unknown parameters to estimate the likelihood ratio test is based on the likelihood ratio which is often denoted by  the capital greek letter lambda the likelihood ratio is defined as follows\u000a\u000aor\u000a\u000awhere  is the likelihood function and  is the supremum function note that some references may use the reciprocal as the definition in the form stated here the likelihood ratio is small if the alternative model is better than the null model and the likelihood ratio test provides the decision rule as follows\u000aif  do not reject \u000aif  reject \u000areject with probability  if \u000athe values  are usually chosen to obtain a specified significance level  through the relation  the neymanpearson lemma states that this likelihood ratio test is the most powerful among all level  tests for this problem\u000a\u000a\u000a definition likelihood ratio test for composite hypothesesedit \u000aa null hypothesis is often stated by saying the parameter  is in a specified subset  of the parameter space \u000a\u000athe likelihood function is  with  being the pdf or pmf which is a function of the parameter  with  held fixed at the value that was actually observed ie the data the likelihood ratio test statistic is \u000a\u000ahere the  notation refers to the supremum function\u000aa likelihood ratio test is any test with critical region or rejection region of the form  where  is any number satisfying  many common test statistics such as the ztest the ftest pearsons chisquared test and the gtest are tests for nested models and can be phrased as loglikelihood ratios or approximations thereof\u000a\u000a\u000a interpretationedit \u000abeing a function of the data  the likelihood ratio is therefore a statistic the likelihood ratio test rejects the null hypothesis if the value of this statistic is too small how small is too small depends on the significance level of the test ie on what probability of type i error is considered tolerable type i errors consist of the rejection of a null hypothesis that is true\u000athe numerator corresponds to the maximum likelihood of an observed outcome under the null hypothesis the denominator corresponds to the maximum likelihood of an observed outcome varying parameters over the whole parameter space the numerator of this ratio is less than the denominator the likelihood ratio hence is between 0 and 1 low values of the likelihood ratio mean that the observed result was less likely to occur under the null hypothesis as compared to the alternative high values of the statistic mean that the observed outcome was nearly as likely to occur under the null hypothesis as the alternative and the null hypothesis cannot be rejected\u000a\u000a\u000a distribution wilkss theoremedit \u000aif the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions to acceptreject the null hypothesis in most cases however the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine a convenient result attributed to samuel s wilks says that as the sample size  approaches  the test statistic  for a nested model will be asymptotically distributed with degrees of freedom equal to the difference in dimensionality of  and  this means that for a great variety of hypotheses a practitioner can compute the likelihood ratio  for the data and compare  to the  value corresponding to a desired statistical significance as an approximate statistical test\u000a\u000a\u000a examplesedit \u000a\u000a\u000a coin tossingedit \u000aan example in the case of pearsons test we might try to compare two coins to determine whether they have the same probability of coming up heads our observation can be put into a contingency table with rows corresponding to the coin and columns corresponding to heads or tails the elements of the contingency table will be the number of times the coin for that row came up heads or tails the contents of this table are our observation \u000ahere  consists of the possible combinations of values of the parameters    and  which are the probability that coins 1 and 2 come up heads or tails in what follows  and  the hypothesis space  is constrained by the usual constraints on a probability distribution  and  the space of the null hypothesis  is the subspace where  writing  for the best values for  under the hypothesis  the maximum likelihood estimate is given by\u000a\u000asimilarly the maximum likelihood estimates of  under the null hypothesis  are given by\u000a\u000awhich does not depend on the coin \u000athe hypothesis and null hypothesis can be rewritten slightly so that they satisfy the constraints for the logarithm of the likelihood ratio to have the desired nice distribution since the constraint causes the twodimensional  to be reduced to the onedimensional  the asymptotic distribution for the test will be  the  distribution with one degree of freedom\u000afor the general contingency table we can write the loglikelihood ratio statistic as\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000apractical application of likelihood ratio test described\u000ar package walds sequential probability ratio test\u000arichard lowrys predictive values and likelihood ratios online clinical calculator
p420
sg14
g17
sg18
Vin statistics a likelihood ratio test is a statistical test used to compare the goodness of fit of two models one of which the null model is a special case of the other the alternative model the test is based on the likelihood ratio which expresses how many times more likely the data are under one model than the other this likelihood ratio or equivalently its logarithm can then be used to compute a pvalue or compared to a critical value to decide whether to reject the null model in favour of the alternative model when the logarithm of the likelihood ratio is used the statistic is known as a loglikelihood ratio statistic and the probability distribution of this test statistic assuming that the null model is true can be approximated using wilkss theorem\u000ain the case of distinguishing between two models each of which has no unknown parameters use of the likelihood ratio test can be justified by the neymanpearson lemma which demonstrates that such a test has the highest power among all competitors\u000a\u000a\u000a useedit \u000aeach of the two competing models the null model and the alternative model is separately fitted to the data and the loglikelihood recorded the test statistic often denoted by d is twice the difference in these loglikelihoods\u000a\u000athe model with more parameters will always fit at least as well have an equal or greater loglikelihood whether it fits significantly better and should thus be preferred is determined by deriving the probability or pvalue of the difference d where the null hypothesis represents a special case of the alternative hypothesis the probability distribution of the test statistic is approximately a chisquared distribution with degrees of freedom equal to df2  df1  symbols df1 and df2 represent the number of free parameters of models 1 and 2 the null model and the alternative model respectively\u000ahere is an example of use if the null model has 1 parameter and a loglikelihood of 8024 and the alternative model has 3 parameters and a loglikelihood of 8012 then the probability of this difference is that of chisquared value of 28024  8012  24 with 3  1  2 degrees of freedom certain assumptions must be met for the statistic to follow a chisquared distribution and often empirical pvalues are computed\u000athe likelihoodratio test requires nested models ie models in which the more complex one can be transformed into the simpler model by imposing a set of constraints on the parameters if the models are not nested then a generalization of the likelihoodratio test can usually be used instead the relative likelihood\u000a\u000a\u000a simplevssimple hypothesesedit \u000a\u000aa statistical model is often a parametrized family of probability density functions or probability mass functions  a simplevssimple hypothesis test has completely specified models under both the null and alternative hypotheses which for convenience are written in terms of fixed values of a notional parameter \u000a\u000anote that under either hypothesis the distribution of the data is fully specified there are no unknown parameters to estimate the likelihood ratio test is based on the likelihood ratio which is often denoted by  the capital greek letter lambda the likelihood ratio is defined as follows\u000a\u000aor\u000a\u000awhere  is the likelihood function and  is the supremum function note that some references may use the reciprocal as the definition in the form stated here the likelihood ratio is small if the alternative model is better than the null model and the likelihood ratio test provides the decision rule as follows\u000aif  do not reject \u000aif  reject \u000areject with probability  if \u000athe values  are usually chosen to obtain a specified significance level  through the relation  the neymanpearson lemma states that this likelihood ratio test is the most powerful among all level  tests for this problem\u000a\u000a\u000a definition likelihood ratio test for composite hypothesesedit \u000aa null hypothesis is often stated by saying the parameter  is in a specified subset  of the parameter space \u000a\u000athe likelihood function is  with  being the pdf or pmf which is a function of the parameter  with  held fixed at the value that was actually observed ie the data the likelihood ratio test statistic is \u000a\u000ahere the  notation refers to the supremum function\u000aa likelihood ratio test is any test with critical region or rejection region of the form  where  is any number satisfying  many common test statistics such as the ztest the ftest pearsons chisquared test and the gtest are tests for nested models and can be phrased as loglikelihood ratios or approximations thereof\u000a\u000a\u000a interpretationedit \u000abeing a function of the data  the likelihood ratio is therefore a statistic the likelihood ratio test rejects the null hypothesis if the value of this statistic is too small how small is too small depends on the significance level of the test ie on what probability of type i error is considered tolerable type i errors consist of the rejection of a null hypothesis that is true\u000athe numerator corresponds to the maximum likelihood of an observed outcome under the null hypothesis the denominator corresponds to the maximum likelihood of an observed outcome varying parameters over the whole parameter space the numerator of this ratio is less than the denominator the likelihood ratio hence is between 0 and 1 low values of the likelihood ratio mean that the observed result was less likely to occur under the null hypothesis as compared to the alternative high values of the statistic mean that the observed outcome was nearly as likely to occur under the null hypothesis as the alternative and the null hypothesis cannot be rejected\u000a\u000a\u000a distribution wilkss theoremedit \u000aif the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions to acceptreject the null hypothesis in most cases however the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine a convenient result attributed to samuel s wilks says that as the sample size  approaches  the test statistic  for a nested model will be asymptotically distributed with degrees of freedom equal to the difference in dimensionality of  and  this means that for a great variety of hypotheses a practitioner can compute the likelihood ratio  for the data and compare  to the  value corresponding to a desired statistical significance as an approximate statistical test\u000a\u000a\u000a examplesedit \u000a\u000a\u000a coin tossingedit \u000aan example in the case of pearsons test we might try to compare two coins to determine whether they have the same probability of coming up heads our observation can be put into a contingency table with rows corresponding to the coin and columns corresponding to heads or tails the elements of the contingency table will be the number of times the coin for that row came up heads or tails the contents of this table are our observation \u000ahere  consists of the possible combinations of values of the parameters    and  which are the probability that coins 1 and 2 come up heads or tails in what follows  and  the hypothesis space  is constrained by the usual constraints on a probability distribution  and  the space of the null hypothesis  is the subspace where  writing  for the best values for  under the hypothesis  the maximum likelihood estimate is given by\u000a\u000asimilarly the maximum likelihood estimates of  under the null hypothesis  are given by\u000a\u000awhich does not depend on the coin \u000athe hypothesis and null hypothesis can be rewritten slightly so that they satisfy the constraints for the logarithm of the likelihood ratio to have the desired nice distribution since the constraint causes the twodimensional  to be reduced to the onedimensional  the asymptotic distribution for the test will be  the  distribution with one degree of freedom\u000afor the general contingency table we can write the loglikelihood ratio statistic as\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000apractical application of likelihood ratio test described\u000ar package walds sequential probability ratio test\u000arichard lowrys predictive values and likelihood ratios online clinical calculator
p421
sg20
g23
sg24
g27
sg30
Vin statistics a likelihood ratio test is a statistical test used to compare the goodness of fit of two models one of which the null model is a special case of the other the alternative model the test is based on the likelihood ratio which expresses how many times more likely the data are under one model than the other this likelihood ratio or equivalently its logarithm can then be used to compute a pvalue or compared to a critical value to decide whether to reject the null model in favour of the alternative model when the logarithm of the likelihood ratio is used the statistic is known as a loglikelihood ratio statistic and the probability distribution of this test statistic assuming that the null model is true can be approximated using wilkss theorem\u000ain the case of distinguishing between two models each of which has no unknown parameters use of the likelihood ratio test can be justified by the neymanpearson lemma which demonstrates that such a test has the highest power among all competitors\u000a\u000a\u000a useedit \u000aeach of the two competing models the null model and the alternative model is separately fitted to the data and the loglikelihood recorded the test statistic often denoted by d is twice the difference in these loglikelihoods\u000a\u000athe model with more parameters will always fit at least as well have an equal or greater loglikelihood whether it fits significantly better and should thus be preferred is determined by deriving the probability or pvalue of the difference d where the null hypothesis represents a special case of the alternative hypothesis the probability distribution of the test statistic is approximately a chisquared distribution with degrees of freedom equal to df2  df1  symbols df1 and df2 represent the number of free parameters of models 1 and 2 the null model and the alternative model respectively\u000ahere is an example of use if the null model has 1 parameter and a loglikelihood of 8024 and the alternative model has 3 parameters and a loglikelihood of 8012 then the probability of this difference is that of chisquared value of 28024  8012  24 with 3  1  2 degrees of freedom certain assumptions must be met for the statistic to follow a chisquared distribution and often empirical pvalues are computed\u000athe likelihoodratio test requires nested models ie models in which the more complex one can be transformed into the simpler model by imposing a set of constraints on the parameters if the models are not nested then a generalization of the likelihoodratio test can usually be used instead the relative likelihood\u000a\u000a\u000a simplevssimple hypothesesedit \u000a\u000aa statistical model is often a parametrized family of probability density functions or probability mass functions  a simplevssimple hypothesis test has completely specified models under both the null and alternative hypotheses which for convenience are written in terms of fixed values of a notional parameter \u000a\u000anote that under either hypothesis the distribution of the data is fully specified there are no unknown parameters to estimate the likelihood ratio test is based on the likelihood ratio which is often denoted by  the capital greek letter lambda the likelihood ratio is defined as follows\u000a\u000aor\u000a\u000awhere  is the likelihood function and  is the supremum function note that some references may use the reciprocal as the definition in the form stated here the likelihood ratio is small if the alternative model is better than the null model and the likelihood ratio test provides the decision rule as follows\u000aif  do not reject \u000aif  reject \u000areject with probability  if \u000athe values  are usually chosen to obtain a specified significance level  through the relation  the neymanpearson lemma states that this likelihood ratio test is the most powerful among all level  tests for this problem\u000a\u000a\u000a definition likelihood ratio test for composite hypothesesedit \u000aa null hypothesis is often stated by saying the parameter  is in a specified subset  of the parameter space \u000a\u000athe likelihood function is  with  being the pdf or pmf which is a function of the parameter  with  held fixed at the value that was actually observed ie the data the likelihood ratio test statistic is \u000a\u000ahere the  notation refers to the supremum function\u000aa likelihood ratio test is any test with critical region or rejection region of the form  where  is any number satisfying  many common test statistics such as the ztest the ftest pearsons chisquared test and the gtest are tests for nested models and can be phrased as loglikelihood ratios or approximations thereof\u000a\u000a\u000a interpretationedit \u000abeing a function of the data  the likelihood ratio is therefore a statistic the likelihood ratio test rejects the null hypothesis if the value of this statistic is too small how small is too small depends on the significance level of the test ie on what probability of type i error is considered tolerable type i errors consist of the rejection of a null hypothesis that is true\u000athe numerator corresponds to the maximum likelihood of an observed outcome under the null hypothesis the denominator corresponds to the maximum likelihood of an observed outcome varying parameters over the whole parameter space the numerator of this ratio is less than the denominator the likelihood ratio hence is between 0 and 1 low values of the likelihood ratio mean that the observed result was less likely to occur under the null hypothesis as compared to the alternative high values of the statistic mean that the observed outcome was nearly as likely to occur under the null hypothesis as the alternative and the null hypothesis cannot be rejected\u000a\u000a\u000a distribution wilkss theoremedit \u000aif the distribution of the likelihood ratio corresponding to a particular null and alternative hypothesis can be explicitly determined then it can directly be used to form decision regions to acceptreject the null hypothesis in most cases however the exact distribution of the likelihood ratio corresponding to specific hypotheses is very difficult to determine a convenient result attributed to samuel s wilks says that as the sample size  approaches  the test statistic  for a nested model will be asymptotically distributed with degrees of freedom equal to the difference in dimensionality of  and  this means that for a great variety of hypotheses a practitioner can compute the likelihood ratio  for the data and compare  to the  value corresponding to a desired statistical significance as an approximate statistical test\u000a\u000a\u000a examplesedit \u000a\u000a\u000a coin tossingedit \u000aan example in the case of pearsons test we might try to compare two coins to determine whether they have the same probability of coming up heads our observation can be put into a contingency table with rows corresponding to the coin and columns corresponding to heads or tails the elements of the contingency table will be the number of times the coin for that row came up heads or tails the contents of this table are our observation \u000ahere  consists of the possible combinations of values of the parameters    and  which are the probability that coins 1 and 2 come up heads or tails in what follows  and  the hypothesis space  is constrained by the usual constraints on a probability distribution  and  the space of the null hypothesis  is the subspace where  writing  for the best values for  under the hypothesis  the maximum likelihood estimate is given by\u000a\u000asimilarly the maximum likelihood estimates of  under the null hypothesis  are given by\u000a\u000awhich does not depend on the coin \u000athe hypothesis and null hypothesis can be rewritten slightly so that they satisfy the constraints for the logarithm of the likelihood ratio to have the desired nice distribution since the constraint causes the twodimensional  to be reduced to the onedimensional  the asymptotic distribution for the test will be  the  distribution with one degree of freedom\u000afor the general contingency table we can write the loglikelihood ratio statistic as\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000apractical application of likelihood ratio test described\u000ar package walds sequential probability ratio test\u000arichard lowrys predictive values and likelihood ratios online clinical calculator
p422
sg32
g35
sg37
NsbsS'berkson_error_model.txt'
p423
g2
(g3
g4
Ntp424
Rp425
(dp426
g8
g11
sg12
Vthe berkson error model is a description of random error or misclassification in measurement unlike classical error berkson error causes little or no bias in the measurement it was proposed by joseph berkson in a paper entitled are there two regressions  published in 1950\u000aan example of berkson error arises in exposure assessment in epidemiological studies berkson error may predominate over classical error in cases where exposure data are highly aggregated while this kind of error reduces the power of a study risk estimates themselves are not themselves attenuated as would be the case where random error predominates\u000a\u000a\u000a
p427
sg14
g17
sg18
Vthe berkson error model is a description of random error or misclassification in measurement unlike classical error berkson error causes little or no bias in the measurement it was proposed by joseph berkson in a paper entitled are there two regressions  published in 1950\u000aan example of berkson error arises in exposure assessment in epidemiological studies berkson error may predominate over classical error in cases where exposure data are highly aggregated while this kind of error reduces the power of a study risk estimates themselves are not themselves attenuated as would be the case where random error predominates
p428
sg20
g23
sg24
g27
sg30
Vthe berkson error model is a description of random error or misclassification in measurement unlike classical error berkson error causes little or no bias in the measurement it was proposed by joseph berkson in a paper entitled are there two regressions  published in 1950\u000aan example of berkson error arises in exposure assessment in epidemiological studies berkson error may predominate over classical error in cases where exposure data are highly aggregated while this kind of error reduces the power of a study risk estimates themselves are not themselves attenuated as would be the case where random error predominates\u000a\u000a\u000a
p429
sg32
g35
sg37
NsbsS'statistical_population.txt'
p430
g2
(g3
g4
Ntp431
Rp432
(dp433
g8
g11
sg12
Vin statistics a population is a complete set of items that share at least one property in common that is the subject of a statistical analysis for example the population of german people share a common geographic origin language literature and genetic heritage among other traits that distinguish them from people of different nationalities as another example the milky way galaxy comprises a star population in contrast a statistical sample is a subset drawn from the population to represent the population in a statistical analysis if a sample is chosen properly characteristics of the entire population that the sample is drawn from can be inferred from corresponding characteristics of the sample\u000a\u000a\u000a subpopulationedit \u000aa subset of a population is called a subpopulation if they share one or more additional properties for example if the population is all german people a subpopulation is all german males if the population is all pharmacies in the world a subpopulation is all pharmacies in egypt\u000ain contrast a subset of a population that does not require the sharing of any additional property is called a sample\u000adescriptive statistics may yield different results for different subpopulations for instance a particular medicine may have different effects on different subpopulations and these effects may be obscured or dismissed if such special subpopulations are not identified and examined in isolation\u000asimilarly one can often estimate parameters more accurately if one separates out subpopulations the distribution of heights among people is better modeled by considering men and women as separate subpopulations for instance\u000apopulations consisting of subpopulations can be modeled by mixture models which combine the distributions within subpopulations into an overall population distribution even if subpopulations are wellmodeled by given simple models the overall population may be poorly fit by a given simple model  poor fit may be evidence for existence of subpopulations for example given two equal subpopulations both normally distributed if they have the same standard deviation and different means the overall distribution will exhibit low kurtosis relative to a single normal distribution  the means of the subpopulations fall on the shoulders of the overall distribution if sufficiently separated these form a bimodal distribution otherwise it simply has a wide peak further it will exhibit overdispersion relative to a single normal distribution with the given variation alternatively given two subpopulations with the same mean and different standard deviations the overall population will exhibit high kurtosis with a sharper peak and heavier tails and correspondingly shallower shoulders than a single distribution\u000a\u000a\u000a see alsoedit \u000asample statistics\u000asampling statistics\u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000astatistical terms made simple
p434
sg14
g17
sg18
Vin statistics a population is a complete set of items that share at least one property in common that is the subject of a statistical analysis for example the population of german people share a common geographic origin language literature and genetic heritage among other traits that distinguish them from people of different nationalities as another example the milky way galaxy comprises a star population in contrast a statistical sample is a subset drawn from the population to represent the population in a statistical analysis if a sample is chosen properly characteristics of the entire population that the sample is drawn from can be inferred from corresponding characteristics of the sample\u000a\u000a\u000a subpopulationedit \u000aa subset of a population is called a subpopulation if they share one or more additional properties for example if the population is all german people a subpopulation is all german males if the population is all pharmacies in the world a subpopulation is all pharmacies in egypt\u000ain contrast a subset of a population that does not require the sharing of any additional property is called a sample\u000adescriptive statistics may yield different results for different subpopulations for instance a particular medicine may have different effects on different subpopulations and these effects may be obscured or dismissed if such special subpopulations are not identified and examined in isolation\u000asimilarly one can often estimate parameters more accurately if one separates out subpopulations the distribution of heights among people is better modeled by considering men and women as separate subpopulations for instance\u000apopulations consisting of subpopulations can be modeled by mixture models which combine the distributions within subpopulations into an overall population distribution even if subpopulations are wellmodeled by given simple models the overall population may be poorly fit by a given simple model  poor fit may be evidence for existence of subpopulations for example given two equal subpopulations both normally distributed if they have the same standard deviation and different means the overall distribution will exhibit low kurtosis relative to a single normal distribution  the means of the subpopulations fall on the shoulders of the overall distribution if sufficiently separated these form a bimodal distribution otherwise it simply has a wide peak further it will exhibit overdispersion relative to a single normal distribution with the given variation alternatively given two subpopulations with the same mean and different standard deviations the overall population will exhibit high kurtosis with a sharper peak and heavier tails and correspondingly shallower shoulders than a single distribution\u000a\u000a\u000a see alsoedit \u000asample statistics\u000asampling statistics\u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000astatistical terms made simple
p435
sg20
g23
sg24
g27
sg30
Vin statistics a population is a complete set of items that share at least one property in common that is the subject of a statistical analysis for example the population of german people share a common geographic origin language literature and genetic heritage among other traits that distinguish them from people of different nationalities as another example the milky way galaxy comprises a star population in contrast a statistical sample is a subset drawn from the population to represent the population in a statistical analysis if a sample is chosen properly characteristics of the entire population that the sample is drawn from can be inferred from corresponding characteristics of the sample\u000a\u000a\u000a subpopulationedit \u000aa subset of a population is called a subpopulation if they share one or more additional properties for example if the population is all german people a subpopulation is all german males if the population is all pharmacies in the world a subpopulation is all pharmacies in egypt\u000ain contrast a subset of a population that does not require the sharing of any additional property is called a sample\u000adescriptive statistics may yield different results for different subpopulations for instance a particular medicine may have different effects on different subpopulations and these effects may be obscured or dismissed if such special subpopulations are not identified and examined in isolation\u000asimilarly one can often estimate parameters more accurately if one separates out subpopulations the distribution of heights among people is better modeled by considering men and women as separate subpopulations for instance\u000apopulations consisting of subpopulations can be modeled by mixture models which combine the distributions within subpopulations into an overall population distribution even if subpopulations are wellmodeled by given simple models the overall population may be poorly fit by a given simple model  poor fit may be evidence for existence of subpopulations for example given two equal subpopulations both normally distributed if they have the same standard deviation and different means the overall distribution will exhibit low kurtosis relative to a single normal distribution  the means of the subpopulations fall on the shoulders of the overall distribution if sufficiently separated these form a bimodal distribution otherwise it simply has a wide peak further it will exhibit overdispersion relative to a single normal distribution with the given variation alternatively given two subpopulations with the same mean and different standard deviations the overall population will exhibit high kurtosis with a sharper peak and heavier tails and correspondingly shallower shoulders than a single distribution\u000a\u000a\u000a see alsoedit \u000asample statistics\u000asampling statistics\u000a\u000a\u000a referencesedit \u000a\u000a\u000a external linksedit \u000astatistical terms made simple
p436
sg32
g35
sg37
NsbsS'semiparametric_model.txt'
p437
g2
(g3
g4
Ntp438
Rp439
(dp440
g8
g11
sg12
Vin statistics a semiparametric model is a model that has parametric and nonparametric components\u000aa model is a collection of distributions  indexed by a parameter \u000aa parametric model is one in which the indexing parameter is a finitedimensional vector in dimensional euclidean space for some integer  ie the set of possible values for  is a subset of  or  in this case we say that  is finitedimensional\u000ain nonparametric models the set of possible values of the parameter  is a subset of some space not necessarily finitedimensional for example we might consider the set of all distributions with mean 0 such spaces are vector spaces with topological structure but may not be finitedimensional as vector spaces thus  for some possibly infinitedimensional space \u000ain semiparametric models the parameter has both a finitedimensional component and an infinitedimensional component often a realvalued function defined on the real line thus the parameter space  in a semiparametric model satisfies  where  is an infinitedimensional space\u000ait may appear at first that semiparametric models include nonparametric models since they have an infinitedimensional as well as a finitedimensional component however a semiparametric model is considered to be smaller than a completely nonparametric model because we are often interested only in the finitedimensional component of  that is we are not interested in estimating the infinitedimensional component in nonparametric models by contrast the primary interest is in estimating the infinitedimensional parameter thus the estimation task is statistically harder in nonparametric models\u000athese models often use smoothing or kernels\u000a\u000a\u000a exampleedit \u000aa wellknown example of a semiparametric model is the cox proportional hazards model if we are interested in studying the time  to an event such as death due to cancer or failure of a light bulb the cox model specifies the following distribution function for \u000a\u000awhere  is the covariate vector and  and  are unknown parameters  here  is finitedimensional and is of interest  is an unknown nonnegative function of time known as the baseline hazard function and is often a nuisance parameter the collection of possible candidates for  is infinitedimensional\u000a\u000a\u000a see alsoedit \u000asemiparametric regression\u000astatistical model\u000a\u000a\u000a referencesedit 
p441
sg14
g17
sg18
Vin statistics a semiparametric model is a model that has parametric and nonparametric components\u000aa model is a collection of distributions  indexed by a parameter \u000aa parametric model is one in which the indexing parameter is a finitedimensional vector in dimensional euclidean space for some integer  ie the set of possible values for  is a subset of  or  in this case we say that  is finitedimensional\u000ain nonparametric models the set of possible values of the parameter  is a subset of some space not necessarily finitedimensional for example we might consider the set of all distributions with mean 0 such spaces are vector spaces with topological structure but may not be finitedimensional as vector spaces thus  for some possibly infinitedimensional space \u000ain semiparametric models the parameter has both a finitedimensional component and an infinitedimensional component often a realvalued function defined on the real line thus the parameter space  in a semiparametric model satisfies  where  is an infinitedimensional space\u000ait may appear at first that semiparametric models include nonparametric models since they have an infinitedimensional as well as a finitedimensional component however a semiparametric model is considered to be smaller than a completely nonparametric model because we are often interested only in the finitedimensional component of  that is we are not interested in estimating the infinitedimensional component in nonparametric models by contrast the primary interest is in estimating the infinitedimensional parameter thus the estimation task is statistically harder in nonparametric models\u000athese models often use smoothing or kernels\u000a\u000a\u000a exampleedit \u000aa wellknown example of a semiparametric model is the cox proportional hazards model if we are interested in studying the time  to an event such as death due to cancer or failure of a light bulb the cox model specifies the following distribution function for \u000a\u000awhere  is the covariate vector and  and  are unknown parameters  here  is finitedimensional and is of interest  is an unknown nonnegative function of time known as the baseline hazard function and is often a nuisance parameter the collection of possible candidates for  is infinitedimensional\u000a\u000a\u000a see alsoedit \u000asemiparametric regression\u000astatistical model\u000a\u000a\u000a referencesedit
p442
sg20
g23
sg24
g27
sg30
Vin statistics a semiparametric model is a model that has parametric and nonparametric components\u000aa model is a collection of distributions  indexed by a parameter \u000aa parametric model is one in which the indexing parameter is a finitedimensional vector in dimensional euclidean space for some integer  ie the set of possible values for  is a subset of  or  in this case we say that  is finitedimensional\u000ain nonparametric models the set of possible values of the parameter  is a subset of some space not necessarily finitedimensional for example we might consider the set of all distributions with mean 0 such spaces are vector spaces with topological structure but may not be finitedimensional as vector spaces thus  for some possibly infinitedimensional space \u000ain semiparametric models the parameter has both a finitedimensional component and an infinitedimensional component often a realvalued function defined on the real line thus the parameter space  in a semiparametric model satisfies  where  is an infinitedimensional space\u000ait may appear at first that semiparametric models include nonparametric models since they have an infinitedimensional as well as a finitedimensional component however a semiparametric model is considered to be smaller than a completely nonparametric model because we are often interested only in the finitedimensional component of  that is we are not interested in estimating the infinitedimensional component in nonparametric models by contrast the primary interest is in estimating the infinitedimensional parameter thus the estimation task is statistically harder in nonparametric models\u000athese models often use smoothing or kernels\u000a\u000a\u000a exampleedit \u000aa wellknown example of a semiparametric model is the cox proportional hazards model if we are interested in studying the time  to an event such as death due to cancer or failure of a light bulb the cox model specifies the following distribution function for \u000a\u000awhere  is the covariate vector and  and  are unknown parameters  here  is finitedimensional and is of interest  is an unknown nonnegative function of time known as the baseline hazard function and is often a nuisance parameter the collection of possible candidates for  is infinitedimensional\u000a\u000a\u000a see alsoedit \u000asemiparametric regression\u000astatistical model\u000a\u000a\u000a referencesedit 
p443
sg32
g35
sg37
NsbsS'mathematical_statistics.txt'
p444
g2
(g3
g4
Ntp445
Rp446
(dp447
g8
g11
sg12
Vmathematical statistics is the application of mathematics to statistics which was originally conceived as the science of the state  the collection and analysis of facts about a country its economy land military population and so forth mathematical techniques which are used for this include mathematical analysis linear algebra stochastic analysis differential equations and measuretheoretic probability theory\u000a\u000a\u000a introductionedit \u000astatistical science is concerned with the planning of studies especially with the design of randomized experiments and with the planning of surveys using random sampling the initial analysis of the data from properly randomized studies often follows the study protocol\u000aof course the data from a randomized study can be analyzed to consider secondary hypotheses or to suggest new ideas a secondary analysis of the data from a planned study uses tools from data analysis\u000adata analysis is divided into\u000adescriptive statistics  the part of statistics that describes data ie summarises the data and their typical properties\u000ainferential statistics  the part of statistics that draws conclusions from data using some model for the data for example inferential statistics involves selecting a model for the data checking whether the data fulfill the conditions of a particular model and with quantifying the involved uncertainty eg using confidence intervals\u000awhile the tools of data analysis work best on data from randomized studies they are also applied to other kinds of data  for example from natural experiments and observational studies in which case the inference is dependent on the model chosen by the statistician and so subjective\u000amathematical statistics has been inspired by and has extended many options in applied statistics\u000a\u000a\u000a topicsedit \u000athe following are some of the important topics in mathematical statistics\u000a\u000a\u000a probability distributionsedit \u000a\u000aa probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment survey or procedure of statistical inference examples are found in experiments whose sample space is nonnumerical where the distribution would be a categorical distribution experiments whose sample space is encoded by discrete random variables where the distribution can be specified by a probability mass function and experiments with sample spaces encoded by continuous random variables where the distribution can be specified by a probability density function more complex experiments such as those involving stochastic processes defined in continuous time may demand the use of more general probability measures\u000aa probability distribution can either be univariate or multivariate a univariate distribution gives the probabilities of a single random variable taking on various alternative values a multivariate distribution a joint probability distribution gives the probabilities of a random vectora set of two or more random variablestaking on various combinations of values important and commonly encountered univariate probability distributions include the binomial distribution the hypergeometric distribution and the normal distribution the multivariate normal distribution is a commonly encountered multivariate distribution\u000a\u000a\u000a special distributionsedit \u000anormal distribution gaussian distribution the most common continuous distribution\u000abernoulli distribution for the outcome of a single bernoulli trial eg successfailure yesno\u000abinomial distribution for the number of positive occurrences eg successes yes votes etc given a fixed total number of independent occurrences\u000anegative binomial distribution for binomialtype observations but where the quantity of interest is the number of failures before a given number of successes occurs\u000ageometric distribution for binomialtype observations but where the quantity of interest is the number of failures before the first success a special cdiscrete uniform distribution for a finite set of values eg the outcome of a fair die\u000acontinuous uniform distribution for continuously distributed values\u000apoisson distribution for the number of occurrences of a poissontype event in a given period of time\u000aexponential distribution for the time before the next poissontype event occurs\u000agamma distribution for the time before the next k poissontype events occur\u000achisquared distribution the distribution of a sum of squared standard normal variables useful eg for inference regarding the sample variance of normally distributed samples see chisquared test\u000astudents t distribution the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable useful for inference regarding the mean of normally distributed samples with unknown variance see students ttest\u000abeta distribution for a single probability real number between 0 and 1 conjugate to the bernoulli distribution and binomial distribution\u000a\u000a\u000a statistical inferencesedit \u000a\u000astatistical inference is the process of drawing conclusions from data that are subject to random variation for example observational errors or sampling variation initial requirements of such a system of procedures for inference and induction are that the system should produce reasonable answers when applied to welldefined situations and that it should be general enough to be applied across a range of situations inferential statistics are used to test hypotheses and make estimations using sample data whereas descriptive statistics describe a sample inferential statistics infer predictions about a larger population that the sample represents\u000athe outcome of statistical inference may be an answer to the question what should be done next where this might be a decision about making further experiments or surveys or about drawing a conclusion before implementing some organizational or governmental policy for the most part statistical inference makes propositions about populations using data drawn from the population of interest via some form of random sampling more generally data about a random process is obtained from its observed behavior during a finite period of time given a parameter or hypothesis about which one wishes to make inference statistical inference most often uses\u000aa statistical model of the random process that is supposed to generate the data which is known when randomization has been used and\u000aa particular realization of the random process ie a set of data\u000a\u000a\u000a regressionedit \u000a\u000ain statistics regression analysis is a statistical process for estimating the relationships among variables it includes many techniques for modeling and analyzing several variables when the focus is on the relationship between a dependent variable and one or more independent variables more specifically regression analysis helps one understand how the typical value of the dependent variable or criterion variable changes when any one of the independent variables is varied while the other independent variables are held fixed most commonly regression analysis estimates the conditional expectation of the dependent variable given the independent variables  that is the average value of the dependent variable when the independent variables are fixed less commonly the focus is on a quantile or other location parameter of the conditional distribution of the dependent variable given the independent variables in all cases the estimation target is a function of the independent variables called the regression function in regression analysis it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution\u000amany techniques for carrying out regression analysis have been developed familiar methods such as linear regression and ordinary least squares regression are parametric in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions which may be infinitedimensional\u000a\u000a\u000a nonparametric statisticsedit \u000a\u000anonparametric statistics are statistics not based on parameterized families of probability distributions they include both descriptive and inferential statistics the typical parameters are the mean variance etc unlike parametric statistics nonparametric statistics make no assumptions about the probability distributions of the variables being assessed\u000anonparametric methods are widely used for studying populations that take on a ranked order such as movie reviews receiving one to four stars the use of nonparametric methods may be necessary when data have a ranking but no clear numerical interpretation such as when assessing preferences in terms of levels of measurement nonparametric methods result in ordinal data\u000aas nonparametric methods make fewer assumptions their applicability is much wider than the corresponding parametric methods in particular they may be applied in situations where less is known about the application in question also due to the reliance on fewer assumptions nonparametric methods are more robust\u000aanother justification for the use of nonparametric methods is simplicity in certain cases even when the use of parametric methods is justified nonparametric methods may be easier to use due both to this simplicity and to their greater robustness nonparametric methods are seen by some statisticians as leaving less room for improper use and misunderstanding\u000a\u000a\u000a statistics mathematics and mathematical statisticsedit \u000amathematical statistics has substantial overlap with the discipline of statistics statistical theorists study and improve statistical procedures with mathematics and statistical research often raises mathematical questions statistical theory relies on probability and decision theory\u000amathematicians and statisticians like gauss laplace and c s peirce used decision theory with probability distributions and loss functions or utility functions the decisiontheoretic approach to statistical inference was reinvigorated by abraham wald and his successors and makes extensive use of scientific computing analysis and optimization for the design of experiments statisticians use algebra and combinatorics\u000a\u000a\u000a see alsoedit \u000aasymptotic theory statistics\u000a\u000a\u000a referencesedit \u000a lakshmikantham ed by d kannan v 2002 handbook of stochastic analysis and applications new york m dekker isbn 0824706609 \u000a schervish mark j 1995 theory of statistics corr 2nd print ed new york springer isbn 0387945466 \u000a freedman da 2005 statistical models theory and practice cambridge university press isbn 9780521671057\u000a hogg r v a craig and j w mckean intro to mathematical statistics 2005\u000a larsen richard j and marx morris l an introduction to mathematical statistics and its applications 2012 prentice hall\u000a upton g cook i 2008 oxford dictionary of statistics oup isbn 9780199541454\u000a wald abraham 1947 sequential analysis new york john wiley and sons isbn 0471918067 see dover reprint 2004 isbn 0486439127 \u000a wald abraham 1950 statistical decision functions john wiley and sons new york \u000a lehmann erich 1997 testing statistical hypotheses 2nd ed isbn 0387949194 \u000a lehmann erich cassella george 1998 theory of point estimation 2nd ed isbn 0387985026 \u000a bickel peter j doksum kjell a 2001 mathematical statistics basic and selected topics 1 second updated printing 2007 ed pearson prenticehall \u000a le cam lucien 1986 asymptotic methods in statistical decision theory springerverlag isbn 0387963073 \u000a liese friedrich and miescke klausj 2008 statistical decision theory estimation testing and selection springer \u000a\u000a\u000a additional readingedit \u000aborovkov a a 1999 mathematical statistics crc press isbn 9056990187\u000avirtual laboratories in probability and statistics univ of alahuntsville\u000astatibot interactive online expert system on statistical tests
p448
sg14
g17
sg18
Vmathematical statistics is the application of mathematics to statistics which was originally conceived as the science of the state  the collection and analysis of facts about a country its economy land military population and so forth mathematical techniques which are used for this include mathematical analysis linear algebra stochastic analysis differential equations and measuretheoretic probability theory\u000a\u000a\u000a introductionedit \u000astatistical science is concerned with the planning of studies especially with the design of randomized experiments and with the planning of surveys using random sampling the initial analysis of the data from properly randomized studies often follows the study protocol\u000aof course the data from a randomized study can be analyzed to consider secondary hypotheses or to suggest new ideas a secondary analysis of the data from a planned study uses tools from data analysis\u000adata analysis is divided into\u000adescriptive statistics  the part of statistics that describes data ie summarises the data and their typical properties\u000ainferential statistics  the part of statistics that draws conclusions from data using some model for the data for example inferential statistics involves selecting a model for the data checking whether the data fulfill the conditions of a particular model and with quantifying the involved uncertainty eg using confidence intervals\u000awhile the tools of data analysis work best on data from randomized studies they are also applied to other kinds of data  for example from natural experiments and observational studies in which case the inference is dependent on the model chosen by the statistician and so subjective\u000amathematical statistics has been inspired by and has extended many options in applied statistics\u000a\u000a\u000a topicsedit \u000athe following are some of the important topics in mathematical statistics\u000a\u000a\u000a probability distributionsedit \u000a\u000aa probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment survey or procedure of statistical inference examples are found in experiments whose sample space is nonnumerical where the distribution would be a categorical distribution experiments whose sample space is encoded by discrete random variables where the distribution can be specified by a probability mass function and experiments with sample spaces encoded by continuous random variables where the distribution can be specified by a probability density function more complex experiments such as those involving stochastic processes defined in continuous time may demand the use of more general probability measures\u000aa probability distribution can either be univariate or multivariate a univariate distribution gives the probabilities of a single random variable taking on various alternative values a multivariate distribution a joint probability distribution gives the probabilities of a random vectora set of two or more random variablestaking on various combinations of values important and commonly encountered univariate probability distributions include the binomial distribution the hypergeometric distribution and the normal distribution the multivariate normal distribution is a commonly encountered multivariate distribution\u000a\u000a\u000a special distributionsedit \u000anormal distribution gaussian distribution the most common continuous distribution\u000abernoulli distribution for the outcome of a single bernoulli trial eg successfailure yesno\u000abinomial distribution for the number of positive occurrences eg successes yes votes etc given a fixed total number of independent occurrences\u000anegative binomial distribution for binomialtype observations but where the quantity of interest is the number of failures before a given number of successes occurs\u000ageometric distribution for binomialtype observations but where the quantity of interest is the number of failures before the first success a special cdiscrete uniform distribution for a finite set of values eg the outcome of a fair die\u000acontinuous uniform distribution for continuously distributed values\u000apoisson distribution for the number of occurrences of a poissontype event in a given period of time\u000aexponential distribution for the time before the next poissontype event occurs\u000agamma distribution for the time before the next k poissontype events occur\u000achisquared distribution the distribution of a sum of squared standard normal variables useful eg for inference regarding the sample variance of normally distributed samples see chisquared test\u000astudents t distribution the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable useful for inference regarding the mean of normally distributed samples with unknown variance see students ttest\u000abeta distribution for a single probability real number between 0 and 1 conjugate to the bernoulli distribution and binomial distribution\u000a\u000a\u000a statistical inferencesedit \u000a\u000astatistical inference is the process of drawing conclusions from data that are subject to random variation for example observational errors or sampling variation initial requirements of such a system of procedures for inference and induction are that the system should produce reasonable answers when applied to welldefined situations and that it should be general enough to be applied across a range of situations inferential statistics are used to test hypotheses and make estimations using sample data whereas descriptive statistics describe a sample inferential statistics infer predictions about a larger population that the sample represents\u000athe outcome of statistical inference may be an answer to the question what should be done next where this might be a decision about making further experiments or surveys or about drawing a conclusion before implementing some organizational or governmental policy for the most part statistical inference makes propositions about populations using data drawn from the population of interest via some form of random sampling more generally data about a random process is obtained from its observed behavior during a finite period of time given a parameter or hypothesis about which one wishes to make inference statistical inference most often uses\u000aa statistical model of the random process that is supposed to generate the data which is known when randomization has been used and\u000aa particular realization of the random process ie a set of data\u000a\u000a\u000a regressionedit \u000a\u000ain statistics regression analysis is a statistical process for estimating the relationships among variables it includes many techniques for modeling and analyzing several variables when the focus is on the relationship between a dependent variable and one or more independent variables more specifically regression analysis helps one understand how the typical value of the dependent variable or criterion variable changes when any one of the independent variables is varied while the other independent variables are held fixed most commonly regression analysis estimates the conditional expectation of the dependent variable given the independent variables  that is the average value of the dependent variable when the independent variables are fixed less commonly the focus is on a quantile or other location parameter of the conditional distribution of the dependent variable given the independent variables in all cases the estimation target is a function of the independent variables called the regression function in regression analysis it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution\u000amany techniques for carrying out regression analysis have been developed familiar methods such as linear regression and ordinary least squares regression are parametric in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions which may be infinitedimensional\u000a\u000a\u000a nonparametric statisticsedit \u000a\u000anonparametric statistics are statistics not based on parameterized families of probability distributions they include both descriptive and inferential statistics the typical parameters are the mean variance etc unlike parametric statistics nonparametric statistics make no assumptions about the probability distributions of the variables being assessed\u000anonparametric methods are widely used for studying populations that take on a ranked order such as movie reviews receiving one to four stars the use of nonparametric methods may be necessary when data have a ranking but no clear numerical interpretation such as when assessing preferences in terms of levels of measurement nonparametric methods result in ordinal data\u000aas nonparametric methods make fewer assumptions their applicability is much wider than the corresponding parametric methods in particular they may be applied in situations where less is known about the application in question also due to the reliance on fewer assumptions nonparametric methods are more robust\u000aanother justification for the use of nonparametric methods is simplicity in certain cases even when the use of parametric methods is justified nonparametric methods may be easier to use due both to this simplicity and to their greater robustness nonparametric methods are seen by some statisticians as leaving less room for improper use and misunderstanding\u000a\u000a\u000a statistics mathematics and mathematical statisticsedit \u000amathematical statistics has substantial overlap with the discipline of statistics statistical theorists study and improve statistical procedures with mathematics and statistical research often raises mathematical questions statistical theory relies on probability and decision theory\u000amathematicians and statisticians like gauss laplace and c s peirce used decision theory with probability distributions and loss functions or utility functions the decisiontheoretic approach to statistical inference was reinvigorated by abraham wald and his successors and makes extensive use of scientific computing analysis and optimization for the design of experiments statisticians use algebra and combinatorics\u000a\u000a\u000a see alsoedit \u000aasymptotic theory statistics\u000a\u000a\u000a referencesedit \u000a lakshmikantham ed by d kannan v 2002 handbook of stochastic analysis and applications new york m dekker isbn 0824706609 \u000a schervish mark j 1995 theory of statistics corr 2nd print ed new york springer isbn 0387945466 \u000a freedman da 2005 statistical models theory and practice cambridge university press isbn 9780521671057\u000a hogg r v a craig and j w mckean intro to mathematical statistics 2005\u000a larsen richard j and marx morris l an introduction to mathematical statistics and its applications 2012 prentice hall\u000a upton g cook i 2008 oxford dictionary of statistics oup isbn 9780199541454\u000a wald abraham 1947 sequential analysis new york john wiley and sons isbn 0471918067 see dover reprint 2004 isbn 0486439127 \u000a wald abraham 1950 statistical decision functions john wiley and sons new york \u000a lehmann erich 1997 testing statistical hypotheses 2nd ed isbn 0387949194 \u000a lehmann erich cassella george 1998 theory of point estimation 2nd ed isbn 0387985026 \u000a bickel peter j doksum kjell a 2001 mathematical statistics basic and selected topics 1 second updated printing 2007 ed pearson prenticehall \u000a le cam lucien 1986 asymptotic methods in statistical decision theory springerverlag isbn 0387963073 \u000a liese friedrich and miescke klausj 2008 statistical decision theory estimation testing and selection springer \u000a\u000a\u000a additional readingedit \u000aborovkov a a 1999 mathematical statistics crc press isbn 9056990187\u000avirtual laboratories in probability and statistics univ of alahuntsville\u000astatibot interactive online expert system on statistical tests
p449
sg20
g23
sg24
g27
sg30
Vmathematical statistics is the application of mathematics to statistics which was originally conceived as the science of the state  the collection and analysis of facts about a country its economy land military population and so forth mathematical techniques which are used for this include mathematical analysis linear algebra stochastic analysis differential equations and measuretheoretic probability theory\u000a\u000a\u000a introductionedit \u000astatistical science is concerned with the planning of studies especially with the design of randomized experiments and with the planning of surveys using random sampling the initial analysis of the data from properly randomized studies often follows the study protocol\u000aof course the data from a randomized study can be analyzed to consider secondary hypotheses or to suggest new ideas a secondary analysis of the data from a planned study uses tools from data analysis\u000adata analysis is divided into\u000adescriptive statistics  the part of statistics that describes data ie summarises the data and their typical properties\u000ainferential statistics  the part of statistics that draws conclusions from data using some model for the data for example inferential statistics involves selecting a model for the data checking whether the data fulfill the conditions of a particular model and with quantifying the involved uncertainty eg using confidence intervals\u000awhile the tools of data analysis work best on data from randomized studies they are also applied to other kinds of data  for example from natural experiments and observational studies in which case the inference is dependent on the model chosen by the statistician and so subjective\u000amathematical statistics has been inspired by and has extended many options in applied statistics\u000a\u000a\u000a topicsedit \u000athe following are some of the important topics in mathematical statistics\u000a\u000a\u000a probability distributionsedit \u000a\u000aa probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment survey or procedure of statistical inference examples are found in experiments whose sample space is nonnumerical where the distribution would be a categorical distribution experiments whose sample space is encoded by discrete random variables where the distribution can be specified by a probability mass function and experiments with sample spaces encoded by continuous random variables where the distribution can be specified by a probability density function more complex experiments such as those involving stochastic processes defined in continuous time may demand the use of more general probability measures\u000aa probability distribution can either be univariate or multivariate a univariate distribution gives the probabilities of a single random variable taking on various alternative values a multivariate distribution a joint probability distribution gives the probabilities of a random vectora set of two or more random variablestaking on various combinations of values important and commonly encountered univariate probability distributions include the binomial distribution the hypergeometric distribution and the normal distribution the multivariate normal distribution is a commonly encountered multivariate distribution\u000a\u000a\u000a special distributionsedit \u000anormal distribution gaussian distribution the most common continuous distribution\u000abernoulli distribution for the outcome of a single bernoulli trial eg successfailure yesno\u000abinomial distribution for the number of positive occurrences eg successes yes votes etc given a fixed total number of independent occurrences\u000anegative binomial distribution for binomialtype observations but where the quantity of interest is the number of failures before a given number of successes occurs\u000ageometric distribution for binomialtype observations but where the quantity of interest is the number of failures before the first success a special cdiscrete uniform distribution for a finite set of values eg the outcome of a fair die\u000acontinuous uniform distribution for continuously distributed values\u000apoisson distribution for the number of occurrences of a poissontype event in a given period of time\u000aexponential distribution for the time before the next poissontype event occurs\u000agamma distribution for the time before the next k poissontype events occur\u000achisquared distribution the distribution of a sum of squared standard normal variables useful eg for inference regarding the sample variance of normally distributed samples see chisquared test\u000astudents t distribution the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable useful for inference regarding the mean of normally distributed samples with unknown variance see students ttest\u000abeta distribution for a single probability real number between 0 and 1 conjugate to the bernoulli distribution and binomial distribution\u000a\u000a\u000a statistical inferencesedit \u000a\u000astatistical inference is the process of drawing conclusions from data that are subject to random variation for example observational errors or sampling variation initial requirements of such a system of procedures for inference and induction are that the system should produce reasonable answers when applied to welldefined situations and that it should be general enough to be applied across a range of situations inferential statistics are used to test hypotheses and make estimations using sample data whereas descriptive statistics describe a sample inferential statistics infer predictions about a larger population that the sample represents\u000athe outcome of statistical inference may be an answer to the question what should be done next where this might be a decision about making further experiments or surveys or about drawing a conclusion before implementing some organizational or governmental policy for the most part statistical inference makes propositions about populations using data drawn from the population of interest via some form of random sampling more generally data about a random process is obtained from its observed behavior during a finite period of time given a parameter or hypothesis about which one wishes to make inference statistical inference most often uses\u000aa statistical model of the random process that is supposed to generate the data which is known when randomization has been used and\u000aa particular realization of the random process ie a set of data\u000a\u000a\u000a regressionedit \u000a\u000ain statistics regression analysis is a statistical process for estimating the relationships among variables it includes many techniques for modeling and analyzing several variables when the focus is on the relationship between a dependent variable and one or more independent variables more specifically regression analysis helps one understand how the typical value of the dependent variable or criterion variable changes when any one of the independent variables is varied while the other independent variables are held fixed most commonly regression analysis estimates the conditional expectation of the dependent variable given the independent variables  that is the average value of the dependent variable when the independent variables are fixed less commonly the focus is on a quantile or other location parameter of the conditional distribution of the dependent variable given the independent variables in all cases the estimation target is a function of the independent variables called the regression function in regression analysis it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution\u000amany techniques for carrying out regression analysis have been developed familiar methods such as linear regression and ordinary least squares regression are parametric in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions which may be infinitedimensional\u000a\u000a\u000a nonparametric statisticsedit \u000a\u000anonparametric statistics are statistics not based on parameterized families of probability distributions they include both descriptive and inferential statistics the typical parameters are the mean variance etc unlike parametric statistics nonparametric statistics make no assumptions about the probability distributions of the variables being assessed\u000anonparametric methods are widely used for studying populations that take on a ranked order such as movie reviews receiving one to four stars the use of nonparametric methods may be necessary when data have a ranking but no clear numerical interpretation such as when assessing preferences in terms of levels of measurement nonparametric methods result in ordinal data\u000aas nonparametric methods make fewer assumptions their applicability is much wider than the corresponding parametric methods in particular they may be applied in situations where less is known about the application in question also due to the reliance on fewer assumptions nonparametric methods are more robust\u000aanother justification for the use of nonparametric methods is simplicity in certain cases even when the use of parametric methods is justified nonparametric methods may be easier to use due both to this simplicity and to their greater robustness nonparametric methods are seen by some statisticians as leaving less room for improper use and misunderstanding\u000a\u000a\u000a statistics mathematics and mathematical statisticsedit \u000amathematical statistics has substantial overlap with the discipline of statistics statistical theorists study and improve statistical procedures with mathematics and statistical research often raises mathematical questions statistical theory relies on probability and decision theory\u000amathematicians and statisticians like gauss laplace and c s peirce used decision theory with probability distributions and loss functions or utility functions the decisiontheoretic approach to statistical inference was reinvigorated by abraham wald and his successors and makes extensive use of scientific computing analysis and optimization for the design of experiments statisticians use algebra and combinatorics\u000a\u000a\u000a see alsoedit \u000aasymptotic theory statistics\u000a\u000a\u000a referencesedit \u000a lakshmikantham ed by d kannan v 2002 handbook of stochastic analysis and applications new york m dekker isbn 0824706609 \u000a schervish mark j 1995 theory of statistics corr 2nd print ed new york springer isbn 0387945466 \u000a freedman da 2005 statistical models theory and practice cambridge university press isbn 9780521671057\u000a hogg r v a craig and j w mckean intro to mathematical statistics 2005\u000a larsen richard j and marx morris l an introduction to mathematical statistics and its applications 2012 prentice hall\u000a upton g cook i 2008 oxford dictionary of statistics oup isbn 9780199541454\u000a wald abraham 1947 sequential analysis new york john wiley and sons isbn 0471918067 see dover reprint 2004 isbn 0486439127 \u000a wald abraham 1950 statistical decision functions john wiley and sons new york \u000a lehmann erich 1997 testing statistical hypotheses 2nd ed isbn 0387949194 \u000a lehmann erich cassella george 1998 theory of point estimation 2nd ed isbn 0387985026 \u000a bickel peter j doksum kjell a 2001 mathematical statistics basic and selected topics 1 second updated printing 2007 ed pearson prenticehall \u000a le cam lucien 1986 asymptotic methods in statistical decision theory springerverlag isbn 0387963073 \u000a liese friedrich and miescke klausj 2008 statistical decision theory estimation testing and selection springer \u000a\u000a\u000a additional readingedit \u000aborovkov a a 1999 mathematical statistics crc press isbn 9056990187\u000avirtual laboratories in probability and statistics univ of alahuntsville\u000astatibot interactive online expert system on statistical tests
p450
sg32
g35
sg37
NsbsS'principle_of_maximum_entropy.txt'
p451
g2
(g3
g4
Ntp452
Rp453
(dp454
g8
g11
sg12
Vthe principle of maximum entropy states that subject to precisely stated prior data such as a proposition that expresses testable information the probability distribution which best represents the current state of knowledge is the one with largest entropy\u000aanother way of stating this take precisely stated prior data or testable information about a probability distribution function consider the set of all trial probability distributions that would encode the prior data of those the one with maximal information entropy is the proper distribution according to this principle\u000a\u000a\u000a historyedit \u000athe principle was first expounded by e t jaynes in two papers in 1957 where he emphasized a natural correspondence between statistical mechanics and information theory in particular jaynes offered a new and very general rationale why the gibbsian method of statistical mechanics works he argued that the entropy of statistical mechanics and the information entropy of information theory are principally the same thing consequently statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory\u000a\u000a\u000a overviewedit \u000ain most practical cases the stated prior data or testable information is given by a set of conserved quantities average values of some moment functions associated with the probability distribution in question this is the way the maximum entropy principle is most often used in statistical thermodynamics another possibility is to prescribe some symmetries of the probability distribution the equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method\u000athe maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods statistical mechanics and logical inference in particular\u000athe maximum entropy principle makes explicit our freedom in using different forms of prior data as a special case a uniform prior probability density laplaces principle of indifference sometimes called the principle of insufficient reason may be adopted thus the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics but represents a significant conceptual generalization of those methods it means that thermodynamics systems need not be shown to be ergodic to justify treatment as a statistical ensemble\u000ain ordinary language the principle of maximum entropy can be said to express a claim of epistemic modesty or of maximum ignorance the selected distribution is the one that makes the least claim to being informed beyond the stated prior data that is to say the one that admits the most ignorance beyond the stated prior data\u000a\u000a\u000a testable informationedit \u000athe principle of maximum entropy is useful explicitly only when applied to testable information testable information is a statement about a probability distribution whose truth or falsity is welldefined for example the statements\u000athe expectation of the variable x is 287\u000aand\u000ap2  p3  06\u000awhere p2  p3 are probabilities of events are statements of testable information\u000agiven testable information the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy subject to the constraints of the information this constrained optimization problem is typically solved using the method of lagrange multipliers\u000aentropy maximization with no testable information respects the universal constraint that the sum of the probabilities is one under this constraint the maximum entropy discrete probability distribution is the uniform distribution\u000a\u000a\u000a applicationsedit \u000athe principle of maximum entropy is commonly applied in two ways to inferential problems\u000a\u000a\u000a prior probabilitiesedit \u000athe principle of maximum entropy is often used to obtain prior probability distributions for bayesian inference jaynes was a strong advocate of this approach claiming the maximum entropy distribution represented the least informative distribution a large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding\u000a\u000a\u000a maximum entropy modelsedit \u000aalternatively the principle is often invoked for model specification in this case the observed data itself is assumed to be the testable information such models are widely used in natural language processing an example of such a model is logistic regression which corresponds to the maximum entropy classifier for independent observations\u000a\u000a\u000a general solution for the maximum entropy distribution with linear constraintsedit \u000a\u000a\u000a discrete caseedit \u000awe have some testable information i about a quantity x taking values in x1 x2 xn we assume this information has the form of m constraints on the expectations of the functions fk that is we require our probability distribution to satisfy\u000a\u000afurthermore the probabilities must sum to one giving the constraint\u000a\u000athe probability distribution with maximum information entropy subject to these constraints is\u000a\u000ait is sometimes called the gibbs distribution the normalization constant is determined by\u000a\u000aand is conventionally called the partition function interestingly the pitmankoopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution\u000athe k parameters are lagrange multipliers whose particular values are determined by the constraints according to\u000a\u000athese m simultaneous equations do not generally possess a closed form solution and are usually solved by numerical methods\u000a\u000a\u000a continuous caseedit \u000afor continuous distributions the shannon entropy cannot be used as it is only defined for discrete probability spaces instead edwin jaynes 1963 1968 2003 gave the following formula which is closely related to the relative entropy see also differential entropy\u000a\u000awhere mx which jaynes called the invariant measure is proportional to the limiting density of discrete points for now we shall assume that m is known we will discuss it further after the solution equations are given\u000aa closely related quantity the relative entropy is usually defined as the kullbackleibler divergence of m from p although it is sometimes confusingly defined as the negative of this the inference principle of minimizing this due to kullback is known as the principle of minimum discrimination information\u000awe have some testable information i about a quantity x which takes values in some interval of the real numbers all integrals below are over this interval we assume this information has the form of m constraints on the expectations of the functions fk ie we require our probability density function to satisfy\u000a\u000aand of course the probability density must integrate to one giving the constraint\u000a\u000athe probability density function with maximum hc subject to these constraints is\u000a\u000awith the partition function determined by\u000a\u000aas in the discrete case the values of the  parameters are determined by the constraints according to\u000a\u000athe invariant measure function mx can be best understood by supposing that x is known to take values only in the bounded interval a b and that no other information is given then the maximum entropy probability density function is\u000a\u000awhere a is a normalization constant the invariant measure function is actually the prior density function encoding lack of relevant information it cannot be determined by the principle of maximum entropy and must be determined by some other logical method such as the principle of transformation groups or marginalization theory\u000a\u000a\u000a examplesedit \u000afor several examples of maximum entropy distributions see the article on maximum entropy probability distributions\u000a\u000a\u000a justifications for the principle of maximum entropyedit \u000aproponents of the principle of maximum entropy justify its use in assigning probabilities in several ways including the following two arguments these arguments take the use of bayesian probability as given and are thus subject to the same postulates\u000a\u000a\u000a information entropy as a measure of uninformativenessedit \u000aconsider a discrete probability distribution among m mutually exclusive propositions the most informative distribution would occur when one of the propositions was known to be true in that case the information entropy would be equal to zero the least informative distribution would occur when there is no reason to favor any one of the propositions over the others in that case the only reasonable probability distribution would be uniform and then the information entropy would be equal to its maximum possible value log m the information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is ranging from zero completely informative to log m completely uninformative\u000aby choosing to use the distribution with the maximum entropy allowed by our information the argument goes we are choosing the most uninformative distribution possible to choose a distribution with lower entropy would be to assume information we do not possess thus the maximum entropy distribution is the only reasonable distribution\u000a\u000a\u000a the wallis derivationedit \u000athe following argument is the result of a suggestion made by graham wallis to e t jaynes in 1962 it is essentially the same mathematical argument used for the maxwellboltzmann statistics in statistical mechanics although the conceptual emphasis is quite different it has the advantage of being strictly combinatorial in nature making no reference to information entropy as a measure of uncertainty uninformativeness or any other imprecisely defined concept the information entropy function is not assumed a priori but rather is found in the course of the argument and the argument leads naturally to the procedure of maximizing the information entropy rather than treating it in some other way\u000asuppose an individual wishes to make a probability assignment among m mutually exclusive propositions she has some testable information but is not sure how to go about including this information in her probability assessment she therefore conceives of the following random experiment she will distribute n quanta of probability each worth 1n at random among the m possibilities one might imagine that she will throw n balls into m buckets while blindfolded in order to be as fair as possible each throw is to be independent of any other and every bucket is to be the same size once the experiment is done she will check if the probability assignment thus obtained is consistent with her information for this step to be successful the information must be a constraint given by an open set in the space of probability measures if it is inconsistent she will reject it and try again if it is consistent her assessment will be\u000a\u000awhere pi is the probability of the ith proposition while ni is the number of quanta that were assigned to the ith proposition ie the number of balls that ended up in bucket i\u000anow in order to reduce the graininess of the probability assignment it will be necessary to use quite a large number of quanta of probability rather than actually carry out and possibly have to repeat the rather long random experiment the protagonist decides to simply calculate and use the most probable result the probability of any particular result is the multinomial distribution\u000a\u000awhere\u000a\u000ais sometimes known as the multiplicity of the outcome\u000athe most probable result is the one which maximizes the multiplicity w rather than maximizing w directly the protagonist could equivalently maximize any monotonic increasing function of w she decides to maximize\u000a\u000aat this point in order to simplify the expression the protagonist takes the limit as  ie as the probability levels go from grainy discrete values to smooth continuous values using stirlings approximation she finds\u000a\u000aall that remains for the protagonist to do is to maximize entropy under the constraints of her testable information she has found that the maximum entropy distribution is the most probable of all fair random distributions in the limit as the probability levels go from discrete to continuous\u000a\u000a\u000a compatibility with bayes theoremedit \u000agiffin et al 2007 state that bayes theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the method of maximum relative entropy they state that this method reproduces every aspect of orthodox bayesian inference methods in addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox bayesian methods individually moreover recent contributions lazar 2003 and schennach 2005 show that frequentist relativeentropybased inference approaches such as empirical likelihood and exponentially tilted empirical likelihood  see eg owen 2001 and kitamura 2006 can be combined with prior information to perform bayesian posterior analysis\u000ajaynes stated bayes theorem was a way to calculate a probability while maximum entropy was a way to assign a prior probability distribution\u000ait is however possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross entropy or the principle of maximum entropy being a special case of using a uniform distribution as the given prior independently of any bayesian considerations by treating the problem formally as a constrained optimisation problem the entropy functional being the objective function for the case of given average values as testable information averaged over the sought after probability distribution the sought after distribution is formally the gibbs or boltzmann distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information\u000a\u000a\u000a see alsoedit \u000aakaike information criterion\u000adissipation\u000aentropy maximization\u000amaximum entropy classifier\u000amaximum entropy probability distribution\u000amaximum entropy spectral estimation\u000amaximum entropy thermodynamics\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000ajaynes e t 1963 information theory and statistical mechanics in ford k ed statistical physics new york benjamin p 181 \u000ajaynes e t 1986 new version online 1996 monkeys kangaroos and  in maximumentropy and bayesian methods in applied statistics j h justice ed cambridge university press cambridge p 26\u000abajkova a t 1992 the generalization of maximum entropy method for reconstruction of complex functions astronomical and astrophysical transactions v1 issue 4 p 313320\u000agiffin a and caticha a 2007 updating probabilities with data and moments\u000aguiasu s and shenitzer a 1985 the principle of maximum entropy the mathematical intelligencer 71 4248\u000aharremos p and topse f 2001 maximum entropy fundamentals entropy 33 191226\u000akapur j n and kesavan h k 1992 entropy optimization principles with applications boston academic press isbn 0123976707\u000akitamura y 2006 empirical likelihood methods in econometrics theory and practice cowles foundation discussion papers 1569 cowles foundation yale university\u000alazar n 2003 bayesian empirical likelihood biometrika 90 319326\u000aowen a b empirical likelihood chapman and hall\u000aschennach s m 2005 bayesian exponentially tilted empirical likelihood biometrika 921 3146\u000auffink jos 1995 can the maximum entropy principle be explained as a consistency requirement studies in history and philosophy of modern physics 26b 223261\u000a\u000a\u000a further readingedit \u000aratnaparkhi a 1997 a simple introduction to maximum entropy models for natural language processing technical report 9708 institute for research in cognitive science university of pennsylvania an easytoread introduction to maximum entropy methods in the context of natural language processing\u000atang a jackson d hobbs j chen w smith j l patel h prieto a petrusca d grivich m i sher a hottowy p dabrowski w litke a m beggs j m 2008 a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro journal of neuroscience 28 2 505518 doi101523jneurosci3359072008 pmid 18184793  open access article containing pointers to various papers and software implementations of maximum entropy model on the net\u000a\u000a\u000a external linksedit \u000amaximum entropy modeling links to publications software and resources\u000amaxent and exponential models links to pedagogicallyoriented material on maximum entropy and exponential models
p455
sg14
g17
sg18
Vthe principle of maximum entropy states that subject to precisely stated prior data such as a proposition that expresses testable information the probability distribution which best represents the current state of knowledge is the one with largest entropy\u000aanother way of stating this take precisely stated prior data or testable information about a probability distribution function consider the set of all trial probability distributions that would encode the prior data of those the one with maximal information entropy is the proper distribution according to this principle\u000a\u000a\u000a historyedit \u000athe principle was first expounded by e t jaynes in two papers in 1957 where he emphasized a natural correspondence between statistical mechanics and information theory in particular jaynes offered a new and very general rationale why the gibbsian method of statistical mechanics works he argued that the entropy of statistical mechanics and the information entropy of information theory are principally the same thing consequently statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory\u000a\u000a\u000a overviewedit \u000ain most practical cases the stated prior data or testable information is given by a set of conserved quantities average values of some moment functions associated with the probability distribution in question this is the way the maximum entropy principle is most often used in statistical thermodynamics another possibility is to prescribe some symmetries of the probability distribution the equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method\u000athe maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods statistical mechanics and logical inference in particular\u000athe maximum entropy principle makes explicit our freedom in using different forms of prior data as a special case a uniform prior probability density laplaces principle of indifference sometimes called the principle of insufficient reason may be adopted thus the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics but represents a significant conceptual generalization of those methods it means that thermodynamics systems need not be shown to be ergodic to justify treatment as a statistical ensemble\u000ain ordinary language the principle of maximum entropy can be said to express a claim of epistemic modesty or of maximum ignorance the selected distribution is the one that makes the least claim to being informed beyond the stated prior data that is to say the one that admits the most ignorance beyond the stated prior data\u000a\u000a\u000a testable informationedit \u000athe principle of maximum entropy is useful explicitly only when applied to testable information testable information is a statement about a probability distribution whose truth or falsity is welldefined for example the statements\u000athe expectation of the variable x is 287\u000aand\u000ap2  p3  06\u000awhere p2  p3 are probabilities of events are statements of testable information\u000agiven testable information the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy subject to the constraints of the information this constrained optimization problem is typically solved using the method of lagrange multipliers\u000aentropy maximization with no testable information respects the universal constraint that the sum of the probabilities is one under this constraint the maximum entropy discrete probability distribution is the uniform distribution\u000a\u000a\u000a applicationsedit \u000athe principle of maximum entropy is commonly applied in two ways to inferential problems\u000a\u000a\u000a prior probabilitiesedit \u000athe principle of maximum entropy is often used to obtain prior probability distributions for bayesian inference jaynes was a strong advocate of this approach claiming the maximum entropy distribution represented the least informative distribution a large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding\u000a\u000a\u000a maximum entropy modelsedit \u000aalternatively the principle is often invoked for model specification in this case the observed data itself is assumed to be the testable information such models are widely used in natural language processing an example of such a model is logistic regression which corresponds to the maximum entropy classifier for independent observations\u000a\u000a\u000a general solution for the maximum entropy distribution with linear constraintsedit \u000a\u000a\u000a discrete caseedit \u000awe have some testable information i about a quantity x taking values in x1 x2 xn we assume this information has the form of m constraints on the expectations of the functions fk that is we require our probability distribution to satisfy\u000a\u000afurthermore the probabilities must sum to one giving the constraint\u000a\u000athe probability distribution with maximum information entropy subject to these constraints is\u000a\u000ait is sometimes called the gibbs distribution the normalization constant is determined by\u000a\u000aand is conventionally called the partition function interestingly the pitmankoopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution\u000athe k parameters are lagrange multipliers whose particular values are determined by the constraints according to\u000a\u000athese m simultaneous equations do not generally possess a closed form solution and are usually solved by numerical methods\u000a\u000a\u000a continuous caseedit \u000afor continuous distributions the shannon entropy cannot be used as it is only defined for discrete probability spaces instead edwin jaynes 1963 1968 2003 gave the following formula which is closely related to the relative entropy see also differential entropy\u000a\u000awhere mx which jaynes called the invariant measure is proportional to the limiting density of discrete points for now we shall assume that m is known we will discuss it further after the solution equations are given\u000aa closely related quantity the relative entropy is usually defined as the kullbackleibler divergence of m from p although it is sometimes confusingly defined as the negative of this the inference principle of minimizing this due to kullback is known as the principle of minimum discrimination information\u000awe have some testable information i about a quantity x which takes values in some interval of the real numbers all integrals below are over this interval we assume this information has the form of m constraints on the expectations of the functions fk ie we require our probability density function to satisfy\u000a\u000aand of course the probability density must integrate to one giving the constraint\u000a\u000athe probability density function with maximum hc subject to these constraints is\u000a\u000awith the partition function determined by\u000a\u000aas in the discrete case the values of the  parameters are determined by the constraints according to\u000a\u000athe invariant measure function mx can be best understood by supposing that x is known to take values only in the bounded interval a b and that no other information is given then the maximum entropy probability density function is\u000a\u000awhere a is a normalization constant the invariant measure function is actually the prior density function encoding lack of relevant information it cannot be determined by the principle of maximum entropy and must be determined by some other logical method such as the principle of transformation groups or marginalization theory\u000a\u000a\u000a examplesedit \u000afor several examples of maximum entropy distributions see the article on maximum entropy probability distributions\u000a\u000a\u000a justifications for the principle of maximum entropyedit \u000aproponents of the principle of maximum entropy justify its use in assigning probabilities in several ways including the following two arguments these arguments take the use of bayesian probability as given and are thus subject to the same postulates\u000a\u000a\u000a information entropy as a measure of uninformativenessedit \u000aconsider a discrete probability distribution among m mutually exclusive propositions the most informative distribution would occur when one of the propositions was known to be true in that case the information entropy would be equal to zero the least informative distribution would occur when there is no reason to favor any one of the propositions over the others in that case the only reasonable probability distribution would be uniform and then the information entropy would be equal to its maximum possible value log m the information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is ranging from zero completely informative to log m completely uninformative\u000aby choosing to use the distribution with the maximum entropy allowed by our information the argument goes we are choosing the most uninformative distribution possible to choose a distribution with lower entropy would be to assume information we do not possess thus the maximum entropy distribution is the only reasonable distribution\u000a\u000a\u000a the wallis derivationedit \u000athe following argument is the result of a suggestion made by graham wallis to e t jaynes in 1962 it is essentially the same mathematical argument used for the maxwellboltzmann statistics in statistical mechanics although the conceptual emphasis is quite different it has the advantage of being strictly combinatorial in nature making no reference to information entropy as a measure of uncertainty uninformativeness or any other imprecisely defined concept the information entropy function is not assumed a priori but rather is found in the course of the argument and the argument leads naturally to the procedure of maximizing the information entropy rather than treating it in some other way\u000asuppose an individual wishes to make a probability assignment among m mutually exclusive propositions she has some testable information but is not sure how to go about including this information in her probability assessment she therefore conceives of the following random experiment she will distribute n quanta of probability each worth 1n at random among the m possibilities one might imagine that she will throw n balls into m buckets while blindfolded in order to be as fair as possible each throw is to be independent of any other and every bucket is to be the same size once the experiment is done she will check if the probability assignment thus obtained is consistent with her information for this step to be successful the information must be a constraint given by an open set in the space of probability measures if it is inconsistent she will reject it and try again if it is consistent her assessment will be\u000a\u000awhere pi is the probability of the ith proposition while ni is the number of quanta that were assigned to the ith proposition ie the number of balls that ended up in bucket i\u000anow in order to reduce the graininess of the probability assignment it will be necessary to use quite a large number of quanta of probability rather than actually carry out and possibly have to repeat the rather long random experiment the protagonist decides to simply calculate and use the most probable result the probability of any particular result is the multinomial distribution\u000a\u000awhere\u000a\u000ais sometimes known as the multiplicity of the outcome\u000athe most probable result is the one which maximizes the multiplicity w rather than maximizing w directly the protagonist could equivalently maximize any monotonic increasing function of w she decides to maximize\u000a\u000aat this point in order to simplify the expression the protagonist takes the limit as  ie as the probability levels go from grainy discrete values to smooth continuous values using stirlings approximation she finds\u000a\u000aall that remains for the protagonist to do is to maximize entropy under the constraints of her testable information she has found that the maximum entropy distribution is the most probable of all fair random distributions in the limit as the probability levels go from discrete to continuous\u000a\u000a\u000a compatibility with bayes theoremedit \u000agiffin et al 2007 state that bayes theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the method of maximum relative entropy they state that this method reproduces every aspect of orthodox bayesian inference methods in addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox bayesian methods individually moreover recent contributions lazar 2003 and schennach 2005 show that frequentist relativeentropybased inference approaches such as empirical likelihood and exponentially tilted empirical likelihood  see eg owen 2001 and kitamura 2006 can be combined with prior information to perform bayesian posterior analysis\u000ajaynes stated bayes theorem was a way to calculate a probability while maximum entropy was a way to assign a prior probability distribution\u000ait is however possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross entropy or the principle of maximum entropy being a special case of using a uniform distribution as the given prior independently of any bayesian considerations by treating the problem formally as a constrained optimisation problem the entropy functional being the objective function for the case of given average values as testable information averaged over the sought after probability distribution the sought after distribution is formally the gibbs or boltzmann distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information\u000a\u000a\u000a see alsoedit \u000aakaike information criterion\u000adissipation\u000aentropy maximization\u000amaximum entropy classifier\u000amaximum entropy probability distribution\u000amaximum entropy spectral estimation\u000amaximum entropy thermodynamics\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000ajaynes e t 1963 information theory and statistical mechanics in ford k ed statistical physics new york benjamin p 181 \u000ajaynes e t 1986 new version online 1996 monkeys kangaroos and  in maximumentropy and bayesian methods in applied statistics j h justice ed cambridge university press cambridge p 26\u000abajkova a t 1992 the generalization of maximum entropy method for reconstruction of complex functions astronomical and astrophysical transactions v1 issue 4 p 313320\u000agiffin a and caticha a 2007 updating probabilities with data and moments\u000aguiasu s and shenitzer a 1985 the principle of maximum entropy the mathematical intelligencer 71 4248\u000aharremos p and topse f 2001 maximum entropy fundamentals entropy 33 191226\u000akapur j n and kesavan h k 1992 entropy optimization principles with applications boston academic press isbn 0123976707\u000akitamura y 2006 empirical likelihood methods in econometrics theory and practice cowles foundation discussion papers 1569 cowles foundation yale university\u000alazar n 2003 bayesian empirical likelihood biometrika 90 319326\u000aowen a b empirical likelihood chapman and hall\u000aschennach s m 2005 bayesian exponentially tilted empirical likelihood biometrika 921 3146\u000auffink jos 1995 can the maximum entropy principle be explained as a consistency requirement studies in history and philosophy of modern physics 26b 223261\u000a\u000a\u000a further readingedit \u000aratnaparkhi a 1997 a simple introduction to maximum entropy models for natural language processing technical report 9708 institute for research in cognitive science university of pennsylvania an easytoread introduction to maximum entropy methods in the context of natural language processing\u000atang a jackson d hobbs j chen w smith j l patel h prieto a petrusca d grivich m i sher a hottowy p dabrowski w litke a m beggs j m 2008 a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro journal of neuroscience 28 2 505518 doi101523jneurosci3359072008 pmid 18184793  open access article containing pointers to various papers and software implementations of maximum entropy model on the net\u000a\u000a\u000a external linksedit \u000amaximum entropy modeling links to publications software and resources\u000amaxent and exponential models links to pedagogicallyoriented material on maximum entropy and exponential models
p456
sg20
g23
sg24
g27
sg30
Vthe principle of maximum entropy states that subject to precisely stated prior data such as a proposition that expresses testable information the probability distribution which best represents the current state of knowledge is the one with largest entropy\u000aanother way of stating this take precisely stated prior data or testable information about a probability distribution function consider the set of all trial probability distributions that would encode the prior data of those the one with maximal information entropy is the proper distribution according to this principle\u000a\u000a\u000a historyedit \u000athe principle was first expounded by e t jaynes in two papers in 1957 where he emphasized a natural correspondence between statistical mechanics and information theory in particular jaynes offered a new and very general rationale why the gibbsian method of statistical mechanics works he argued that the entropy of statistical mechanics and the information entropy of information theory are principally the same thing consequently statistical mechanics should be seen just as a particular application of a general tool of logical inference and information theory\u000a\u000a\u000a overviewedit \u000ain most practical cases the stated prior data or testable information is given by a set of conserved quantities average values of some moment functions associated with the probability distribution in question this is the way the maximum entropy principle is most often used in statistical thermodynamics another possibility is to prescribe some symmetries of the probability distribution the equivalence between conserved quantities and corresponding symmetry groups implies a similar equivalence for these two ways of specifying the testable information in the maximum entropy method\u000athe maximum entropy principle is also needed to guarantee the uniqueness and consistency of probability assignments obtained by different methods statistical mechanics and logical inference in particular\u000athe maximum entropy principle makes explicit our freedom in using different forms of prior data as a special case a uniform prior probability density laplaces principle of indifference sometimes called the principle of insufficient reason may be adopted thus the maximum entropy principle is not merely an alternative way to view the usual methods of inference of classical statistics but represents a significant conceptual generalization of those methods it means that thermodynamics systems need not be shown to be ergodic to justify treatment as a statistical ensemble\u000ain ordinary language the principle of maximum entropy can be said to express a claim of epistemic modesty or of maximum ignorance the selected distribution is the one that makes the least claim to being informed beyond the stated prior data that is to say the one that admits the most ignorance beyond the stated prior data\u000a\u000a\u000a testable informationedit \u000athe principle of maximum entropy is useful explicitly only when applied to testable information testable information is a statement about a probability distribution whose truth or falsity is welldefined for example the statements\u000athe expectation of the variable x is 287\u000aand\u000ap2  p3  06\u000awhere p2  p3 are probabilities of events are statements of testable information\u000agiven testable information the maximum entropy procedure consists of seeking the probability distribution which maximizes information entropy subject to the constraints of the information this constrained optimization problem is typically solved using the method of lagrange multipliers\u000aentropy maximization with no testable information respects the universal constraint that the sum of the probabilities is one under this constraint the maximum entropy discrete probability distribution is the uniform distribution\u000a\u000a\u000a applicationsedit \u000athe principle of maximum entropy is commonly applied in two ways to inferential problems\u000a\u000a\u000a prior probabilitiesedit \u000athe principle of maximum entropy is often used to obtain prior probability distributions for bayesian inference jaynes was a strong advocate of this approach claiming the maximum entropy distribution represented the least informative distribution a large amount of literature is now dedicated to the elicitation of maximum entropy priors and links with channel coding\u000a\u000a\u000a maximum entropy modelsedit \u000aalternatively the principle is often invoked for model specification in this case the observed data itself is assumed to be the testable information such models are widely used in natural language processing an example of such a model is logistic regression which corresponds to the maximum entropy classifier for independent observations\u000a\u000a\u000a general solution for the maximum entropy distribution with linear constraintsedit \u000a\u000a\u000a discrete caseedit \u000awe have some testable information i about a quantity x taking values in x1 x2 xn we assume this information has the form of m constraints on the expectations of the functions fk that is we require our probability distribution to satisfy\u000a\u000afurthermore the probabilities must sum to one giving the constraint\u000a\u000athe probability distribution with maximum information entropy subject to these constraints is\u000a\u000ait is sometimes called the gibbs distribution the normalization constant is determined by\u000a\u000aand is conventionally called the partition function interestingly the pitmankoopman theorem states that the necessary and sufficient condition for a sampling distribution to admit sufficient statistics of bounded dimension is that it have the general form of a maximum entropy distribution\u000athe k parameters are lagrange multipliers whose particular values are determined by the constraints according to\u000a\u000athese m simultaneous equations do not generally possess a closed form solution and are usually solved by numerical methods\u000a\u000a\u000a continuous caseedit \u000afor continuous distributions the shannon entropy cannot be used as it is only defined for discrete probability spaces instead edwin jaynes 1963 1968 2003 gave the following formula which is closely related to the relative entropy see also differential entropy\u000a\u000awhere mx which jaynes called the invariant measure is proportional to the limiting density of discrete points for now we shall assume that m is known we will discuss it further after the solution equations are given\u000aa closely related quantity the relative entropy is usually defined as the kullbackleibler divergence of m from p although it is sometimes confusingly defined as the negative of this the inference principle of minimizing this due to kullback is known as the principle of minimum discrimination information\u000awe have some testable information i about a quantity x which takes values in some interval of the real numbers all integrals below are over this interval we assume this information has the form of m constraints on the expectations of the functions fk ie we require our probability density function to satisfy\u000a\u000aand of course the probability density must integrate to one giving the constraint\u000a\u000athe probability density function with maximum hc subject to these constraints is\u000a\u000awith the partition function determined by\u000a\u000aas in the discrete case the values of the  parameters are determined by the constraints according to\u000a\u000athe invariant measure function mx can be best understood by supposing that x is known to take values only in the bounded interval a b and that no other information is given then the maximum entropy probability density function is\u000a\u000awhere a is a normalization constant the invariant measure function is actually the prior density function encoding lack of relevant information it cannot be determined by the principle of maximum entropy and must be determined by some other logical method such as the principle of transformation groups or marginalization theory\u000a\u000a\u000a examplesedit \u000afor several examples of maximum entropy distributions see the article on maximum entropy probability distributions\u000a\u000a\u000a justifications for the principle of maximum entropyedit \u000aproponents of the principle of maximum entropy justify its use in assigning probabilities in several ways including the following two arguments these arguments take the use of bayesian probability as given and are thus subject to the same postulates\u000a\u000a\u000a information entropy as a measure of uninformativenessedit \u000aconsider a discrete probability distribution among m mutually exclusive propositions the most informative distribution would occur when one of the propositions was known to be true in that case the information entropy would be equal to zero the least informative distribution would occur when there is no reason to favor any one of the propositions over the others in that case the only reasonable probability distribution would be uniform and then the information entropy would be equal to its maximum possible value log m the information entropy can therefore be seen as a numerical measure which describes how uninformative a particular probability distribution is ranging from zero completely informative to log m completely uninformative\u000aby choosing to use the distribution with the maximum entropy allowed by our information the argument goes we are choosing the most uninformative distribution possible to choose a distribution with lower entropy would be to assume information we do not possess thus the maximum entropy distribution is the only reasonable distribution\u000a\u000a\u000a the wallis derivationedit \u000athe following argument is the result of a suggestion made by graham wallis to e t jaynes in 1962 it is essentially the same mathematical argument used for the maxwellboltzmann statistics in statistical mechanics although the conceptual emphasis is quite different it has the advantage of being strictly combinatorial in nature making no reference to information entropy as a measure of uncertainty uninformativeness or any other imprecisely defined concept the information entropy function is not assumed a priori but rather is found in the course of the argument and the argument leads naturally to the procedure of maximizing the information entropy rather than treating it in some other way\u000asuppose an individual wishes to make a probability assignment among m mutually exclusive propositions she has some testable information but is not sure how to go about including this information in her probability assessment she therefore conceives of the following random experiment she will distribute n quanta of probability each worth 1n at random among the m possibilities one might imagine that she will throw n balls into m buckets while blindfolded in order to be as fair as possible each throw is to be independent of any other and every bucket is to be the same size once the experiment is done she will check if the probability assignment thus obtained is consistent with her information for this step to be successful the information must be a constraint given by an open set in the space of probability measures if it is inconsistent she will reject it and try again if it is consistent her assessment will be\u000a\u000awhere pi is the probability of the ith proposition while ni is the number of quanta that were assigned to the ith proposition ie the number of balls that ended up in bucket i\u000anow in order to reduce the graininess of the probability assignment it will be necessary to use quite a large number of quanta of probability rather than actually carry out and possibly have to repeat the rather long random experiment the protagonist decides to simply calculate and use the most probable result the probability of any particular result is the multinomial distribution\u000a\u000awhere\u000a\u000ais sometimes known as the multiplicity of the outcome\u000athe most probable result is the one which maximizes the multiplicity w rather than maximizing w directly the protagonist could equivalently maximize any monotonic increasing function of w she decides to maximize\u000a\u000aat this point in order to simplify the expression the protagonist takes the limit as  ie as the probability levels go from grainy discrete values to smooth continuous values using stirlings approximation she finds\u000a\u000aall that remains for the protagonist to do is to maximize entropy under the constraints of her testable information she has found that the maximum entropy distribution is the most probable of all fair random distributions in the limit as the probability levels go from discrete to continuous\u000a\u000a\u000a compatibility with bayes theoremedit \u000agiffin et al 2007 state that bayes theorem and the principle of maximum entropy are completely compatible and can be seen as special cases of the method of maximum relative entropy they state that this method reproduces every aspect of orthodox bayesian inference methods in addition this new method opens the door to tackling problems that could not be addressed by either the maximal entropy principle or orthodox bayesian methods individually moreover recent contributions lazar 2003 and schennach 2005 show that frequentist relativeentropybased inference approaches such as empirical likelihood and exponentially tilted empirical likelihood  see eg owen 2001 and kitamura 2006 can be combined with prior information to perform bayesian posterior analysis\u000ajaynes stated bayes theorem was a way to calculate a probability while maximum entropy was a way to assign a prior probability distribution\u000ait is however possible in concept to solve for a posterior distribution directly from a stated prior distribution using the principle of minimum cross entropy or the principle of maximum entropy being a special case of using a uniform distribution as the given prior independently of any bayesian considerations by treating the problem formally as a constrained optimisation problem the entropy functional being the objective function for the case of given average values as testable information averaged over the sought after probability distribution the sought after distribution is formally the gibbs or boltzmann distribution the parameters of which must be solved for in order to achieve minimum cross entropy and satisfy the given testable information\u000a\u000a\u000a see alsoedit \u000aakaike information criterion\u000adissipation\u000aentropy maximization\u000amaximum entropy classifier\u000amaximum entropy probability distribution\u000amaximum entropy spectral estimation\u000amaximum entropy thermodynamics\u000a\u000a\u000a notesedit \u000a\u000a\u000a referencesedit \u000ajaynes e t 1963 information theory and statistical mechanics in ford k ed statistical physics new york benjamin p 181 \u000ajaynes e t 1986 new version online 1996 monkeys kangaroos and  in maximumentropy and bayesian methods in applied statistics j h justice ed cambridge university press cambridge p 26\u000abajkova a t 1992 the generalization of maximum entropy method for reconstruction of complex functions astronomical and astrophysical transactions v1 issue 4 p 313320\u000agiffin a and caticha a 2007 updating probabilities with data and moments\u000aguiasu s and shenitzer a 1985 the principle of maximum entropy the mathematical intelligencer 71 4248\u000aharremos p and topse f 2001 maximum entropy fundamentals entropy 33 191226\u000akapur j n and kesavan h k 1992 entropy optimization principles with applications boston academic press isbn 0123976707\u000akitamura y 2006 empirical likelihood methods in econometrics theory and practice cowles foundation discussion papers 1569 cowles foundation yale university\u000alazar n 2003 bayesian empirical likelihood biometrika 90 319326\u000aowen a b empirical likelihood chapman and hall\u000aschennach s m 2005 bayesian exponentially tilted empirical likelihood biometrika 921 3146\u000auffink jos 1995 can the maximum entropy principle be explained as a consistency requirement studies in history and philosophy of modern physics 26b 223261\u000a\u000a\u000a further readingedit \u000aratnaparkhi a 1997 a simple introduction to maximum entropy models for natural language processing technical report 9708 institute for research in cognitive science university of pennsylvania an easytoread introduction to maximum entropy methods in the context of natural language processing\u000atang a jackson d hobbs j chen w smith j l patel h prieto a petrusca d grivich m i sher a hottowy p dabrowski w litke a m beggs j m 2008 a maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro journal of neuroscience 28 2 505518 doi101523jneurosci3359072008 pmid 18184793  open access article containing pointers to various papers and software implementations of maximum entropy model on the net\u000a\u000a\u000a external linksedit \u000amaximum entropy modeling links to publications software and resources\u000amaxent and exponential models links to pedagogicallyoriented material on maximum entropy and exponential models
p457
sg32
g35
sg37
NsbsS'binomial_proportion_confidence_interval.txt'
p458
g2
(g3
g4
Ntp459
Rp460
(dp461
g8
g11
sg12
Vin statistics a binomial proportion confidence interval is a confidence interval for a proportion in a statistical population it uses the proportion estimated in a statistical sample and allows for sampling error there are several formulas for a binomial confidence interval but all of them rely on the assumption of a binomial distribution in general a binomial distribution applies when an experiment is repeated a fixed number of times each trial of the experiment has two possible outcomes labeled arbitrarily success and failure the probability of success is the same for each trial and the trials are statistically independent\u000aa simple example of a binomial distribution is the set of various possible outcomes and their probabilities for the number of heads observed when a not necessarily fair coin is flipped ten times the observed binomial proportion is the fraction of the flips which turn out to be heads given this observed proportion the confidence interval for the true proportion innate in that coin is a range of possible proportions which may contain the true proportion a 95 confidence interval for the proportion for instance will contain the true proportion 95 of the times that the procedure for constructing the confidence interval is employed note that this does not mean that a calculated 95 confidence interval will contain the true proportion with 95 probability instead one should interpret it as follows the process of drawing a random sample and calculating an accompanying 95 confidence interval will generate a confidence interval that contains the true proportion in 95 of all cases\u000athere are several ways to compute a confidence interval for a binomial proportion the normal approximation interval is the simplest formula and the one introduced in most basic statistics classes and textbooks this formula however is based on an approximation that does not always work well several competing formulas are available that perform better especially for situations with a small sample size and a proportion very close to zero or one the choice of interval will depend on how important it is to use a simple and easytoexplain interval versus the desire for better accuracy\u000a\u000a\u000a normal approximation interval \u000athe most commonly used formula for a binomial confidence interval relies on approximating the distribution of error about a binomiallydistributed observation  with a normal distribution however although this distribution is frequently confused with a binomial distribution it should be noted that the error distribution itself is not binomial and hence other methods below are preferred\u000athe approximation is usually justified by the central limit theorem the formula is\u000a\u000awhere  is the proportion of successes in a bernoulli trial process estimated from the statistical sample  is the  quantile of a standard normal distribution  is the error quantile and n is the sample size for example for a 95 confidence level the error  is 5 so   0975 and   196\u000athe central limit theorem applies poorly to this distribution with a sample size less than 30 or where the proportion is close to 0 or 1 the normal approximation fails totally when the sample proportion is exactly zero or exactly one a frequently cited rule of thumb is that the normal approximation is a reasonable one as long as np  5 and n1  p  5 see brown et al 2001\u000aan important theoretical derivation of this confidence interval involves the inversion of a hypothesis test under this formulation the confidence interval represents those values of the population parameter that would have large pvalues if they were tested as a hypothesized population proportion the collection of values  for which the normal approximation is valid can be represented as\u000a\u000awhere  is the  quantile of a standard normal distribution\u000asince the test in the middle of the inequality is a wald test the normal approximation interval is sometimes called the wald interval but pierresimon laplace first described it in his 1812 book thorie analytique des probabilits page 283\u000a\u000a\u000a wilson score interval \u000athe wilson interval is an improvement the actual coverage probability is closer to the nominal value over the normal approximation interval and was first developed by edwin bidwell wilson 1927\u000a\u000athis interval has good properties even for a small number of trials andor an extreme probability\u000athese properties obtain from its derivation from the binomial model consider a binomial population probability  whose distribution may be approximated by the normal distribution with standard deviation  however the distribution of true values about an observation is not binomial rather an observation  will have an error interval with a lower bound equal to  when  is at the equivalent normal interval upper bound ie for the same  of  and vice versa\u000athe wilson interval can also be derived from pearsons chisquared test with two categories the resulting interval\u000a\u000acan then be solved for  to produce the wilson interval the test in the middle of the inequality is a score test so the wilson interval is sometimes called the wilson score interval\u000athe center of the wilson interval\u000a\u000acan be shown to be a weighted average of  and  with  receiving greater weight as the sample size increases for the 95 interval the wilson interval is nearly identical to the normal approximation interval using  instead of \u000a\u000a\u000a wilson score interval with continuity correction \u000athe wilson interval may be modified by employing a continuity correction in order to align the minimum coverage probability rather than the average with the nominal value\u000ajust as the wilson interval mirrors pearsons chisquared test the wilson interval with continuity correction mirrors the equivalent yates chisquared test\u000athe following formulae for the lower and upper bounds of the wilson score interval with continuity correction  are derived from newcombe 1998\u000a\u000a\u000a jeffreys interval \u000athe jeffreys interval has a bayesian derivation but it has good frequentist properties in particular it has coverage properties that are similar to the wilson interval but it is one of the few intervals with the advantage of being equaltailed eg for a 95 confidence interval the probabilities of the interval lying above or below the true value are both close to 25 in contrast the wilson interval has a systematic bias such that it is centred too close to p  05\u000athe jeffreys interval is the bayesian credible interval obtained when using the noninformative jeffreys prior for the binomial proportion p the jeffreys prior for this problem is a beta distribution with parameters 12 12 after observing x successes in n trials the posterior distribution for p is a beta distribution with parameters x  12 n  x  12\u000awhen x 0 and x  n the jeffreys interval is taken to be the 1001   equaltailed posterior probability interval ie the 2 and 1  2 quantiles of a beta distribution with parameters x  12 n  x  12 these quantiles need to be computed numerically although this is reasonably simple with modern statistical software\u000ain order to avoid the coverage probability tending to zero when p  0 or 1 when x  0 the upper limit is calculated as before but the lower limit is set to 0 and when x  n the lower limit is calculated as before but the upper limit is set to 1\u000a\u000a\u000a clopperpearson interval \u000athe clopperpearson interval is an early and very common method for calculating binomial confidence intervals this is often called an exact method but that is because it is based on the cumulative probabilities of the binomial distribution ie exactly the correct distribution rather than an approximation but the intervals are not exact in the way that one might assume the discontinuous nature of the binomial distribution precludes any interval with exact coverage for all population proportions the clopperpearson interval can be written as\u000a\u000awith\u000a\u000awhere 0  x  n is the number of successes observed in the sample and binn  is a binomial random variable with n trials and probability of success \u000abecause of a relationship between the cumulative binomial distribution and the beta distribution the clopperpearson interval is sometimes presented in an alternate format that uses quantiles from the beta distribution\u000a\u000awhere x is the number of successes n is the number of trials and bp vw is the pth quantile from a beta distribution with shape parameters v and w the beta distribution is in turn related to the fdistribution so a third formulation of the clopperpearson interval can be written using f quantiles\u000a\u000awhere x is the number of successes n is the number of trials and fc d1 d2 is the 1  c quantile from an fdistribution with d1 and d2 degrees of freedom\u000athe clopperpearson interval is an exact interval since it is based directly on the binomial distribution rather than any approximation to the binomial distribution this interval never has less than the nominal coverage for any population proportion but that means that it is usually conservative for example the true coverage rate of a 95 clopperpearson interval may be well above 95 depending on n and  thus the interval may be wider than it needs to be to achieve 95 confidence in contrast it is worth noting that other confidence bounds may be narrower than their nominal confidence with ie the normal approximation or standard interval wilson interval agresticoull interval etc with a nominal coverage of 95 may in fact cover less than 95\u000a\u000a\u000a agresticoull interval \u000athe agresticoull interval is also another approximate binomial confidence interval\u000agiven  successes in  trials define\u000a\u000aand\u000a\u000athen a confidence interval for  is given by\u000a\u000awhere  is the  quantile of a standard normal distribution as before for example for a 95 confidence interval let  so   196 and   384 if we use 2 instead of 196 for  this is the add 2 successes and 2 failures interval in \u000a\u000a\u000a arcsine transformation \u000a\u000alet x be the number of successes in n trials and let p  xn the variance of p is\u000a\u000ausing the arc sine transform the variance of the arcsine of p is\u000a\u000aso confidence interval itself has the following form\u000a\u000awhere  is  quantile of a standard normal distribution\u000athis method may be used to estimate the variance of p but its use is problematic when p is close to 0 or 1\u000a\u000a\u000a ta transform \u000alet p be the proportion of successes for 0  a  2\u000a\u000athis family is a generalisation of the logit transform which is a special case with a  1 and can be used to transform a proportional data distribution to an approximately normal distribution the parameter a has to be estimated for the data set\u000a\u000a\u000a special cases \u000ain medicine the rule of three is used to provide a simple way of stating an approximate 95 confidence interval for p in the special case that no successes  have been observed the interval is 03n\u000aby symmetry one could expect for only successes  the interval is 13n1\u000a\u000a\u000a comparison of different intervals \u000athere are several research papers that compare these and other confidence intervals for the binomial proportion both agresti and coull 1998 and ross 2003 point out that exact methods such as the clopperpearson interval may not work as well as certain approximations\u000amany of these intervals can be calculated in r using the binom package\u000a\u000a\u000a see also \u000acoverage probability\u000aestimation theory\u000a\u000a\u000a
p462
sg14
g17
sg18
Vin statistics a binomial proportion confidence interval is a confidence interval for a proportion in a statistical population it uses the proportion estimated in a statistical sample and allows for sampling error there are several formulas for a binomial confidence interval but all of them rely on the assumption of a binomial distribution in general a binomial distribution applies when an experiment is repeated a fixed number of times each trial of the experiment has two possible outcomes labeled arbitrarily success and failure the probability of success is the same for each trial and the trials are statistically independent\u000aa simple example of a binomial distribution is the set of various possible outcomes and their probabilities for the number of heads observed when a not necessarily fair coin is flipped ten times the observed binomial proportion is the fraction of the flips which turn out to be heads given this observed proportion the confidence interval for the true proportion innate in that coin is a range of possible proportions which may contain the true proportion a 95 confidence interval for the proportion for instance will contain the true proportion 95 of the times that the procedure for constructing the confidence interval is employed note that this does not mean that a calculated 95 confidence interval will contain the true proportion with 95 probability instead one should interpret it as follows the process of drawing a random sample and calculating an accompanying 95 confidence interval will generate a confidence interval that contains the true proportion in 95 of all cases\u000athere are several ways to compute a confidence interval for a binomial proportion the normal approximation interval is the simplest formula and the one introduced in most basic statistics classes and textbooks this formula however is based on an approximation that does not always work well several competing formulas are available that perform better especially for situations with a small sample size and a proportion very close to zero or one the choice of interval will depend on how important it is to use a simple and easytoexplain interval versus the desire for better accuracy\u000a\u000a\u000a normal approximation interval \u000athe most commonly used formula for a binomial confidence interval relies on approximating the distribution of error about a binomiallydistributed observation  with a normal distribution however although this distribution is frequently confused with a binomial distribution it should be noted that the error distribution itself is not binomial and hence other methods below are preferred\u000athe approximation is usually justified by the central limit theorem the formula is\u000a\u000awhere  is the proportion of successes in a bernoulli trial process estimated from the statistical sample  is the  quantile of a standard normal distribution  is the error quantile and n is the sample size for example for a 95 confidence level the error  is 5 so   0975 and   196\u000athe central limit theorem applies poorly to this distribution with a sample size less than 30 or where the proportion is close to 0 or 1 the normal approximation fails totally when the sample proportion is exactly zero or exactly one a frequently cited rule of thumb is that the normal approximation is a reasonable one as long as np  5 and n1  p  5 see brown et al 2001\u000aan important theoretical derivation of this confidence interval involves the inversion of a hypothesis test under this formulation the confidence interval represents those values of the population parameter that would have large pvalues if they were tested as a hypothesized population proportion the collection of values  for which the normal approximation is valid can be represented as\u000a\u000awhere  is the  quantile of a standard normal distribution\u000asince the test in the middle of the inequality is a wald test the normal approximation interval is sometimes called the wald interval but pierresimon laplace first described it in his 1812 book thorie analytique des probabilits page 283\u000a\u000a\u000a wilson score interval \u000athe wilson interval is an improvement the actual coverage probability is closer to the nominal value over the normal approximation interval and was first developed by edwin bidwell wilson 1927\u000a\u000athis interval has good properties even for a small number of trials andor an extreme probability\u000athese properties obtain from its derivation from the binomial model consider a binomial population probability  whose distribution may be approximated by the normal distribution with standard deviation  however the distribution of true values about an observation is not binomial rather an observation  will have an error interval with a lower bound equal to  when  is at the equivalent normal interval upper bound ie for the same  of  and vice versa\u000athe wilson interval can also be derived from pearsons chisquared test with two categories the resulting interval\u000a\u000acan then be solved for  to produce the wilson interval the test in the middle of the inequality is a score test so the wilson interval is sometimes called the wilson score interval\u000athe center of the wilson interval\u000a\u000acan be shown to be a weighted average of  and  with  receiving greater weight as the sample size increases for the 95 interval the wilson interval is nearly identical to the normal approximation interval using  instead of \u000a\u000a\u000a wilson score interval with continuity correction \u000athe wilson interval may be modified by employing a continuity correction in order to align the minimum coverage probability rather than the average with the nominal value\u000ajust as the wilson interval mirrors pearsons chisquared test the wilson interval with continuity correction mirrors the equivalent yates chisquared test\u000athe following formulae for the lower and upper bounds of the wilson score interval with continuity correction  are derived from newcombe 1998\u000a\u000a\u000a jeffreys interval \u000athe jeffreys interval has a bayesian derivation but it has good frequentist properties in particular it has coverage properties that are similar to the wilson interval but it is one of the few intervals with the advantage of being equaltailed eg for a 95 confidence interval the probabilities of the interval lying above or below the true value are both close to 25 in contrast the wilson interval has a systematic bias such that it is centred too close to p  05\u000athe jeffreys interval is the bayesian credible interval obtained when using the noninformative jeffreys prior for the binomial proportion p the jeffreys prior for this problem is a beta distribution with parameters 12 12 after observing x successes in n trials the posterior distribution for p is a beta distribution with parameters x  12 n  x  12\u000awhen x 0 and x  n the jeffreys interval is taken to be the 1001   equaltailed posterior probability interval ie the 2 and 1  2 quantiles of a beta distribution with parameters x  12 n  x  12 these quantiles need to be computed numerically although this is reasonably simple with modern statistical software\u000ain order to avoid the coverage probability tending to zero when p  0 or 1 when x  0 the upper limit is calculated as before but the lower limit is set to 0 and when x  n the lower limit is calculated as before but the upper limit is set to 1\u000a\u000a\u000a clopperpearson interval \u000athe clopperpearson interval is an early and very common method for calculating binomial confidence intervals this is often called an exact method but that is because it is based on the cumulative probabilities of the binomial distribution ie exactly the correct distribution rather than an approximation but the intervals are not exact in the way that one might assume the discontinuous nature of the binomial distribution precludes any interval with exact coverage for all population proportions the clopperpearson interval can be written as\u000a\u000awith\u000a\u000awhere 0  x  n is the number of successes observed in the sample and binn  is a binomial random variable with n trials and probability of success \u000abecause of a relationship between the cumulative binomial distribution and the beta distribution the clopperpearson interval is sometimes presented in an alternate format that uses quantiles from the beta distribution\u000a\u000awhere x is the number of successes n is the number of trials and bp vw is the pth quantile from a beta distribution with shape parameters v and w the beta distribution is in turn related to the fdistribution so a third formulation of the clopperpearson interval can be written using f quantiles\u000a\u000awhere x is the number of successes n is the number of trials and fc d1 d2 is the 1  c quantile from an fdistribution with d1 and d2 degrees of freedom\u000athe clopperpearson interval is an exact interval since it is based directly on the binomial distribution rather than any approximation to the binomial distribution this interval never has less than the nominal coverage for any population proportion but that means that it is usually conservative for example the true coverage rate of a 95 clopperpearson interval may be well above 95 depending on n and  thus the interval may be wider than it needs to be to achieve 95 confidence in contrast it is worth noting that other confidence bounds may be narrower than their nominal confidence with ie the normal approximation or standard interval wilson interval agresticoull interval etc with a nominal coverage of 95 may in fact cover less than 95\u000a\u000a\u000a agresticoull interval \u000athe agresticoull interval is also another approximate binomial confidence interval\u000agiven  successes in  trials define\u000a\u000aand\u000a\u000athen a confidence interval for  is given by\u000a\u000awhere  is the  quantile of a standard normal distribution as before for example for a 95 confidence interval let  so   196 and   384 if we use 2 instead of 196 for  this is the add 2 successes and 2 failures interval in \u000a\u000a\u000a arcsine transformation \u000a\u000alet x be the number of successes in n trials and let p  xn the variance of p is\u000a\u000ausing the arc sine transform the variance of the arcsine of p is\u000a\u000aso confidence interval itself has the following form\u000a\u000awhere  is  quantile of a standard normal distribution\u000athis method may be used to estimate the variance of p but its use is problematic when p is close to 0 or 1\u000a\u000a\u000a ta transform \u000alet p be the proportion of successes for 0  a  2\u000a\u000athis family is a generalisation of the logit transform which is a special case with a  1 and can be used to transform a proportional data distribution to an approximately normal distribution the parameter a has to be estimated for the data set\u000a\u000a\u000a special cases \u000ain medicine the rule of three is used to provide a simple way of stating an approximate 95 confidence interval for p in the special case that no successes  have been observed the interval is 03n\u000aby symmetry one could expect for only successes  the interval is 13n1\u000a\u000a\u000a comparison of different intervals \u000athere are several research papers that compare these and other confidence intervals for the binomial proportion both agresti and coull 1998 and ross 2003 point out that exact methods such as the clopperpearson interval may not work as well as certain approximations\u000amany of these intervals can be calculated in r using the binom package\u000a\u000a\u000a see also \u000acoverage probability\u000aestimation theory
p463
sg20
g23
sg24
g27
sg30
Vin statistics a binomial proportion confidence interval is a confidence interval for a proportion in a statistical population it uses the proportion estimated in a statistical sample and allows for sampling error there are several formulas for a binomial confidence interval but all of them rely on the assumption of a binomial distribution in general a binomial distribution applies when an experiment is repeated a fixed number of times each trial of the experiment has two possible outcomes labeled arbitrarily success and failure the probability of success is the same for each trial and the trials are statistically independent\u000aa simple example of a binomial distribution is the set of various possible outcomes and their probabilities for the number of heads observed when a not necessarily fair coin is flipped ten times the observed binomial proportion is the fraction of the flips which turn out to be heads given this observed proportion the confidence interval for the true proportion innate in that coin is a range of possible proportions which may contain the true proportion a 95 confidence interval for the proportion for instance will contain the true proportion 95 of the times that the procedure for constructing the confidence interval is employed note that this does not mean that a calculated 95 confidence interval will contain the true proportion with 95 probability instead one should interpret it as follows the process of drawing a random sample and calculating an accompanying 95 confidence interval will generate a confidence interval that contains the true proportion in 95 of all cases\u000athere are several ways to compute a confidence interval for a binomial proportion the normal approximation interval is the simplest formula and the one introduced in most basic statistics classes and textbooks this formula however is based on an approximation that does not always work well several competing formulas are available that perform better especially for situations with a small sample size and a proportion very close to zero or one the choice of interval will depend on how important it is to use a simple and easytoexplain interval versus the desire for better accuracy\u000a\u000a\u000a normal approximation interval \u000athe most commonly used formula for a binomial confidence interval relies on approximating the distribution of error about a binomiallydistributed observation  with a normal distribution however although this distribution is frequently confused with a binomial distribution it should be noted that the error distribution itself is not binomial and hence other methods below are preferred\u000athe approximation is usually justified by the central limit theorem the formula is\u000a\u000awhere  is the proportion of successes in a bernoulli trial process estimated from the statistical sample  is the  quantile of a standard normal distribution  is the error quantile and n is the sample size for example for a 95 confidence level the error  is 5 so   0975 and   196\u000athe central limit theorem applies poorly to this distribution with a sample size less than 30 or where the proportion is close to 0 or 1 the normal approximation fails totally when the sample proportion is exactly zero or exactly one a frequently cited rule of thumb is that the normal approximation is a reasonable one as long as np  5 and n1  p  5 see brown et al 2001\u000aan important theoretical derivation of this confidence interval involves the inversion of a hypothesis test under this formulation the confidence interval represents those values of the population parameter that would have large pvalues if they were tested as a hypothesized population proportion the collection of values  for which the normal approximation is valid can be represented as\u000a\u000awhere  is the  quantile of a standard normal distribution\u000asince the test in the middle of the inequality is a wald test the normal approximation interval is sometimes called the wald interval but pierresimon laplace first described it in his 1812 book thorie analytique des probabilits page 283\u000a\u000a\u000a wilson score interval \u000athe wilson interval is an improvement the actual coverage probability is closer to the nominal value over the normal approximation interval and was first developed by edwin bidwell wilson 1927\u000a\u000athis interval has good properties even for a small number of trials andor an extreme probability\u000athese properties obtain from its derivation from the binomial model consider a binomial population probability  whose distribution may be approximated by the normal distribution with standard deviation  however the distribution of true values about an observation is not binomial rather an observation  will have an error interval with a lower bound equal to  when  is at the equivalent normal interval upper bound ie for the same  of  and vice versa\u000athe wilson interval can also be derived from pearsons chisquared test with two categories the resulting interval\u000a\u000acan then be solved for  to produce the wilson interval the test in the middle of the inequality is a score test so the wilson interval is sometimes called the wilson score interval\u000athe center of the wilson interval\u000a\u000acan be shown to be a weighted average of  and  with  receiving greater weight as the sample size increases for the 95 interval the wilson interval is nearly identical to the normal approximation interval using  instead of \u000a\u000a\u000a wilson score interval with continuity correction \u000athe wilson interval may be modified by employing a continuity correction in order to align the minimum coverage probability rather than the average with the nominal value\u000ajust as the wilson interval mirrors pearsons chisquared test the wilson interval with continuity correction mirrors the equivalent yates chisquared test\u000athe following formulae for the lower and upper bounds of the wilson score interval with continuity correction  are derived from newcombe 1998\u000a\u000a\u000a jeffreys interval \u000athe jeffreys interval has a bayesian derivation but it has good frequentist properties in particular it has coverage properties that are similar to the wilson interval but it is one of the few intervals with the advantage of being equaltailed eg for a 95 confidence interval the probabilities of the interval lying above or below the true value are both close to 25 in contrast the wilson interval has a systematic bias such that it is centred too close to p  05\u000athe jeffreys interval is the bayesian credible interval obtained when using the noninformative jeffreys prior for the binomial proportion p the jeffreys prior for this problem is a beta distribution with parameters 12 12 after observing x successes in n trials the posterior distribution for p is a beta distribution with parameters x  12 n  x  12\u000awhen x 0 and x  n the jeffreys interval is taken to be the 1001   equaltailed posterior probability interval ie the 2 and 1  2 quantiles of a beta distribution with parameters x  12 n  x  12 these quantiles need to be computed numerically although this is reasonably simple with modern statistical software\u000ain order to avoid the coverage probability tending to zero when p  0 or 1 when x  0 the upper limit is calculated as before but the lower limit is set to 0 and when x  n the lower limit is calculated as before but the upper limit is set to 1\u000a\u000a\u000a clopperpearson interval \u000athe clopperpearson interval is an early and very common method for calculating binomial confidence intervals this is often called an exact method but that is because it is based on the cumulative probabilities of the binomial distribution ie exactly the correct distribution rather than an approximation but the intervals are not exact in the way that one might assume the discontinuous nature of the binomial distribution precludes any interval with exact coverage for all population proportions the clopperpearson interval can be written as\u000a\u000awith\u000a\u000awhere 0  x  n is the number of successes observed in the sample and binn  is a binomial random variable with n trials and probability of success \u000abecause of a relationship between the cumulative binomial distribution and the beta distribution the clopperpearson interval is sometimes presented in an alternate format that uses quantiles from the beta distribution\u000a\u000awhere x is the number of successes n is the number of trials and bp vw is the pth quantile from a beta distribution with shape parameters v and w the beta distribution is in turn related to the fdistribution so a third formulation of the clopperpearson interval can be written using f quantiles\u000a\u000awhere x is the number of successes n is the number of trials and fc d1 d2 is the 1  c quantile from an fdistribution with d1 and d2 degrees of freedom\u000athe clopperpearson interval is an exact interval since it is based directly on the binomial distribution rather than any approximation to the binomial distribution this interval never has less than the nominal coverage for any population proportion but that means that it is usually conservative for example the true coverage rate of a 95 clopperpearson interval may be well above 95 depending on n and  thus the interval may be wider than it needs to be to achieve 95 confidence in contrast it is worth noting that other confidence bounds may be narrower than their nominal confidence with ie the normal approximation or standard interval wilson interval agresticoull interval etc with a nominal coverage of 95 may in fact cover less than 95\u000a\u000a\u000a agresticoull interval \u000athe agresticoull interval is also another approximate binomial confidence interval\u000agiven  successes in  trials define\u000a\u000aand\u000a\u000athen a confidence interval for  is given by\u000a\u000awhere  is the  quantile of a standard normal distribution as before for example for a 95 confidence interval let  so   196 and   384 if we use 2 instead of 196 for  this is the add 2 successes and 2 failures interval in \u000a\u000a\u000a arcsine transformation \u000a\u000alet x be the number of successes in n trials and let p  xn the variance of p is\u000a\u000ausing the arc sine transform the variance of the arcsine of p is\u000a\u000aso confidence interval itself has the following form\u000a\u000awhere  is  quantile of a standard normal distribution\u000athis method may be used to estimate the variance of p but its use is problematic when p is close to 0 or 1\u000a\u000a\u000a ta transform \u000alet p be the proportion of successes for 0  a  2\u000a\u000athis family is a generalisation of the logit transform which is a special case with a  1 and can be used to transform a proportional data distribution to an approximately normal distribution the parameter a has to be estimated for the data set\u000a\u000a\u000a special cases \u000ain medicine the rule of three is used to provide a simple way of stating an approximate 95 confidence interval for p in the special case that no successes  have been observed the interval is 03n\u000aby symmetry one could expect for only successes  the interval is 13n1\u000a\u000a\u000a comparison of different intervals \u000athere are several research papers that compare these and other confidence intervals for the binomial proportion both agresti and coull 1998 and ross 2003 point out that exact methods such as the clopperpearson interval may not work as well as certain approximations\u000amany of these intervals can be calculated in r using the binom package\u000a\u000a\u000a see also \u000acoverage probability\u000aestimation theory\u000a\u000a\u000a
p464
sg32
g35
sg37
NsbsS'behrens\xc3\xa2\xc2\x80\xc2\x93fisher_problem.txt'
p465
g2
(g3
g4
Ntp466
Rp467
(dp468
g8
g11
sg12
Vin statistics the behrensfisher problem named after walter ulrich behrens and ronald fisher is the problem of interval estimation and hypothesis testing concerning the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal based on two independent samples\u000a\u000a\u000a specification \u000aone difficulty with discussing the behrensfisher problem and proposed solutions is that there are many different interpretations of what is meant by the behrensfisher problem these differences involve not only what is counted as being a relevant solution but even the basic statement of the context being considered\u000a\u000a\u000a context \u000alet x1  xn and y1  ym be iid samples from two populations which both come from the same locationscale family of distributions the scale parameters are assumed to be unknown and not necessarily equal and the problem is to assess whether the location parameters can reasonably be treated as equal lehmann states that the behrensfisher problem is used both for this general form of model when the family of distributions is arbitrary and for when the restriction to a normal distribution is made while lehmann discusses a number of approaches to the more general problem mainly based on nonparametrics most other sources appear to use the behrensfisher problem to refer only to the case where the distribution is assumed to be normal most of this article makes this assumption\u000a\u000a\u000a requirements of solutions \u000asolutions to the behrensfisher problem have been presented that make use of either a classical or a bayesian inference point of view and either solution would be notionally invalid judged from the other point of view if consideration is restricted to classical statistical inference only it is possible to seek solutions to the inference problem that are simple to apply in a practical sense giving preference to this simplicity over any inaccuracy in the corresponding probability statements where exactness of the significance levels of statistical tests is required there may be an additional requirement that the procedure should make maximum use of the statistical information in the dataset it is well known that an exact test can be gained by randomly discarding data from the larger dataset until the sample sizes are equal assembling data in pairs and taking differences and then using an ordinary ttest to test for the meandifference being zero clearly this would not be optimal in any sense\u000athe task of specifying interval estimates for this problem is one where a frequentist approach fails to provide an exact solution although some approximations are available standard bayesian approaches also fail to provide an answer that can be expressed as straightforward simple formulae but modern computational methods of bayesian analysis do allow essentially exact solutions to be found thus study of the problem can be used to elucidate the differences between the frequentist and bayesian approaches to interval estimation\u000a\u000a\u000a outline of different approaches \u000a\u000a\u000a behrens and fisher approach \u000aronald fisher in 1935 introduced fiducial inference in order to apply it to this problem he referred to an earlier paper by walter ulrich behrens from 1929 behrens and fisher proposed to find the probability distribution of\u000a\u000awhere  and  are the two sample means and s1 and s2 are their standard deviations see behrensfisher distribution fisher approximated the distribution of this by ignoring the random variation of the relative sizes of the standard deviations\u000a\u000afishers solution provoked controversy because it did not have the property that the hypothesis of equal means would be rejected with probability  if the means were in fact equal many other methods of treating the problem have been proposed since\u000a\u000a\u000a welchs approximate t solution \u000a\u000aa widely used method is that of b l welch who like fisher was at university college london the variance of the mean difference\u000a\u000aresults in\u000a\u000awelch 1938 approximated the distribution of  by the type iii pearson distribution a scaled chisquared distribution whose first two moments agree with that of  this applies to the following number of degrees of freedom df which is generally noninteger\u000a\u000aunder the null hypothesis of equal expectations 1  2 the distribution of the behrensfisher statistic t which also depends on the variance ratio 1222 could now be approximated by students t distribution with these  degrees of freedom but this  contains the population variances i2 and these are unknown the following estimate only replaces the population variances by the sample variances\u000a\u000athis  is a random variable a t distribution with a random number of degrees of freedom does not exist nevertheless the behrensfisher t can be compared with a corresponding quantile of students t distribution with these estimated number of degrees of freedom  which is generally noninteger in this way the boundary between acceptance and rejection region of the test statistic t is calculated based on the empirical variances si2 in a way that is a smooth function of these\u000athis method also does not give exactly the nominal rate but is generally not too far off however if the population variances are equal or if the samples are rather small and the population variances can be assumed to be approximately equal it is more accurate to use students ttest\u000a\u000a\u000a other approaches \u000aa number of different approaches to the general problem have been proposed some of which claim to solve some version of the problem among these are\u000a\u000athat of chapman in 1950\u000athat of prokofyev and shishkin in 1974\u000athat of dudewicz and ahmed in 1998\u000a\u000ain dudewiczs comparison of selected methods it was found that the dudewiczahmed procedure is recommended for practical use\u000a\u000a\u000a variants \u000aa minor variant of the behrensfisher problem has been studied in this instance the problem is assuming that the two populationmeans are in fact the same to make inferences about the common mean for example one could require a confidence interval for the common mean\u000a\u000a\u000a generalisations \u000athe immediate generalisation of the problem involves multivariate normal distributions with unknown covariance matrices and is known as the multivariate behrensfisher problem\u000a\u000a\u000a notes \u000a lehmann 1975 p95\u000a lehmann 1975 section 7\u000a fisher r a 1935 the fiducial argument in statistical inference annals of eugenics 8 391398\u000a r a fishers fiducial argument and bayes theorem by teddy seidenfeld\u000a welch 1938 1947\u000a a b dudewicz ma mai and su 2007\u000a chapman d g 1950 some two sample tests annals of mathematical statistics 21 4 601606 doi101214aoms1177729755 \u000a prokofyev v n shishkin a d 1974 successive classification of normal sets with unknown variances radio engng electron phys 19 2 141143 \u000a dudewicz  ahmed 1998 1999\u000a young ga smith rl 2005 essentials of statistical inference cup isbn 0521839718 page 204\u000a belloni  didier 2008\u000a\u000a\u000a
p469
sg14
g17
sg18
Vin statistics the behrensfisher problem named after walter ulrich behrens and ronald fisher is the problem of interval estimation and hypothesis testing concerning the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal based on two independent samples\u000a\u000a\u000a specification \u000aone difficulty with discussing the behrensfisher problem and proposed solutions is that there are many different interpretations of what is meant by the behrensfisher problem these differences involve not only what is counted as being a relevant solution but even the basic statement of the context being considered\u000a\u000a\u000a context \u000alet x1  xn and y1  ym be iid samples from two populations which both come from the same locationscale family of distributions the scale parameters are assumed to be unknown and not necessarily equal and the problem is to assess whether the location parameters can reasonably be treated as equal lehmann states that the behrensfisher problem is used both for this general form of model when the family of distributions is arbitrary and for when the restriction to a normal distribution is made while lehmann discusses a number of approaches to the more general problem mainly based on nonparametrics most other sources appear to use the behrensfisher problem to refer only to the case where the distribution is assumed to be normal most of this article makes this assumption\u000a\u000a\u000a requirements of solutions \u000asolutions to the behrensfisher problem have been presented that make use of either a classical or a bayesian inference point of view and either solution would be notionally invalid judged from the other point of view if consideration is restricted to classical statistical inference only it is possible to seek solutions to the inference problem that are simple to apply in a practical sense giving preference to this simplicity over any inaccuracy in the corresponding probability statements where exactness of the significance levels of statistical tests is required there may be an additional requirement that the procedure should make maximum use of the statistical information in the dataset it is well known that an exact test can be gained by randomly discarding data from the larger dataset until the sample sizes are equal assembling data in pairs and taking differences and then using an ordinary ttest to test for the meandifference being zero clearly this would not be optimal in any sense\u000athe task of specifying interval estimates for this problem is one where a frequentist approach fails to provide an exact solution although some approximations are available standard bayesian approaches also fail to provide an answer that can be expressed as straightforward simple formulae but modern computational methods of bayesian analysis do allow essentially exact solutions to be found thus study of the problem can be used to elucidate the differences between the frequentist and bayesian approaches to interval estimation\u000a\u000a\u000a outline of different approaches \u000a\u000a\u000a behrens and fisher approach \u000aronald fisher in 1935 introduced fiducial inference in order to apply it to this problem he referred to an earlier paper by walter ulrich behrens from 1929 behrens and fisher proposed to find the probability distribution of\u000a\u000awhere  and  are the two sample means and s1 and s2 are their standard deviations see behrensfisher distribution fisher approximated the distribution of this by ignoring the random variation of the relative sizes of the standard deviations\u000a\u000afishers solution provoked controversy because it did not have the property that the hypothesis of equal means would be rejected with probability  if the means were in fact equal many other methods of treating the problem have been proposed since\u000a\u000a\u000a welchs approximate t solution \u000a\u000aa widely used method is that of b l welch who like fisher was at university college london the variance of the mean difference\u000a\u000aresults in\u000a\u000awelch 1938 approximated the distribution of  by the type iii pearson distribution a scaled chisquared distribution whose first two moments agree with that of  this applies to the following number of degrees of freedom df which is generally noninteger\u000a\u000aunder the null hypothesis of equal expectations 1  2 the distribution of the behrensfisher statistic t which also depends on the variance ratio 1222 could now be approximated by students t distribution with these  degrees of freedom but this  contains the population variances i2 and these are unknown the following estimate only replaces the population variances by the sample variances\u000a\u000athis  is a random variable a t distribution with a random number of degrees of freedom does not exist nevertheless the behrensfisher t can be compared with a corresponding quantile of students t distribution with these estimated number of degrees of freedom  which is generally noninteger in this way the boundary between acceptance and rejection region of the test statistic t is calculated based on the empirical variances si2 in a way that is a smooth function of these\u000athis method also does not give exactly the nominal rate but is generally not too far off however if the population variances are equal or if the samples are rather small and the population variances can be assumed to be approximately equal it is more accurate to use students ttest\u000a\u000a\u000a other approaches \u000aa number of different approaches to the general problem have been proposed some of which claim to solve some version of the problem among these are\u000a\u000athat of chapman in 1950\u000athat of prokofyev and shishkin in 1974\u000athat of dudewicz and ahmed in 1998\u000a\u000ain dudewiczs comparison of selected methods it was found that the dudewiczahmed procedure is recommended for practical use\u000a\u000a\u000a variants \u000aa minor variant of the behrensfisher problem has been studied in this instance the problem is assuming that the two populationmeans are in fact the same to make inferences about the common mean for example one could require a confidence interval for the common mean\u000a\u000a\u000a generalisations \u000athe immediate generalisation of the problem involves multivariate normal distributions with unknown covariance matrices and is known as the multivariate behrensfisher problem\u000a\u000a\u000a notes \u000a lehmann 1975 p95\u000a lehmann 1975 section 7\u000a fisher r a 1935 the fiducial argument in statistical inference annals of eugenics 8 391398\u000a r a fishers fiducial argument and bayes theorem by teddy seidenfeld\u000a welch 1938 1947\u000a a b dudewicz ma mai and su 2007\u000a chapman d g 1950 some two sample tests annals of mathematical statistics 21 4 601606 doi101214aoms1177729755 \u000a prokofyev v n shishkin a d 1974 successive classification of normal sets with unknown variances radio engng electron phys 19 2 141143 \u000a dudewicz  ahmed 1998 1999\u000a young ga smith rl 2005 essentials of statistical inference cup isbn 0521839718 page 204\u000a belloni  didier 2008
p470
sg20
g23
sg24
g27
sg30
Vin statistics the behrensfisher problem named after walter ulrich behrens and ronald fisher is the problem of interval estimation and hypothesis testing concerning the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal based on two independent samples\u000a\u000a\u000a specification \u000aone difficulty with discussing the behrensfisher problem and proposed solutions is that there are many different interpretations of what is meant by the behrensfisher problem these differences involve not only what is counted as being a relevant solution but even the basic statement of the context being considered\u000a\u000a\u000a context \u000alet x1  xn and y1  ym be iid samples from two populations which both come from the same locationscale family of distributions the scale parameters are assumed to be unknown and not necessarily equal and the problem is to assess whether the location parameters can reasonably be treated as equal lehmann states that the behrensfisher problem is used both for this general form of model when the family of distributions is arbitrary and for when the restriction to a normal distribution is made while lehmann discusses a number of approaches to the more general problem mainly based on nonparametrics most other sources appear to use the behrensfisher problem to refer only to the case where the distribution is assumed to be normal most of this article makes this assumption\u000a\u000a\u000a requirements of solutions \u000asolutions to the behrensfisher problem have been presented that make use of either a classical or a bayesian inference point of view and either solution would be notionally invalid judged from the other point of view if consideration is restricted to classical statistical inference only it is possible to seek solutions to the inference problem that are simple to apply in a practical sense giving preference to this simplicity over any inaccuracy in the corresponding probability statements where exactness of the significance levels of statistical tests is required there may be an additional requirement that the procedure should make maximum use of the statistical information in the dataset it is well known that an exact test can be gained by randomly discarding data from the larger dataset until the sample sizes are equal assembling data in pairs and taking differences and then using an ordinary ttest to test for the meandifference being zero clearly this would not be optimal in any sense\u000athe task of specifying interval estimates for this problem is one where a frequentist approach fails to provide an exact solution although some approximations are available standard bayesian approaches also fail to provide an answer that can be expressed as straightforward simple formulae but modern computational methods of bayesian analysis do allow essentially exact solutions to be found thus study of the problem can be used to elucidate the differences between the frequentist and bayesian approaches to interval estimation\u000a\u000a\u000a outline of different approaches \u000a\u000a\u000a behrens and fisher approach \u000aronald fisher in 1935 introduced fiducial inference in order to apply it to this problem he referred to an earlier paper by walter ulrich behrens from 1929 behrens and fisher proposed to find the probability distribution of\u000a\u000awhere  and  are the two sample means and s1 and s2 are their standard deviations see behrensfisher distribution fisher approximated the distribution of this by ignoring the random variation of the relative sizes of the standard deviations\u000a\u000afishers solution provoked controversy because it did not have the property that the hypothesis of equal means would be rejected with probability  if the means were in fact equal many other methods of treating the problem have been proposed since\u000a\u000a\u000a welchs approximate t solution \u000a\u000aa widely used method is that of b l welch who like fisher was at university college london the variance of the mean difference\u000a\u000aresults in\u000a\u000awelch 1938 approximated the distribution of  by the type iii pearson distribution a scaled chisquared distribution whose first two moments agree with that of  this applies to the following number of degrees of freedom df which is generally noninteger\u000a\u000aunder the null hypothesis of equal expectations 1  2 the distribution of the behrensfisher statistic t which also depends on the variance ratio 1222 could now be approximated by students t distribution with these  degrees of freedom but this  contains the population variances i2 and these are unknown the following estimate only replaces the population variances by the sample variances\u000a\u000athis  is a random variable a t distribution with a random number of degrees of freedom does not exist nevertheless the behrensfisher t can be compared with a corresponding quantile of students t distribution with these estimated number of degrees of freedom  which is generally noninteger in this way the boundary between acceptance and rejection region of the test statistic t is calculated based on the empirical variances si2 in a way that is a smooth function of these\u000athis method also does not give exactly the nominal rate but is generally not too far off however if the population variances are equal or if the samples are rather small and the population variances can be assumed to be approximately equal it is more accurate to use students ttest\u000a\u000a\u000a other approaches \u000aa number of different approaches to the general problem have been proposed some of which claim to solve some version of the problem among these are\u000a\u000athat of chapman in 1950\u000athat of prokofyev and shishkin in 1974\u000athat of dudewicz and ahmed in 1998\u000a\u000ain dudewiczs comparison of selected methods it was found that the dudewiczahmed procedure is recommended for practical use\u000a\u000a\u000a variants \u000aa minor variant of the behrensfisher problem has been studied in this instance the problem is assuming that the two populationmeans are in fact the same to make inferences about the common mean for example one could require a confidence interval for the common mean\u000a\u000a\u000a generalisations \u000athe immediate generalisation of the problem involves multivariate normal distributions with unknown covariance matrices and is known as the multivariate behrensfisher problem\u000a\u000a\u000a notes \u000a lehmann 1975 p95\u000a lehmann 1975 section 7\u000a fisher r a 1935 the fiducial argument in statistical inference annals of eugenics 8 391398\u000a r a fishers fiducial argument and bayes theorem by teddy seidenfeld\u000a welch 1938 1947\u000a a b dudewicz ma mai and su 2007\u000a chapman d g 1950 some two sample tests annals of mathematical statistics 21 4 601606 doi101214aoms1177729755 \u000a prokofyev v n shishkin a d 1974 successive classification of normal sets with unknown variances radio engng electron phys 19 2 141143 \u000a dudewicz  ahmed 1998 1999\u000a young ga smith rl 2005 essentials of statistical inference cup isbn 0521839718 page 204\u000a belloni  didier 2008\u000a\u000a\u000a
p471
sg32
g35
sg37
NsbsS'invariant_estimator.txt'
p472
g2
(g3
g4
Ntp473
Rp474
(dp475
g8
g11
sg12
Vin statistics the concept of being an invariant estimator is a criterion that can be used to compare the properties of different estimators for the same quantity it is a way of formalising the idea that an estimator should have certain intuitively appealing qualities strictly speaking invariant would mean that the estimates themselves are unchanged when both the measurements and the parameters are transformed in a compatible way but the meaning has been extended to allow the estimates to change in appropriate ways with such transformations the term equivariant estimator is used in formal mathematical contexts that include a precise description of the relation of the way the estimator changes in response to changes to the dataset and parameterisation this corresponds to the use of equivariance in more general mathematics\u000a\u000a\u000a general setting \u000a\u000a\u000a background \u000ain statistical inference there are several approaches to estimation theory that can be used to decide immediately what estimators should be used according to those approaches for example ideas from bayesian inference would lead directly to bayesian estimators similarly the theory of classical statistical inference can sometimes lead to strong conclusions about what estimator should be used however the usefulness of these theories depends on having a fully prescribed statistical model and may also depend on having a relevant loss function to determine the estimator thus a bayesian analysis might be undertaken leading to a posterior distribution for relevant parameters but the use of a specific utility or loss function may be unclear ideas of invariance can then be applied to the task of summarising the posterior distribution in other cases statistical analyses are undertaken without a fully defined statistical model or the classical theory of statistical inference cannot be readily applied because the family of models being considered are not amenable to such treatment in addition to these cases where general theory does not prescribe an estimator the concept of invariance of an estimator can be applied when seeking estimators of alternative forms either for the sake of simplicity of application of the estimator or so that the estimator is robust\u000athe concept of invariance is sometimes used on its own as a way of choosing between estimators but this is not necessarily definitive for example a requirement of invariance may be incompatible with the requirement that the estimator be meanunbiased on the other hand the criterion of medianunbiasedness is defined in terms of the estimators sampling distribution and so is invariant under many transformations\u000aone use of the concept of invariance is where a class or family of estimators is proposed and a particular formulation must be selected amongst these one procedure is to impose relevant invariance properties and then to find the formulation within this class that has the best properties leading to what is called the optimal invariant estimator\u000a\u000a\u000a some classes of invariant estimators \u000athere are several types of transformations that are usefully considered when dealing with invariant estimators each gives rise to a class of estimators which are invariant to those particular types of transformation\u000ashift invariance notionally estimates of a location parameter should be invariant to simple shifts of the data values if all data values are increased by a given amount the estimate should change by the same amount when considering estimation using a weighted average this invariance requirement immediately implies that the weights should sum to one while the same result is often derived from a requirement for unbiasedness the use of invariance does not require that a mean value exists and makes no use of any probability distribution at all\u000ascale invariance note that this topic about the invariance of the estimator scale parameter not to be confused with the more general scale invariance about the behavior of systems under aggregate properties in physics\u000aparametertransformation invariance here the transformation applies to the parameters alone the concept here is that essentially the same inference should be made from data and a model involving a parameter  as would be made from the same data if the model used a parameter  where  is a onetoone transformation of  h according to this type of invariance results from transformationinvariant estimators should also be related by h maximum likelihood estimators have this property though the asymptotic properties of the estimator might be invariant the small sample properties can be different and a specific distribution needs to be derived\u000apermutation invariance where a set of data values can be represented by a statistical model that they are outcomes from independent and identically distributed random variables it is reasonable to impose the requirement that any estimator of any property of the common distribution should be permutationinvariant specifically that the estimator considered as a function of the set of datavalues should not change if items of data are swapped within the dataset\u000athe combination of permutation invariance and location invariance for estimating a location parameter from an independent and identically distributed dataset using a weighted average implies that the weights should be identical and sum to one of course estimators other than a weighted average may be preferable\u000a\u000a\u000a optimal invariant estimators \u000aunder this setting we are given a set of measurements  which contains information about an unknown parameter  the measurements  are modelled as a vector random variable having a probability density function  which depends on a parameter vector \u000athe problem is to estimate  given  the estimate denoted by  is a function of the measurements and belongs to a set  the quality of the result is defined by a loss function  which determines a risk function  the sets of possible values of   and  are denoted by   and  respectively\u000a\u000a\u000a in classification \u000ain statistical classification the rule which assigns a class to a new dataitem can be consider to be a special type of estimator a number of invariancetype considerations can be brought to bear in formulating prior knowledge for pattern recognition\u000a\u000a\u000a mathematical setting \u000a\u000a\u000a definition \u000aan invariant estimator is an estimator which obeys the following two rules\u000aprinciple of rational invariance the action taken in a decision problem should not depend on transformation on the measurement used\u000ainvariance principle if two decision problems have the same formal structure in terms of    and  then the same decision rule should be used in each problem\u000ato define an invariant or equivariant estimator formally some definitions related to groups of transformations are needed first let  denote the set of possible datasamples a group of transformations of  to be denoted by  is a set of measurable 11 and onto transformations of  into itself which satisfies the following conditions\u000aif  and  then \u000aif  then  where  that is each transformation has an inverse within the group\u000a ie there is an identity transformation \u000adatasets  and  in  are equivalent if  for some  all the equivalent points form an equivalence class such an equivalence class is called an orbit in  the  orbit  is the set  if  consists of a single orbit then  is said to be transitive\u000aa family of densities  is said to be invariant under the group  if for every  and  there exists a unique  such that  has density   will be denoted \u000aif  is invariant under the group  then the loss function  is said to be invariant under  if for every  and  there exists an  such that  for all  the transformed value  will be denoted by \u000ain the above  is a group of transformations from  to itself and  is a group of transformations from  to itself\u000aan estimation problem is invariantequivariant under  if there exist three groups  as defined above\u000afor an estimation problem that is invariant under  estimator  is an invariant estimator under  if for all  and \u000a\u000a\u000a properties \u000athe risk function of an invariant estimator  is constant on orbits of  equivalently  for all  and \u000athe risk function of an invariant estimator with transitive  is constant\u000afor a given problem the invariant estimator with the lowest risk is termed the best invariant estimator best invariant estimator cannot always be achieved a special case for which it can be achieved is the case when  is transitive\u000a\u000a\u000a example location parameter \u000asuppose  is a location parameter if the density of  is of the form  for  and  the problem is invariant under  the invariant estimator in this case must satisfy\u000a\u000athus it is of the form    is transitive on  so the risk does not vary with  that is  the best invariant estimator is the one that brings the risk  to minimum\u000ain the case that l is the squared error \u000a\u000a\u000a pitman estimator \u000athe estimation problem is that  has density  where  is a parameter to be estimated and where the loss function is  this problem is invariant with the following additive transformation groups\u000a\u000athe best invariant estimator  is the one that minimizes\u000a\u000aand this is pitmans estimator 1939\u000afor the squared error loss case the result is\u000a\u000aif  ie a multivariate normal distribution with independent unitvariance components then\u000a\u000aif  independent components having a cauchy distribution with scale parameter  then  however the result is\u000a\u000awith\u000a\u000a\u000a
p476
sg14
g17
sg18
Vin statistics the concept of being an invariant estimator is a criterion that can be used to compare the properties of different estimators for the same quantity it is a way of formalising the idea that an estimator should have certain intuitively appealing qualities strictly speaking invariant would mean that the estimates themselves are unchanged when both the measurements and the parameters are transformed in a compatible way but the meaning has been extended to allow the estimates to change in appropriate ways with such transformations the term equivariant estimator is used in formal mathematical contexts that include a precise description of the relation of the way the estimator changes in response to changes to the dataset and parameterisation this corresponds to the use of equivariance in more general mathematics\u000a\u000a\u000a general setting \u000a\u000a\u000a background \u000ain statistical inference there are several approaches to estimation theory that can be used to decide immediately what estimators should be used according to those approaches for example ideas from bayesian inference would lead directly to bayesian estimators similarly the theory of classical statistical inference can sometimes lead to strong conclusions about what estimator should be used however the usefulness of these theories depends on having a fully prescribed statistical model and may also depend on having a relevant loss function to determine the estimator thus a bayesian analysis might be undertaken leading to a posterior distribution for relevant parameters but the use of a specific utility or loss function may be unclear ideas of invariance can then be applied to the task of summarising the posterior distribution in other cases statistical analyses are undertaken without a fully defined statistical model or the classical theory of statistical inference cannot be readily applied because the family of models being considered are not amenable to such treatment in addition to these cases where general theory does not prescribe an estimator the concept of invariance of an estimator can be applied when seeking estimators of alternative forms either for the sake of simplicity of application of the estimator or so that the estimator is robust\u000athe concept of invariance is sometimes used on its own as a way of choosing between estimators but this is not necessarily definitive for example a requirement of invariance may be incompatible with the requirement that the estimator be meanunbiased on the other hand the criterion of medianunbiasedness is defined in terms of the estimators sampling distribution and so is invariant under many transformations\u000aone use of the concept of invariance is where a class or family of estimators is proposed and a particular formulation must be selected amongst these one procedure is to impose relevant invariance properties and then to find the formulation within this class that has the best properties leading to what is called the optimal invariant estimator\u000a\u000a\u000a some classes of invariant estimators \u000athere are several types of transformations that are usefully considered when dealing with invariant estimators each gives rise to a class of estimators which are invariant to those particular types of transformation\u000ashift invariance notionally estimates of a location parameter should be invariant to simple shifts of the data values if all data values are increased by a given amount the estimate should change by the same amount when considering estimation using a weighted average this invariance requirement immediately implies that the weights should sum to one while the same result is often derived from a requirement for unbiasedness the use of invariance does not require that a mean value exists and makes no use of any probability distribution at all\u000ascale invariance note that this topic about the invariance of the estimator scale parameter not to be confused with the more general scale invariance about the behavior of systems under aggregate properties in physics\u000aparametertransformation invariance here the transformation applies to the parameters alone the concept here is that essentially the same inference should be made from data and a model involving a parameter  as would be made from the same data if the model used a parameter  where  is a onetoone transformation of  h according to this type of invariance results from transformationinvariant estimators should also be related by h maximum likelihood estimators have this property though the asymptotic properties of the estimator might be invariant the small sample properties can be different and a specific distribution needs to be derived\u000apermutation invariance where a set of data values can be represented by a statistical model that they are outcomes from independent and identically distributed random variables it is reasonable to impose the requirement that any estimator of any property of the common distribution should be permutationinvariant specifically that the estimator considered as a function of the set of datavalues should not change if items of data are swapped within the dataset\u000athe combination of permutation invariance and location invariance for estimating a location parameter from an independent and identically distributed dataset using a weighted average implies that the weights should be identical and sum to one of course estimators other than a weighted average may be preferable\u000a\u000a\u000a optimal invariant estimators \u000aunder this setting we are given a set of measurements  which contains information about an unknown parameter  the measurements  are modelled as a vector random variable having a probability density function  which depends on a parameter vector \u000athe problem is to estimate  given  the estimate denoted by  is a function of the measurements and belongs to a set  the quality of the result is defined by a loss function  which determines a risk function  the sets of possible values of   and  are denoted by   and  respectively\u000a\u000a\u000a in classification \u000ain statistical classification the rule which assigns a class to a new dataitem can be consider to be a special type of estimator a number of invariancetype considerations can be brought to bear in formulating prior knowledge for pattern recognition\u000a\u000a\u000a mathematical setting \u000a\u000a\u000a definition \u000aan invariant estimator is an estimator which obeys the following two rules\u000aprinciple of rational invariance the action taken in a decision problem should not depend on transformation on the measurement used\u000ainvariance principle if two decision problems have the same formal structure in terms of    and  then the same decision rule should be used in each problem\u000ato define an invariant or equivariant estimator formally some definitions related to groups of transformations are needed first let  denote the set of possible datasamples a group of transformations of  to be denoted by  is a set of measurable 11 and onto transformations of  into itself which satisfies the following conditions\u000aif  and  then \u000aif  then  where  that is each transformation has an inverse within the group\u000a ie there is an identity transformation \u000adatasets  and  in  are equivalent if  for some  all the equivalent points form an equivalence class such an equivalence class is called an orbit in  the  orbit  is the set  if  consists of a single orbit then  is said to be transitive\u000aa family of densities  is said to be invariant under the group  if for every  and  there exists a unique  such that  has density   will be denoted \u000aif  is invariant under the group  then the loss function  is said to be invariant under  if for every  and  there exists an  such that  for all  the transformed value  will be denoted by \u000ain the above  is a group of transformations from  to itself and  is a group of transformations from  to itself\u000aan estimation problem is invariantequivariant under  if there exist three groups  as defined above\u000afor an estimation problem that is invariant under  estimator  is an invariant estimator under  if for all  and \u000a\u000a\u000a properties \u000athe risk function of an invariant estimator  is constant on orbits of  equivalently  for all  and \u000athe risk function of an invariant estimator with transitive  is constant\u000afor a given problem the invariant estimator with the lowest risk is termed the best invariant estimator best invariant estimator cannot always be achieved a special case for which it can be achieved is the case when  is transitive\u000a\u000a\u000a example location parameter \u000asuppose  is a location parameter if the density of  is of the form  for  and  the problem is invariant under  the invariant estimator in this case must satisfy\u000a\u000athus it is of the form    is transitive on  so the risk does not vary with  that is  the best invariant estimator is the one that brings the risk  to minimum\u000ain the case that l is the squared error \u000a\u000a\u000a pitman estimator \u000athe estimation problem is that  has density  where  is a parameter to be estimated and where the loss function is  this problem is invariant with the following additive transformation groups\u000a\u000athe best invariant estimator  is the one that minimizes\u000a\u000aand this is pitmans estimator 1939\u000afor the squared error loss case the result is\u000a\u000aif  ie a multivariate normal distribution with independent unitvariance components then\u000a\u000aif  independent components having a cauchy distribution with scale parameter  then  however the result is\u000a\u000awith
p477
sg20
g23
sg24
g27
sg30
Vin statistics the concept of being an invariant estimator is a criterion that can be used to compare the properties of different estimators for the same quantity it is a way of formalising the idea that an estimator should have certain intuitively appealing qualities strictly speaking invariant would mean that the estimates themselves are unchanged when both the measurements and the parameters are transformed in a compatible way but the meaning has been extended to allow the estimates to change in appropriate ways with such transformations the term equivariant estimator is used in formal mathematical contexts that include a precise description of the relation of the way the estimator changes in response to changes to the dataset and parameterisation this corresponds to the use of equivariance in more general mathematics\u000a\u000a\u000a general setting \u000a\u000a\u000a background \u000ain statistical inference there are several approaches to estimation theory that can be used to decide immediately what estimators should be used according to those approaches for example ideas from bayesian inference would lead directly to bayesian estimators similarly the theory of classical statistical inference can sometimes lead to strong conclusions about what estimator should be used however the usefulness of these theories depends on having a fully prescribed statistical model and may also depend on having a relevant loss function to determine the estimator thus a bayesian analysis might be undertaken leading to a posterior distribution for relevant parameters but the use of a specific utility or loss function may be unclear ideas of invariance can then be applied to the task of summarising the posterior distribution in other cases statistical analyses are undertaken without a fully defined statistical model or the classical theory of statistical inference cannot be readily applied because the family of models being considered are not amenable to such treatment in addition to these cases where general theory does not prescribe an estimator the concept of invariance of an estimator can be applied when seeking estimators of alternative forms either for the sake of simplicity of application of the estimator or so that the estimator is robust\u000athe concept of invariance is sometimes used on its own as a way of choosing between estimators but this is not necessarily definitive for example a requirement of invariance may be incompatible with the requirement that the estimator be meanunbiased on the other hand the criterion of medianunbiasedness is defined in terms of the estimators sampling distribution and so is invariant under many transformations\u000aone use of the concept of invariance is where a class or family of estimators is proposed and a particular formulation must be selected amongst these one procedure is to impose relevant invariance properties and then to find the formulation within this class that has the best properties leading to what is called the optimal invariant estimator\u000a\u000a\u000a some classes of invariant estimators \u000athere are several types of transformations that are usefully considered when dealing with invariant estimators each gives rise to a class of estimators which are invariant to those particular types of transformation\u000ashift invariance notionally estimates of a location parameter should be invariant to simple shifts of the data values if all data values are increased by a given amount the estimate should change by the same amount when considering estimation using a weighted average this invariance requirement immediately implies that the weights should sum to one while the same result is often derived from a requirement for unbiasedness the use of invariance does not require that a mean value exists and makes no use of any probability distribution at all\u000ascale invariance note that this topic about the invariance of the estimator scale parameter not to be confused with the more general scale invariance about the behavior of systems under aggregate properties in physics\u000aparametertransformation invariance here the transformation applies to the parameters alone the concept here is that essentially the same inference should be made from data and a model involving a parameter  as would be made from the same data if the model used a parameter  where  is a onetoone transformation of  h according to this type of invariance results from transformationinvariant estimators should also be related by h maximum likelihood estimators have this property though the asymptotic properties of the estimator might be invariant the small sample properties can be different and a specific distribution needs to be derived\u000apermutation invariance where a set of data values can be represented by a statistical model that they are outcomes from independent and identically distributed random variables it is reasonable to impose the requirement that any estimator of any property of the common distribution should be permutationinvariant specifically that the estimator considered as a function of the set of datavalues should not change if items of data are swapped within the dataset\u000athe combination of permutation invariance and location invariance for estimating a location parameter from an independent and identically distributed dataset using a weighted average implies that the weights should be identical and sum to one of course estimators other than a weighted average may be preferable\u000a\u000a\u000a optimal invariant estimators \u000aunder this setting we are given a set of measurements  which contains information about an unknown parameter  the measurements  are modelled as a vector random variable having a probability density function  which depends on a parameter vector \u000athe problem is to estimate  given  the estimate denoted by  is a function of the measurements and belongs to a set  the quality of the result is defined by a loss function  which determines a risk function  the sets of possible values of   and  are denoted by   and  respectively\u000a\u000a\u000a in classification \u000ain statistical classification the rule which assigns a class to a new dataitem can be consider to be a special type of estimator a number of invariancetype considerations can be brought to bear in formulating prior knowledge for pattern recognition\u000a\u000a\u000a mathematical setting \u000a\u000a\u000a definition \u000aan invariant estimator is an estimator which obeys the following two rules\u000aprinciple of rational invariance the action taken in a decision problem should not depend on transformation on the measurement used\u000ainvariance principle if two decision problems have the same formal structure in terms of    and  then the same decision rule should be used in each problem\u000ato define an invariant or equivariant estimator formally some definitions related to groups of transformations are needed first let  denote the set of possible datasamples a group of transformations of  to be denoted by  is a set of measurable 11 and onto transformations of  into itself which satisfies the following conditions\u000aif  and  then \u000aif  then  where  that is each transformation has an inverse within the group\u000a ie there is an identity transformation \u000adatasets  and  in  are equivalent if  for some  all the equivalent points form an equivalence class such an equivalence class is called an orbit in  the  orbit  is the set  if  consists of a single orbit then  is said to be transitive\u000aa family of densities  is said to be invariant under the group  if for every  and  there exists a unique  such that  has density   will be denoted \u000aif  is invariant under the group  then the loss function  is said to be invariant under  if for every  and  there exists an  such that  for all  the transformed value  will be denoted by \u000ain the above  is a group of transformations from  to itself and  is a group of transformations from  to itself\u000aan estimation problem is invariantequivariant under  if there exist three groups  as defined above\u000afor an estimation problem that is invariant under  estimator  is an invariant estimator under  if for all  and \u000a\u000a\u000a properties \u000athe risk function of an invariant estimator  is constant on orbits of  equivalently  for all  and \u000athe risk function of an invariant estimator with transitive  is constant\u000afor a given problem the invariant estimator with the lowest risk is termed the best invariant estimator best invariant estimator cannot always be achieved a special case for which it can be achieved is the case when  is transitive\u000a\u000a\u000a example location parameter \u000asuppose  is a location parameter if the density of  is of the form  for  and  the problem is invariant under  the invariant estimator in this case must satisfy\u000a\u000athus it is of the form    is transitive on  so the risk does not vary with  that is  the best invariant estimator is the one that brings the risk  to minimum\u000ain the case that l is the squared error \u000a\u000a\u000a pitman estimator \u000athe estimation problem is that  has density  where  is a parameter to be estimated and where the loss function is  this problem is invariant with the following additive transformation groups\u000a\u000athe best invariant estimator  is the one that minimizes\u000a\u000aand this is pitmans estimator 1939\u000afor the squared error loss case the result is\u000a\u000aif  ie a multivariate normal distribution with independent unitvariance components then\u000a\u000aif  independent components having a cauchy distribution with scale parameter  then  however the result is\u000a\u000awith\u000a\u000a\u000a
p478
sg32
g35
sg37
NsbsS'sampling_distribution.txt'
p479
g2
(g3
g4
Ntp480
Rp481
(dp482
g8
g11
sg12
Vin statistics a sampling distribution or finitesample distribution is the probability distribution of a given statistic based on a random sample sampling distributions are important in statistics because they provide a major simplification en route to statistical inference more specifically they allow analytical considerations to be based on the sampling distribution of a statistic rather than on the joint probability distribution of all the individual sample values\u000a\u000a\u000a introductionedit \u000athe sampling distribution of a statistic is the distribution of that statistic considered as a random variable when derived from a random sample of size n it may be considered as the distribution of the statistic for all possible samples from the same population of a given size the sampling distribution depends on the underlying distribution of the population the statistic being considered the sampling procedure employed and the sample size used there is often considerable interest in whether the sampling distribution can be approximated by an asymptotic distribution which corresponds to the limiting case either as the number of random samples of finite size taken from an infinite population and used to produce the distribution tends to infinity or when just one equallyinfinitesize sample is taken of that same population\u000afor example consider a normal population with mean  and variance  assume we repeatedly take samples of a given size from this population and calculate the arithmetic mean  for each sample  this statistic is called the sample mean each sample has its own average value and the distribution of these averages is called the sampling distribution of the sample mean this distribution is normal  n is the sample size since the underlying population is normal although sampling distributions may also often be close to normal even when the population distribution is not see central limit theorem an alternative to the sample mean is the sample median when calculated from the same population it has a different sampling distribution to that of the mean and is generally not normal but it may be close for large sample sizes\u000athe mean of a sample from a population having a normal distribution is an example of a simple statistic taken from one of the simplest statistical populations for other statistics and other populations the formulas are more complicated and often they dont exist in closedform in such cases the sampling distributions may be approximated through montecarlo simulationsp 2 bootstrap methods or asymptotic distribution theory\u000a\u000a\u000a standard erroredit \u000athe standard deviation of the sampling distribution of a statistic is referred to as the standard error of that quantity for the case where the statistic is the sample mean and samples are uncorrelated the standard error is\u000a\u000awhere  is the standard deviation of the population distribution of that quantity and n is the sample size number of items in the sample\u000aan important implication of this formula is that the sample size must be quadrupled multiplied by 4 to achieve half 12 the measurement error when designing statistical studies where cost is a factor this may have a role in understanding costbenefit tradeoffs\u000a\u000a\u000a examplesedit \u000a\u000a\u000a statistical inferenceedit \u000ain the theory of statistical inference the idea of a sufficient statistic provides the basis of choosing a statistic as a function of the sample data points in such a way that no information is lost by replacing the full probabilistic description of the sample with the sampling distribution of the selected statistic\u000ain frequentist inference for example in the development of a statistical hypothesis test or a confidence interval the availability of the sampling distribution of a statistic or an approximation to this in the form of an asymptotic distribution can allow the ready formulation of such procedures whereas the development of procedures starting from the joint distribution of the sample would be less straightforward\u000ain bayesian inference when the sampling distribution of a statistic is available one can consider replacing the final outcome of such procedures specifically the conditional distributions of any unknown quantities given the sample data by the conditional distributions of any unknown quantities given selected sample statistics such a procedure would involve the sampling distribution of the statistics the results would be identical provided the statistics chosen are jointly sufficient statistics\u000a\u000a\u000a referencesedit \u000a\u000amerberg a and sj miller 2008 the sample distribution of the median course notes for math 162 mathematical statistics on the web at httpwebwilliamsedumathematicssjmillerpublichtmlbrownclasses162handoutsmedianthm04pdf pgs 19\u000a\u000a\u000a external linksedit \u000agenerate sampling distributions in excel\u000amathematica demonstration showing the sampling distribution of various statistics eg x for a normal population
p483
sg14
g17
sg18
Vin statistics a sampling distribution or finitesample distribution is the probability distribution of a given statistic based on a random sample sampling distributions are important in statistics because they provide a major simplification en route to statistical inference more specifically they allow analytical considerations to be based on the sampling distribution of a statistic rather than on the joint probability distribution of all the individual sample values\u000a\u000a\u000a introductionedit \u000athe sampling distribution of a statistic is the distribution of that statistic considered as a random variable when derived from a random sample of size n it may be considered as the distribution of the statistic for all possible samples from the same population of a given size the sampling distribution depends on the underlying distribution of the population the statistic being considered the sampling procedure employed and the sample size used there is often considerable interest in whether the sampling distribution can be approximated by an asymptotic distribution which corresponds to the limiting case either as the number of random samples of finite size taken from an infinite population and used to produce the distribution tends to infinity or when just one equallyinfinitesize sample is taken of that same population\u000afor example consider a normal population with mean  and variance  assume we repeatedly take samples of a given size from this population and calculate the arithmetic mean  for each sample  this statistic is called the sample mean each sample has its own average value and the distribution of these averages is called the sampling distribution of the sample mean this distribution is normal  n is the sample size since the underlying population is normal although sampling distributions may also often be close to normal even when the population distribution is not see central limit theorem an alternative to the sample mean is the sample median when calculated from the same population it has a different sampling distribution to that of the mean and is generally not normal but it may be close for large sample sizes\u000athe mean of a sample from a population having a normal distribution is an example of a simple statistic taken from one of the simplest statistical populations for other statistics and other populations the formulas are more complicated and often they dont exist in closedform in such cases the sampling distributions may be approximated through montecarlo simulationsp 2 bootstrap methods or asymptotic distribution theory\u000a\u000a\u000a standard erroredit \u000athe standard deviation of the sampling distribution of a statistic is referred to as the standard error of that quantity for the case where the statistic is the sample mean and samples are uncorrelated the standard error is\u000a\u000awhere  is the standard deviation of the population distribution of that quantity and n is the sample size number of items in the sample\u000aan important implication of this formula is that the sample size must be quadrupled multiplied by 4 to achieve half 12 the measurement error when designing statistical studies where cost is a factor this may have a role in understanding costbenefit tradeoffs\u000a\u000a\u000a examplesedit \u000a\u000a\u000a statistical inferenceedit \u000ain the theory of statistical inference the idea of a sufficient statistic provides the basis of choosing a statistic as a function of the sample data points in such a way that no information is lost by replacing the full probabilistic description of the sample with the sampling distribution of the selected statistic\u000ain frequentist inference for example in the development of a statistical hypothesis test or a confidence interval the availability of the sampling distribution of a statistic or an approximation to this in the form of an asymptotic distribution can allow the ready formulation of such procedures whereas the development of procedures starting from the joint distribution of the sample would be less straightforward\u000ain bayesian inference when the sampling distribution of a statistic is available one can consider replacing the final outcome of such procedures specifically the conditional distributions of any unknown quantities given the sample data by the conditional distributions of any unknown quantities given selected sample statistics such a procedure would involve the sampling distribution of the statistics the results would be identical provided the statistics chosen are jointly sufficient statistics\u000a\u000a\u000a referencesedit \u000a\u000amerberg a and sj miller 2008 the sample distribution of the median course notes for math 162 mathematical statistics on the web at httpwebwilliamsedumathematicssjmillerpublichtmlbrownclasses162handoutsmedianthm04pdf pgs 19\u000a\u000a\u000a external linksedit \u000agenerate sampling distributions in excel\u000amathematica demonstration showing the sampling distribution of various statistics eg x for a normal population
p484
sg20
g23
sg24
g27
sg30
Vin statistics a sampling distribution or finitesample distribution is the probability distribution of a given statistic based on a random sample sampling distributions are important in statistics because they provide a major simplification en route to statistical inference more specifically they allow analytical considerations to be based on the sampling distribution of a statistic rather than on the joint probability distribution of all the individual sample values\u000a\u000a\u000a introductionedit \u000athe sampling distribution of a statistic is the distribution of that statistic considered as a random variable when derived from a random sample of size n it may be considered as the distribution of the statistic for all possible samples from the same population of a given size the sampling distribution depends on the underlying distribution of the population the statistic being considered the sampling procedure employed and the sample size used there is often considerable interest in whether the sampling distribution can be approximated by an asymptotic distribution which corresponds to the limiting case either as the number of random samples of finite size taken from an infinite population and used to produce the distribution tends to infinity or when just one equallyinfinitesize sample is taken of that same population\u000afor example consider a normal population with mean  and variance  assume we repeatedly take samples of a given size from this population and calculate the arithmetic mean  for each sample  this statistic is called the sample mean each sample has its own average value and the distribution of these averages is called the sampling distribution of the sample mean this distribution is normal  n is the sample size since the underlying population is normal although sampling distributions may also often be close to normal even when the population distribution is not see central limit theorem an alternative to the sample mean is the sample median when calculated from the same population it has a different sampling distribution to that of the mean and is generally not normal but it may be close for large sample sizes\u000athe mean of a sample from a population having a normal distribution is an example of a simple statistic taken from one of the simplest statistical populations for other statistics and other populations the formulas are more complicated and often they dont exist in closedform in such cases the sampling distributions may be approximated through montecarlo simulationsp 2 bootstrap methods or asymptotic distribution theory\u000a\u000a\u000a standard erroredit \u000athe standard deviation of the sampling distribution of a statistic is referred to as the standard error of that quantity for the case where the statistic is the sample mean and samples are uncorrelated the standard error is\u000a\u000awhere  is the standard deviation of the population distribution of that quantity and n is the sample size number of items in the sample\u000aan important implication of this formula is that the sample size must be quadrupled multiplied by 4 to achieve half 12 the measurement error when designing statistical studies where cost is a factor this may have a role in understanding costbenefit tradeoffs\u000a\u000a\u000a examplesedit \u000a\u000a\u000a statistical inferenceedit \u000ain the theory of statistical inference the idea of a sufficient statistic provides the basis of choosing a statistic as a function of the sample data points in such a way that no information is lost by replacing the full probabilistic description of the sample with the sampling distribution of the selected statistic\u000ain frequentist inference for example in the development of a statistical hypothesis test or a confidence interval the availability of the sampling distribution of a statistic or an approximation to this in the form of an asymptotic distribution can allow the ready formulation of such procedures whereas the development of procedures starting from the joint distribution of the sample would be less straightforward\u000ain bayesian inference when the sampling distribution of a statistic is available one can consider replacing the final outcome of such procedures specifically the conditional distributions of any unknown quantities given the sample data by the conditional distributions of any unknown quantities given selected sample statistics such a procedure would involve the sampling distribution of the statistics the results would be identical provided the statistics chosen are jointly sufficient statistics\u000a\u000a\u000a referencesedit \u000a\u000amerberg a and sj miller 2008 the sample distribution of the median course notes for math 162 mathematical statistics on the web at httpwebwilliamsedumathematicssjmillerpublichtmlbrownclasses162handoutsmedianthm04pdf pgs 19\u000a\u000a\u000a external linksedit \u000agenerate sampling distributions in excel\u000amathematica demonstration showing the sampling distribution of various statistics eg x for a normal population
p485
sg32
g35
sg37
NsbsS'l-statistic.txt'
p486
g2
(g3
g4
Ntp487
Rp488
(dp489
g8
g11
sg12
Vin statistics an lstatistic is a statistic function of a data set that is a linear combination of order statistics the l is for linear these are more often referred to by narrower terms according to use namely\u000alestimator using lstatistics as estimators for parameters\u000almoment lstatistic analogs of the conventional moments
p490
sg14
g17
sg18
Vin statistics an lstatistic is a statistic function of a data set that is a linear combination of order statistics the l is for linear these are more often referred to by narrower terms according to use namely\u000alestimator using lstatistics as estimators for parameters\u000almoment lstatistic analogs of the conventional moments
p491
sg20
g23
sg24
g27
sg30
Vin statistics an lstatistic is a statistic function of a data set that is a linear combination of order statistics the l is for linear these are more often referred to by narrower terms according to use namely\u000alestimator using lstatistics as estimators for parameters\u000almoment lstatistic analogs of the conventional moments
p492
sg32
g35
sg37
NsbsS'bias_(statistics).txt'
p493
g2
(g3
g4
Ntp494
Rp495
(dp496
g8
g11
sg12
Va statistic is biased if it is calculated in such a way that it is only systematically different from the population parameter of interest the following lists some types of biases which can overlap\u000aselection bias involves individuals being more likely to be selected for study than others biasing the sample this can also be termed berksonian biasspectrum bias arises from evaluating diagnostic tests on biased patient samples leading to an overestimate of the sensitivity and specificity of the test\u000a\u000athe bias of an estimator is the difference between an estimators expectations and the true value of the parameter being estimated\u000aomittedvariable bias is the bias that appears in estimates of parameters in a regression analysis when the assumed specification omits an independent variable that should be in the model\u000a\u000ain statistical hypothesis testing a test is said to be unbiased when the probability of committing a type i error ie false positive is less than the significance level and that of getting a true positive rejecting the null hypothesis when the alternative hypothesis is true is at least that of the significance level\u000adetection bias occurs when a phenomenon is more likely to be observed for a particular set of study subjects for instance the syndemic involving obesity and diabetes may mean doctors are more likely to look for diabetes in obese patients than in thinner patients leading to an inflation in diabetes among obese patients because of skewed detection efforts\u000afunding bias may lead to selection of outcomes test samples or test procedures that favor a studys financial sponsor\u000areporting bias involves a skew in the availability of data such that observations of a certain kind are more likely to be reported\u000aanalytical bias arise due to the way that the results are evaluated\u000aexclusion bias arise due to the systematic exclusion of certain individuals from the study\u000aattrition bias arises due to a loss of participants eg loss to follow up during a study\u000arecall bias arises due to differences in the accuracy or completeness of participant recollections of past events eg a patient cannot recall how many cigarettes they smoked last week exactly leading to overestimation or underestimation\u000aobserver bias arises when the researcher unconsciously influences the experiment due to cognitive bias where judgement may alter how an experiment is carried out  how results are recorded\u000a\u000a\u000a see alsoedit \u000atrueness\u000asystematic error\u000a\u000a\u000a referencesedit 
p497
sg14
g17
sg18
Va statistic is biased if it is calculated in such a way that it is only systematically different from the population parameter of interest the following lists some types of biases which can overlap\u000aselection bias involves individuals being more likely to be selected for study than others biasing the sample this can also be termed berksonian biasspectrum bias arises from evaluating diagnostic tests on biased patient samples leading to an overestimate of the sensitivity and specificity of the test\u000a\u000athe bias of an estimator is the difference between an estimators expectations and the true value of the parameter being estimated\u000aomittedvariable bias is the bias that appears in estimates of parameters in a regression analysis when the assumed specification omits an independent variable that should be in the model\u000a\u000ain statistical hypothesis testing a test is said to be unbiased when the probability of committing a type i error ie false positive is less than the significance level and that of getting a true positive rejecting the null hypothesis when the alternative hypothesis is true is at least that of the significance level\u000adetection bias occurs when a phenomenon is more likely to be observed for a particular set of study subjects for instance the syndemic involving obesity and diabetes may mean doctors are more likely to look for diabetes in obese patients than in thinner patients leading to an inflation in diabetes among obese patients because of skewed detection efforts\u000afunding bias may lead to selection of outcomes test samples or test procedures that favor a studys financial sponsor\u000areporting bias involves a skew in the availability of data such that observations of a certain kind are more likely to be reported\u000aanalytical bias arise due to the way that the results are evaluated\u000aexclusion bias arise due to the systematic exclusion of certain individuals from the study\u000aattrition bias arises due to a loss of participants eg loss to follow up during a study\u000arecall bias arises due to differences in the accuracy or completeness of participant recollections of past events eg a patient cannot recall how many cigarettes they smoked last week exactly leading to overestimation or underestimation\u000aobserver bias arises when the researcher unconsciously influences the experiment due to cognitive bias where judgement may alter how an experiment is carried out  how results are recorded\u000a\u000a\u000a see alsoedit \u000atrueness\u000asystematic error\u000a\u000a\u000a referencesedit
p498
sg20
g23
sg24
g27
sg30
Va statistic is biased if it is calculated in such a way that it is only systematically different from the population parameter of interest the following lists some types of biases which can overlap\u000aselection bias involves individuals being more likely to be selected for study than others biasing the sample this can also be termed berksonian biasspectrum bias arises from evaluating diagnostic tests on biased patient samples leading to an overestimate of the sensitivity and specificity of the test\u000a\u000athe bias of an estimator is the difference between an estimators expectations and the true value of the parameter being estimated\u000aomittedvariable bias is the bias that appears in estimates of parameters in a regression analysis when the assumed specification omits an independent variable that should be in the model\u000a\u000ain statistical hypothesis testing a test is said to be unbiased when the probability of committing a type i error ie false positive is less than the significance level and that of getting a true positive rejecting the null hypothesis when the alternative hypothesis is true is at least that of the significance level\u000adetection bias occurs when a phenomenon is more likely to be observed for a particular set of study subjects for instance the syndemic involving obesity and diabetes may mean doctors are more likely to look for diabetes in obese patients than in thinner patients leading to an inflation in diabetes among obese patients because of skewed detection efforts\u000afunding bias may lead to selection of outcomes test samples or test procedures that favor a studys financial sponsor\u000areporting bias involves a skew in the availability of data such that observations of a certain kind are more likely to be reported\u000aanalytical bias arise due to the way that the results are evaluated\u000aexclusion bias arise due to the systematic exclusion of certain individuals from the study\u000aattrition bias arises due to a loss of participants eg loss to follow up during a study\u000arecall bias arises due to differences in the accuracy or completeness of participant recollections of past events eg a patient cannot recall how many cigarettes they smoked last week exactly leading to overestimation or underestimation\u000aobserver bias arises when the researcher unconsciously influences the experiment due to cognitive bias where judgement may alter how an experiment is carried out  how results are recorded\u000a\u000a\u000a see alsoedit \u000atrueness\u000asystematic error\u000a\u000a\u000a referencesedit 
p499
sg32
g35
sg37
NsbsS'robust_statistics.txt'
p500
g2
(g3
g4
Ntp501
Rp502
(dp503
g8
g11
sg12
Vrobust statistics are statistics with good performance for data drawn from a wide range of probability distributions especially for distributions that are not normal robust statistical methods have been developed for many common problems such as estimating location scale and regression parameters one motivation is to produce statistical methods that are not unduly affected by outliers another motivation is to provide methods with good performance when there are small departures from parametric distributions for example robust methods work well for mixtures of two normal distributions with different standarddeviations under this model nonrobust methods like a ttest work badly\u000a\u000a\u000a introduction \u000arobust statistics seeks to provide methods that emulate popular statistical methods but which are not unduly affected by outliers or other small departures from model assumptions in statistics classical estimation methods rely heavily on assumptions which are often not met in practice in particular it is often assumed that the data errors are normally distributed at least approximately or that the central limit theorem can be relied on to produce normally distributed estimates unfortunately when there are outliers in the data classical estimators often have very poor performance when judged using the breakdown point and the influence function described below\u000athe practical effect of problems seen in the influence function can be studied empirically by examining the sampling distribution of proposed estimators under a mixture model where one mixes in a small amount 15 is often sufficient of contamination for instance one may use a mixture of 95 a normal distribution and 5 a normal distribution with the same mean but significantly higher standard deviation representing outliers\u000arobust parametric statistics can proceed in two ways\u000aby designing estimators so that a preselected behaviour of the influence function is achieved\u000aby replacing estimators that are optimal under the assumption of a normal distribution with estimators that are optimal for or at least derived for other distributions for example using the tdistribution with low degrees of freedom high kurtosis degrees of freedom between 4 and 6 have often been found to be useful in practice or with a mixture of two or more distributions\u000arobust estimates have been studied for the following problems\u000aestimating location parameters\u000aestimating scale parameters\u000aestimating regression coefficients\u000aestimation of modelstates in models expressed in statespace form for which the standard method is equivalent to a kalman filter\u000a\u000a\u000a examples \u000athe median is a robust measure of central tendency while the mean is not the median has a breakdown point of 50 while the mean has a breakdown point of 0 a single large observation can throw it off\u000athe median absolute deviation and interquartile range are robust measures of statistical dispersion while the standard deviation and range are not\u000atrimmed estimators and winsorised estimators are general methods to make statistics more robust lestimators are a general class of simple statistics often robust while mestimators are a general class of robust statistics and are now the preferred solution though they can be quite involved to calculate\u000a\u000a\u000a definition \u000athere are various definitions of a robust statistic strictly speaking a robust statistic is resistant to errors in the results produced by deviations from assumptions eg of normality this means that if the assumptions are only approximately met the robust estimator will still have a reasonable efficiency and reasonably small bias as well as being asymptotically unbiased meaning having a bias tending towards 0 as the sample size tends towards infinity\u000aone of the most important cases is distributional robustness classical statistical procedures are typically sensitive to longtailedness eg when the distribution of the data has longer tails than the assumed normal distribution thus in the context of robust statistics distributionally robust and outlierresistant are effectively synonymous for one perspective on research in robust statistics up to 2000 see portnoy and he 2000\u000aa related topic is that of resistant statistics which are resistant to the effect of extreme scores\u000a\u000a\u000a example speed of light data \u000agelman et al in bayesian data analysis 2004 consider a data set relating to speed of light measurements made by simon newcomb the data sets for that book can be found via the classic data sets page and the books website contains more information on the data\u000aalthough the bulk of the data look to be more or less normally distributed there are two obvious outliers these outliers have a large effect on the mean dragging it towards them and away from the center of the bulk of the data thus if the mean is intended as a measure of the location of the center of the data it is in a sense biased when outliers are present\u000aalso the distribution of the mean is known to be asymptotically normal due to the central limit theorem however outliers can make the distribution of the mean nonnormal even for fairly large data sets besides this nonnormality the mean is also inefficient in the presence of outliers and less variable measures of location are available\u000a\u000a\u000a estimation of location \u000athe plot below shows a density plot of the speed of light data together with a rug plot panel a also shown is a normal qq plot panel b the outliers are clearly visible in these plots\u000apanels c and d of the plot show the bootstrap distribution of the mean c and the 10 trimmed mean d the trimmed mean is a simple robust estimator of location that deletes a certain percentage of observations 10 here from each end of the data then computes the mean in the usual way the analysis was performed in r and 10000 bootstrap samples were used for each of the raw and trimmed means\u000athe distribution of the mean is clearly much wider than that of the 10 trimmed mean the plots are on the same scale also note that whereas the distribution of the trimmed mean appears to be close to normal the distribution of the raw mean is quite skewed to the left so in this sample of 66 observations only 2 outliers cause the central limit theorem to be inapplicable\u000a\u000arobust statistical methods of which the trimmed mean is a simple example seek to outperform classical statistical methods in the presence of outliers or more generally when underlying parametric assumptions are not quite correct\u000awhilst the trimmed mean performs well relative to the mean in this example better robust estimates are available in fact the mean median and trimmed mean are all special cases of mestimators details appear in the sections below\u000a\u000a\u000a estimation of scale \u000a\u000athe outliers in the speed of light data have more than just an adverse effect on the mean the usual estimate of scale is the standard deviation and this quantity is even more badly affected by outliers because the squares of the deviations from the mean go into the calculation so the outliers effects are exacerbated\u000athe plots below show the bootstrap distributions of the standard deviation median absolute deviation mad and qn estimator of scale rousseeuw and croux 1993 the plots are based on 10000 bootstrap samples for each estimator with some gaussian noise added to the resampled data smoothed bootstrap panel a shows the distribution of the standard deviation b of the mad and c of qn\u000a\u000athe distribution of standard deviation is erratic and wide a result of the outliers the mad is better behaved and qn is a little bit more efficient than mad this simple example demonstrates that when outliers are present the standard deviation cannot be recommended as an estimate of scale\u000a\u000a\u000a manual screening for outliers \u000atraditionally statisticians would manually screen data for outliers and remove them usually checking the source of the data to see if the outliers were erroneously recorded indeed in the speed of light example above it is easy to see and remove the two outliers prior to proceeding with any further analysis however in modern times data sets often consist of large numbers of variables being measured on large numbers of experimental units therefore manual screening for outliers is often impractical\u000aoutliers can often interact in such a way that they mask each other as a simple example consider a small univariate data set containing one modest and one large outlier the estimated standard deviation will be grossly inflated by the large outlier the result is that the modest outlier looks relatively normal as soon as the large outlier is removed the estimated standard deviation shrinks and the modest outlier now looks unusual\u000athis problem of masking gets worse as the complexity of the data increases for example in regression problems diagnostic plots are used to identify outliers however it is common that once a few outliers have been removed others become visible the problem is even worse in higher dimensions\u000arobust methods provide automatic ways of detecting downweighting or removing and flagging outliers largely removing the need for manual screening care must be taken initial data showing the ozone hole first appearing over antarctica were rejected as outliers by nonhuman screening\u000a\u000a\u000a variety of applications \u000aalthough this article deals with general principles for univariate statistical methods robust methods also exist for regression problems generalized linear models and parameter estimation of various distributions\u000a\u000a\u000a measures of robustness \u000athe basic tools used to describe and measure robustness are the breakdown point the influence function and the sensitivity curve\u000a\u000a\u000a breakdown point \u000aintuitively the breakdown point of an estimator is the proportion of incorrect observations eg arbitrarily large observations an estimator can handle before giving an incorrect eg arbitrarily large result for example given  independent random variables  and the corresponding realizations  we can use  to estimate the mean such an estimator has a breakdown point of 0 because we can make  arbitrarily large just by changing any of \u000athe higher the breakdown point of an estimator the more robust it is intuitively we can understand that a breakdown point cannot exceed 50 because if more than half of the observations are contaminated it is not possible to distinguish between the underlying distribution and the contaminating distribution therefore the maximum breakdown point is 05 and there are estimators which achieve such a breakdown point for example the median has a breakdown point of 05 the x trimmed mean has breakdown point of x for the chosen level of x huber 1981 and maronna et al 2006 contain more details the level and the power breakdown points of tests are investigated in he et al 1990\u000astatistics with high breakdown points are sometimes called resistant statistics\u000a\u000a\u000a example speed of light data \u000ain the speed of light example removing the two lowest observations causes the mean to change from 262 to 2775 a change of 155 the estimate of scale produced by the qn method is 63 we can divide this by the square root of the sample size to get a robust standard error and we find this quantity to be 078 thus the change in the mean resulting from removing two outliers is approximately twice the robust standard error\u000athe 10 trimmed mean for the speed of light data is 2743 removing the two lowest observations and recomputing gives 2767 clearly the trimmed mean is less affected by the outliers and has a higher breakdown point\u000anotice that if we replace the lowest observation 44 by 1000 the mean becomes 1173 whereas the 10 trimmed mean is still 2743 in many areas of applied statistics it is common for data to be logtransformed to make them near symmetrical very small values become large negative when logtransformed and zeroes become negatively infinite therefore this example is of practical interest\u000a\u000a\u000a empirical influence function \u000a\u000athe empirical influence function is a measure of the dependence of the estimator on the value of one of the points in the sample it is a modelfree measure in the sense that it simply relies on calculating the estimator again with a different sample on the right is tukeys biweight function which as we will later see is an example of what a good in a sense defined later on empirical influence function should look like\u000ain mathematical terms an influence function is defined as a vector in the space of the estimator which is in turn defined for a sample which is a subset of the population\u000a is a probability space\u000a is a measure space state space\u000a is a parameter space of dimension \u000a is a measure space\u000afor example\u000a is any probability space\u000a\u000a\u000a\u000athe definition of an empirical influence function is let  and  are iid and  is a sample from these variables  is an estimator let  the empirical influence function  at observation  is defined by\u000a\u000anote that \u000awhat this actually means is that we are replacing the ith value in the sample by an arbitrary value and looking at the output of the estimator alternatively the eif is defined as the scaled by n1 instead of n effect on the estimator of adding the point  to the sample\u000a\u000a\u000a influence function and sensitivity curve \u000ainstead of relying solely on the data we could use the distribution of the random variables the approach is quite different from that of the previous paragraph what we are now trying to do is to see what happens to an estimator when we change the distribution of the data slightly it assumes a distribution and measures sensitivity to change in this distribution by contrast the empirical influence assumes a sample set and measures sensitivity to change in the samples\u000alet  be a convex subset of the set of all finite signed measures on  we want to estimate the parameter  of a distribution  in  let the functional  be the asymptotic value of some estimator sequence  we will suppose that this functional is fisher consistent ie  this means that at the model  the estimator sequence asymptotically measures the correct quantity\u000alet  be some distribution in  what happens when the data doesnt follow the model  exactly but another slightly different going towards \u000awere looking at \u000awhich is the onesided directional derivative of  at  in the direction of \u000alet   is the probability measure which gives mass 1 to  we choose  the influence function is then defined by\u000a\u000ait describes the effect of an infinitesimal contamination at the point  on the estimate we are seeking standardized by the mass  of the contamination the asymptotic bias caused by contamination in the observations for a robust estimator we want a bounded influence function that is one which does not go to infinity as x becomes arbitrarily large\u000a\u000a\u000a desirable properties \u000aproperties of an influence function which bestow it with desirable performance are\u000afinite rejection point \u000asmall grosserror sensitivity \u000asmall localshift sensitivity \u000a\u000a\u000a rejection point \u000a\u000a\u000a grosserror sensitivity \u000a\u000a\u000a localshift sensitivity \u000a\u000athis value which looks a lot like a lipschitz constant represents the effect of shifting an observation slightly from  to a neighbouring point  ie add an observation at  and remove one at \u000a\u000a\u000a mestimators \u000a\u000athe mathematical context of this paragraph is given in the section on empirical influence functions\u000ahistorically several approaches to robust estimation were proposed including restimators and lestimators however mestimators now appear to dominate the field as a result of their generality high breakdown point and their efficiency see huber 1981\u000amestimators are a generalization of maximum likelihood estimators mles what we try to do with mles is to maximize  or equivalently minimize  in 1964 huber proposed to generalize this to the minimization of  where  is some function mle are therefore a special case of mestimators hence the name maximum likelihood type estimators\u000aminimizing  can often be done by differentiating  and solving  where  if  has a derivative\u000aseveral choices of  and  have been proposed the two figures below show four  functions and their corresponding  functions\u000a\u000afor squared errors  increases at an accelerating rate whilst for absolute errors it increases at a constant rate when winsorizing is used a mixture of these two effects is introduced for small values of x  increases at the squared rate but once the chosen threshold is reached 15 in this example the rate of increase becomes constant this winsorised estimator is also known as the huber loss function\u000atukeys biweight also known as bisquare function behaves in a similar way to the squared error function at first but for larger errors the function tapers off\u000a\u000a\u000a properties of mestimators \u000anotice that mestimators do not necessarily relate to a probability density function therefore offtheshelf approaches to inference that arise from likelihood theory can not in general be used\u000ait can be shown that mestimators are asymptotically normally distributed so that as long as their standard errors can be computed an approximate approach to inference is available\u000asince mestimators are normal only asymptotically for small sample sizes it might be appropriate to use an alternative approach to inference such as the bootstrap however mestimates are not necessarily unique ie there might be more than one solution that satisfies the equations also it is possible that any particular bootstrap sample can contain more outliers than the estimators breakdown point therefore some care is needed when designing bootstrap schemes\u000aof course as we saw with the speed of light example the mean is only normally distributed asymptotically and when outliers are present the approximation can be very poor even for quite large samples however classical statistical tests including those based on the mean are typically bounded above by the nominal size of the test the same is not true of mestimators and the type i error rate can be substantially above the nominal level\u000athese considerations do not invalidate mestimation in any way they merely make clear that some care is needed in their use as is true of any other method of estimation\u000a\u000a\u000a influence function of an mestimator \u000ait can be shown that the influence function of an mestimator  is proportional to  see huber 1981 and 2004 page 45 which means we can derive the properties of such an estimator such as its rejection point grosserror sensitivity or localshift sensitivity when we know its  function\u000a with the  given by \u000a\u000a\u000a choice of  and  \u000ain many practical situations the choice of the  function is not critical to gaining a good robust estimate and many choices will give similar results that offer great improvements in terms of efficiency and bias over classical estimates in the presence of outliers huber 1981\u000atheoretically  functions are to be preferred and tukeys biweight also known as bisquare function is a popular choice maronna et al 2006 recommend the biweight function with efficiency at the normal set to 85\u000a\u000a\u000a robust parametric approaches \u000amestimators do not necessarily relate to a density function and so are not fully parametric fully parametric approaches to robust modeling and inference both bayesian and likelihood approaches usually deal with heavy tailed distributions such as students tdistribution\u000afor the tdistribution with  degrees of freedom it can be shown that\u000a\u000afor  the tdistribution is equivalent to the cauchy distribution notice that the degrees of freedom is sometimes known as the kurtosis parameter it is the parameter that controls how heavy the tails are in principle  can be estimated from the data in the same way as any other parameter in practice it is common for there to be multiple local maxima when  is allowed to vary as such it is common to fix  at a value around 4 or 6 the figure below displays the function for 4 different values of \u000a\u000a\u000a example speed of light data \u000afor the speed of light data allowing the kurtosis parameter to vary and maximizing the likelihood we get\u000a\u000afixing  and maximizing the likelihood gives\u000a\u000a\u000a related concepts \u000aa pivotal quantity is a function of data whose underlying population distribution is a member of a parametric family that is not dependent on the values of the parameters an ancillary statistic is such a function that is also a statistic meaning that it is computed in terms of the data alone such functions are robust to parameters in the sense that they are independent of the values of the parameters but not robust to the model in the sense that they assume an underlying model parametric family and in fact such functions are often very sensitive to violations of the model assumptions thus test statistics frequently constructed in terms of these to not be sensitive to assumptions about parameters are still very sensitive to model assumptions\u000a\u000a\u000a replacing outliers and missing values \u000aif there are relatively few missing points there are some models which can be used to estimate values to complete the series such as replacing missing values with the mean or median of the data simple linear regression can also be used to estimate missing values macdonald and zucchini 1997 harvey 1989 in addition outliers can sometimes be accommodated in the data through the use of trimmed means other scale estimators apart from standard deviation eg mad and winsorization mcbean and rovers 1998 in calculations of a trimmed mean a fixed percentage of data is dropped from each end of an ordered data thus eliminating the outliers the mean is then calculated using the remaining data winsorizing involves accommodating an outlier by replacing it with the next highest or next smallest value as appropriate rustum  adeloye 2007\u000ahowever using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable particularly if the number of values to be infilled is relatively high in comparison with total record length the accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends rosen and lennox 2001 the in a case of a dynamic process so any variable is dependent not just on the historical time series of the same variable but also on several other variables or parameters of the process in other words the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers a multivariate model will therefore be more representative than a univariate one for predicting missing values the kohonin self organising map ksom offers a simple and robust multivariate model for data analysis thus providing good possibilities to estimate missing values taking into account its relationship or correlation with other pertinent variables in the data record rustum  adeloye 2007\u000astandard kalman filters are not robust to outliers to this end ting theodorou and schaal have recently shown that a modification of masreliezs theorem can deal with outliers\u000aone common approach to handle outliers in data analysis is to perform outlier detection first followed by an efficient estimation method eg the least squares while this approach is often useful one must keep in mind two challenges first an outlier detection method that relies on a nonrobust initial fit can suffer from the effect of masking that is a group of outliers can mask each other and escape detection rousseeuw and leroy 2007 second if a high breakdown initial fit is used for outlier detection the followup analysis might inherit some of the inefficiencies of the initial estimator he and portnoy 1992\u000a\u000a\u000a see also \u000arobust confidence intervals\u000arobust regression\u000aunitweighted regression\u000a\u000a\u000a
p504
sg14
g17
sg18
Vrobust statistics are statistics with good performance for data drawn from a wide range of probability distributions especially for distributions that are not normal robust statistical methods have been developed for many common problems such as estimating location scale and regression parameters one motivation is to produce statistical methods that are not unduly affected by outliers another motivation is to provide methods with good performance when there are small departures from parametric distributions for example robust methods work well for mixtures of two normal distributions with different standarddeviations under this model nonrobust methods like a ttest work badly\u000a\u000a\u000a introduction \u000arobust statistics seeks to provide methods that emulate popular statistical methods but which are not unduly affected by outliers or other small departures from model assumptions in statistics classical estimation methods rely heavily on assumptions which are often not met in practice in particular it is often assumed that the data errors are normally distributed at least approximately or that the central limit theorem can be relied on to produce normally distributed estimates unfortunately when there are outliers in the data classical estimators often have very poor performance when judged using the breakdown point and the influence function described below\u000athe practical effect of problems seen in the influence function can be studied empirically by examining the sampling distribution of proposed estimators under a mixture model where one mixes in a small amount 15 is often sufficient of contamination for instance one may use a mixture of 95 a normal distribution and 5 a normal distribution with the same mean but significantly higher standard deviation representing outliers\u000arobust parametric statistics can proceed in two ways\u000aby designing estimators so that a preselected behaviour of the influence function is achieved\u000aby replacing estimators that are optimal under the assumption of a normal distribution with estimators that are optimal for or at least derived for other distributions for example using the tdistribution with low degrees of freedom high kurtosis degrees of freedom between 4 and 6 have often been found to be useful in practice or with a mixture of two or more distributions\u000arobust estimates have been studied for the following problems\u000aestimating location parameters\u000aestimating scale parameters\u000aestimating regression coefficients\u000aestimation of modelstates in models expressed in statespace form for which the standard method is equivalent to a kalman filter\u000a\u000a\u000a examples \u000athe median is a robust measure of central tendency while the mean is not the median has a breakdown point of 50 while the mean has a breakdown point of 0 a single large observation can throw it off\u000athe median absolute deviation and interquartile range are robust measures of statistical dispersion while the standard deviation and range are not\u000atrimmed estimators and winsorised estimators are general methods to make statistics more robust lestimators are a general class of simple statistics often robust while mestimators are a general class of robust statistics and are now the preferred solution though they can be quite involved to calculate\u000a\u000a\u000a definition \u000athere are various definitions of a robust statistic strictly speaking a robust statistic is resistant to errors in the results produced by deviations from assumptions eg of normality this means that if the assumptions are only approximately met the robust estimator will still have a reasonable efficiency and reasonably small bias as well as being asymptotically unbiased meaning having a bias tending towards 0 as the sample size tends towards infinity\u000aone of the most important cases is distributional robustness classical statistical procedures are typically sensitive to longtailedness eg when the distribution of the data has longer tails than the assumed normal distribution thus in the context of robust statistics distributionally robust and outlierresistant are effectively synonymous for one perspective on research in robust statistics up to 2000 see portnoy and he 2000\u000aa related topic is that of resistant statistics which are resistant to the effect of extreme scores\u000a\u000a\u000a example speed of light data \u000agelman et al in bayesian data analysis 2004 consider a data set relating to speed of light measurements made by simon newcomb the data sets for that book can be found via the classic data sets page and the books website contains more information on the data\u000aalthough the bulk of the data look to be more or less normally distributed there are two obvious outliers these outliers have a large effect on the mean dragging it towards them and away from the center of the bulk of the data thus if the mean is intended as a measure of the location of the center of the data it is in a sense biased when outliers are present\u000aalso the distribution of the mean is known to be asymptotically normal due to the central limit theorem however outliers can make the distribution of the mean nonnormal even for fairly large data sets besides this nonnormality the mean is also inefficient in the presence of outliers and less variable measures of location are available\u000a\u000a\u000a estimation of location \u000athe plot below shows a density plot of the speed of light data together with a rug plot panel a also shown is a normal qq plot panel b the outliers are clearly visible in these plots\u000apanels c and d of the plot show the bootstrap distribution of the mean c and the 10 trimmed mean d the trimmed mean is a simple robust estimator of location that deletes a certain percentage of observations 10 here from each end of the data then computes the mean in the usual way the analysis was performed in r and 10000 bootstrap samples were used for each of the raw and trimmed means\u000athe distribution of the mean is clearly much wider than that of the 10 trimmed mean the plots are on the same scale also note that whereas the distribution of the trimmed mean appears to be close to normal the distribution of the raw mean is quite skewed to the left so in this sample of 66 observations only 2 outliers cause the central limit theorem to be inapplicable\u000a\u000arobust statistical methods of which the trimmed mean is a simple example seek to outperform classical statistical methods in the presence of outliers or more generally when underlying parametric assumptions are not quite correct\u000awhilst the trimmed mean performs well relative to the mean in this example better robust estimates are available in fact the mean median and trimmed mean are all special cases of mestimators details appear in the sections below\u000a\u000a\u000a estimation of scale \u000a\u000athe outliers in the speed of light data have more than just an adverse effect on the mean the usual estimate of scale is the standard deviation and this quantity is even more badly affected by outliers because the squares of the deviations from the mean go into the calculation so the outliers effects are exacerbated\u000athe plots below show the bootstrap distributions of the standard deviation median absolute deviation mad and qn estimator of scale rousseeuw and croux 1993 the plots are based on 10000 bootstrap samples for each estimator with some gaussian noise added to the resampled data smoothed bootstrap panel a shows the distribution of the standard deviation b of the mad and c of qn\u000a\u000athe distribution of standard deviation is erratic and wide a result of the outliers the mad is better behaved and qn is a little bit more efficient than mad this simple example demonstrates that when outliers are present the standard deviation cannot be recommended as an estimate of scale\u000a\u000a\u000a manual screening for outliers \u000atraditionally statisticians would manually screen data for outliers and remove them usually checking the source of the data to see if the outliers were erroneously recorded indeed in the speed of light example above it is easy to see and remove the two outliers prior to proceeding with any further analysis however in modern times data sets often consist of large numbers of variables being measured on large numbers of experimental units therefore manual screening for outliers is often impractical\u000aoutliers can often interact in such a way that they mask each other as a simple example consider a small univariate data set containing one modest and one large outlier the estimated standard deviation will be grossly inflated by the large outlier the result is that the modest outlier looks relatively normal as soon as the large outlier is removed the estimated standard deviation shrinks and the modest outlier now looks unusual\u000athis problem of masking gets worse as the complexity of the data increases for example in regression problems diagnostic plots are used to identify outliers however it is common that once a few outliers have been removed others become visible the problem is even worse in higher dimensions\u000arobust methods provide automatic ways of detecting downweighting or removing and flagging outliers largely removing the need for manual screening care must be taken initial data showing the ozone hole first appearing over antarctica were rejected as outliers by nonhuman screening\u000a\u000a\u000a variety of applications \u000aalthough this article deals with general principles for univariate statistical methods robust methods also exist for regression problems generalized linear models and parameter estimation of various distributions\u000a\u000a\u000a measures of robustness \u000athe basic tools used to describe and measure robustness are the breakdown point the influence function and the sensitivity curve\u000a\u000a\u000a breakdown point \u000aintuitively the breakdown point of an estimator is the proportion of incorrect observations eg arbitrarily large observations an estimator can handle before giving an incorrect eg arbitrarily large result for example given  independent random variables  and the corresponding realizations  we can use  to estimate the mean such an estimator has a breakdown point of 0 because we can make  arbitrarily large just by changing any of \u000athe higher the breakdown point of an estimator the more robust it is intuitively we can understand that a breakdown point cannot exceed 50 because if more than half of the observations are contaminated it is not possible to distinguish between the underlying distribution and the contaminating distribution therefore the maximum breakdown point is 05 and there are estimators which achieve such a breakdown point for example the median has a breakdown point of 05 the x trimmed mean has breakdown point of x for the chosen level of x huber 1981 and maronna et al 2006 contain more details the level and the power breakdown points of tests are investigated in he et al 1990\u000astatistics with high breakdown points are sometimes called resistant statistics\u000a\u000a\u000a example speed of light data \u000ain the speed of light example removing the two lowest observations causes the mean to change from 262 to 2775 a change of 155 the estimate of scale produced by the qn method is 63 we can divide this by the square root of the sample size to get a robust standard error and we find this quantity to be 078 thus the change in the mean resulting from removing two outliers is approximately twice the robust standard error\u000athe 10 trimmed mean for the speed of light data is 2743 removing the two lowest observations and recomputing gives 2767 clearly the trimmed mean is less affected by the outliers and has a higher breakdown point\u000anotice that if we replace the lowest observation 44 by 1000 the mean becomes 1173 whereas the 10 trimmed mean is still 2743 in many areas of applied statistics it is common for data to be logtransformed to make them near symmetrical very small values become large negative when logtransformed and zeroes become negatively infinite therefore this example is of practical interest\u000a\u000a\u000a empirical influence function \u000a\u000athe empirical influence function is a measure of the dependence of the estimator on the value of one of the points in the sample it is a modelfree measure in the sense that it simply relies on calculating the estimator again with a different sample on the right is tukeys biweight function which as we will later see is an example of what a good in a sense defined later on empirical influence function should look like\u000ain mathematical terms an influence function is defined as a vector in the space of the estimator which is in turn defined for a sample which is a subset of the population\u000a is a probability space\u000a is a measure space state space\u000a is a parameter space of dimension \u000a is a measure space\u000afor example\u000a is any probability space\u000a\u000a\u000a\u000athe definition of an empirical influence function is let  and  are iid and  is a sample from these variables  is an estimator let  the empirical influence function  at observation  is defined by\u000a\u000anote that \u000awhat this actually means is that we are replacing the ith value in the sample by an arbitrary value and looking at the output of the estimator alternatively the eif is defined as the scaled by n1 instead of n effect on the estimator of adding the point  to the sample\u000a\u000a\u000a influence function and sensitivity curve \u000ainstead of relying solely on the data we could use the distribution of the random variables the approach is quite different from that of the previous paragraph what we are now trying to do is to see what happens to an estimator when we change the distribution of the data slightly it assumes a distribution and measures sensitivity to change in this distribution by contrast the empirical influence assumes a sample set and measures sensitivity to change in the samples\u000alet  be a convex subset of the set of all finite signed measures on  we want to estimate the parameter  of a distribution  in  let the functional  be the asymptotic value of some estimator sequence  we will suppose that this functional is fisher consistent ie  this means that at the model  the estimator sequence asymptotically measures the correct quantity\u000alet  be some distribution in  what happens when the data doesnt follow the model  exactly but another slightly different going towards \u000awere looking at \u000awhich is the onesided directional derivative of  at  in the direction of \u000alet   is the probability measure which gives mass 1 to  we choose  the influence function is then defined by\u000a\u000ait describes the effect of an infinitesimal contamination at the point  on the estimate we are seeking standardized by the mass  of the contamination the asymptotic bias caused by contamination in the observations for a robust estimator we want a bounded influence function that is one which does not go to infinity as x becomes arbitrarily large\u000a\u000a\u000a desirable properties \u000aproperties of an influence function which bestow it with desirable performance are\u000afinite rejection point \u000asmall grosserror sensitivity \u000asmall localshift sensitivity \u000a\u000a\u000a rejection point \u000a\u000a\u000a grosserror sensitivity \u000a\u000a\u000a localshift sensitivity \u000a\u000athis value which looks a lot like a lipschitz constant represents the effect of shifting an observation slightly from  to a neighbouring point  ie add an observation at  and remove one at \u000a\u000a\u000a mestimators \u000a\u000athe mathematical context of this paragraph is given in the section on empirical influence functions\u000ahistorically several approaches to robust estimation were proposed including restimators and lestimators however mestimators now appear to dominate the field as a result of their generality high breakdown point and their efficiency see huber 1981\u000amestimators are a generalization of maximum likelihood estimators mles what we try to do with mles is to maximize  or equivalently minimize  in 1964 huber proposed to generalize this to the minimization of  where  is some function mle are therefore a special case of mestimators hence the name maximum likelihood type estimators\u000aminimizing  can often be done by differentiating  and solving  where  if  has a derivative\u000aseveral choices of  and  have been proposed the two figures below show four  functions and their corresponding  functions\u000a\u000afor squared errors  increases at an accelerating rate whilst for absolute errors it increases at a constant rate when winsorizing is used a mixture of these two effects is introduced for small values of x  increases at the squared rate but once the chosen threshold is reached 15 in this example the rate of increase becomes constant this winsorised estimator is also known as the huber loss function\u000atukeys biweight also known as bisquare function behaves in a similar way to the squared error function at first but for larger errors the function tapers off\u000a\u000a\u000a properties of mestimators \u000anotice that mestimators do not necessarily relate to a probability density function therefore offtheshelf approaches to inference that arise from likelihood theory can not in general be used\u000ait can be shown that mestimators are asymptotically normally distributed so that as long as their standard errors can be computed an approximate approach to inference is available\u000asince mestimators are normal only asymptotically for small sample sizes it might be appropriate to use an alternative approach to inference such as the bootstrap however mestimates are not necessarily unique ie there might be more than one solution that satisfies the equations also it is possible that any particular bootstrap sample can contain more outliers than the estimators breakdown point therefore some care is needed when designing bootstrap schemes\u000aof course as we saw with the speed of light example the mean is only normally distributed asymptotically and when outliers are present the approximation can be very poor even for quite large samples however classical statistical tests including those based on the mean are typically bounded above by the nominal size of the test the same is not true of mestimators and the type i error rate can be substantially above the nominal level\u000athese considerations do not invalidate mestimation in any way they merely make clear that some care is needed in their use as is true of any other method of estimation\u000a\u000a\u000a influence function of an mestimator \u000ait can be shown that the influence function of an mestimator  is proportional to  see huber 1981 and 2004 page 45 which means we can derive the properties of such an estimator such as its rejection point grosserror sensitivity or localshift sensitivity when we know its  function\u000a with the  given by \u000a\u000a\u000a choice of  and  \u000ain many practical situations the choice of the  function is not critical to gaining a good robust estimate and many choices will give similar results that offer great improvements in terms of efficiency and bias over classical estimates in the presence of outliers huber 1981\u000atheoretically  functions are to be preferred and tukeys biweight also known as bisquare function is a popular choice maronna et al 2006 recommend the biweight function with efficiency at the normal set to 85\u000a\u000a\u000a robust parametric approaches \u000amestimators do not necessarily relate to a density function and so are not fully parametric fully parametric approaches to robust modeling and inference both bayesian and likelihood approaches usually deal with heavy tailed distributions such as students tdistribution\u000afor the tdistribution with  degrees of freedom it can be shown that\u000a\u000afor  the tdistribution is equivalent to the cauchy distribution notice that the degrees of freedom is sometimes known as the kurtosis parameter it is the parameter that controls how heavy the tails are in principle  can be estimated from the data in the same way as any other parameter in practice it is common for there to be multiple local maxima when  is allowed to vary as such it is common to fix  at a value around 4 or 6 the figure below displays the function for 4 different values of \u000a\u000a\u000a example speed of light data \u000afor the speed of light data allowing the kurtosis parameter to vary and maximizing the likelihood we get\u000a\u000afixing  and maximizing the likelihood gives\u000a\u000a\u000a related concepts \u000aa pivotal quantity is a function of data whose underlying population distribution is a member of a parametric family that is not dependent on the values of the parameters an ancillary statistic is such a function that is also a statistic meaning that it is computed in terms of the data alone such functions are robust to parameters in the sense that they are independent of the values of the parameters but not robust to the model in the sense that they assume an underlying model parametric family and in fact such functions are often very sensitive to violations of the model assumptions thus test statistics frequently constructed in terms of these to not be sensitive to assumptions about parameters are still very sensitive to model assumptions\u000a\u000a\u000a replacing outliers and missing values \u000aif there are relatively few missing points there are some models which can be used to estimate values to complete the series such as replacing missing values with the mean or median of the data simple linear regression can also be used to estimate missing values macdonald and zucchini 1997 harvey 1989 in addition outliers can sometimes be accommodated in the data through the use of trimmed means other scale estimators apart from standard deviation eg mad and winsorization mcbean and rovers 1998 in calculations of a trimmed mean a fixed percentage of data is dropped from each end of an ordered data thus eliminating the outliers the mean is then calculated using the remaining data winsorizing involves accommodating an outlier by replacing it with the next highest or next smallest value as appropriate rustum  adeloye 2007\u000ahowever using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable particularly if the number of values to be infilled is relatively high in comparison with total record length the accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends rosen and lennox 2001 the in a case of a dynamic process so any variable is dependent not just on the historical time series of the same variable but also on several other variables or parameters of the process in other words the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers a multivariate model will therefore be more representative than a univariate one for predicting missing values the kohonin self organising map ksom offers a simple and robust multivariate model for data analysis thus providing good possibilities to estimate missing values taking into account its relationship or correlation with other pertinent variables in the data record rustum  adeloye 2007\u000astandard kalman filters are not robust to outliers to this end ting theodorou and schaal have recently shown that a modification of masreliezs theorem can deal with outliers\u000aone common approach to handle outliers in data analysis is to perform outlier detection first followed by an efficient estimation method eg the least squares while this approach is often useful one must keep in mind two challenges first an outlier detection method that relies on a nonrobust initial fit can suffer from the effect of masking that is a group of outliers can mask each other and escape detection rousseeuw and leroy 2007 second if a high breakdown initial fit is used for outlier detection the followup analysis might inherit some of the inefficiencies of the initial estimator he and portnoy 1992\u000a\u000a\u000a see also \u000arobust confidence intervals\u000arobust regression\u000aunitweighted regression
p505
sg20
g23
sg24
g27
sg30
Vrobust statistics are statistics with good performance for data drawn from a wide range of probability distributions especially for distributions that are not normal robust statistical methods have been developed for many common problems such as estimating location scale and regression parameters one motivation is to produce statistical methods that are not unduly affected by outliers another motivation is to provide methods with good performance when there are small departures from parametric distributions for example robust methods work well for mixtures of two normal distributions with different standarddeviations under this model nonrobust methods like a ttest work badly\u000a\u000a\u000a introduction \u000arobust statistics seeks to provide methods that emulate popular statistical methods but which are not unduly affected by outliers or other small departures from model assumptions in statistics classical estimation methods rely heavily on assumptions which are often not met in practice in particular it is often assumed that the data errors are normally distributed at least approximately or that the central limit theorem can be relied on to produce normally distributed estimates unfortunately when there are outliers in the data classical estimators often have very poor performance when judged using the breakdown point and the influence function described below\u000athe practical effect of problems seen in the influence function can be studied empirically by examining the sampling distribution of proposed estimators under a mixture model where one mixes in a small amount 15 is often sufficient of contamination for instance one may use a mixture of 95 a normal distribution and 5 a normal distribution with the same mean but significantly higher standard deviation representing outliers\u000arobust parametric statistics can proceed in two ways\u000aby designing estimators so that a preselected behaviour of the influence function is achieved\u000aby replacing estimators that are optimal under the assumption of a normal distribution with estimators that are optimal for or at least derived for other distributions for example using the tdistribution with low degrees of freedom high kurtosis degrees of freedom between 4 and 6 have often been found to be useful in practice or with a mixture of two or more distributions\u000arobust estimates have been studied for the following problems\u000aestimating location parameters\u000aestimating scale parameters\u000aestimating regression coefficients\u000aestimation of modelstates in models expressed in statespace form for which the standard method is equivalent to a kalman filter\u000a\u000a\u000a examples \u000athe median is a robust measure of central tendency while the mean is not the median has a breakdown point of 50 while the mean has a breakdown point of 0 a single large observation can throw it off\u000athe median absolute deviation and interquartile range are robust measures of statistical dispersion while the standard deviation and range are not\u000atrimmed estimators and winsorised estimators are general methods to make statistics more robust lestimators are a general class of simple statistics often robust while mestimators are a general class of robust statistics and are now the preferred solution though they can be quite involved to calculate\u000a\u000a\u000a definition \u000athere are various definitions of a robust statistic strictly speaking a robust statistic is resistant to errors in the results produced by deviations from assumptions eg of normality this means that if the assumptions are only approximately met the robust estimator will still have a reasonable efficiency and reasonably small bias as well as being asymptotically unbiased meaning having a bias tending towards 0 as the sample size tends towards infinity\u000aone of the most important cases is distributional robustness classical statistical procedures are typically sensitive to longtailedness eg when the distribution of the data has longer tails than the assumed normal distribution thus in the context of robust statistics distributionally robust and outlierresistant are effectively synonymous for one perspective on research in robust statistics up to 2000 see portnoy and he 2000\u000aa related topic is that of resistant statistics which are resistant to the effect of extreme scores\u000a\u000a\u000a example speed of light data \u000agelman et al in bayesian data analysis 2004 consider a data set relating to speed of light measurements made by simon newcomb the data sets for that book can be found via the classic data sets page and the books website contains more information on the data\u000aalthough the bulk of the data look to be more or less normally distributed there are two obvious outliers these outliers have a large effect on the mean dragging it towards them and away from the center of the bulk of the data thus if the mean is intended as a measure of the location of the center of the data it is in a sense biased when outliers are present\u000aalso the distribution of the mean is known to be asymptotically normal due to the central limit theorem however outliers can make the distribution of the mean nonnormal even for fairly large data sets besides this nonnormality the mean is also inefficient in the presence of outliers and less variable measures of location are available\u000a\u000a\u000a estimation of location \u000athe plot below shows a density plot of the speed of light data together with a rug plot panel a also shown is a normal qq plot panel b the outliers are clearly visible in these plots\u000apanels c and d of the plot show the bootstrap distribution of the mean c and the 10 trimmed mean d the trimmed mean is a simple robust estimator of location that deletes a certain percentage of observations 10 here from each end of the data then computes the mean in the usual way the analysis was performed in r and 10000 bootstrap samples were used for each of the raw and trimmed means\u000athe distribution of the mean is clearly much wider than that of the 10 trimmed mean the plots are on the same scale also note that whereas the distribution of the trimmed mean appears to be close to normal the distribution of the raw mean is quite skewed to the left so in this sample of 66 observations only 2 outliers cause the central limit theorem to be inapplicable\u000a\u000arobust statistical methods of which the trimmed mean is a simple example seek to outperform classical statistical methods in the presence of outliers or more generally when underlying parametric assumptions are not quite correct\u000awhilst the trimmed mean performs well relative to the mean in this example better robust estimates are available in fact the mean median and trimmed mean are all special cases of mestimators details appear in the sections below\u000a\u000a\u000a estimation of scale \u000a\u000athe outliers in the speed of light data have more than just an adverse effect on the mean the usual estimate of scale is the standard deviation and this quantity is even more badly affected by outliers because the squares of the deviations from the mean go into the calculation so the outliers effects are exacerbated\u000athe plots below show the bootstrap distributions of the standard deviation median absolute deviation mad and qn estimator of scale rousseeuw and croux 1993 the plots are based on 10000 bootstrap samples for each estimator with some gaussian noise added to the resampled data smoothed bootstrap panel a shows the distribution of the standard deviation b of the mad and c of qn\u000a\u000athe distribution of standard deviation is erratic and wide a result of the outliers the mad is better behaved and qn is a little bit more efficient than mad this simple example demonstrates that when outliers are present the standard deviation cannot be recommended as an estimate of scale\u000a\u000a\u000a manual screening for outliers \u000atraditionally statisticians would manually screen data for outliers and remove them usually checking the source of the data to see if the outliers were erroneously recorded indeed in the speed of light example above it is easy to see and remove the two outliers prior to proceeding with any further analysis however in modern times data sets often consist of large numbers of variables being measured on large numbers of experimental units therefore manual screening for outliers is often impractical\u000aoutliers can often interact in such a way that they mask each other as a simple example consider a small univariate data set containing one modest and one large outlier the estimated standard deviation will be grossly inflated by the large outlier the result is that the modest outlier looks relatively normal as soon as the large outlier is removed the estimated standard deviation shrinks and the modest outlier now looks unusual\u000athis problem of masking gets worse as the complexity of the data increases for example in regression problems diagnostic plots are used to identify outliers however it is common that once a few outliers have been removed others become visible the problem is even worse in higher dimensions\u000arobust methods provide automatic ways of detecting downweighting or removing and flagging outliers largely removing the need for manual screening care must be taken initial data showing the ozone hole first appearing over antarctica were rejected as outliers by nonhuman screening\u000a\u000a\u000a variety of applications \u000aalthough this article deals with general principles for univariate statistical methods robust methods also exist for regression problems generalized linear models and parameter estimation of various distributions\u000a\u000a\u000a measures of robustness \u000athe basic tools used to describe and measure robustness are the breakdown point the influence function and the sensitivity curve\u000a\u000a\u000a breakdown point \u000aintuitively the breakdown point of an estimator is the proportion of incorrect observations eg arbitrarily large observations an estimator can handle before giving an incorrect eg arbitrarily large result for example given  independent random variables  and the corresponding realizations  we can use  to estimate the mean such an estimator has a breakdown point of 0 because we can make  arbitrarily large just by changing any of \u000athe higher the breakdown point of an estimator the more robust it is intuitively we can understand that a breakdown point cannot exceed 50 because if more than half of the observations are contaminated it is not possible to distinguish between the underlying distribution and the contaminating distribution therefore the maximum breakdown point is 05 and there are estimators which achieve such a breakdown point for example the median has a breakdown point of 05 the x trimmed mean has breakdown point of x for the chosen level of x huber 1981 and maronna et al 2006 contain more details the level and the power breakdown points of tests are investigated in he et al 1990\u000astatistics with high breakdown points are sometimes called resistant statistics\u000a\u000a\u000a example speed of light data \u000ain the speed of light example removing the two lowest observations causes the mean to change from 262 to 2775 a change of 155 the estimate of scale produced by the qn method is 63 we can divide this by the square root of the sample size to get a robust standard error and we find this quantity to be 078 thus the change in the mean resulting from removing two outliers is approximately twice the robust standard error\u000athe 10 trimmed mean for the speed of light data is 2743 removing the two lowest observations and recomputing gives 2767 clearly the trimmed mean is less affected by the outliers and has a higher breakdown point\u000anotice that if we replace the lowest observation 44 by 1000 the mean becomes 1173 whereas the 10 trimmed mean is still 2743 in many areas of applied statistics it is common for data to be logtransformed to make them near symmetrical very small values become large negative when logtransformed and zeroes become negatively infinite therefore this example is of practical interest\u000a\u000a\u000a empirical influence function \u000a\u000athe empirical influence function is a measure of the dependence of the estimator on the value of one of the points in the sample it is a modelfree measure in the sense that it simply relies on calculating the estimator again with a different sample on the right is tukeys biweight function which as we will later see is an example of what a good in a sense defined later on empirical influence function should look like\u000ain mathematical terms an influence function is defined as a vector in the space of the estimator which is in turn defined for a sample which is a subset of the population\u000a is a probability space\u000a is a measure space state space\u000a is a parameter space of dimension \u000a is a measure space\u000afor example\u000a is any probability space\u000a\u000a\u000a\u000athe definition of an empirical influence function is let  and  are iid and  is a sample from these variables  is an estimator let  the empirical influence function  at observation  is defined by\u000a\u000anote that \u000awhat this actually means is that we are replacing the ith value in the sample by an arbitrary value and looking at the output of the estimator alternatively the eif is defined as the scaled by n1 instead of n effect on the estimator of adding the point  to the sample\u000a\u000a\u000a influence function and sensitivity curve \u000ainstead of relying solely on the data we could use the distribution of the random variables the approach is quite different from that of the previous paragraph what we are now trying to do is to see what happens to an estimator when we change the distribution of the data slightly it assumes a distribution and measures sensitivity to change in this distribution by contrast the empirical influence assumes a sample set and measures sensitivity to change in the samples\u000alet  be a convex subset of the set of all finite signed measures on  we want to estimate the parameter  of a distribution  in  let the functional  be the asymptotic value of some estimator sequence  we will suppose that this functional is fisher consistent ie  this means that at the model  the estimator sequence asymptotically measures the correct quantity\u000alet  be some distribution in  what happens when the data doesnt follow the model  exactly but another slightly different going towards \u000awere looking at \u000awhich is the onesided directional derivative of  at  in the direction of \u000alet   is the probability measure which gives mass 1 to  we choose  the influence function is then defined by\u000a\u000ait describes the effect of an infinitesimal contamination at the point  on the estimate we are seeking standardized by the mass  of the contamination the asymptotic bias caused by contamination in the observations for a robust estimator we want a bounded influence function that is one which does not go to infinity as x becomes arbitrarily large\u000a\u000a\u000a desirable properties \u000aproperties of an influence function which bestow it with desirable performance are\u000afinite rejection point \u000asmall grosserror sensitivity \u000asmall localshift sensitivity \u000a\u000a\u000a rejection point \u000a\u000a\u000a grosserror sensitivity \u000a\u000a\u000a localshift sensitivity \u000a\u000athis value which looks a lot like a lipschitz constant represents the effect of shifting an observation slightly from  to a neighbouring point  ie add an observation at  and remove one at \u000a\u000a\u000a mestimators \u000a\u000athe mathematical context of this paragraph is given in the section on empirical influence functions\u000ahistorically several approaches to robust estimation were proposed including restimators and lestimators however mestimators now appear to dominate the field as a result of their generality high breakdown point and their efficiency see huber 1981\u000amestimators are a generalization of maximum likelihood estimators mles what we try to do with mles is to maximize  or equivalently minimize  in 1964 huber proposed to generalize this to the minimization of  where  is some function mle are therefore a special case of mestimators hence the name maximum likelihood type estimators\u000aminimizing  can often be done by differentiating  and solving  where  if  has a derivative\u000aseveral choices of  and  have been proposed the two figures below show four  functions and their corresponding  functions\u000a\u000afor squared errors  increases at an accelerating rate whilst for absolute errors it increases at a constant rate when winsorizing is used a mixture of these two effects is introduced for small values of x  increases at the squared rate but once the chosen threshold is reached 15 in this example the rate of increase becomes constant this winsorised estimator is also known as the huber loss function\u000atukeys biweight also known as bisquare function behaves in a similar way to the squared error function at first but for larger errors the function tapers off\u000a\u000a\u000a properties of mestimators \u000anotice that mestimators do not necessarily relate to a probability density function therefore offtheshelf approaches to inference that arise from likelihood theory can not in general be used\u000ait can be shown that mestimators are asymptotically normally distributed so that as long as their standard errors can be computed an approximate approach to inference is available\u000asince mestimators are normal only asymptotically for small sample sizes it might be appropriate to use an alternative approach to inference such as the bootstrap however mestimates are not necessarily unique ie there might be more than one solution that satisfies the equations also it is possible that any particular bootstrap sample can contain more outliers than the estimators breakdown point therefore some care is needed when designing bootstrap schemes\u000aof course as we saw with the speed of light example the mean is only normally distributed asymptotically and when outliers are present the approximation can be very poor even for quite large samples however classical statistical tests including those based on the mean are typically bounded above by the nominal size of the test the same is not true of mestimators and the type i error rate can be substantially above the nominal level\u000athese considerations do not invalidate mestimation in any way they merely make clear that some care is needed in their use as is true of any other method of estimation\u000a\u000a\u000a influence function of an mestimator \u000ait can be shown that the influence function of an mestimator  is proportional to  see huber 1981 and 2004 page 45 which means we can derive the properties of such an estimator such as its rejection point grosserror sensitivity or localshift sensitivity when we know its  function\u000a with the  given by \u000a\u000a\u000a choice of  and  \u000ain many practical situations the choice of the  function is not critical to gaining a good robust estimate and many choices will give similar results that offer great improvements in terms of efficiency and bias over classical estimates in the presence of outliers huber 1981\u000atheoretically  functions are to be preferred and tukeys biweight also known as bisquare function is a popular choice maronna et al 2006 recommend the biweight function with efficiency at the normal set to 85\u000a\u000a\u000a robust parametric approaches \u000amestimators do not necessarily relate to a density function and so are not fully parametric fully parametric approaches to robust modeling and inference both bayesian and likelihood approaches usually deal with heavy tailed distributions such as students tdistribution\u000afor the tdistribution with  degrees of freedom it can be shown that\u000a\u000afor  the tdistribution is equivalent to the cauchy distribution notice that the degrees of freedom is sometimes known as the kurtosis parameter it is the parameter that controls how heavy the tails are in principle  can be estimated from the data in the same way as any other parameter in practice it is common for there to be multiple local maxima when  is allowed to vary as such it is common to fix  at a value around 4 or 6 the figure below displays the function for 4 different values of \u000a\u000a\u000a example speed of light data \u000afor the speed of light data allowing the kurtosis parameter to vary and maximizing the likelihood we get\u000a\u000afixing  and maximizing the likelihood gives\u000a\u000a\u000a related concepts \u000aa pivotal quantity is a function of data whose underlying population distribution is a member of a parametric family that is not dependent on the values of the parameters an ancillary statistic is such a function that is also a statistic meaning that it is computed in terms of the data alone such functions are robust to parameters in the sense that they are independent of the values of the parameters but not robust to the model in the sense that they assume an underlying model parametric family and in fact such functions are often very sensitive to violations of the model assumptions thus test statistics frequently constructed in terms of these to not be sensitive to assumptions about parameters are still very sensitive to model assumptions\u000a\u000a\u000a replacing outliers and missing values \u000aif there are relatively few missing points there are some models which can be used to estimate values to complete the series such as replacing missing values with the mean or median of the data simple linear regression can also be used to estimate missing values macdonald and zucchini 1997 harvey 1989 in addition outliers can sometimes be accommodated in the data through the use of trimmed means other scale estimators apart from standard deviation eg mad and winsorization mcbean and rovers 1998 in calculations of a trimmed mean a fixed percentage of data is dropped from each end of an ordered data thus eliminating the outliers the mean is then calculated using the remaining data winsorizing involves accommodating an outlier by replacing it with the next highest or next smallest value as appropriate rustum  adeloye 2007\u000ahowever using these types of models to predict missing values or outliers in a long time series is difficult and often unreliable particularly if the number of values to be infilled is relatively high in comparison with total record length the accuracy of the estimate depends on how good and representative the model is and how long the period of missing values extends rosen and lennox 2001 the in a case of a dynamic process so any variable is dependent not just on the historical time series of the same variable but also on several other variables or parameters of the process in other words the problem is an exercise in multivariate analysis rather than the univariate approach of most of the traditional methods of estimating missing values and outliers a multivariate model will therefore be more representative than a univariate one for predicting missing values the kohonin self organising map ksom offers a simple and robust multivariate model for data analysis thus providing good possibilities to estimate missing values taking into account its relationship or correlation with other pertinent variables in the data record rustum  adeloye 2007\u000astandard kalman filters are not robust to outliers to this end ting theodorou and schaal have recently shown that a modification of masreliezs theorem can deal with outliers\u000aone common approach to handle outliers in data analysis is to perform outlier detection first followed by an efficient estimation method eg the least squares while this approach is often useful one must keep in mind two challenges first an outlier detection method that relies on a nonrobust initial fit can suffer from the effect of masking that is a group of outliers can mask each other and escape detection rousseeuw and leroy 2007 second if a high breakdown initial fit is used for outlier detection the followup analysis might inherit some of the inefficiencies of the initial estimator he and portnoy 1992\u000a\u000a\u000a see also \u000arobust confidence intervals\u000arobust regression\u000aunitweighted regression\u000a\u000a\u000a
p506
sg32
g35
sg37
NsbsS'shrinkage_(statistics).txt'
p507
g2
(g3
g4
Ntp508
Rp509
(dp510
g8
g11
sg12
Vin statistics shrinkage has two meanings\u000ain relation to the general observation that in regression analysis a fitted relationship appears to perform less well on a new data set than on the data set used for fitting in particular the value of the coefficient of determination shrinks this idea is complementary to overfitting and separately to the standard adjustment made in the coefficient of determination to compensate for the subjunctive effects of further sampling like controlling for the potential of new explanatory terms improving the model by chance that is the adjustment formula itself provides shrinkage but the adjustment formula yields an artificial shrinkage in contrast to the first definition\u000ato describe general types of estimators or the effects of some types of estimation whereby a naive or raw estimate is improved by combining it with other information see shrinkage estimator the term relates to the notion that the improved estimate is at a reduced distance from the value supplied by the other information than is the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000aa common idea underlying both of these meanings is the reduction in the effects of sampling variation\u000a\u000a\u000a
p511
sg14
g17
sg18
Vin statistics shrinkage has two meanings\u000ain relation to the general observation that in regression analysis a fitted relationship appears to perform less well on a new data set than on the data set used for fitting in particular the value of the coefficient of determination shrinks this idea is complementary to overfitting and separately to the standard adjustment made in the coefficient of determination to compensate for the subjunctive effects of further sampling like controlling for the potential of new explanatory terms improving the model by chance that is the adjustment formula itself provides shrinkage but the adjustment formula yields an artificial shrinkage in contrast to the first definition\u000ato describe general types of estimators or the effects of some types of estimation whereby a naive or raw estimate is improved by combining it with other information see shrinkage estimator the term relates to the notion that the improved estimate is at a reduced distance from the value supplied by the other information than is the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000aa common idea underlying both of these meanings is the reduction in the effects of sampling variation
p512
sg20
g23
sg24
g27
sg30
Vin statistics shrinkage has two meanings\u000ain relation to the general observation that in regression analysis a fitted relationship appears to perform less well on a new data set than on the data set used for fitting in particular the value of the coefficient of determination shrinks this idea is complementary to overfitting and separately to the standard adjustment made in the coefficient of determination to compensate for the subjunctive effects of further sampling like controlling for the potential of new explanatory terms improving the model by chance that is the adjustment formula itself provides shrinkage but the adjustment formula yields an artificial shrinkage in contrast to the first definition\u000ato describe general types of estimators or the effects of some types of estimation whereby a naive or raw estimate is improved by combining it with other information see shrinkage estimator the term relates to the notion that the improved estimate is at a reduced distance from the value supplied by the other information than is the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000aa common idea underlying both of these meanings is the reduction in the effects of sampling variation\u000a\u000a\u000a
p513
sg32
g35
sg37
NsbsS'fisher_transformation.txt'
p514
g2
(g3
g4
Ntp515
Rp516
(dp517
g8
g11
sg12
Vin statistics hypotheses about the value of the population correlation coefficient  between variables x and y can be tested using the fisher transformation aka fisher ztransformation applied to the sample correlation coefficient \u000a\u000a\u000a definition \u000agiven a set of n bivariate sample pairs xi yi i  1  n the sample correlation coefficient r is given by\u000a\u000afishers ztransformation of r is defined as\u000a\u000awhere ln is the natural logarithm function and arctanh is the inverse hyperbolic tangent function\u000aif x y has a bivariate normal distribution and if the pairs xi yi are independent then z is approximately normally distributed with mean\u000a\u000aand standard error\u000a\u000awhere n is the sample size and  is the true correlation coefficient\u000athis transformation and its inverse\u000a\u000acan be used to construct a confidence interval for \u000a\u000a\u000a discussion \u000athe fisher transformation is an approximate variancestabilizing transformation for r when x and y follow a bivariate normal distribution this means that the variance of z is approximately constant for all values of the population correlation coefficient  without the fisher transformation the variance of r grows smaller as  gets closer to 1 since the fisher transformation is approximately the identity function when r  12 it is sometimes useful to remember that the variance of r is well approximated by 1n as long as  is not too large and n is not too small this is related to the fact that the asymptotic variance of r is 1 for bivariate normal data\u000athe behavior of this transform has been extensively studied since fisher introduced it in 1915 fisher himself found the exact distribution of z for data from a bivariate normal distribution in 1921 gayen 1951 determined the exact distribution of z for data from a bivariate type a edgeworth distribution hotelling in 1953 calculated the taylor series expressions for the moments of z and several related statistics and hawkins in 1989 discovered the asymptotic distribution of z for data from a distribution with bounded fourth moments\u000a\u000a\u000a other uses \u000awhile the fisher transformation is mainly associated with the pearson productmoment correlation coefficient for bivariate normal observations it can also be applied to spearmans rank correlation coefficient in more general cases a similar result for the asymptotic distribution applies but with a minor adjustment factor see the latter article for details\u000a\u000a\u000a
p518
sg14
g17
sg18
Vin statistics hypotheses about the value of the population correlation coefficient  between variables x and y can be tested using the fisher transformation aka fisher ztransformation applied to the sample correlation coefficient \u000a\u000a\u000a definition \u000agiven a set of n bivariate sample pairs xi yi i  1  n the sample correlation coefficient r is given by\u000a\u000afishers ztransformation of r is defined as\u000a\u000awhere ln is the natural logarithm function and arctanh is the inverse hyperbolic tangent function\u000aif x y has a bivariate normal distribution and if the pairs xi yi are independent then z is approximately normally distributed with mean\u000a\u000aand standard error\u000a\u000awhere n is the sample size and  is the true correlation coefficient\u000athis transformation and its inverse\u000a\u000acan be used to construct a confidence interval for \u000a\u000a\u000a discussion \u000athe fisher transformation is an approximate variancestabilizing transformation for r when x and y follow a bivariate normal distribution this means that the variance of z is approximately constant for all values of the population correlation coefficient  without the fisher transformation the variance of r grows smaller as  gets closer to 1 since the fisher transformation is approximately the identity function when r  12 it is sometimes useful to remember that the variance of r is well approximated by 1n as long as  is not too large and n is not too small this is related to the fact that the asymptotic variance of r is 1 for bivariate normal data\u000athe behavior of this transform has been extensively studied since fisher introduced it in 1915 fisher himself found the exact distribution of z for data from a bivariate normal distribution in 1921 gayen 1951 determined the exact distribution of z for data from a bivariate type a edgeworth distribution hotelling in 1953 calculated the taylor series expressions for the moments of z and several related statistics and hawkins in 1989 discovered the asymptotic distribution of z for data from a distribution with bounded fourth moments\u000a\u000a\u000a other uses \u000awhile the fisher transformation is mainly associated with the pearson productmoment correlation coefficient for bivariate normal observations it can also be applied to spearmans rank correlation coefficient in more general cases a similar result for the asymptotic distribution applies but with a minor adjustment factor see the latter article for details
p519
sg20
g23
sg24
g27
sg30
Vin statistics hypotheses about the value of the population correlation coefficient  between variables x and y can be tested using the fisher transformation aka fisher ztransformation applied to the sample correlation coefficient \u000a\u000a\u000a definition \u000agiven a set of n bivariate sample pairs xi yi i  1  n the sample correlation coefficient r is given by\u000a\u000afishers ztransformation of r is defined as\u000a\u000awhere ln is the natural logarithm function and arctanh is the inverse hyperbolic tangent function\u000aif x y has a bivariate normal distribution and if the pairs xi yi are independent then z is approximately normally distributed with mean\u000a\u000aand standard error\u000a\u000awhere n is the sample size and  is the true correlation coefficient\u000athis transformation and its inverse\u000a\u000acan be used to construct a confidence interval for \u000a\u000a\u000a discussion \u000athe fisher transformation is an approximate variancestabilizing transformation for r when x and y follow a bivariate normal distribution this means that the variance of z is approximately constant for all values of the population correlation coefficient  without the fisher transformation the variance of r grows smaller as  gets closer to 1 since the fisher transformation is approximately the identity function when r  12 it is sometimes useful to remember that the variance of r is well approximated by 1n as long as  is not too large and n is not too small this is related to the fact that the asymptotic variance of r is 1 for bivariate normal data\u000athe behavior of this transform has been extensively studied since fisher introduced it in 1915 fisher himself found the exact distribution of z for data from a bivariate normal distribution in 1921 gayen 1951 determined the exact distribution of z for data from a bivariate type a edgeworth distribution hotelling in 1953 calculated the taylor series expressions for the moments of z and several related statistics and hawkins in 1989 discovered the asymptotic distribution of z for data from a distribution with bounded fourth moments\u000a\u000a\u000a other uses \u000awhile the fisher transformation is mainly associated with the pearson productmoment correlation coefficient for bivariate normal observations it can also be applied to spearmans rank correlation coefficient in more general cases a similar result for the asymptotic distribution applies but with a minor adjustment factor see the latter article for details\u000a\u000a\u000a
p520
sg32
g35
sg37
NsbsS'optimal_design.txt'
p521
g2
(g3
g4
Ntp522
Rp523
(dp524
g8
g11
sg12
Vin the design of experiments optimal designs are a class of experimental designs that are optimal with respect to some statistical criterion the creation of this field of statistics has been credited to danish statistician kirstine smith\u000ain the design of experiments for estimating statistical models optimal designs allow parameters to be estimated without bias and with minimumvariance a nonoptimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design in practical terms optimal experiments can reduce the costs of experimentation\u000athe optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion which is related to the variancematrix of the estimator specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments\u000aoptimal designs are also called optimum designs\u000a\u000a\u000a advantages \u000aoptimal designs offer three advantages over suboptimal experimental designs\u000aoptimal designs reduce the costs of experimentation by allowing statistical models to be estimated with fewer experimental runs\u000aoptimal designs can accommodate multiple types of factors such as process mixture and discrete factors\u000adesigns can be optimized when the designspace is constrained for example when the mathematical processspace contains factorsettings that are practically infeasible eg due to safety concerns\u000a\u000a\u000a minimizing the variance of estimators \u000aexperimental designs are evaluated using statistical criteria\u000ait is known that the least squares estimator minimizes the variance of meanunbiased estimators under the conditions of the gaussmarkov theorem in the estimation theory for statistical models with one real parameter the reciprocal of the variance of an efficient estimator is called the fisher information for that estimator because of this reciprocity minimizing the variance corresponds to maximizing the information\u000awhen the statistical model has several parameters however the mean of the parameterestimator is a vector and its variance is a matrix the inverse matrix of the variancematrix is called the information matrix because the variance of the estimator of a parameter vector is a matrix the problem of minimizing the variance is complicated using statistical theory statisticians compress the informationmatrix using realvalued summary statistics being realvalued functions these information criteria can be maximized the traditional optimalitycriteria are invariants of the information matrix algebraically the traditional optimalitycriteria are functionals of the eigenvalues of the information matrix\u000aaoptimality average or trace\u000aone criterion is aoptimality which seeks to minimize the trace of the inverse of the information matrix this criterion results in minimizing the average variance of the estimates of the regression coefficients\u000a\u000acoptimality\u000athis criterion minimizes the variance of a best linear unbiased estimator of a predetermined linear combination of model parameters\u000a\u000adoptimality determinant\u000aa popular criterion is doptimality which seeks to minimize xx1 or equivalently maximize the determinant of the information matrix xx of the design this criterion results in maximizing the differential shannon information content of the parameter estimates\u000a\u000aeoptimality eigenvalue\u000aanother design is eoptimality which maximizes the minimum eigenvalue of the information matrix\u000a\u000atoptimality\u000athis criterion maximizes the trace of the information matrix\u000a\u000aother optimalitycriteria are concerned with the variance of predictions\u000agoptimality\u000aa popular criterion is goptimality which seeks to minimize the maximum entry in the diagonal of the hat matrix xxx1x this has the effect of minimizing the maximum variance of the predicted values\u000a\u000aioptimality integrated\u000aa second criterion on prediction variance is ioptimality which seeks to minimize the average prediction variance over the design space\u000a\u000avoptimality variance\u000aa third criterion on prediction variance is voptimality which seeks to minimize the average prediction variance over a set of m specific points\u000a\u000a\u000a contrasts \u000a\u000ain many applications the statistician is most concerned with a parameter of interest rather than with nuisance parameters more generally statisticians consider linear combinations of parameters which are estimated via linear combinations of treatmentmeans in the design of experiments and in the analysis of variance such linear combinations are called contrasts statisticians can use appropriate optimalitycriteria for such parameters of interest and for more generally for contrasts\u000a\u000a\u000a implementation \u000acatalogs of optimal designs occur in books and in software libraries\u000ain addition major statistical systems like sas and r have procedures for optimizing a design according to a users specification the experimenter must specify a model for the design and an optimalitycriterion before the method can compute an optimal design\u000a\u000a\u000a practical considerations \u000asome advanced topics in optimal design require more statistical theory and practical knowledge in designing experiments\u000a\u000a\u000a model dependence and robustness \u000asince the optimality criterion of most optimal designs is based on some function of the information matrix the optimality of a given design is model dependent while an optimal design is best for that model its performance may deteriorate on other models on other models an optimal design can be either better or worse than a nonoptimal design therefore it is important to benchmark the performance of designs under alternative models\u000a\u000a\u000a choosing an optimality criterion and robustness \u000athe choice of an appropriate optimality criterion requires some thought and it is useful to benchmark the performance of designs with respect to several optimality criteria cornell writes that\u000a\u000asince the traditional optimality criteria    are varianceminimizing criteria    a design that is optimal for a given model using one of the    criteria is usually nearoptimal for the same model with respect to the other criteria\u000a\u000aindeed there are several classes of designs for which all the traditional optimalitycriteria agree according to the theory of universal optimality of kiefer the experience of practitioners like cornell and the universal optimality theory of kiefer suggest that robustness with respect to changes in the optimalitycriterion is much greater than is robustness with respect to changes in the model\u000a\u000a\u000a flexible optimality criteria and convex analysis \u000ahighquality statistical software provide a combination of libraries of optimal designs or iterative methods for constructing approximately optimal designs depending on the model specified and the optimality criterion users may use a standard optimalitycriterion or may program a custommade criterion\u000aall of the traditional optimalitycriteria are convex or concave functions and therefore optimaldesigns are amenable to the mathematical theory of convex analysis and their computation can use specialized methods of convex minimization the practitioner need not select exactly one traditional optimalitycriterion but can specify a custom criterion in particular the practitioner can specify a convex criterion using the maxima of convex optimalitycriteria and nonnegative combinations of optimality criteria since these operations preserve convex functions for convex optimality criteria the kieferwolfowitz equivalence theorem allows the practitioner to verify that a given design is globally optimal the kieferwolfowitz equivalence theorem is related with the legendrefenchel conjugacy for convex functions\u000aif an optimalitycriterion lacks convexity then finding a global optimum and verifying its optimality often are difficult\u000a\u000a\u000a model uncertainty and bayesian approaches \u000a\u000a\u000a model selection \u000a\u000awhen scientists wish to test several theories then a statistician can design an experiment that allows optimal tests between specified models such discrimination experiments are especially important in the biostatistics supporting pharmacokinetics and pharmacodynamics following the work of cox and atkinson\u000a\u000a\u000a bayesian experimental design \u000a\u000awhen practitioners need to consider multiple models they can specify a probabilitymeasure on the models and then select any design maximizing the expected value of such an experiment such probabilitybased optimaldesigns are called optimal bayesian designs such bayesian designs are used especially for generalized linear models where the response follows an exponentialfamily distribution\u000athe use of a bayesian design does not force statisticians to use bayesian methods to analyze the data however indeed the bayesian label for probabilitybased experimentaldesigns is disliked by some researchers alternative terminology for bayesian optimality includes onaverage optimality or population optimality\u000a\u000a\u000a iterative experimentation \u000ascientific experimentation is an iterative process and statisticians have developed several approaches to the optimal design of sequential experiments\u000a\u000a\u000a sequential analysis \u000a\u000asequential analysis was pioneered by abraham wald in 1972 herman chernoff wrote an overview of optimal sequential designs while adaptive designs were surveyed later by s zacks of course much work on the optimal design of experiments is related to the theory of optimal decisions especially the statistical decision theory of abraham wald\u000a\u000a\u000a responsesurface methodology \u000a\u000aoptimal designs for responsesurface models are discussed in the textbook by atkinson donev and tobias and in the survey of gaffke and heiligers and in the mathematical text of pukelsheim the blocking of optimal designs is discussed in the textbook of atkinson donev and tobias and also in the monograph by goos\u000athe earliest optimal designs were developed to estimate the parameters of regression models with continuous variables for example by j d gergonne in 1815 stigler in english two early contributions were made by charles s peirce and kirstine smith\u000apioneering designs for multivariate responsesurfaces were proposed by george e p box however boxs designs have few optimality properties indeed the boxbehnken design requires excessive experimental runs when the number of variables exceeds three boxs centralcomposite designs require more experimental runs than do the optimal designs of kno\u000a\u000a\u000a system identification and stochastic approximation \u000a\u000athe optimization of sequential experimentation is studied also in stochastic programming and in systems and control popular methods include stochastic approximation and other methods of stochastic optimization much of this research has been associated with the subdiscipline of system identification in computational optimal control d judin  a nemirovskii and boris polyak has described methods that are more efficient than the armijostyle stepsize rules introduced by g e p box in responsesurface methodology\u000aadaptive designs are used in clinical trials and optimal adaptive designs are surveyed in the handbook of experimental designs chapter by shelemyahu zacks\u000a\u000a\u000a specifying the number of experimental runs \u000a\u000a\u000a using a computer to find a good design \u000athere are several methods of finding an optimal design given an a priori restriction on the number of experimental runs or replications some of these methods are discussed by atkinson donev and tobias and in the paper by hardin and sloane of course fixing the number of experimental runs a priori would be impractical prudent statisticians examine the other optimal designs whose number of experimental runs differ\u000a\u000a\u000a discretizing probabilitymeasure designs \u000ain the mathematical theory on optimal experiments an optimal design can be a probability measure that is supported on an infinite set of observationlocations such optimal probabilitymeasure designs solve a mathematical problem that neglected to specify the cost of observations and experimental runs nonetheless such optimal probabilitymeasure designs can be discretized to furnish approximately optimal designs\u000ain some cases a finite set of observationlocations suffices to support an optimal design such a result was proved by kno and kiefer in their works on responsesurface designs for quadratic models the knokiefer analysis explains why optimal designs for responsesurfaces can have discrete supports which are very similar as do the less efficient designs that have been traditional in response surface methodology\u000a\u000a\u000a history \u000athe prophet of scientific experimentation francis bacon foresaw that experimental designs should be improved researchers who improved experiments were praised in bacons utopian novel new atlantis\u000a\u000athen after divers meetings and consults of our whole number to consider of the former labors and collections we have three that take care out of them to direct new experiments of a higher light more penetrating into nature than the former these we call lamps\u000a\u000ain 1815 an article on optimal designs for polynomial regression was published by joseph diaz gergonne according to stigler\u000acharles s peirce proposed an economic theory of scientific experimentation in 1876 which sought to maximize the precision of the estimates peirces optimal allocation immediately improved the accuracy of gravitational experiments and was used for decades by peirce and his colleagues in his 1882 published lecture at johns hopkins university peirce introduced experimental design with these words\u000a\u000alogic will not undertake to inform you what kind of experiments you ought to make in order best to determine the acceleration of gravity or the value of the ohm but it will tell you how to proceed to form a plan of experimentation\u000a unfortunately practice generally precedes theory and it is the usual fate of mankind to get things done in some boggling way first and find out afterward how they could have been done much more easily and perfectly\u000a\u000alike bacon peirce was aware that experimental methods should strive for substantial improvement even optimality\u000akirstine smith proposed optimal designs for polynomial models in 1918 kirstine smith had been a student of the danish statistician thorvald n thiele and was working with karl pearson in london\u000a\u000a\u000a see also \u000a\u000a\u000a notes \u000a\u000a\u000a
p525
sg14
g17
sg18
Vin the design of experiments optimal designs are a class of experimental designs that are optimal with respect to some statistical criterion the creation of this field of statistics has been credited to danish statistician kirstine smith\u000ain the design of experiments for estimating statistical models optimal designs allow parameters to be estimated without bias and with minimumvariance a nonoptimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design in practical terms optimal experiments can reduce the costs of experimentation\u000athe optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion which is related to the variancematrix of the estimator specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments\u000aoptimal designs are also called optimum designs\u000a\u000a\u000a advantages \u000aoptimal designs offer three advantages over suboptimal experimental designs\u000aoptimal designs reduce the costs of experimentation by allowing statistical models to be estimated with fewer experimental runs\u000aoptimal designs can accommodate multiple types of factors such as process mixture and discrete factors\u000adesigns can be optimized when the designspace is constrained for example when the mathematical processspace contains factorsettings that are practically infeasible eg due to safety concerns\u000a\u000a\u000a minimizing the variance of estimators \u000aexperimental designs are evaluated using statistical criteria\u000ait is known that the least squares estimator minimizes the variance of meanunbiased estimators under the conditions of the gaussmarkov theorem in the estimation theory for statistical models with one real parameter the reciprocal of the variance of an efficient estimator is called the fisher information for that estimator because of this reciprocity minimizing the variance corresponds to maximizing the information\u000awhen the statistical model has several parameters however the mean of the parameterestimator is a vector and its variance is a matrix the inverse matrix of the variancematrix is called the information matrix because the variance of the estimator of a parameter vector is a matrix the problem of minimizing the variance is complicated using statistical theory statisticians compress the informationmatrix using realvalued summary statistics being realvalued functions these information criteria can be maximized the traditional optimalitycriteria are invariants of the information matrix algebraically the traditional optimalitycriteria are functionals of the eigenvalues of the information matrix\u000aaoptimality average or trace\u000aone criterion is aoptimality which seeks to minimize the trace of the inverse of the information matrix this criterion results in minimizing the average variance of the estimates of the regression coefficients\u000a\u000acoptimality\u000athis criterion minimizes the variance of a best linear unbiased estimator of a predetermined linear combination of model parameters\u000a\u000adoptimality determinant\u000aa popular criterion is doptimality which seeks to minimize xx1 or equivalently maximize the determinant of the information matrix xx of the design this criterion results in maximizing the differential shannon information content of the parameter estimates\u000a\u000aeoptimality eigenvalue\u000aanother design is eoptimality which maximizes the minimum eigenvalue of the information matrix\u000a\u000atoptimality\u000athis criterion maximizes the trace of the information matrix\u000a\u000aother optimalitycriteria are concerned with the variance of predictions\u000agoptimality\u000aa popular criterion is goptimality which seeks to minimize the maximum entry in the diagonal of the hat matrix xxx1x this has the effect of minimizing the maximum variance of the predicted values\u000a\u000aioptimality integrated\u000aa second criterion on prediction variance is ioptimality which seeks to minimize the average prediction variance over the design space\u000a\u000avoptimality variance\u000aa third criterion on prediction variance is voptimality which seeks to minimize the average prediction variance over a set of m specific points\u000a\u000a\u000a contrasts \u000a\u000ain many applications the statistician is most concerned with a parameter of interest rather than with nuisance parameters more generally statisticians consider linear combinations of parameters which are estimated via linear combinations of treatmentmeans in the design of experiments and in the analysis of variance such linear combinations are called contrasts statisticians can use appropriate optimalitycriteria for such parameters of interest and for more generally for contrasts\u000a\u000a\u000a implementation \u000acatalogs of optimal designs occur in books and in software libraries\u000ain addition major statistical systems like sas and r have procedures for optimizing a design according to a users specification the experimenter must specify a model for the design and an optimalitycriterion before the method can compute an optimal design\u000a\u000a\u000a practical considerations \u000asome advanced topics in optimal design require more statistical theory and practical knowledge in designing experiments\u000a\u000a\u000a model dependence and robustness \u000asince the optimality criterion of most optimal designs is based on some function of the information matrix the optimality of a given design is model dependent while an optimal design is best for that model its performance may deteriorate on other models on other models an optimal design can be either better or worse than a nonoptimal design therefore it is important to benchmark the performance of designs under alternative models\u000a\u000a\u000a choosing an optimality criterion and robustness \u000athe choice of an appropriate optimality criterion requires some thought and it is useful to benchmark the performance of designs with respect to several optimality criteria cornell writes that\u000a\u000asince the traditional optimality criteria    are varianceminimizing criteria    a design that is optimal for a given model using one of the    criteria is usually nearoptimal for the same model with respect to the other criteria\u000a\u000aindeed there are several classes of designs for which all the traditional optimalitycriteria agree according to the theory of universal optimality of kiefer the experience of practitioners like cornell and the universal optimality theory of kiefer suggest that robustness with respect to changes in the optimalitycriterion is much greater than is robustness with respect to changes in the model\u000a\u000a\u000a flexible optimality criteria and convex analysis \u000ahighquality statistical software provide a combination of libraries of optimal designs or iterative methods for constructing approximately optimal designs depending on the model specified and the optimality criterion users may use a standard optimalitycriterion or may program a custommade criterion\u000aall of the traditional optimalitycriteria are convex or concave functions and therefore optimaldesigns are amenable to the mathematical theory of convex analysis and their computation can use specialized methods of convex minimization the practitioner need not select exactly one traditional optimalitycriterion but can specify a custom criterion in particular the practitioner can specify a convex criterion using the maxima of convex optimalitycriteria and nonnegative combinations of optimality criteria since these operations preserve convex functions for convex optimality criteria the kieferwolfowitz equivalence theorem allows the practitioner to verify that a given design is globally optimal the kieferwolfowitz equivalence theorem is related with the legendrefenchel conjugacy for convex functions\u000aif an optimalitycriterion lacks convexity then finding a global optimum and verifying its optimality often are difficult\u000a\u000a\u000a model uncertainty and bayesian approaches \u000a\u000a\u000a model selection \u000a\u000awhen scientists wish to test several theories then a statistician can design an experiment that allows optimal tests between specified models such discrimination experiments are especially important in the biostatistics supporting pharmacokinetics and pharmacodynamics following the work of cox and atkinson\u000a\u000a\u000a bayesian experimental design \u000a\u000awhen practitioners need to consider multiple models they can specify a probabilitymeasure on the models and then select any design maximizing the expected value of such an experiment such probabilitybased optimaldesigns are called optimal bayesian designs such bayesian designs are used especially for generalized linear models where the response follows an exponentialfamily distribution\u000athe use of a bayesian design does not force statisticians to use bayesian methods to analyze the data however indeed the bayesian label for probabilitybased experimentaldesigns is disliked by some researchers alternative terminology for bayesian optimality includes onaverage optimality or population optimality\u000a\u000a\u000a iterative experimentation \u000ascientific experimentation is an iterative process and statisticians have developed several approaches to the optimal design of sequential experiments\u000a\u000a\u000a sequential analysis \u000a\u000asequential analysis was pioneered by abraham wald in 1972 herman chernoff wrote an overview of optimal sequential designs while adaptive designs were surveyed later by s zacks of course much work on the optimal design of experiments is related to the theory of optimal decisions especially the statistical decision theory of abraham wald\u000a\u000a\u000a responsesurface methodology \u000a\u000aoptimal designs for responsesurface models are discussed in the textbook by atkinson donev and tobias and in the survey of gaffke and heiligers and in the mathematical text of pukelsheim the blocking of optimal designs is discussed in the textbook of atkinson donev and tobias and also in the monograph by goos\u000athe earliest optimal designs were developed to estimate the parameters of regression models with continuous variables for example by j d gergonne in 1815 stigler in english two early contributions were made by charles s peirce and kirstine smith\u000apioneering designs for multivariate responsesurfaces were proposed by george e p box however boxs designs have few optimality properties indeed the boxbehnken design requires excessive experimental runs when the number of variables exceeds three boxs centralcomposite designs require more experimental runs than do the optimal designs of kno\u000a\u000a\u000a system identification and stochastic approximation \u000a\u000athe optimization of sequential experimentation is studied also in stochastic programming and in systems and control popular methods include stochastic approximation and other methods of stochastic optimization much of this research has been associated with the subdiscipline of system identification in computational optimal control d judin  a nemirovskii and boris polyak has described methods that are more efficient than the armijostyle stepsize rules introduced by g e p box in responsesurface methodology\u000aadaptive designs are used in clinical trials and optimal adaptive designs are surveyed in the handbook of experimental designs chapter by shelemyahu zacks\u000a\u000a\u000a specifying the number of experimental runs \u000a\u000a\u000a using a computer to find a good design \u000athere are several methods of finding an optimal design given an a priori restriction on the number of experimental runs or replications some of these methods are discussed by atkinson donev and tobias and in the paper by hardin and sloane of course fixing the number of experimental runs a priori would be impractical prudent statisticians examine the other optimal designs whose number of experimental runs differ\u000a\u000a\u000a discretizing probabilitymeasure designs \u000ain the mathematical theory on optimal experiments an optimal design can be a probability measure that is supported on an infinite set of observationlocations such optimal probabilitymeasure designs solve a mathematical problem that neglected to specify the cost of observations and experimental runs nonetheless such optimal probabilitymeasure designs can be discretized to furnish approximately optimal designs\u000ain some cases a finite set of observationlocations suffices to support an optimal design such a result was proved by kno and kiefer in their works on responsesurface designs for quadratic models the knokiefer analysis explains why optimal designs for responsesurfaces can have discrete supports which are very similar as do the less efficient designs that have been traditional in response surface methodology\u000a\u000a\u000a history \u000athe prophet of scientific experimentation francis bacon foresaw that experimental designs should be improved researchers who improved experiments were praised in bacons utopian novel new atlantis\u000a\u000athen after divers meetings and consults of our whole number to consider of the former labors and collections we have three that take care out of them to direct new experiments of a higher light more penetrating into nature than the former these we call lamps\u000a\u000ain 1815 an article on optimal designs for polynomial regression was published by joseph diaz gergonne according to stigler\u000acharles s peirce proposed an economic theory of scientific experimentation in 1876 which sought to maximize the precision of the estimates peirces optimal allocation immediately improved the accuracy of gravitational experiments and was used for decades by peirce and his colleagues in his 1882 published lecture at johns hopkins university peirce introduced experimental design with these words\u000a\u000alogic will not undertake to inform you what kind of experiments you ought to make in order best to determine the acceleration of gravity or the value of the ohm but it will tell you how to proceed to form a plan of experimentation\u000a unfortunately practice generally precedes theory and it is the usual fate of mankind to get things done in some boggling way first and find out afterward how they could have been done much more easily and perfectly\u000a\u000alike bacon peirce was aware that experimental methods should strive for substantial improvement even optimality\u000akirstine smith proposed optimal designs for polynomial models in 1918 kirstine smith had been a student of the danish statistician thorvald n thiele and was working with karl pearson in london\u000a\u000a\u000a see also \u000a\u000a\u000a notes
p526
sg20
g23
sg24
g27
sg30
Vin the design of experiments optimal designs are a class of experimental designs that are optimal with respect to some statistical criterion the creation of this field of statistics has been credited to danish statistician kirstine smith\u000ain the design of experiments for estimating statistical models optimal designs allow parameters to be estimated without bias and with minimumvariance a nonoptimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design in practical terms optimal experiments can reduce the costs of experimentation\u000athe optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion which is related to the variancematrix of the estimator specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments\u000aoptimal designs are also called optimum designs\u000a\u000a\u000a advantages \u000aoptimal designs offer three advantages over suboptimal experimental designs\u000aoptimal designs reduce the costs of experimentation by allowing statistical models to be estimated with fewer experimental runs\u000aoptimal designs can accommodate multiple types of factors such as process mixture and discrete factors\u000adesigns can be optimized when the designspace is constrained for example when the mathematical processspace contains factorsettings that are practically infeasible eg due to safety concerns\u000a\u000a\u000a minimizing the variance of estimators \u000aexperimental designs are evaluated using statistical criteria\u000ait is known that the least squares estimator minimizes the variance of meanunbiased estimators under the conditions of the gaussmarkov theorem in the estimation theory for statistical models with one real parameter the reciprocal of the variance of an efficient estimator is called the fisher information for that estimator because of this reciprocity minimizing the variance corresponds to maximizing the information\u000awhen the statistical model has several parameters however the mean of the parameterestimator is a vector and its variance is a matrix the inverse matrix of the variancematrix is called the information matrix because the variance of the estimator of a parameter vector is a matrix the problem of minimizing the variance is complicated using statistical theory statisticians compress the informationmatrix using realvalued summary statistics being realvalued functions these information criteria can be maximized the traditional optimalitycriteria are invariants of the information matrix algebraically the traditional optimalitycriteria are functionals of the eigenvalues of the information matrix\u000aaoptimality average or trace\u000aone criterion is aoptimality which seeks to minimize the trace of the inverse of the information matrix this criterion results in minimizing the average variance of the estimates of the regression coefficients\u000a\u000acoptimality\u000athis criterion minimizes the variance of a best linear unbiased estimator of a predetermined linear combination of model parameters\u000a\u000adoptimality determinant\u000aa popular criterion is doptimality which seeks to minimize xx1 or equivalently maximize the determinant of the information matrix xx of the design this criterion results in maximizing the differential shannon information content of the parameter estimates\u000a\u000aeoptimality eigenvalue\u000aanother design is eoptimality which maximizes the minimum eigenvalue of the information matrix\u000a\u000atoptimality\u000athis criterion maximizes the trace of the information matrix\u000a\u000aother optimalitycriteria are concerned with the variance of predictions\u000agoptimality\u000aa popular criterion is goptimality which seeks to minimize the maximum entry in the diagonal of the hat matrix xxx1x this has the effect of minimizing the maximum variance of the predicted values\u000a\u000aioptimality integrated\u000aa second criterion on prediction variance is ioptimality which seeks to minimize the average prediction variance over the design space\u000a\u000avoptimality variance\u000aa third criterion on prediction variance is voptimality which seeks to minimize the average prediction variance over a set of m specific points\u000a\u000a\u000a contrasts \u000a\u000ain many applications the statistician is most concerned with a parameter of interest rather than with nuisance parameters more generally statisticians consider linear combinations of parameters which are estimated via linear combinations of treatmentmeans in the design of experiments and in the analysis of variance such linear combinations are called contrasts statisticians can use appropriate optimalitycriteria for such parameters of interest and for more generally for contrasts\u000a\u000a\u000a implementation \u000acatalogs of optimal designs occur in books and in software libraries\u000ain addition major statistical systems like sas and r have procedures for optimizing a design according to a users specification the experimenter must specify a model for the design and an optimalitycriterion before the method can compute an optimal design\u000a\u000a\u000a practical considerations \u000asome advanced topics in optimal design require more statistical theory and practical knowledge in designing experiments\u000a\u000a\u000a model dependence and robustness \u000asince the optimality criterion of most optimal designs is based on some function of the information matrix the optimality of a given design is model dependent while an optimal design is best for that model its performance may deteriorate on other models on other models an optimal design can be either better or worse than a nonoptimal design therefore it is important to benchmark the performance of designs under alternative models\u000a\u000a\u000a choosing an optimality criterion and robustness \u000athe choice of an appropriate optimality criterion requires some thought and it is useful to benchmark the performance of designs with respect to several optimality criteria cornell writes that\u000a\u000asince the traditional optimality criteria    are varianceminimizing criteria    a design that is optimal for a given model using one of the    criteria is usually nearoptimal for the same model with respect to the other criteria\u000a\u000aindeed there are several classes of designs for which all the traditional optimalitycriteria agree according to the theory of universal optimality of kiefer the experience of practitioners like cornell and the universal optimality theory of kiefer suggest that robustness with respect to changes in the optimalitycriterion is much greater than is robustness with respect to changes in the model\u000a\u000a\u000a flexible optimality criteria and convex analysis \u000ahighquality statistical software provide a combination of libraries of optimal designs or iterative methods for constructing approximately optimal designs depending on the model specified and the optimality criterion users may use a standard optimalitycriterion or may program a custommade criterion\u000aall of the traditional optimalitycriteria are convex or concave functions and therefore optimaldesigns are amenable to the mathematical theory of convex analysis and their computation can use specialized methods of convex minimization the practitioner need not select exactly one traditional optimalitycriterion but can specify a custom criterion in particular the practitioner can specify a convex criterion using the maxima of convex optimalitycriteria and nonnegative combinations of optimality criteria since these operations preserve convex functions for convex optimality criteria the kieferwolfowitz equivalence theorem allows the practitioner to verify that a given design is globally optimal the kieferwolfowitz equivalence theorem is related with the legendrefenchel conjugacy for convex functions\u000aif an optimalitycriterion lacks convexity then finding a global optimum and verifying its optimality often are difficult\u000a\u000a\u000a model uncertainty and bayesian approaches \u000a\u000a\u000a model selection \u000a\u000awhen scientists wish to test several theories then a statistician can design an experiment that allows optimal tests between specified models such discrimination experiments are especially important in the biostatistics supporting pharmacokinetics and pharmacodynamics following the work of cox and atkinson\u000a\u000a\u000a bayesian experimental design \u000a\u000awhen practitioners need to consider multiple models they can specify a probabilitymeasure on the models and then select any design maximizing the expected value of such an experiment such probabilitybased optimaldesigns are called optimal bayesian designs such bayesian designs are used especially for generalized linear models where the response follows an exponentialfamily distribution\u000athe use of a bayesian design does not force statisticians to use bayesian methods to analyze the data however indeed the bayesian label for probabilitybased experimentaldesigns is disliked by some researchers alternative terminology for bayesian optimality includes onaverage optimality or population optimality\u000a\u000a\u000a iterative experimentation \u000ascientific experimentation is an iterative process and statisticians have developed several approaches to the optimal design of sequential experiments\u000a\u000a\u000a sequential analysis \u000a\u000asequential analysis was pioneered by abraham wald in 1972 herman chernoff wrote an overview of optimal sequential designs while adaptive designs were surveyed later by s zacks of course much work on the optimal design of experiments is related to the theory of optimal decisions especially the statistical decision theory of abraham wald\u000a\u000a\u000a responsesurface methodology \u000a\u000aoptimal designs for responsesurface models are discussed in the textbook by atkinson donev and tobias and in the survey of gaffke and heiligers and in the mathematical text of pukelsheim the blocking of optimal designs is discussed in the textbook of atkinson donev and tobias and also in the monograph by goos\u000athe earliest optimal designs were developed to estimate the parameters of regression models with continuous variables for example by j d gergonne in 1815 stigler in english two early contributions were made by charles s peirce and kirstine smith\u000apioneering designs for multivariate responsesurfaces were proposed by george e p box however boxs designs have few optimality properties indeed the boxbehnken design requires excessive experimental runs when the number of variables exceeds three boxs centralcomposite designs require more experimental runs than do the optimal designs of kno\u000a\u000a\u000a system identification and stochastic approximation \u000a\u000athe optimization of sequential experimentation is studied also in stochastic programming and in systems and control popular methods include stochastic approximation and other methods of stochastic optimization much of this research has been associated with the subdiscipline of system identification in computational optimal control d judin  a nemirovskii and boris polyak has described methods that are more efficient than the armijostyle stepsize rules introduced by g e p box in responsesurface methodology\u000aadaptive designs are used in clinical trials and optimal adaptive designs are surveyed in the handbook of experimental designs chapter by shelemyahu zacks\u000a\u000a\u000a specifying the number of experimental runs \u000a\u000a\u000a using a computer to find a good design \u000athere are several methods of finding an optimal design given an a priori restriction on the number of experimental runs or replications some of these methods are discussed by atkinson donev and tobias and in the paper by hardin and sloane of course fixing the number of experimental runs a priori would be impractical prudent statisticians examine the other optimal designs whose number of experimental runs differ\u000a\u000a\u000a discretizing probabilitymeasure designs \u000ain the mathematical theory on optimal experiments an optimal design can be a probability measure that is supported on an infinite set of observationlocations such optimal probabilitymeasure designs solve a mathematical problem that neglected to specify the cost of observations and experimental runs nonetheless such optimal probabilitymeasure designs can be discretized to furnish approximately optimal designs\u000ain some cases a finite set of observationlocations suffices to support an optimal design such a result was proved by kno and kiefer in their works on responsesurface designs for quadratic models the knokiefer analysis explains why optimal designs for responsesurfaces can have discrete supports which are very similar as do the less efficient designs that have been traditional in response surface methodology\u000a\u000a\u000a history \u000athe prophet of scientific experimentation francis bacon foresaw that experimental designs should be improved researchers who improved experiments were praised in bacons utopian novel new atlantis\u000a\u000athen after divers meetings and consults of our whole number to consider of the former labors and collections we have three that take care out of them to direct new experiments of a higher light more penetrating into nature than the former these we call lamps\u000a\u000ain 1815 an article on optimal designs for polynomial regression was published by joseph diaz gergonne according to stigler\u000acharles s peirce proposed an economic theory of scientific experimentation in 1876 which sought to maximize the precision of the estimates peirces optimal allocation immediately improved the accuracy of gravitational experiments and was used for decades by peirce and his colleagues in his 1882 published lecture at johns hopkins university peirce introduced experimental design with these words\u000a\u000alogic will not undertake to inform you what kind of experiments you ought to make in order best to determine the acceleration of gravity or the value of the ohm but it will tell you how to proceed to form a plan of experimentation\u000a unfortunately practice generally precedes theory and it is the usual fate of mankind to get things done in some boggling way first and find out afterward how they could have been done much more easily and perfectly\u000a\u000alike bacon peirce was aware that experimental methods should strive for substantial improvement even optimality\u000akirstine smith proposed optimal designs for polynomial models in 1918 kirstine smith had been a student of the danish statistician thorvald n thiele and was working with karl pearson in london\u000a\u000a\u000a see also \u000a\u000a\u000a notes \u000a\u000a\u000a
p527
sg32
g35
sg37
NsbsS'shrinkage_estimator.txt'
p528
g2
(g3
g4
Ntp529
Rp530
(dp531
g8
g11
sg12
Vin statistics a shrinkage estimator is an estimator that either explicitly or implicitly incorporates the effects of shrinkage in loose terms this means that a naive or raw estimate is improved by combining it with other information the term relates to the notion that the improved estimate is made closer to the value supplied by the other information than the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000a\u000a\u000a examples \u000aone general result is that many standard estimators can be improved in terms of mean squared error mse by shrinking them towards zero or any other fixed constant value assume that the expected value of the raw estimate is not zero and consider other estimators obtained by multiplying the raw estimate by a certain parameter a value for this parameter can be specified so as to minimize the mse of the new estimate for this value of the parameter the new estimate will have a smaller mse than the raw one thus it has been improved an effect here may be to convert an unbiased raw estimate to an improved biased one\u000aa wellknown example arises in the estimation of the population variance by sample variance for a sample size of n the use of a divisor n  1 in the usual formula bessels correction gives an unbiased estimator while other divisors have lower mse at the expense of bias the optimal choice of divisor weighting of shrinkage depends on the excess kurtosis of the population as discussed at mean squared error variance but one can always do better in terms of mse than the unbiased estimator for the normal distribution a divisor of n  1 gives one which has the minimum mean square error\u000a\u000a\u000a background \u000ashrinkage is implicit in bayesian inference and penalized likelihood inference and explicit in jamessteintype inference in contrast simple types of maximumlikelihood and leastsquares estimation procedures do not include shrinkage effects although they can be used within shrinkage estimation schemes\u000a\u000a\u000a applications \u000a\u000a\u000a copas \u000athe use of shrinkage estimators in the context of regression analysis where there may be a large number of explanatory variables has been described by copas here the values of the estimated regression coefficients are shrunken towards zero with the effect of reducing the mean square error of predicted values from the model when applied to new data a later paper by copas applies shrinkage in a context where the problem is to predict a binary response on the basis of binary explanatory variables\u000a\u000a\u000a hausser and strimmer \u000ahausser and strimmer develop a jamessteintype shrinkage estimator resulting in a procedure that is highly efficient statistically as well as computationally despite its simplicity it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and datagenerating models even in cases of severe undersampling method is fully analytic and hence computationally inexpensive moreover procedure simultaneously provides estimates of the entropy and of the cell frequencies the proposed shrinkage estimators of entropy and mutual information as well as all other investigated entropy estimators have been implemented in r r development core team 2008 a corresponding r package entropy was deposited in the r archive cran and is accessible at the url httpcranrprojectorgwebpackagesentropy under the gnu general public license \u000a\u000a\u000a see also \u000asteins example\u000ashrinkage estimation in estimation of covariance matrices\u000aregularization mathematics\u000atikhonov regularization\u000a\u000a\u000a
p532
sg14
g17
sg18
Vin statistics a shrinkage estimator is an estimator that either explicitly or implicitly incorporates the effects of shrinkage in loose terms this means that a naive or raw estimate is improved by combining it with other information the term relates to the notion that the improved estimate is made closer to the value supplied by the other information than the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000a\u000a\u000a examples \u000aone general result is that many standard estimators can be improved in terms of mean squared error mse by shrinking them towards zero or any other fixed constant value assume that the expected value of the raw estimate is not zero and consider other estimators obtained by multiplying the raw estimate by a certain parameter a value for this parameter can be specified so as to minimize the mse of the new estimate for this value of the parameter the new estimate will have a smaller mse than the raw one thus it has been improved an effect here may be to convert an unbiased raw estimate to an improved biased one\u000aa wellknown example arises in the estimation of the population variance by sample variance for a sample size of n the use of a divisor n  1 in the usual formula bessels correction gives an unbiased estimator while other divisors have lower mse at the expense of bias the optimal choice of divisor weighting of shrinkage depends on the excess kurtosis of the population as discussed at mean squared error variance but one can always do better in terms of mse than the unbiased estimator for the normal distribution a divisor of n  1 gives one which has the minimum mean square error\u000a\u000a\u000a background \u000ashrinkage is implicit in bayesian inference and penalized likelihood inference and explicit in jamessteintype inference in contrast simple types of maximumlikelihood and leastsquares estimation procedures do not include shrinkage effects although they can be used within shrinkage estimation schemes\u000a\u000a\u000a applications \u000a\u000a\u000a copas \u000athe use of shrinkage estimators in the context of regression analysis where there may be a large number of explanatory variables has been described by copas here the values of the estimated regression coefficients are shrunken towards zero with the effect of reducing the mean square error of predicted values from the model when applied to new data a later paper by copas applies shrinkage in a context where the problem is to predict a binary response on the basis of binary explanatory variables\u000a\u000a\u000a hausser and strimmer \u000ahausser and strimmer develop a jamessteintype shrinkage estimator resulting in a procedure that is highly efficient statistically as well as computationally despite its simplicity it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and datagenerating models even in cases of severe undersampling method is fully analytic and hence computationally inexpensive moreover procedure simultaneously provides estimates of the entropy and of the cell frequencies the proposed shrinkage estimators of entropy and mutual information as well as all other investigated entropy estimators have been implemented in r r development core team 2008 a corresponding r package entropy was deposited in the r archive cran and is accessible at the url httpcranrprojectorgwebpackagesentropy under the gnu general public license \u000a\u000a\u000a see also \u000asteins example\u000ashrinkage estimation in estimation of covariance matrices\u000aregularization mathematics\u000atikhonov regularization
p533
sg20
g23
sg24
g27
sg30
Vin statistics a shrinkage estimator is an estimator that either explicitly or implicitly incorporates the effects of shrinkage in loose terms this means that a naive or raw estimate is improved by combining it with other information the term relates to the notion that the improved estimate is made closer to the value supplied by the other information than the raw estimate in this sense shrinkage is used to regularize illposed inference problems\u000a\u000a\u000a examples \u000aone general result is that many standard estimators can be improved in terms of mean squared error mse by shrinking them towards zero or any other fixed constant value assume that the expected value of the raw estimate is not zero and consider other estimators obtained by multiplying the raw estimate by a certain parameter a value for this parameter can be specified so as to minimize the mse of the new estimate for this value of the parameter the new estimate will have a smaller mse than the raw one thus it has been improved an effect here may be to convert an unbiased raw estimate to an improved biased one\u000aa wellknown example arises in the estimation of the population variance by sample variance for a sample size of n the use of a divisor n  1 in the usual formula bessels correction gives an unbiased estimator while other divisors have lower mse at the expense of bias the optimal choice of divisor weighting of shrinkage depends on the excess kurtosis of the population as discussed at mean squared error variance but one can always do better in terms of mse than the unbiased estimator for the normal distribution a divisor of n  1 gives one which has the minimum mean square error\u000a\u000a\u000a background \u000ashrinkage is implicit in bayesian inference and penalized likelihood inference and explicit in jamessteintype inference in contrast simple types of maximumlikelihood and leastsquares estimation procedures do not include shrinkage effects although they can be used within shrinkage estimation schemes\u000a\u000a\u000a applications \u000a\u000a\u000a copas \u000athe use of shrinkage estimators in the context of regression analysis where there may be a large number of explanatory variables has been described by copas here the values of the estimated regression coefficients are shrunken towards zero with the effect of reducing the mean square error of predicted values from the model when applied to new data a later paper by copas applies shrinkage in a context where the problem is to predict a binary response on the basis of binary explanatory variables\u000a\u000a\u000a hausser and strimmer \u000ahausser and strimmer develop a jamessteintype shrinkage estimator resulting in a procedure that is highly efficient statistically as well as computationally despite its simplicity it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and datagenerating models even in cases of severe undersampling method is fully analytic and hence computationally inexpensive moreover procedure simultaneously provides estimates of the entropy and of the cell frequencies the proposed shrinkage estimators of entropy and mutual information as well as all other investigated entropy estimators have been implemented in r r development core team 2008 a corresponding r package entropy was deposited in the r archive cran and is accessible at the url httpcranrprojectorgwebpackagesentropy under the gnu general public license \u000a\u000a\u000a see also \u000asteins example\u000ashrinkage estimation in estimation of covariance matrices\u000aregularization mathematics\u000atikhonov regularization\u000a\u000a\u000a
p534
sg32
g35
sg37
NsbsS"galton's_problem.txt"
p535
g2
(g3
g4
Ntp536
Rp537
(dp538
g8
g11
sg12
Vgaltons problem named after sir francis galton is the problem of drawing inferences from crosscultural data due to the statistical phenomenon now called autocorrelation the problem is now recognized as a general one that applies to all nonexperimental studies and to experimental design as well it is most simply described as the problem of external dependencies in making statistical estimates when the elements sampled are not statistically independent asking two people in the same household whether they watch tv for example does not give you statistically independent answers the sample size n for independent observations in this case is one not two once proper adjustments are made that deal with external dependencies then the axioms of probability theory concerning statistical independence will apply these axioms are important for deriving measures of variance for example or tests of statistical significance\u000a\u000a\u000a origin \u000ain 1888 galton was present when sir edward tylor presented a paper at the royal anthropological institute tylor had compiled information on institutions of marriage and descent for 350 cultures and examined the correlations between these institutions and measures of societal complexity tylor interpreted his results as indications of a general evolutionary sequence in which institutions change focus from the maternal line to the paternal line as societies become increasingly complex galton disagreed pointing out that similarity between cultures could be due to borrowing could be due to common descent or could be due to evolutionary development he maintained that without controlling for borrowing and common descent one cannot make valid inferences regarding evolutionary development galtons critique has become the eponymous galtons problem as named by raoul naroll who proposed the first statistical solutions\u000aby the early 20th century unilineal evolutionism was abandoned and along with it the drawing of direct inferences from correlations to evolutionary sequences galtons criticisms proved equally valid however for inferring functional relations from correlations the problem of autocorrelation remained\u000a\u000a\u000a solutions \u000astatistician william s gosset in 1914 developed methods of eliminating spurious correlation due to how position in time or space affects similarities todays election polls have a similar problem the closer the poll to the election the less individuals make up their mind independently and the greater the unreliability of the polling results especially the margin of error or confidence limits the effective n of independent cases from their sample drops as the election nears statistical significance falls with lower effective sample size\u000athe problem pops up in sample surveys when sociologists want to reduce the travel time to do their interviews and hence they divide their population into local clusters and sample the clusters randomly then sample again within the clusters if they interview n people in clusters of size m the effective sample size efs would have a lower limit of 1  n  1  m if everyone in each cluster were identical when there are only partial similarities within clusters the m in this formula has to be lowered accordingly a formula of this sort is 1  d n  1 where d is the intraclass correlation for the statistic in question in general estimations of the appropriate efs depends on the statistic estimated as for example mean chisquare r regression coefficient and their variances\u000afor crosscultural studies murdock and white estimated the size of patches of similarities in their sample of 186 societies the four variables they tested  language economy political integration and descent  had patches of similarities that varied from size three to size ten a very crude rule of thumb might be to divide the square root of the similaritypatch sizes into n so that the effective sample sizes are 58 and 107 for these patches respectively again statistical significance falls with lower effective sample size\u000ain modern analysis spatial lags have been modelled in order to estimate the degree of globalization on modern societies\u000aspatial dependency or autocorrelation is a fundamental concept in geography methods developed by geographers that measure and control for spatial autocorrelation do far more than reduce the effective n for tests of significance of a correlation one example is the complicated hypothesis that the presence of gambling in a society is directly proportional to the presence of a commercial money and to the presence of considerable socioeconomic differences and is inversely related to whether or not the society is a nomadic herding society tests of this hypothesis in a sample of 60 societies failed to reject the null hypothesis autocorrelation analysis however showed a significant effect of socioeconomic differences\u000ahow prevalent is autocorrelation among the variables studied in crosscultural research a test by anthon eff on 1700 variables in the cumulative database for the standard crosscultural sample published in world cultures measured morans i for spatial autocorrelation distance linguistic autocorrelation common descent and autoccorrelation in cultural complexity mainline evolution the results suggest that  it would be prudent to test for spatial and phylogenetic autoccorrelation when conducting regression analyses with the standard crosscultural sample the use of autocorrelation tests in exploratory data analysis is illustrated showing how all variables in a given study can be evaluated for nonindependence of cases in terms of distance language and cultural complexity the methods for estimating these autocorrelation effects are then explained and illustrated for ordinary least squares regression using again the moran i significance measure of autocorrelation\u000awhen autocorrelation is present it can often be removed to get unbiased estimates of regression coefficients and their variances by constructing a respecified dependent variable that is lagged by weightings on the dependent variable on other locations where the weights are degree of relationship this lagged dependent variable is endogenous and estimation requires either twostage least squares or maximum likelihood methods\u000aa public server at httpsocscicomputessuciedu offers ethnographic data and tools for inference they are using custom r scripts incorporated into the galaxy httpgetgalaxyorg framework supported on httpswwwxsedeorg this is an extraordinary public resource  eg take a look at a random tool manual from the site httpcaponemtsuedueaeffdownloadsmanualdefhtm\u000a\u000a\u000a opportunities \u000ain anthropology where tylors problem was first recognized by the statistician galton in 1889 it is still not widely recognized that there are standard statistical adjustments for the problem of patches of similarity in observed cases and opportunities for new discoveries using autocorrelation methods some crosscultural researchers see eg korotayev and de munck 2003 have begun to realize that evidence of diffusion historical origin and other sources of similarity among related societies or individuals should be renamed galtons opportunity and galtons asset rather than galtons problem researchers now use longitudinal crosscultural and regional variation analysis routinely to analyze all the competing hypotheses functional relationships diffusion common historical origin multilineal evolution coadaptation with environment and complex social interaction dynamics\u000a\u000a\u000a controversies \u000awithin anthropology galtons problem is often given as a cause to reject comparative studies altogether since the problem is a general one common to the sciences and statistical inference generally this particular criticism of crosscultural or comparative studies  and there are many  is one that logically speaking amounts to a rejection of science and statistics altogether any data collected and analyzed by ethnographers for example is equally subject to galtons problem understood in its most general sense a critique of the anticomparative critique is not limited to statistical comparison since it would apply as well to the analysis of text that is the analysis and use of text in argumentation is subject to critique as to the evidential basis of inference reliance purely on rhetoric is no protection against critique as to the validity of argument and its evidentiary basis\u000athere is little doubt however that the community of crosscultural researchers have been remiss in ignoring galtons problem expert investigation of this question shows results that strongly suggest that the extensive reporting of nave chisquare independence tests using crosscultural data sets over the past several decades has led to incorrect rejection of null hypotheses at levels much higher than the expected 5 rate the investigator concludes that incorrect theories that have been saved by nave chisquare tests with comparative data may yet be more rigorously tested another day once again the adjusted variance of a cluster sample is given as one multiplied by 1  d k  1 where k is the average size of a cluster and a more complicated correction is given for the variance of contingency table correlations with r rows and c columns since this critique was published in 1993 and others like it more authors have begun to adopt corrections for galtons problem but the majority in the crosscultural field have not consequently a large proportion of published results that rely on naive significance tests and that adopt the p  005 rather than a p  0005 standard are likely to be in error because they are more susceptible to type i error which is to reject the null hypothesis when it is true\u000asome crosscultural researchers reject the seriousness of galtons problem because they argue estimates of correlations and means may be unbiased even if autocorrelation weak or strong is present without investigating autocorrelation however they may still misestimate statistics dealing with relationships among variables in regression analysis for example examining the patterns of autocorrelated residuals may give important clues to third factors that may affect the relationships among variables but that have not been included in the regression model second if there are clusters of similar and related societies in the sample measures of variance will be underestimated leading to spurious statistical conclusions for example exaggerating the statistical significance of correlations third the underestimation of variance makes it difficult to test for replication of results from two different samples as the results will more often be rejected as similar\u000a\u000a\u000a see also \u000alist of cultures in the standard cross cultural sample\u000a\u000a\u000a further reading \u000aintersciwiki 2007 using autocorrelation in model specification including software and tutorial\u000aintersciwiki 2009 galtons problem and autocorrelation bibliography\u000astudent w s gosset 1914 the elimination of spurious correlation due to position in time or space biometrika 10179181\u000atylor edward e 1889 on a method of investigating the development of institutions applied to the laws of marriage and descent journal of the royal anthropological institute 18324572\u000awitkowski stanley 1974 galtons opportunity  hologeistic study of historical processes behavior science research 911115\u000a\u000a\u000a
p539
sg14
g17
sg18
Vgaltons problem named after sir francis galton is the problem of drawing inferences from crosscultural data due to the statistical phenomenon now called autocorrelation the problem is now recognized as a general one that applies to all nonexperimental studies and to experimental design as well it is most simply described as the problem of external dependencies in making statistical estimates when the elements sampled are not statistically independent asking two people in the same household whether they watch tv for example does not give you statistically independent answers the sample size n for independent observations in this case is one not two once proper adjustments are made that deal with external dependencies then the axioms of probability theory concerning statistical independence will apply these axioms are important for deriving measures of variance for example or tests of statistical significance\u000a\u000a\u000a origin \u000ain 1888 galton was present when sir edward tylor presented a paper at the royal anthropological institute tylor had compiled information on institutions of marriage and descent for 350 cultures and examined the correlations between these institutions and measures of societal complexity tylor interpreted his results as indications of a general evolutionary sequence in which institutions change focus from the maternal line to the paternal line as societies become increasingly complex galton disagreed pointing out that similarity between cultures could be due to borrowing could be due to common descent or could be due to evolutionary development he maintained that without controlling for borrowing and common descent one cannot make valid inferences regarding evolutionary development galtons critique has become the eponymous galtons problem as named by raoul naroll who proposed the first statistical solutions\u000aby the early 20th century unilineal evolutionism was abandoned and along with it the drawing of direct inferences from correlations to evolutionary sequences galtons criticisms proved equally valid however for inferring functional relations from correlations the problem of autocorrelation remained\u000a\u000a\u000a solutions \u000astatistician william s gosset in 1914 developed methods of eliminating spurious correlation due to how position in time or space affects similarities todays election polls have a similar problem the closer the poll to the election the less individuals make up their mind independently and the greater the unreliability of the polling results especially the margin of error or confidence limits the effective n of independent cases from their sample drops as the election nears statistical significance falls with lower effective sample size\u000athe problem pops up in sample surveys when sociologists want to reduce the travel time to do their interviews and hence they divide their population into local clusters and sample the clusters randomly then sample again within the clusters if they interview n people in clusters of size m the effective sample size efs would have a lower limit of 1  n  1  m if everyone in each cluster were identical when there are only partial similarities within clusters the m in this formula has to be lowered accordingly a formula of this sort is 1  d n  1 where d is the intraclass correlation for the statistic in question in general estimations of the appropriate efs depends on the statistic estimated as for example mean chisquare r regression coefficient and their variances\u000afor crosscultural studies murdock and white estimated the size of patches of similarities in their sample of 186 societies the four variables they tested  language economy political integration and descent  had patches of similarities that varied from size three to size ten a very crude rule of thumb might be to divide the square root of the similaritypatch sizes into n so that the effective sample sizes are 58 and 107 for these patches respectively again statistical significance falls with lower effective sample size\u000ain modern analysis spatial lags have been modelled in order to estimate the degree of globalization on modern societies\u000aspatial dependency or autocorrelation is a fundamental concept in geography methods developed by geographers that measure and control for spatial autocorrelation do far more than reduce the effective n for tests of significance of a correlation one example is the complicated hypothesis that the presence of gambling in a society is directly proportional to the presence of a commercial money and to the presence of considerable socioeconomic differences and is inversely related to whether or not the society is a nomadic herding society tests of this hypothesis in a sample of 60 societies failed to reject the null hypothesis autocorrelation analysis however showed a significant effect of socioeconomic differences\u000ahow prevalent is autocorrelation among the variables studied in crosscultural research a test by anthon eff on 1700 variables in the cumulative database for the standard crosscultural sample published in world cultures measured morans i for spatial autocorrelation distance linguistic autocorrelation common descent and autoccorrelation in cultural complexity mainline evolution the results suggest that  it would be prudent to test for spatial and phylogenetic autoccorrelation when conducting regression analyses with the standard crosscultural sample the use of autocorrelation tests in exploratory data analysis is illustrated showing how all variables in a given study can be evaluated for nonindependence of cases in terms of distance language and cultural complexity the methods for estimating these autocorrelation effects are then explained and illustrated for ordinary least squares regression using again the moran i significance measure of autocorrelation\u000awhen autocorrelation is present it can often be removed to get unbiased estimates of regression coefficients and their variances by constructing a respecified dependent variable that is lagged by weightings on the dependent variable on other locations where the weights are degree of relationship this lagged dependent variable is endogenous and estimation requires either twostage least squares or maximum likelihood methods\u000aa public server at httpsocscicomputessuciedu offers ethnographic data and tools for inference they are using custom r scripts incorporated into the galaxy httpgetgalaxyorg framework supported on httpswwwxsedeorg this is an extraordinary public resource  eg take a look at a random tool manual from the site httpcaponemtsuedueaeffdownloadsmanualdefhtm\u000a\u000a\u000a opportunities \u000ain anthropology where tylors problem was first recognized by the statistician galton in 1889 it is still not widely recognized that there are standard statistical adjustments for the problem of patches of similarity in observed cases and opportunities for new discoveries using autocorrelation methods some crosscultural researchers see eg korotayev and de munck 2003 have begun to realize that evidence of diffusion historical origin and other sources of similarity among related societies or individuals should be renamed galtons opportunity and galtons asset rather than galtons problem researchers now use longitudinal crosscultural and regional variation analysis routinely to analyze all the competing hypotheses functional relationships diffusion common historical origin multilineal evolution coadaptation with environment and complex social interaction dynamics\u000a\u000a\u000a controversies \u000awithin anthropology galtons problem is often given as a cause to reject comparative studies altogether since the problem is a general one common to the sciences and statistical inference generally this particular criticism of crosscultural or comparative studies  and there are many  is one that logically speaking amounts to a rejection of science and statistics altogether any data collected and analyzed by ethnographers for example is equally subject to galtons problem understood in its most general sense a critique of the anticomparative critique is not limited to statistical comparison since it would apply as well to the analysis of text that is the analysis and use of text in argumentation is subject to critique as to the evidential basis of inference reliance purely on rhetoric is no protection against critique as to the validity of argument and its evidentiary basis\u000athere is little doubt however that the community of crosscultural researchers have been remiss in ignoring galtons problem expert investigation of this question shows results that strongly suggest that the extensive reporting of nave chisquare independence tests using crosscultural data sets over the past several decades has led to incorrect rejection of null hypotheses at levels much higher than the expected 5 rate the investigator concludes that incorrect theories that have been saved by nave chisquare tests with comparative data may yet be more rigorously tested another day once again the adjusted variance of a cluster sample is given as one multiplied by 1  d k  1 where k is the average size of a cluster and a more complicated correction is given for the variance of contingency table correlations with r rows and c columns since this critique was published in 1993 and others like it more authors have begun to adopt corrections for galtons problem but the majority in the crosscultural field have not consequently a large proportion of published results that rely on naive significance tests and that adopt the p  005 rather than a p  0005 standard are likely to be in error because they are more susceptible to type i error which is to reject the null hypothesis when it is true\u000asome crosscultural researchers reject the seriousness of galtons problem because they argue estimates of correlations and means may be unbiased even if autocorrelation weak or strong is present without investigating autocorrelation however they may still misestimate statistics dealing with relationships among variables in regression analysis for example examining the patterns of autocorrelated residuals may give important clues to third factors that may affect the relationships among variables but that have not been included in the regression model second if there are clusters of similar and related societies in the sample measures of variance will be underestimated leading to spurious statistical conclusions for example exaggerating the statistical significance of correlations third the underestimation of variance makes it difficult to test for replication of results from two different samples as the results will more often be rejected as similar\u000a\u000a\u000a see also \u000alist of cultures in the standard cross cultural sample\u000a\u000a\u000a further reading \u000aintersciwiki 2007 using autocorrelation in model specification including software and tutorial\u000aintersciwiki 2009 galtons problem and autocorrelation bibliography\u000astudent w s gosset 1914 the elimination of spurious correlation due to position in time or space biometrika 10179181\u000atylor edward e 1889 on a method of investigating the development of institutions applied to the laws of marriage and descent journal of the royal anthropological institute 18324572\u000awitkowski stanley 1974 galtons opportunity  hologeistic study of historical processes behavior science research 911115
p540
sg20
g23
sg24
g27
sg30
Vgaltons problem named after sir francis galton is the problem of drawing inferences from crosscultural data due to the statistical phenomenon now called autocorrelation the problem is now recognized as a general one that applies to all nonexperimental studies and to experimental design as well it is most simply described as the problem of external dependencies in making statistical estimates when the elements sampled are not statistically independent asking two people in the same household whether they watch tv for example does not give you statistically independent answers the sample size n for independent observations in this case is one not two once proper adjustments are made that deal with external dependencies then the axioms of probability theory concerning statistical independence will apply these axioms are important for deriving measures of variance for example or tests of statistical significance\u000a\u000a\u000a origin \u000ain 1888 galton was present when sir edward tylor presented a paper at the royal anthropological institute tylor had compiled information on institutions of marriage and descent for 350 cultures and examined the correlations between these institutions and measures of societal complexity tylor interpreted his results as indications of a general evolutionary sequence in which institutions change focus from the maternal line to the paternal line as societies become increasingly complex galton disagreed pointing out that similarity between cultures could be due to borrowing could be due to common descent or could be due to evolutionary development he maintained that without controlling for borrowing and common descent one cannot make valid inferences regarding evolutionary development galtons critique has become the eponymous galtons problem as named by raoul naroll who proposed the first statistical solutions\u000aby the early 20th century unilineal evolutionism was abandoned and along with it the drawing of direct inferences from correlations to evolutionary sequences galtons criticisms proved equally valid however for inferring functional relations from correlations the problem of autocorrelation remained\u000a\u000a\u000a solutions \u000astatistician william s gosset in 1914 developed methods of eliminating spurious correlation due to how position in time or space affects similarities todays election polls have a similar problem the closer the poll to the election the less individuals make up their mind independently and the greater the unreliability of the polling results especially the margin of error or confidence limits the effective n of independent cases from their sample drops as the election nears statistical significance falls with lower effective sample size\u000athe problem pops up in sample surveys when sociologists want to reduce the travel time to do their interviews and hence they divide their population into local clusters and sample the clusters randomly then sample again within the clusters if they interview n people in clusters of size m the effective sample size efs would have a lower limit of 1  n  1  m if everyone in each cluster were identical when there are only partial similarities within clusters the m in this formula has to be lowered accordingly a formula of this sort is 1  d n  1 where d is the intraclass correlation for the statistic in question in general estimations of the appropriate efs depends on the statistic estimated as for example mean chisquare r regression coefficient and their variances\u000afor crosscultural studies murdock and white estimated the size of patches of similarities in their sample of 186 societies the four variables they tested  language economy political integration and descent  had patches of similarities that varied from size three to size ten a very crude rule of thumb might be to divide the square root of the similaritypatch sizes into n so that the effective sample sizes are 58 and 107 for these patches respectively again statistical significance falls with lower effective sample size\u000ain modern analysis spatial lags have been modelled in order to estimate the degree of globalization on modern societies\u000aspatial dependency or autocorrelation is a fundamental concept in geography methods developed by geographers that measure and control for spatial autocorrelation do far more than reduce the effective n for tests of significance of a correlation one example is the complicated hypothesis that the presence of gambling in a society is directly proportional to the presence of a commercial money and to the presence of considerable socioeconomic differences and is inversely related to whether or not the society is a nomadic herding society tests of this hypothesis in a sample of 60 societies failed to reject the null hypothesis autocorrelation analysis however showed a significant effect of socioeconomic differences\u000ahow prevalent is autocorrelation among the variables studied in crosscultural research a test by anthon eff on 1700 variables in the cumulative database for the standard crosscultural sample published in world cultures measured morans i for spatial autocorrelation distance linguistic autocorrelation common descent and autoccorrelation in cultural complexity mainline evolution the results suggest that  it would be prudent to test for spatial and phylogenetic autoccorrelation when conducting regression analyses with the standard crosscultural sample the use of autocorrelation tests in exploratory data analysis is illustrated showing how all variables in a given study can be evaluated for nonindependence of cases in terms of distance language and cultural complexity the methods for estimating these autocorrelation effects are then explained and illustrated for ordinary least squares regression using again the moran i significance measure of autocorrelation\u000awhen autocorrelation is present it can often be removed to get unbiased estimates of regression coefficients and their variances by constructing a respecified dependent variable that is lagged by weightings on the dependent variable on other locations where the weights are degree of relationship this lagged dependent variable is endogenous and estimation requires either twostage least squares or maximum likelihood methods\u000aa public server at httpsocscicomputessuciedu offers ethnographic data and tools for inference they are using custom r scripts incorporated into the galaxy httpgetgalaxyorg framework supported on httpswwwxsedeorg this is an extraordinary public resource  eg take a look at a random tool manual from the site httpcaponemtsuedueaeffdownloadsmanualdefhtm\u000a\u000a\u000a opportunities \u000ain anthropology where tylors problem was first recognized by the statistician galton in 1889 it is still not widely recognized that there are standard statistical adjustments for the problem of patches of similarity in observed cases and opportunities for new discoveries using autocorrelation methods some crosscultural researchers see eg korotayev and de munck 2003 have begun to realize that evidence of diffusion historical origin and other sources of similarity among related societies or individuals should be renamed galtons opportunity and galtons asset rather than galtons problem researchers now use longitudinal crosscultural and regional variation analysis routinely to analyze all the competing hypotheses functional relationships diffusion common historical origin multilineal evolution coadaptation with environment and complex social interaction dynamics\u000a\u000a\u000a controversies \u000awithin anthropology galtons problem is often given as a cause to reject comparative studies altogether since the problem is a general one common to the sciences and statistical inference generally this particular criticism of crosscultural or comparative studies  and there are many  is one that logically speaking amounts to a rejection of science and statistics altogether any data collected and analyzed by ethnographers for example is equally subject to galtons problem understood in its most general sense a critique of the anticomparative critique is not limited to statistical comparison since it would apply as well to the analysis of text that is the analysis and use of text in argumentation is subject to critique as to the evidential basis of inference reliance purely on rhetoric is no protection against critique as to the validity of argument and its evidentiary basis\u000athere is little doubt however that the community of crosscultural researchers have been remiss in ignoring galtons problem expert investigation of this question shows results that strongly suggest that the extensive reporting of nave chisquare independence tests using crosscultural data sets over the past several decades has led to incorrect rejection of null hypotheses at levels much higher than the expected 5 rate the investigator concludes that incorrect theories that have been saved by nave chisquare tests with comparative data may yet be more rigorously tested another day once again the adjusted variance of a cluster sample is given as one multiplied by 1  d k  1 where k is the average size of a cluster and a more complicated correction is given for the variance of contingency table correlations with r rows and c columns since this critique was published in 1993 and others like it more authors have begun to adopt corrections for galtons problem but the majority in the crosscultural field have not consequently a large proportion of published results that rely on naive significance tests and that adopt the p  005 rather than a p  0005 standard are likely to be in error because they are more susceptible to type i error which is to reject the null hypothesis when it is true\u000asome crosscultural researchers reject the seriousness of galtons problem because they argue estimates of correlations and means may be unbiased even if autocorrelation weak or strong is present without investigating autocorrelation however they may still misestimate statistics dealing with relationships among variables in regression analysis for example examining the patterns of autocorrelated residuals may give important clues to third factors that may affect the relationships among variables but that have not been included in the regression model second if there are clusters of similar and related societies in the sample measures of variance will be underestimated leading to spurious statistical conclusions for example exaggerating the statistical significance of correlations third the underestimation of variance makes it difficult to test for replication of results from two different samples as the results will more often be rejected as similar\u000a\u000a\u000a see also \u000alist of cultures in the standard cross cultural sample\u000a\u000a\u000a further reading \u000aintersciwiki 2007 using autocorrelation in model specification including software and tutorial\u000aintersciwiki 2009 galtons problem and autocorrelation bibliography\u000astudent w s gosset 1914 the elimination of spurious correlation due to position in time or space biometrika 10179181\u000atylor edward e 1889 on a method of investigating the development of institutions applied to the laws of marriage and descent journal of the royal anthropological institute 18324572\u000awitkowski stanley 1974 galtons opportunity  hologeistic study of historical processes behavior science research 911115\u000a\u000a\u000a
p541
sg32
g35
sg37
Nsbs.